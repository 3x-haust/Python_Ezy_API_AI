{"repo_info": {"repo_name": "pdf_translator", "repo_owner": "ppisljar", "repo_url": "https://github.com/ppisljar/pdf_translator"}}
{"type": "source_file", "path": "cli.py", "content": "import argparse\nimport subprocess\nfrom pathlib import Path\n\nimport requests\n\nTRANSLATE_URL = \"http://localhost:8765/translate_pdf/\"\nCLEAR_TEMP_URL = \"http://localhost:8765/clear_temp_dir/\"\n\n\ndef translate_request(input_pdf_path: Path, output_dir: Path) -> None:\n    \"\"\"Sends a POST request to the translator server to translate a PDF.\n\n    Parameters\n    ----------\n    input_pdf_path : Path\n        Path to the PDF to be translated.\n    output_dir : Path\n        Path to the directory where the translated PDF will be saved.\n    \"\"\"\n    print(f\"Translating {input_pdf_path}...\")\n    with open(input_pdf_path, \"rb\") as input_pdf:\n        response = requests.post(TRANSLATE_URL, files={\"input_pdf\": input_pdf})\n\n    if response.status_code == 200:\n        with open(output_dir / input_pdf_path.name, \"wb\") as output_pdf:\n            output_pdf.write(response.content)\n        print(f\"Converted PDF saved to {output_dir / input_pdf_path.name}\")\n        requests.get(CLEAR_TEMP_URL)\n    else:\n        print(f\"An error occurred: {response.status_code}\")\n\n\ndef main(args: argparse.Namespace) -> None:\n    \"\"\"Translates a PDF or all PDFs in a directory.\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        Arguments passed to the script.\n\n    Raises\n    ------\n     ValueError\n        If the input path is not a valid path to file or directory.\n\n    Notes\n    -----\n    args must have the following attributes:\n        input_pdf_path_or_dir : Path\n            Path to the PDF or directory of PDFs to be translated.\n        output_dir : Path\n            Path to the directory where the translated PDFs\n            will be saved.\n    \"\"\"\n    args.output_dir.mkdir(parents=True, exist_ok=True)\n\n    if args.input_pdf_path_or_dir.is_file():\n        if args.input_pdf_path_or_dir.suffix != \".pdf\":\n            raise ValueError(\n                f\"Input file must be a PDF or directory: {args.input_pdf_path_or_dir}\"\n            )\n\n        translate_request(args.input_pdf_path_or_dir, args.output_dir)\n    elif args.input_pdf_path_or_dir.is_dir():\n        input_pdf_paths = args.input_pdf_path_or_dir.glob(\"*.pdf\")\n\n        if not input_pdf_paths:\n            raise ValueError(f\"Input directory is empty: {args.input_pdf_path_or_dir}\")\n\n        for input_pdf_path in input_pdf_paths:\n            translate_request(input_pdf_path, args.output_dir)\n    else:\n        raise ValueError(\n            f\"Input path must be a file or directory: {args.input_pdf_path_or_dir}\"\n        )\n\n    print(\"Done.\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-i\",\n        \"--input_pdf_path_or_dir\",\n        type=Path,\n        required=True,\n        help=\"Path to the PDF or directory of PDFs to be translated.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output_dir\",\n        type=Path,\n        default=\"./outputs\",\n        help=\"Path to the directory where the translated PDFs will be saved. (default: ./outputs)\",\n    )\n    args = parser.parse_args()\n    main(args)\n"}
{"type": "source_file", "path": "modules/translate/openai_gpt.py", "content": "from .base import TranslateBase \nfrom openai import OpenAI\n\ndef system_prompt(from_lang, to_lang):\n    p  = \"You are an %s-to-%s translator. \" % (from_lang, to_lang)\n    p += \"Keep all special characters and HTML tags as in the source text. Return only %s translation.\" % to_lang\n    return p\n\nlangs = [\n    \"Albanian\",\n    \"Arabic\",\n    \"Armenian\",\n    \"Awadhi\",\n    \"Azerbaijani\",\n    \"Bashkir\",\n    \"Basque\",\n    \"Belarusian\",\n    \"Bengali\",\n    \"Bhojpuri\",\n    \"Bosnian\",\n    \"Brazilian Portuguese\",\n    \"Bulgarian\",\n    \"Cantonese (Yue)\",\n    \"Catalan\",\n    \"Chhattisgarhi\",\n    \"Chinese\",\n    \"Croatian\",\n    \"Czech\",\n    \"Danish\",\n    \"Dogri\",\n    \"Dutch\",\n    \"English\",\n    \"Estonian\",\n    \"Faroese\",\n    \"Finnish\",\n    \"French\",\n    \"Galician\",\n    \"Georgian\",\n    \"German\",\n    \"Greek\",\n    \"Gujarati\",\n    \"Haryanvi\",\n    \"Hindi\",\n    \"Hungarian\",\n    \"Indonesian\",\n    \"Irish\",\n    \"Italian\",\n    \"Japanese\",\n    \"Javanese\",\n    \"Kannada\",\n    \"Kashmiri\",\n    \"Kazakh\",\n    \"Konkani\",\n    \"Korean\",\n    \"Kyrgyz\",\n    \"Latvian\",\n    \"Lithuanian\",\n    \"Macedonian\",\n    \"Maithili\",\n    \"Malay\",\n    \"Maltese\",\n    \"Mandarin\",\n    \"Mandarin Chinese\",\n    \"Marathi\",\n    \"Marwari\",\n    \"Min Nan\",\n    \"Moldovan\",\n    \"Mongolian\",\n    \"Montenegrin\",\n    \"Nepali\",\n    \"Norwegian\",\n    \"Oriya\",\n    \"Pashto\",\n    \"Persian (Farsi)\",\n    \"Polish\",\n    \"Portuguese\",\n    \"Punjabi\",\n    \"Rajasthani\",\n    \"Romanian\",\n    \"Russian\",\n    \"Sanskrit\",\n    \"Santali\",\n    \"Serbian\",\n    \"Sindhi\",\n    \"Sinhala\",\n    \"Slovak\",\n    \"Slovene\",\n    \"Slovenian\",\n    \"Ukrainian\",\n    \"Urdu\",\n    \"Uzbek\",\n    \"Vietnamese\",\n    \"Welsh\",\n    \"Wu\"\n]\n\nclass TranslateOpenAIGPT(TranslateBase):\n    def init(self, cfg: dict):\n        self.client = OpenAI(api_key=cfg['openai_api_key'])\n\n    def get_languages(self):\n        return langs\n\n    def translate(self, text: str, from_lang='ENGLISH', to_lang='SLOVENIAN') -> str:\n        response = self.client.chat.completions.create(\n            model='gpt-4-1106-preview',\n            temperature=0.2,\n            messages=[\n                { 'role': 'system', 'content': system_prompt(from_lang, to_lang) },\n                { 'role': 'user', 'content': text },\n            ]\n        )\n\n        translated_text = response.choices[0].message.content\n        return translated_text"}
{"type": "source_file", "path": "modules/__init__.py", "content": "\n\ndef load_translator(cfg: dict):\n    if cfg['type'] == 'openai':\n        from .translate.openai_gpt import TranslateOpenAIGPT\n        translator = TranslateOpenAIGPT()\n        translator.init(cfg)\n        return translator\n    \n    raise(\"unknown translator\")\n\ndef load_layout_engine(cfg: dict):\n    if cfg['type'] == 'dit':\n        from .layout.ditod import DiTLayout\n        engine = DiTLayout()\n        engine.init(cfg)\n        return engine\n    \n    raise(\"unknown layout engine\")\n\ndef load_ocr_engine(cfg: dict):\n    if cfg['type'] == 'paddle':\n        from .ocr.paddle import PaddleOCR\n        engine = PaddleOCR()\n        engine.init(cfg)\n        return engine\n    \n    raise(\"unknown ocr engine\")\n\ndef load_font_engine(cfg: dict):\n    if cfg['type'] == 'simple':\n        from .font.simple import SimpleFont\n        engine = SimpleFont()\n        engine.init(cfg)\n        return engine\n    \n    raise(\"unknown ocr engine\")\n"}
{"type": "source_file", "path": "utils/ditod/config.py", "content": "from detectron2.config import CfgNode as CN\n\n\ndef add_vit_config(cfg):\n    \"\"\"\n    Add config for VIT.\n    \"\"\"\n    _C = cfg\n\n    _C.MODEL.VIT = CN()\n\n    # CoaT model name.\n    _C.MODEL.VIT.NAME = \"\"\n\n    # Output features from CoaT backbone.\n    _C.MODEL.VIT.OUT_FEATURES = [\"layer3\", \"layer5\", \"layer7\", \"layer11\"]\n\n    _C.MODEL.VIT.IMG_SIZE = [224, 224]\n\n    _C.MODEL.VIT.POS_TYPE = \"shared_rel\"\n\n    _C.MODEL.VIT.DROP_PATH = 0.\n\n    _C.MODEL.VIT.MODEL_KWARGS = \"{}\"\n\n    _C.SOLVER.OPTIMIZER = \"ADAMW\"\n\n    _C.SOLVER.BACKBONE_MULTIPLIER = 1.0\n\n    _C.AUG = CN()\n\n    _C.AUG.DETR = False\n"}
{"type": "source_file", "path": "modules/layout/ditod.py", "content": "import cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom pdf2image import convert_from_bytes, convert_from_path\nfrom .base import LayoutBase \nfrom utils import LayoutAnalyzer\n\nclass DiTLayout(LayoutBase):\n    def init(self, cfg: dict):\n        self.layout_model = LayoutAnalyzer(\n            model_root_dir= Path(\"models/unilm\"), device=cfg['device']\n        )\n\n        self.DPI = cfg['DPI'] if 'DPI' in cfg else 200\n \n    def get_layout(self, pdf_path_or_bytes: str, p_from, p_to) -> str:\n        \n        if isinstance(pdf_path_or_bytes, Path):\n            pdf_images = convert_from_path(pdf_path_or_bytes, dpi=self.DPI)\n        else:\n            pdf_images = convert_from_bytes(pdf_path_or_bytes, dpi=self.DPI)\n\n        data = []\n        images = []\n\n        for i, image in tqdm(enumerate(pdf_images)):\n            if i < p_from: continue\n            if i > p_to and p_to != 0: break\n\n            result = self.get_single_layout(image)\n            images.append(image)\n            data.append(result)\n\n        return data, images\n    \n    def get_single_layout(self, image):\n        img = np.array(image, dtype=np.uint8)\n        result = self.layout_model(cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n        return result\n                "}
{"type": "source_file", "path": "utils/ditod/deit.py", "content": "\"\"\"\nMostly copy-paste from DINO and timm library:\nhttps://github.com/facebookresearch/dino\nhttps://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n\"\"\"\nimport warnings\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import trunc_normal_, drop_path, to_2tuple\nfrom functools import partial\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        q, k, v = self.qkv(x).reshape(B, N, 3, self.num_heads,\n                                      C // self.num_heads).permute(2, 0, 3, 1, 4)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(\n            drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim,\n                       act_layer=act_layer, drop=drop)\n\n    def forward(self, x):\n        x = x + self.drop_path(self.attn(self.norm1(x)))\n        x = x + self.drop_path(self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n\n        self.window_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n\n        self.num_patches_w, self.num_patches_h = self.window_size\n\n        self.num_patches = self.window_size[0] * self.window_size[1]\n        self.img_size = img_size\n        self.patch_size = patch_size\n\n        self.proj = nn.Conv2d(in_chans, embed_dim,\n                              kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)\n        return x\n\n\nclass HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n\n    def __init__(self, backbone, img_size=224, feature_size=None, in_chans=3, embed_dim=768):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = to_2tuple(img_size)\n        self.img_size = img_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n                # map for all networks, the feature metadata has reliable channel and stride info, but using\n                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(\n                    1, in_chans, img_size[0], img_size[1]))[-1]\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = to_2tuple(feature_size)\n            feature_dim = self.backbone.feature_info.channels()[-1]\n        self.num_patches = feature_size[0] * feature_size[1]\n        self.proj = nn.Linear(feature_dim, embed_dim)\n\n    def forward(self, x):\n        x = self.backbone(x)[-1]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x\n\n\nclass ViT(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 model_name='vit_base_patch16_224',\n                 img_size=384,\n                 patch_size=16,\n                 in_chans=3,\n                 embed_dim=1024,\n                 depth=24,\n                 num_heads=16,\n                 num_classes=19,\n                 mlp_ratio=4.,\n                 qkv_bias=True,\n                 qk_scale=None,\n                 drop_rate=0.1,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 hybrid_backbone=None,\n                 norm_layer=partial(nn.LayerNorm, eps=1e-6),\n                 norm_cfg=None,\n                 pos_embed_interp=False,\n                 random_init=False,\n                 align_corners=False,\n                 use_checkpoint=False,\n                 num_extra_tokens=1,\n                 out_features=None,\n                 **kwargs,\n                 ):\n\n        super(ViT, self).__init__()\n        self.model_name = model_name\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.depth = depth\n        self.num_heads = num_heads\n        self.num_classes = num_classes\n        self.mlp_ratio = mlp_ratio\n        self.qkv_bias = qkv_bias\n        self.qk_scale = qk_scale\n        self.drop_rate = drop_rate\n        self.attn_drop_rate = attn_drop_rate\n        self.drop_path_rate = drop_path_rate\n        self.hybrid_backbone = hybrid_backbone\n        self.norm_layer = norm_layer\n        self.norm_cfg = norm_cfg\n        self.pos_embed_interp = pos_embed_interp\n        self.random_init = random_init\n        self.align_corners = align_corners\n        self.use_checkpoint = use_checkpoint\n        self.num_extra_tokens = num_extra_tokens\n        self.out_features = out_features\n        self.out_indices = [int(name[5:]) for name in out_features]\n\n        # self.num_stages = self.depth\n        # self.out_indices = tuple(range(self.num_stages))\n\n        if self.hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                self.hybrid_backbone, img_size=self.img_size, in_chans=self.in_chans, embed_dim=self.embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=self.img_size, patch_size=self.patch_size, in_chans=self.in_chans, embed_dim=self.embed_dim)\n        self.num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n\n        if self.num_extra_tokens == 2:\n            self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n\n        self.pos_embed = nn.Parameter(torch.zeros(\n            1, self.num_patches + self.num_extra_tokens, self.embed_dim))\n        self.pos_drop = nn.Dropout(p=self.drop_rate)\n\n        # self.num_extra_tokens = self.pos_embed.shape[-2] - self.num_patches\n        dpr = [x.item() for x in torch.linspace(0, self.drop_path_rate,\n                                                self.depth)]  # stochastic depth decay rule\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=self.embed_dim, num_heads=self.num_heads, mlp_ratio=self.mlp_ratio, qkv_bias=self.qkv_bias,\n                qk_scale=self.qk_scale,\n                drop=self.drop_rate, attn_drop=self.attn_drop_rate, drop_path=dpr[i], norm_layer=self.norm_layer)\n            for i in range(self.depth)])\n\n        # NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\n        # self.repr = nn.Linear(embed_dim, representation_size)\n        # self.repr_act = nn.Tanh()\n\n        if patch_size == 16:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                nn.SyncBatchNorm(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn3 = nn.Identity()\n\n            self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        elif patch_size == 8:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Identity()\n\n            self.fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n\n            self.fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        if self.num_extra_tokens==2:\n            trunc_normal_(self.dist_token, std=0.2)\n        self.apply(self._init_weights)\n        # self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    '''\n    def init_weights(self):\n        logger = get_root_logger()\n\n        trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n\n        if self.init_cfg is None:\n            logger.warn(f'No pre-trained weights for '\n                        f'{self.__class__.__name__}, '\n                        f'training start from scratch')\n        else:\n            assert 'checkpoint' in self.init_cfg, f'Only support ' \\\n                                                  f'specify `Pretrained` in ' \\\n                                                  f'`init_cfg` in ' \\\n                                                  f'{self.__class__.__name__} '\n            logger.info(f\"Will load ckpt from {self.init_cfg['checkpoint']}\")\n            load_checkpoint(self, filename=self.init_cfg['checkpoint'], strict=False, logger=logger)\n    '''\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def _conv_filter(self, state_dict, patch_size=16):\n        \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n        out_dict = {}\n        for k, v in state_dict.items():\n            if 'patch_embed.proj.weight' in k:\n                v = v.reshape((v.shape[0], 3, patch_size, patch_size))\n            out_dict[k] = v\n        return out_dict\n\n    def to_2D(self, x):\n        n, hw, c = x.shape\n        h = w = int(math.sqrt(hw))\n        x = x.transpose(1, 2).reshape(n, c, h, w)\n        return x\n\n    def to_1D(self, x):\n        n, c, h, w = x.shape\n        x = x.reshape(n, c, -1).transpose(1, 2)\n        return x\n\n    def interpolate_pos_encoding(self, x, w, h):\n        npatch = x.shape[1] - self.num_extra_tokens\n        N = self.pos_embed.shape[1] - self.num_extra_tokens\n        if npatch == N and w == h:\n            return self.pos_embed\n\n        class_ORdist_pos_embed = self.pos_embed[:, 0:self.num_extra_tokens]\n\n        patch_pos_embed = self.pos_embed[:, self.num_extra_tokens:]\n\n        dim = x.shape[-1]\n        w0 = w // self.patch_embed.patch_size[0]\n        h0 = h // self.patch_embed.patch_size[1]\n        # we add a small number to avoid floating point error in the interpolation\n        # see discussion at https://github.com/facebookresearch/dino/issues/8\n        w0, h0 = w0 + 0.1, h0 + 0.1\n        patch_pos_embed = nn.functional.interpolate(\n            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n            mode='bicubic',\n        )\n        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n\n        return torch.cat((class_ORdist_pos_embed, patch_pos_embed), dim=1)\n\n    def prepare_tokens(self, x, mask=None):\n        B, nc, w, h = x.shape\n        # patch linear embedding\n        x = self.patch_embed(x)\n\n        # mask image modeling\n        if mask is not None:\n            x = self.mask_model(x, mask)\n        x = x.flatten(2).transpose(1, 2)\n\n        # add the [CLS] token to the embed patch tokens\n        all_tokens = [self.cls_token.expand(B, -1, -1)]\n\n        if self.num_extra_tokens == 2:\n            dist_tokens = self.dist_token.expand(B, -1, -1)\n            all_tokens.append(dist_tokens)\n        all_tokens.append(x)\n\n        x = torch.cat(all_tokens, dim=1)\n\n        # add positional encoding to each token\n        x = x + self.interpolate_pos_encoding(x, w, h)\n\n        return self.pos_drop(x)\n\n    def forward_features(self, x):\n        # print(f\"==========shape of x is {x.shape}==========\")\n        B, _, H, W = x.shape\n        Hp, Wp = H // self.patch_size, W // self.patch_size\n        x = self.prepare_tokens(x)\n\n        features = []\n        for i, blk in enumerate(self.blocks):\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x)\n            else:\n                x = blk(x)\n            if i in self.out_indices:\n                xp = x[:, self.num_extra_tokens:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n\n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n\n        feat_out = {}\n\n        for name, value in zip(self.out_features, features):\n            feat_out[name] = value\n\n        return feat_out\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef deit_base_patch16(pretrained=False, **kwargs):\n    model = ViT(\n        patch_size=16,\n        drop_rate=0.,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        num_classes=1000,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        use_checkpoint=True,\n        num_extra_tokens=2,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef mae_base_patch16(pretrained=False, **kwargs):\n    model = ViT(\n        patch_size=16,\n        drop_rate=0.,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        num_classes=1000,\n        mlp_ratio=4.,\n        qkv_bias=True,\n        use_checkpoint=True,\n        num_extra_tokens=1,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model"}
{"type": "source_file", "path": "modules/ocr/paddle.py", "content": "import re\nimport numpy as np\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom .base import OCRBase \nfrom utils import OCRModel\n\nclass PaddleOCR(OCRBase):\n    def init(self, cfg: dict):\n        self.ocr_model = OCRModel(\n            model_root_dir= Path(\"models/paddle-ocr\"), device=cfg['device']\n        )\n \n    def get_all_text(self, layout) -> str:\n\n        for i, line in tqdm(enumerate(layout)):\n            if line.type in [\"text\", \"list\", \"title\"]:\n                # update this so image is created from images and layout bbox info\n                image = line.image\n                ocr_results = self.get_text(image)\n                text = list(map(lambda x: x[0],ocr_results[1]))\n                text = \" \".join(text)\n                clean_text = re.sub(r\"\\n|\\t\", \" \", text)\n                line.text = clean_text\n\n                lasty = 0\n                cnt = 0\n                for x in ocr_results[0]:\n                    if x[0][1] > lasty:\n                        cnt+=1\n                        lasty = x[2][1]\n\n                line.line_cnt = cnt\n\n\n        return layout\n    \n    def get_text(self, image):\n        return self.ocr_model(image)\n                "}
{"type": "source_file", "path": "modules/ocr/base.py", "content": "from abc import ABC, abstractmethod\n\nclass OCRBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n\n    @abstractmethod\n    def get_text(self, image):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def get_all_text(self, layout):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n"}
{"type": "source_file", "path": "utils/ditod/table_evaluation/__init__.py", "content": "from .evaluate import calc_table_score"}
{"type": "source_file", "path": "utils/ditod/icdar_evaluation.py", "content": "import copy\nimport itertools\nimport os\nimport os.path as osp\nimport shutil\nfrom collections import OrderedDict\nfrom xml.dom.minidom import Document\n\nimport detectron2.utils.comm as comm\nimport torch\nfrom detectron2.evaluation import COCOEvaluator\nfrom detectron2.utils.file_io import PathManager\n\nfrom .table_evaluation.evaluate import calc_table_score\n\n\nclass ICDAREvaluator(COCOEvaluator):\n    def evaluate(self, img_ids=None):\n        \"\"\"\n        Args:\n            img_ids: a list of image IDs to evaluate on. Default to None for the whole dataset\n        \"\"\"\n        if self._distributed:\n            comm.synchronize()\n            predictions = comm.gather(self._predictions, dst=0)\n            predictions = list(itertools.chain(*predictions))\n\n            if not comm.is_main_process():\n                return {}\n        else:\n            predictions = self._predictions\n\n        if len(predictions) == 0:\n            self._logger.warning(\"[COCOEvaluator] Did not receive valid predictions.\")\n            return {}\n\n        if self._output_dir:\n            PathManager.mkdirs(self._output_dir)\n            file_path = os.path.join(self._output_dir, \"instances_predictions.pth\")\n            with PathManager.open(file_path, \"wb\") as f:\n                torch.save(predictions, f)\n\n        self._results = OrderedDict()\n        if \"proposals\" in predictions[0]:\n            self._eval_box_proposals(predictions)\n        if \"instances\" in predictions[0]:\n            self._eval_predictions(predictions, img_ids=img_ids)\n            self.evaluate_table(predictions)\n        # Copy so the caller can do whatever with results\n        return copy.deepcopy(self._results)\n\n    def evaluate_table(self, predictions):\n        xml_dir = self.convert_to_xml(predictions)\n        results = calc_table_score(xml_dir)\n        self._results[\"wF1\"] = results['wF1']\n\n    def convert_to_xml(self, predictions):\n        output_dir = osp.join(self._output_dir, \"xml_results\")\n        if os.path.exists(output_dir):\n            shutil.rmtree(output_dir)\n        os.makedirs(output_dir, exist_ok=True)\n        coco_results = list(itertools.chain(*[x[\"instances\"] for x in predictions]))\n        results_dict = {}\n        for result in coco_results:\n            if result[\"score\"] < 0.7:\n                continue\n            image_id = result[\"image_id\"]\n            if image_id not in results_dict:\n                results_dict[image_id] = []\n\n            results_dict[image_id].append(result)\n\n        for image_id, tables in results_dict.items():\n            file_name = f\"cTDaR_t{image_id:05d}.jpg\"\n            doc = Document()\n            root = doc.createElement('document')\n            root.setAttribute('filename', file_name)\n            doc.appendChild(root)\n            for table_id, table in enumerate(tables, start=1):\n                nodeManager = doc.createElement('table')\n                nodeManager.setAttribute('id', str(table_id))\n                bbox = list(map(int, table['bbox']))\n                bbox_str = '{},{} {},{} {},{} {},{}'.format(bbox[0], bbox[1],\n                                                            bbox[0], bbox[1] + bbox[3],\n                                                            bbox[0] + bbox[2], bbox[1] + bbox[3],\n                                                            bbox[0] + bbox[2], bbox[1])\n                nodeCoords = doc.createElement('Coords')\n                nodeCoords.setAttribute('points', bbox_str)\n                nodeManager.appendChild(nodeCoords)\n                root.appendChild(nodeManager)\n            filename = '{}-result.xml'.format(file_name[:-4])\n            fp = open(os.path.join(output_dir, filename), 'w')\n            doc.writexml(fp, indent='', addindent='\\t', newl='\\n', encoding=\"utf-8\")\n            fp.flush()\n            fp.close()\n        return output_dir\n\n\nif __name__ == '__main__':\n    pass\n"}
{"type": "source_file", "path": "utils/ditod/mytrainer.py", "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\"\"\"\nThis file contains components with some default boilerplate logic user may need\nin training / testing. They will not work for everyone, but many users may find them useful.\n\nThe behavior of functions/classes in this file is subject to change,\nsince they are meant to represent the \"common default behavior\" people need in their projects.\n\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport sys\nimport weakref\nfrom collections import OrderedDict\nfrom typing import Optional\nimport torch\nfrom fvcore.nn.precise_bn import get_bn_modules\nfrom omegaconf import OmegaConf\nfrom torch.nn.parallel import DistributedDataParallel\n\nimport detectron2.data.transforms as T\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.config import CfgNode, LazyConfig\nfrom detectron2.data import (\n    MetadataCatalog,\n    build_detection_test_loader,\n    build_detection_train_loader,\n)\nfrom detectron2.evaluation import (\n    DatasetEvaluator,\n    inference_on_dataset,\n    print_csv_format,\n    verify_results,\n)\nfrom detectron2.modeling import build_model\nfrom detectron2.solver import build_lr_scheduler, build_optimizer\nfrom detectron2.utils import comm\nfrom detectron2.utils.collect_env import collect_env_info\nfrom detectron2.utils.env import seed_all_rng\nfrom detectron2.utils.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter\nfrom detectron2.utils.file_io import PathManager\nfrom detectron2.utils.logger import setup_logger\n\nfrom detectron2.engine import hooks\nfrom detectron2.engine.train_loop import AMPTrainer, SimpleTrainer, TrainerBase\n\nfrom .mycheckpointer import MyDetectionCheckpointer\nfrom typing import Any, Dict, List, Set\nimport itertools\nfrom detectron2.solver.build import maybe_add_gradient_clipping\nfrom .dataset_mapper import DetrDatasetMapper\nfrom .icdar_evaluation import ICDAREvaluator\nfrom detectron2.evaluation import COCOEvaluator\n\n__all__ = [\n    \"create_ddp_model\",\n    \"default_argument_parser\",\n    \"default_setup\",\n    \"default_writers\",\n    \"DefaultPredictor\",\n    \"MyTrainer\",\n]\n\n\ndef create_ddp_model(model, *, fp16_compression=False, **kwargs):\n    \"\"\"\n    Create a DistributedDataParallel model if there are >1 processes.\n\n    Args:\n        model: a torch.nn.Module\n        fp16_compression: add fp16 compression hooks to the ddp object.\n            See more at https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook\n        kwargs: other arguments of :module:`torch.nn.parallel.DistributedDataParallel`.\n    \"\"\"  # noqa\n    if comm.get_world_size() == 1:\n        return model\n    if \"device_ids\" not in kwargs:\n        kwargs[\"device_ids\"] = [comm.get_local_rank()]\n    ddp = DistributedDataParallel(model, **kwargs)\n    if fp16_compression:\n        from torch.distributed.algorithms.ddp_comm_hooks import default as comm_hooks\n\n        ddp.register_comm_hook(state=None, hook=comm_hooks.fp16_compress_hook)\n    return ddp\n\n\ndef default_argument_parser(epilog=None):\n    \"\"\"\n    Create a parser with some common arguments used by detectron2 users.\n\n    Args:\n        epilog (str): epilog passed to ArgumentParser describing the usage.\n\n    Returns:\n        argparse.ArgumentParser:\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        epilog=epilog\n        or f\"\"\"\nExamples:\n\nRun on single machine:\n    $ {sys.argv[0]} --num-gpus 8 --config-file cfg.yaml\n\nChange some config options:\n    $ {sys.argv[0]} --config-file cfg.yaml MODEL.WEIGHTS /path/to/weight.pth SOLVER.BASE_LR 0.001\n\nRun on multiple machines:\n    (machine0)$ {sys.argv[0]} --machine-rank 0 --num-machines 2 --dist-url <URL> [--other-flags]\n    (machine1)$ {sys.argv[0]} --machine-rank 1 --num-machines 2 --dist-url <URL> [--other-flags]\n\"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--resume\",\n        action=\"store_true\",\n        help=\"Whether to attempt to resume from the checkpoint directory. \"\n        \"See documentation of `MyTrainer.resume_or_load()` for what it means.\",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")\n    parser.add_argument(\"--num-gpus\", type=int, default=1, help=\"number of gpus *per machine*\")\n    parser.add_argument(\"--num-machines\", type=int, default=1, help=\"total number of machines\")\n    parser.add_argument(\n        \"--machine-rank\", type=int, default=0, help=\"the rank of this machine (unique per machine)\"\n    )\n\n    # PyTorch still may leave orphan processes in multi-gpu training.\n    # Therefore we use a deterministic way to obtain port,\n    # so that users are aware of orphan processes by seeing the port occupied.\n    port = 2 ** 15 + 2 ** 14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2 ** 14\n    parser.add_argument(\n        \"--dist-url\",\n        default=\"tcp://127.0.0.1:{}\".format(port),\n        help=\"initialization URL for pytorch distributed backend. See \"\n        \"https://pytorch.org/docs/stable/distributed.html for details.\",\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"\"\"\nModify config options at the end of the command. For Yacs configs, use\nspace-separated \"PATH.KEY VALUE\" pairs.\nFor python-based LazyConfig, use \"path.key=value\".\n        \"\"\".strip(),\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    return parser\n\n\ndef _try_get_key(cfg, *keys, default=None):\n    \"\"\"\n    Try select keys from cfg until the first key that exists. Otherwise return default.\n    \"\"\"\n    if isinstance(cfg, CfgNode):\n        cfg = OmegaConf.create(cfg.dump())\n    for k in keys:\n        none = object()\n        p = OmegaConf.select(cfg, k, default=none)\n        if p is not none:\n            return p\n    return default\n\n\ndef _highlight(code, filename):\n    try:\n        import pygments\n    except ImportError:\n        return code\n\n    from pygments.lexers import Python3Lexer, YamlLexer\n    from pygments.formatters import Terminal256Formatter\n\n    lexer = Python3Lexer() if filename.endswith(\".py\") else YamlLexer()\n    code = pygments.highlight(code, lexer, Terminal256Formatter(style=\"monokai\"))\n    return code\n\n\ndef default_setup(cfg, args):\n    \"\"\"\n    Perform some basic common setups at the beginning of a job, including:\n\n    1. Set up the detectron2 logger\n    2. Log basic information about environment, cmdline arguments, and config\n    3. Backup the config to the output directory\n\n    Args:\n        cfg (CfgNode or omegaconf.DictConfig): the full config to be used\n        args (argparse.NameSpace): the command line arguments to be logged\n    \"\"\"\n    output_dir = _try_get_key(cfg, \"OUTPUT_DIR\", \"output_dir\", \"train.output_dir\")\n    if comm.is_main_process() and output_dir:\n        PathManager.mkdirs(output_dir)\n\n    rank = comm.get_rank()\n    setup_logger(output_dir, distributed_rank=rank, name=\"fvcore\")\n    logger = setup_logger(output_dir, distributed_rank=rank)\n\n    logger.info(\"Rank of current process: {}. World size: {}\".format(rank, comm.get_world_size()))\n    logger.info(\"Environment info:\\n\" + collect_env_info())\n\n    logger.info(\"Command line arguments: \" + str(args))\n    if hasattr(args, \"config_file\") and args.config_file != \"\":\n        logger.info(\n            \"Contents of args.config_file={}:\\n{}\".format(\n                args.config_file,\n                _highlight(PathManager.open(args.config_file, \"r\").read(), args.config_file),\n            )\n        )\n\n    if comm.is_main_process() and output_dir:\n        # Note: some of our scripts may expect the existence of\n        # config.yaml in output directory\n        path = os.path.join(output_dir, \"config.yaml\")\n        if isinstance(cfg, CfgNode):\n            logger.info(\"Running with full config:\\n{}\".format(_highlight(cfg.dump(), \".yaml\")))\n            with PathManager.open(path, \"w\") as f:\n                f.write(cfg.dump())\n        else:\n            LazyConfig.save(cfg, path)\n        logger.info(\"Full config saved to {}\".format(path))\n\n    # make sure each worker has a different, yet deterministic seed if specified\n    seed = _try_get_key(cfg, \"SEED\", \"train.seed\", default=-1)\n    seed_all_rng(None if seed < 0 else seed + rank)\n\n    # cudnn benchmark has large overhead. It shouldn't be used considering the small size of\n    # typical validation set.\n    if not (hasattr(args, \"eval_only\") and args.eval_only):\n        torch.backends.cudnn.benchmark = _try_get_key(\n            cfg, \"CUDNN_BENCHMARK\", \"train.cudnn_benchmark\", default=False\n        )\n\n\ndef default_writers(output_dir: str, max_iter: Optional[int] = None):\n    \"\"\"\n    Build a list of :class:`EventWriter` to be used.\n    It now consists of a :class:`CommonMetricPrinter`,\n    :class:`TensorboardXWriter` and :class:`JSONWriter`.\n\n    Args:\n        output_dir: directory to store JSON metrics and tensorboard events\n        max_iter: the total number of iterations\n\n    Returns:\n        list[EventWriter]: a list of :class:`EventWriter` objects.\n    \"\"\"\n    PathManager.mkdirs(output_dir)\n    return [\n        # It may not always print what you want to see, since it prints \"common\" metrics only.\n        CommonMetricPrinter(max_iter),\n        JSONWriter(os.path.join(output_dir, \"metrics.json\")),\n        TensorboardXWriter(output_dir),\n    ]\n\n\nclass DefaultPredictor:\n    \"\"\"\n    Create a simple end-to-end predictor with the given config that runs on\n    single device for a single input image.\n\n    Compared to using the model directly, this class does the following additions:\n\n    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.\n    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.\n    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.\n    4. Take one input image and produce a single output, instead of a batch.\n\n    This is meant for simple demo purposes, so it does the above steps automatically.\n    This is not meant for benchmarks or running complicated inference logic.\n    If you'd like to do anything more complicated, please refer to its source code as\n    examples to build and use the model manually.\n\n    Attributes:\n        metadata (Metadata): the metadata of the underlying dataset, obtained from\n            cfg.DATASETS.TEST.\n\n    Examples:\n    ::\n        pred = DefaultPredictor(cfg)\n        inputs = cv2.imread(\"input.jpg\")\n        outputs = pred(inputs)\n    \"\"\"\n\n    def __init__(self, cfg):\n        self.cfg = cfg.clone()  # cfg can be modified by model\n        self.model = build_model(self.cfg)\n        self.model.eval()\n        if len(cfg.DATASETS.TEST):\n            self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n\n        checkpointer = DetectionCheckpointer(self.model)\n        checkpointer.load(cfg.MODEL.WEIGHTS)\n\n        self.aug = T.ResizeShortestEdge(\n            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n        )\n\n        self.input_format = cfg.INPUT.FORMAT\n        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n\n    def __call__(self, original_image):\n        \"\"\"\n        Args:\n            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n\n        Returns:\n            predictions (dict):\n                the output of the model for one image only.\n                See :doc:`/tutorials/models` for details about the format.\n        \"\"\"\n        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n            # Apply pre-processing to image.\n            if self.input_format == \"RGB\":\n                # whether the model expects BGR inputs or RGB\n                original_image = original_image[:, :, ::-1]\n            height, width = original_image.shape[:2]\n            image = self.aug.get_transform(original_image).apply_image(original_image)\n            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n\n            inputs = {\"image\": image, \"height\": height, \"width\": width}\n            predictions = self.model([inputs])[0]\n            return predictions\n\n\nclass MyTrainer(TrainerBase):\n    \"\"\"\n    A trainer with default training logic. It does the following:\n\n    1. Create a :class:`SimpleTrainer` using model, optimizer, dataloader\n       defined by the given config. Create a LR scheduler defined by the config.\n    2. Load the last checkpoint or `cfg.MODEL.WEIGHTS`, if exists, when\n       `resume_or_load` is called.\n    3. Register a few common hooks defined by the config.\n\n    It is created to simplify the **standard model training workflow** and reduce code boilerplate\n    for users who only need the standard training workflow, with standard features.\n    It means this class makes *many assumptions* about your training logic that\n    may easily become invalid in a new research. In fact, any assumptions beyond those made in the\n    :class:`SimpleTrainer` are too much for research.\n\n    The code of this class has been annotated about restrictive assumptions it makes.\n    When they do not work for you, you're encouraged to:\n\n    1. Overwrite methods of this class, OR:\n    2. Use :class:`SimpleTrainer`, which only does minimal SGD training and\n       nothing else. You can then add your own hooks if needed. OR:\n    3. Write your own training loop similar to `tools/plain_train_net.py`.\n\n    See the :doc:`/tutorials/training` tutorials for more details.\n\n    Note that the behavior of this class, like other functions/classes in\n    this file, is not stable, since it is meant to represent the \"common default behavior\".\n    It is only guaranteed to work well with the standard models and training workflow in detectron2.\n    To obtain more stable behavior, write your own training logic with other public APIs.\n\n    Examples:\n    ::\n        trainer = MyTrainer(cfg)\n        trainer.resume_or_load()  # load last checkpoint or MODEL.WEIGHTS\n        trainer.train()\n\n    Attributes:\n        scheduler:\n        checkpointer (DetectionCheckpointer):\n        cfg (CfgNode):\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode):\n        \"\"\"\n        super().__init__()\n        logger = logging.getLogger(\"detectron2\")\n        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for d2\n            setup_logger()\n        cfg = MyTrainer.auto_scale_workers(cfg, comm.get_world_size())\n\n        self.cfg = cfg\n\n        # Assume these objects must be constructed in this order.\n        model = self.build_model(cfg)\n        optimizer = self.build_optimizer(cfg, model)\n        data_loader = self.build_train_loader(cfg)\n\n        model = create_ddp_model(model, broadcast_buffers=False)\n        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n            model, data_loader, optimizer\n        )\n\n        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n        self.checkpointer = MyDetectionCheckpointer(\n            # Assume you want to save checkpoints together with logs/statistics\n            model,\n            cfg.OUTPUT_DIR,\n            trainer=weakref.proxy(self),\n        )\n        self.start_iter = 0\n        self.max_iter = cfg.SOLVER.MAX_ITER\n        self.cfg = cfg\n\n        self.register_hooks(self.build_hooks())\n\n    def resume_or_load(self, resume=True):\n        \"\"\"\n        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n        a `last_checkpoint` file), resume from the file. Resuming means loading all\n        available states (eg. optimizer and scheduler) and update iteration counter\n        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n\n        Otherwise, this is considered as an independent training. The method will load model\n        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n        from iteration 0.\n\n        Args:\n            resume (bool): whether to do resume or not\n        \"\"\"\n        self.checkpointer.resume_or_load(self.cfg.MODEL.WEIGHTS, resume=resume)\n        if resume and self.checkpointer.has_checkpoint():\n            # The checkpoint stores the training iteration that just finished, thus we start\n            # at the next iteration\n            self.start_iter = self.iter + 1\n\n    def build_hooks(self):\n        \"\"\"\n        Build a list of default hooks, including timing, evaluation,\n        checkpointing, lr scheduling, precise BN, writing events.\n\n        Returns:\n            list[HookBase]:\n        \"\"\"\n        cfg = self.cfg.clone()\n        cfg.defrost()\n        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n\n        ret = [\n            hooks.IterationTimer(),\n            hooks.LRScheduler(),\n            hooks.PreciseBN(\n                # Run at the same freq as (but before) evaluation.\n                cfg.TEST.EVAL_PERIOD,\n                self.model,\n                # Build a new data loader to not affect training\n                self.build_train_loader(cfg),\n                cfg.TEST.PRECISE_BN.NUM_ITER,\n            )\n            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n            else None,\n        ]\n\n        # Do PreciseBN before checkpointer, because it updates the model and need to\n        # be saved by checkpointer.\n        # This is not always the best: if checkpointing has a different frequency,\n        # some checkpoints may have more precise statistics than others.\n        if comm.is_main_process():\n            ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))\n\n        def test_and_save_results():\n            self._last_eval_results = self.test(self.cfg, self.model)\n            return self._last_eval_results\n\n        # Do evaluation after checkpointer, because then if it fails,\n        # we can use the saved checkpoint to debug.\n        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))\n\n        if comm.is_main_process():\n            # Here the default print/log frequency of each writer is used.\n            # run writers in the end, so that evaluation metrics are written\n            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n        return ret\n\n    def build_writers(self):\n        \"\"\"\n        Build a list of writers to be used using :func:`default_writers()`.\n        If you'd like a different list of writers, you can overwrite it in\n        your trainer.\n\n        Returns:\n            list[EventWriter]: a list of :class:`EventWriter` objects.\n        \"\"\"\n        return default_writers(self.cfg.OUTPUT_DIR, self.max_iter)\n\n    def train(self):\n        \"\"\"\n        Run training.\n\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        \"\"\"\n        super().train(self.start_iter, self.max_iter)\n        if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\n            assert hasattr(\n                self, \"_last_eval_results\"\n            ), \"No evaluation results obtained during training!\"\n            verify_results(self.cfg, self._last_eval_results)\n            return self._last_eval_results\n\n    def run_step(self):\n        self._trainer.iter = self.iter\n        self._trainer.run_step()\n\n    @classmethod\n    def build_model(cls, cfg):\n        \"\"\"\n        Returns:\n            torch.nn.Module:\n\n        It now calls :func:`detectron2.modeling.build_model`.\n        Overwrite it if you'd like a different model.\n        \"\"\"\n        model = build_model(cfg)\n        logger = logging.getLogger(__name__)\n        logger.info(\"Model:\\n{}\".format(model))\n        return model\n\n    @classmethod\n    def build_optimizer(cls, cfg, model):\n        params: List[Dict[str, Any]] = []\n        memo: Set[torch.nn.parameter.Parameter] = set()\n        for key, value in model.named_parameters(recurse=True):\n            if not value.requires_grad:\n                continue\n            # Avoid duplicating parameters\n            if value in memo:\n                continue\n            memo.add(value)\n            lr = cfg.SOLVER.BASE_LR\n            weight_decay = cfg.SOLVER.WEIGHT_DECAY\n            if \"backbone\" in key:\n                lr = lr * cfg.SOLVER.BACKBONE_MULTIPLIER\n            params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n\n        def maybe_add_full_model_gradient_clipping(optim):  # optim: the optimizer class\n            # detectron2 doesn't have full model gradient clipping now\n            clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE\n            enable = (\n                    cfg.SOLVER.CLIP_GRADIENTS.ENABLED\n                    and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\"\n                    and clip_norm_val > 0.0\n            )\n\n            class FullModelGradientClippingOptimizer(optim):\n                def step(self, closure=None):\n                    all_params = itertools.chain(*[x[\"params\"] for x in self.param_groups])\n                    torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)\n                    super().step(closure=closure)\n\n            return FullModelGradientClippingOptimizer if enable else optim\n\n        optimizer_type = cfg.SOLVER.OPTIMIZER\n        if optimizer_type == \"SGD\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(\n                params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM\n            )\n        elif optimizer_type == \"ADAMW\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(\n                params, cfg.SOLVER.BASE_LR\n            )\n        else:\n            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n        if not cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\":\n            optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n        return optimizer\n\n    @classmethod\n    def build_lr_scheduler(cls, cfg, optimizer):\n        \"\"\"\n        It now calls :func:`detectron2.solver.build_lr_scheduler`.\n        Overwrite it if you'd like a different scheduler.\n        \"\"\"\n        return build_lr_scheduler(cfg, optimizer)\n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        if cfg.AUG.DETR:\n            mapper = DetrDatasetMapper(cfg, is_train=True)\n        else:\n            mapper = None\n        return build_detection_train_loader(cfg, mapper=mapper)\n\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \"\"\"\n        Returns:\n            iterable\n\n        It now calls :func:`detectron2.data.build_detection_test_loader`.\n        Overwrite it if you'd like a different data loader.\n        \"\"\"\n        return build_detection_test_loader(cfg, dataset_name)\n\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n        if 'icdar' not in dataset_name:\n            return COCOEvaluator(dataset_name, output_dir=output_folder)\n        else:\n            return ICDAREvaluator(dataset_name, output_dir=output_folder)\n\n    @classmethod\n    def test(cls, cfg, model, evaluators=None):\n        \"\"\"\n        Evaluate the given model. The given model is expected to already contain\n        weights to evaluate.\n\n        Args:\n            cfg (CfgNode):\n            model (nn.Module):\n            evaluators (list[DatasetEvaluator] or None): if None, will call\n                :meth:`build_evaluator`. Otherwise, must have the same length as\n                ``cfg.DATASETS.TEST``.\n\n        Returns:\n            dict: a dict of result metrics\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        if isinstance(evaluators, DatasetEvaluator):\n            evaluators = [evaluators]\n        if evaluators is not None:\n            assert len(cfg.DATASETS.TEST) == len(evaluators), \"{} != {}\".format(\n                len(cfg.DATASETS.TEST), len(evaluators)\n            )\n\n        results = OrderedDict()\n        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n            data_loader = cls.build_test_loader(cfg, dataset_name)\n            # When evaluators are passed in as arguments,\n            # implicitly assume that evaluators can be created before data_loader.\n            if evaluators is not None:\n                evaluator = evaluators[idx]\n            else:\n                try:\n                    evaluator = cls.build_evaluator(cfg, dataset_name)\n                except NotImplementedError:\n                    logger.warn(\n                        \"No evaluator found. Use `MyTrainer.test(evaluators=)`, \"\n                        \"or implement its `build_evaluator` method.\"\n                    )\n                    results[dataset_name] = {}\n                    continue\n            results_i = inference_on_dataset(model, data_loader, evaluator)\n            results[dataset_name] = results_i\n            if comm.is_main_process():\n                assert isinstance(\n                    results_i, dict\n                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n                    results_i\n                )\n                logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n                print_csv_format(results_i)\n\n        if len(results) == 1:\n            results = list(results.values())[0]\n        return results\n\n    @staticmethod\n    def auto_scale_workers(cfg, num_workers: int):\n        \"\"\"\n        When the config is defined for certain number of workers (according to\n        ``cfg.SOLVER.REFERENCE_WORLD_SIZE``) that's different from the number of\n        workers currently in use, returns a new cfg where the total batch size\n        is scaled so that the per-GPU batch size stays the same as the\n        original ``IMS_PER_BATCH // REFERENCE_WORLD_SIZE``.\n\n        Other config options are also scaled accordingly:\n        * training steps and warmup steps are scaled inverse proportionally.\n        * learning rate are scaled proportionally, following :paper:`ImageNet in 1h`.\n\n        For example, with the original config like the following:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 16\n            BASE_LR: 0.1\n            REFERENCE_WORLD_SIZE: 8\n            MAX_ITER: 5000\n            STEPS: (4000,)\n            CHECKPOINT_PERIOD: 1000\n\n        When this config is used on 16 GPUs instead of the reference number 8,\n        calling this method will return a new config with:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 32\n            BASE_LR: 0.2\n            REFERENCE_WORLD_SIZE: 16\n            MAX_ITER: 2500\n            STEPS: (2000,)\n            CHECKPOINT_PERIOD: 500\n\n        Note that both the original config and this new config can be trained on 16 GPUs.\n        It's up to user whether to enable this feature (by setting ``REFERENCE_WORLD_SIZE``).\n\n        Returns:\n            CfgNode: a new config. Same as original if ``cfg.SOLVER.REFERENCE_WORLD_SIZE==0``.\n        \"\"\"\n        old_world_size = cfg.SOLVER.REFERENCE_WORLD_SIZE\n        if old_world_size == 0 or old_world_size == num_workers:\n            return cfg\n        cfg = cfg.clone()\n        frozen = cfg.is_frozen()\n        cfg.defrost()\n\n        assert (\n            cfg.SOLVER.IMS_PER_BATCH % old_world_size == 0\n        ), \"Invalid REFERENCE_WORLD_SIZE in config!\"\n        scale = num_workers / old_world_size\n        bs = cfg.SOLVER.IMS_PER_BATCH = int(round(cfg.SOLVER.IMS_PER_BATCH * scale))\n        lr = cfg.SOLVER.BASE_LR = cfg.SOLVER.BASE_LR * scale\n        max_iter = cfg.SOLVER.MAX_ITER = int(round(cfg.SOLVER.MAX_ITER / scale))\n        warmup_iter = cfg.SOLVER.WARMUP_ITERS = int(round(cfg.SOLVER.WARMUP_ITERS / scale))\n        cfg.SOLVER.STEPS = tuple(int(round(s / scale)) for s in cfg.SOLVER.STEPS)\n        cfg.TEST.EVAL_PERIOD = int(round(cfg.TEST.EVAL_PERIOD / scale))\n        cfg.SOLVER.CHECKPOINT_PERIOD = int(round(cfg.SOLVER.CHECKPOINT_PERIOD / scale))\n        cfg.SOLVER.REFERENCE_WORLD_SIZE = num_workers  # maintain invariant\n        logger = logging.getLogger(__name__)\n        logger.info(\n            f\"Auto-scaling the config to batch_size={bs}, learning_rate={lr}, \"\n            f\"max_iter={max_iter}, warmup={warmup_iter}.\"\n        )\n\n        if frozen:\n            cfg.freeze()\n        return cfg\n\n\n# Access basic attributes from the underlying trainer\nfor _attr in [\"model\", \"data_loader\", \"optimizer\"]:\n    setattr(\n        MyTrainer,\n        _attr,\n        property(\n            # getter\n            lambda self, x=_attr: getattr(self._trainer, x),\n            # setter\n            lambda self, value, x=_attr: setattr(self._trainer, x, value),\n        ),\n    )\n"}
{"type": "source_file", "path": "modules/font/base.py", "content": "from abc import ABC, abstractmethod\n\nclass FontBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n\n    @abstractmethod\n    def get_font_info(self, image, line_cnt):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n\n    @abstractmethod\n    def get_all_fonts(self, layout):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n\n"}
{"type": "source_file", "path": "utils/ditod/beit.py", "content": "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929\n\nThe official jax code is released and available at https://github.com/google-research/vision_transformer\n\nStatus/TODO:\n* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.\n* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.\n* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.\n* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.\n\nAcknowledgments:\n* The paper authors for releasing code and weights, thanks!\n* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\nfor some einops/einsum fun\n* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport warnings\nimport math\nimport torch\nfrom functools import partial\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            # trunc_normal_(self.relative_position_bias_table, std=.0)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            if training_window_size == self.window_size:\n                relative_position_bias = \\\n                    self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                        self.window_size[0] * self.window_size[1] + 1,\n                        self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n            else:\n                training_window_size = tuple(training_window_size.tolist())\n                new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n                # new_num_relative_dis  cls-clstok-clscls-tok\n                new_relative_position_bias_table = F.interpolate(\n                    self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                                 2 * self.window_size[0] - 1,\n                                                                                 2 * self.window_size[1] - 1),\n                    size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                    align_corners=False)\n                new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                         new_num_relative_distance - 3).permute(\n                    1, 0)\n                new_relative_position_bias_table = torch.cat(\n                    [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n                # get pair-wise relative position index for each token inside the window\n                coords_h = torch.arange(training_window_size[0])\n                coords_w = torch.arange(training_window_size[1])\n                coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n                coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n                relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n                relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n                relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n                relative_coords[:, :, 1] += training_window_size[1] - 1\n                relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n                relative_position_index = \\\n                    torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                                dtype=relative_coords.dtype)\n                relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n                relative_position_index[0, 0:] = new_num_relative_distance - 3\n                relative_position_index[0:, 0] = new_num_relative_distance - 2\n                relative_position_index[0, 0] = new_num_relative_distance - 1\n\n                relative_position_bias = \\\n                    new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                        training_window_size[0] * training_window_size[1] + 1,\n                        training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(\n                self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, training_window_size=training_window_size))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias,\n                                                            training_window_size=training_window_size))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=[224, 224], patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches_w = self.patch_shape[0]\n        self.num_patches_h = self.patch_shape[1]\n        # the so-called patch_shape is the patch shape during pre-training\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, position_embedding=None, **kwargs):\n        # FIXME look at relaxing size constraints\n        # assert H == self.img_size[0] and W == self.img_size[1], \\\n        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n\n        if position_embedding is not None:\n            # interpolate the position embedding to the corresponding size\n            position_embedding = position_embedding.view(1, self.patch_shape[0], self.patch_shape[1], -1).permute(0, 3,\n                                                                                                                  1, 2)\n            position_embedding = F.interpolate(position_embedding, size=(Hp, Wp), mode='bicubic')\n            x = x + position_embedding\n\n        x = x.flatten(2).transpose(1, 2)\n        return x, (Hp, Wp)\n\n\nclass HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n\n    def __init__(self, backbone, img_size=[224, 224], feature_size=None, in_chans=3, embed_dim=768):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = to_2tuple(img_size)\n        self.img_size = img_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n                # map for all networks, the feature metadata has reliable channel and stride info, but using\n                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = to_2tuple(feature_size)\n            feature_dim = self.backbone.feature_info.channels()[-1]\n        self.num_patches = feature_size[0] * feature_size[1]\n        self.proj = nn.Linear(feature_dim, embed_dim)\n\n    def forward(self, x):\n        x = self.backbone(x)[-1]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        # trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def forward(self, training_window_size):\n        if training_window_size == self.window_size:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        else:\n            training_window_size = tuple(training_window_size.tolist())\n            new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n            # new_num_relative_dis  cls-clstok-clscls-tok\n            new_relative_position_bias_table = F.interpolate(\n                self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                             2 * self.window_size[0] - 1,\n                                                                             2 * self.window_size[1] - 1),\n                size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                align_corners=False)\n            new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                     new_num_relative_distance - 3).permute(\n                1, 0)\n            new_relative_position_bias_table = torch.cat(\n                [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(training_window_size[0])\n            coords_w = torch.arange(training_window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += training_window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                            dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = new_num_relative_distance - 3\n            relative_position_index[0:, 0] = new_num_relative_distance - 2\n            relative_position_index[0, 0] = new_num_relative_distance - 1\n\n            relative_position_bias = \\\n                new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                    training_window_size[0] * training_window_size[1] + 1,\n                    training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n        return relative_position_bias\n\n\nclass BEiT(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 img_size=[224, 224],\n                 patch_size=16,\n                 in_chans=3,\n                 num_classes=80,\n                 embed_dim=768,\n                 depth=12,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 hybrid_backbone=None,\n                 norm_layer=None,\n                 init_values=None,\n                 use_abs_pos_emb=False,\n                 use_rel_pos_bias=False,\n                 use_shared_rel_pos_bias=False,\n                 use_checkpoint=True,\n                 pretrained=None,\n                 out_features=None,\n                 ):\n\n        super(BEiT, self).__init__()\n\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.use_checkpoint = use_checkpoint\n\n        if hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n        self.out_features = out_features\n        self.out_indices = [int(name[5:]) for name in out_features]\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        self.use_shared_rel_pos_bias = use_shared_rel_pos_bias\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(depth)])\n\n        # trunc_normal_(self.mask_token, std=.02)\n\n        if patch_size == 16:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                # nn.SyncBatchNorm(embed_dim),\n                nn.BatchNorm2d(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn3 = nn.Identity()\n\n            self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n        elif patch_size == 8:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n\n            self.fpn2 = nn.Identity()\n\n            self.fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n\n            self.fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    '''\n    def init_weights(self):\n        \"\"\"Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n        logger = get_root_logger()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if self.init_cfg is None:\n            logger.warn(f'No pre-trained weights for '\n                        f'{self.__class__.__name__}, '\n                        f'training start from scratch')\n        else:\n            assert 'checkpoint' in self.init_cfg, f'Only support ' \\\n                                                  f'specify `Pretrained` in ' \\\n                                                  f'`init_cfg` in ' \\\n                                                  f'{self.__class__.__name__} '\n            logger.info(f\"Will load ckpt from {self.init_cfg['checkpoint']}\")\n            load_checkpoint(self,\n                            filename=self.init_cfg['checkpoint'],\n                            strict=False,\n                            logger=logger,\n                            beit_spec_expand_rel_pos = self.use_rel_pos_bias,\n                            )\n    '''\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x):\n        B, C, H, W = x.shape\n        x, (Hp, Wp) = self.patch_embed(x, self.pos_embed[:, 1:, :] if self.pos_embed is not None else None)\n        # Hp, Wp are HW for patches\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        if self.pos_embed is not None:\n            cls_tokens = cls_tokens + self.pos_embed[:, :1, :]\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = self.pos_drop(x)\n\n        features = []\n        training_window_size = torch.tensor([Hp, Wp])\n\n        rel_pos_bias = self.rel_pos_bias(training_window_size) if self.rel_pos_bias is not None else None\n\n        for i, blk in enumerate(self.blocks):\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, rel_pos_bias, training_window_size)\n            else:\n                x = blk(x, rel_pos_bias=rel_pos_bias, training_window_size=training_window_size)\n            if i in self.out_indices:\n                xp = x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n\n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n\n        feat_out = {}\n\n        for name, value in zip(self.out_features, features):\n            feat_out[name] = value\n\n        return feat_out\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        return x\n\n\ndef beit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef beit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.1,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=1e-5,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\nif __name__ == '__main__':\n    model = BEiT(use_checkpoint=True, use_shared_rel_pos_bias=True)\n    model = model.to(\"cuda:0\")\n    input1 = torch.rand(2, 3, 512, 762).to(\"cuda:0\")\n    input2 = torch.rand(2, 3, 800, 1200).to(\"cuda:0\")\n    input3 = torch.rand(2, 3, 720, 1000).to(\"cuda:0\")\n    output1 = model(input1)\n    output2 = model(input2)\n    output3 = model(input3)\n    print(\"all done\")\n"}
{"type": "source_file", "path": "utils/ditod/table_evaluation/evaluate.py", "content": "\"\"\"\nEvaluation of -.tar.gz file.\nYu Fang - March 2019\n\"\"\"\n\nimport os\nimport xml.dom.minidom\n\n# from eval import eval\n\nreg_gt_path = os.path.abspath(\"data/test\")\nreg_gt_path_archival = os.path.abspath(\"data/test\")\nreg_gt_path_modern = os.path.abspath(\"data/test\")\nstr_gt_path_1 = os.path.abspath(\"data/test\")\nstr_gt_path_2 = os.path.abspath(\"data/test\")\nstr_gt_path_archival = os.path.abspath(\"data/test\")\nstr_gt_path_modern = os.path.abspath(\"data/test\")\n\nimport xml.dom.minidom\n# from functools import cmp_to_key\nfrom os.path import join as osj\nfrom .data_structure import *\n\n\nclass eval:\n    STR = \"-str\"\n    REG = \"-reg\"\n    DEFAULT_ENCODING = \"UTF-8\"\n    # reg_gt_path = \"./annotations/trackA/\"\n    # str_gt_path = \"./annotations/trackB/\"\n    # reg_gt_path = os.path.abspath(\"data/test\")\n    # reg_gt_path_archival = os.path.abspath(\"data/test\")\n    # reg_gt_path_modern = os.path.abspath(\"data/test\")\n    # str_gt_path_1 = os.path.abspath(\"data/test\")\n    # str_gt_path_2 = os.path.abspath(\"data/test\")\n    # str_gt_path_archival = os.path.abspath(\"data/test\")\n    # str_gt_path_modern = os.path.abspath(\"data/test\")\n\n    # dummyDom = xml.dom.minidom.parse(\"./dummyXML.xml\")\n\n    def __init__(self, track, res_path):\n        self.return_result = None\n        self.reg = True\n        self.str = False\n\n        self.resultFile = res_path\n        self.inPrefix = os.path.split(res_path)[-1].split(\".\")[0][:-7]\n\n        if track == \"-trackA\":\n            self.reg = True\n            self.GTFile = osj(reg_gt_path, self.inPrefix + \".xml\")\n            # self.GTFile = osj(self.reg_gt_path, self.inPrefix)\n        elif track == \"-trackA1\":  # archival documents\n            self.reg = True\n            self.GTFile = osj(reg_gt_path_archival, self.inPrefix + \".xml\")\n        elif track == \"-trackA2\":  # modern documents\n            self.reg = True\n            self.GTFile = osj(reg_gt_path_modern, self.inPrefix + \".xml\")\n        elif track == \"-trackB1\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_1, self.inPrefix + \".xml\")\n            # self.GTFile = osj(self.str_gt_path_1, self.inPrefix)\n        elif track == \"-trackB2\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_2, self.inPrefix + \".xml\")\n            # print(self.GTFile)\n            # self.GTFile = osj(self.str_gt_path_2, self.inPrefix)\n        elif track == \"-trackB2_a\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_archival, self.inPrefix + \".xml\")\n        elif track == \"-trackB2_m\":\n            self.str = True\n            self.GTFile = osj(str_gt_path_modern, self.inPrefix + \".xml\")\n        else:\n            print(track)\n            print(\"Not a valid track, please check your spelling.\")\n\n        # self.resultFile = res_path\n        # self.inPrefix = os.path.split(res_path)[-1].split(\"-\")[0]\n\n        # if self.str:\n        #     # self.GTFile = osj(self.str_gt_path, self.inPrefix + \"-str.xml\")\n        #     self.GTFile = osj(self.str_gt_path, self.inPrefix + \".xml\")\n        # elif self.reg:\n        #     # self.GTFile = osj(self.reg_gt_path, self.inPrefix + \"-reg.xml\")\n        #     self.GTFile = osj(self.reg_gt_path, self.inPrefix + \".xml\")\n        # else:\n        #     print(\"Not a valid track, please check your spelling.\")\n\n        self.gene_ret_lst()\n\n    @property\n    def result(self):\n        return self.return_result\n\n    def gene_ret_lst(self):\n        ret_lst = []\n        for iou in [0.6, 0.7, 0.8, 0.9]:\n            temp = self.compute_retVal(iou)\n            ret_lst.append(temp)\n            # ret_lst.append(self.compute_retVal(iou))\n\n        ret_lst.append(self.inPrefix + \".xml\")\n        # ret_lst.append(self.inPrefix)\n        # print(\"Done processing {}\\n\".format(self.resultFile))\n        self.return_result = ret_lst\n\n    def compute_retVal(self, iou):\n        gt_dom = xml.dom.minidom.parse(self.GTFile)\n        # incorrect submission format handling\n        try:\n            result_dom = xml.dom.minidom.parse(self.resultFile)\n        except Exception as e:\n            # result_dom = xml.dom.minidom.parse(dummyDom)\n            gt_tables = eval.get_table_list(gt_dom)\n            retVal = ResultStructure(truePos=0, gtTotal=len(gt_tables), resTotal=0)\n            return retVal\n\n        # result_dom = xml.dom.minidom.parse(self.resultFile)\n        if self.reg:\n            ret = self.evaluate_result_reg(gt_dom, result_dom, iou)\n            return ret\n        if self.str:\n            ret = self.evaluate_result_str(gt_dom, result_dom, iou)\n            return ret\n\n    @staticmethod\n    def get_table_list(dom):\n        \"\"\"\n        return a list of Table objects corresponding to the table element of the DOM.\n        \"\"\"\n        return [Table(_nd) for _nd in dom.documentElement.getElementsByTagName(\"table\")]\n\n    @staticmethod\n    def evaluate_result_reg(gt_dom, result_dom, iou_value):\n        # parse the tables in input elements\n        gt_tables = eval.get_table_list(gt_dom)\n        result_tables = eval.get_table_list(result_dom)\n        # duplicate result table list\n        remaining_tables = result_tables.copy()\n\n        # map the tables in gt and result file\n        table_matches = []  # @param: table_matches - list of mapping of tables in gt and res file, in order (gt, res)\n        for gtt in gt_tables:\n            for rest in remaining_tables:\n                if gtt.compute_table_iou(rest) >= iou_value:\n                    remaining_tables.remove(rest)\n                    table_matches.append((gtt, rest))\n                    break\n\n        assert len(table_matches) <= len(gt_tables)\n        assert len(table_matches) <= len(result_tables)\n\n        retVal = ResultStructure(truePos=len(table_matches), gtTotal=len(gt_tables), resTotal=len(result_tables))\n        return retVal\n\n    @staticmethod\n    def evaluate_result_str(gt_dom, result_dom, iou_value, table_iou_value=0.8):\n        # parse the tables in input elements\n        gt_tables = eval.get_table_list(gt_dom)\n        result_tables = eval.get_table_list(result_dom)\n\n        # duplicate result table list\n        remaining_tables = result_tables.copy()\n        gt_remaining = gt_tables.copy()\n\n        # map the tables in gt and result file\n        table_matches = []  # @param: table_matches - list of mapping of tables in gt and res file, in order (gt, res)\n        for gtt in gt_remaining:\n            for rest in remaining_tables:\n                # note: for structural analysis, use 0.8 for table mapping\n                if gtt.compute_table_iou(rest) >= table_iou_value:\n                    table_matches.append((gtt, rest))\n                    remaining_tables.remove(rest)  # unsafe... should be ok with the break below\n                    gt_remaining.remove(gtt)\n                    break\n\n        total_gt_relation, total_res_relation, total_correct_relation = 0, 0, 0\n        for gt_table, ress_table in table_matches:\n\n            # set up the cell mapping for matching tables\n            cell_mapping = gt_table.find_cell_mapping(ress_table, iou_value)\n            # set up the adj relations, convert the one for result table to a dictionary for faster searching\n            gt_AR = gt_table.find_adj_relations()\n            total_gt_relation += len(gt_AR)\n\n            res_AR = ress_table.find_adj_relations()\n            total_res_relation += len(res_AR)\n\n            if False:  # for DEBUG\n                Table.printCellMapping(cell_mapping)\n                Table.printAdjacencyRelationList(gt_AR, \"GT\")\n                Table.printAdjacencyRelationList(res_AR, \"run\")\n\n            # Now map GT adjacency relations to result\n            lMappedAR = []\n            for ar in gt_AR:\n                try:\n                    resFromCell = cell_mapping[ar.fromText]\n                    resToCell = cell_mapping[ar.toText]\n                    # make a mapped adjacency relation\n                    lMappedAR.append(AdjRelation(resFromCell, resToCell, ar.direction))\n                except:\n                    # no mapping is possible\n                    pass\n\n            # compare two list of adjacency relation\n            correct_dect = 0\n            for ar1 in res_AR:\n                for ar2 in lMappedAR:\n                    if ar1.isEqual(ar2):\n                        correct_dect += 1\n                        break\n\n            total_correct_relation += correct_dect\n\n        # handle gt_relations in unmatched gt table\n        for gtt_remain in gt_remaining:\n            total_gt_relation += len(gtt_remain.find_adj_relations())\n\n        # handle gt_relation in unmatched res table\n        for res_remain in remaining_tables:\n            total_res_relation += len(res_remain.find_adj_relations())\n\n        retVal = ResultStructure(truePos=total_correct_relation, gtTotal=total_gt_relation, resTotal=total_res_relation)\n        return retVal\n\n# calculate the gt adj_relations of the missing file\n# @param: file_lst - list of missing ground truth file\n# @param: cur_gt_num - current total of ground truth objects (tables / cells)\ndef process_missing_files(track, gt_file_lst, cur_gt_num):\n    if track in [\"-trackA\", \"-trackA1\", \"-trackA2\"]:\n        gt_file_lst_full = [osj(reg_gt_path, filename) for filename in gt_file_lst]\n        for file in gt_file_lst_full:\n            if os.path.split(file)[-1].split(\".\")[-1] == \"xml\":\n                gt_dom = xml.dom.minidom.parse(file)\n                gt_root = gt_dom.documentElement\n                # tables = []\n                table_elements = gt_root.getElementsByTagName(\"table\")\n                for res_table in table_elements:\n                    # t = Table(res_table)\n                    # tables.append(t)\n                    cur_gt_num += 1\n        return cur_gt_num\n    elif track == \"-trackB1\":\n        gt_file_lst_full = [osj(str_gt_path_1, filename) for filename in gt_file_lst]\n        for file in gt_file_lst_full:\n            if os.path.split(file)[-1].split(\".\")[-1] == \"xml\":\n                gt_dom = xml.dom.minidom.parse(file)\n                gt_root = gt_dom.documentElement\n                tables = []\n                table_elements = gt_root.getElementsByTagName(\"table\")\n                for res_table in table_elements:\n                    t = Table(res_table)\n                    tables.append(t)\n                for table in tables:\n                    cur_gt_num += len(table.find_adj_relations())\n        return cur_gt_num\n    elif track == \"-trackB2\":\n        gt_file_lst_full = [osj(str_gt_path_2, filename) for filename in gt_file_lst]\n        for file in gt_file_lst_full:\n            if os.path.split(file)[-1].split(\".\")[-1] == \"xml\":\n                gt_dom = xml.dom.minidom.parse(file)\n                gt_root = gt_dom.documentElement\n                tables = []\n                table_elements = gt_root.getElementsByTagName(\"table\")\n                for res_table in table_elements:\n                    t = Table(res_table)\n                    tables.append(t)\n                for table in tables:\n                    cur_gt_num += len(table.find_adj_relations())\n        return cur_gt_num\n\ndef calc(F1):\n    sum_a = 0.6 * F1[0] + 0.7 * F1[1] + 0.8 * F1[2] + 0.9 * F1[3]\n    sum_b = 0.6 + 0.7 + 0.8 + 0.9\n\n    return sum_a / sum_b\n\ndef calc_table_score(result_path):\n    # measure = eval(*sys.argv[1:])\n\n    gt_file_lst = os.listdir(reg_gt_path_archival)\n    track = \"-trackA1\"\n    untar_path = result_path\n\n    res_lst = []\n    for root, files, dirs in os.walk(untar_path):\n        for name in dirs:\n            if name.split(\".\")[-1] == \"xml\":\n                cur_filepath = osj(os.path.abspath(root), name)\n                res_lst.append(eval(track, cur_filepath))\n                # printing for debug\n                # print(\"Processing... {}\".format(name))\n    # print(\"DONE WITH FILE PROCESSING\\n\")\n    # note: results are stored as list of each when iou at [0.6, 0.7, 0.8, 0.9, gt_filename]\n    # gt number should be the same for all files\n    gt_num = 0\n    correct_six, res_six = 0, 0\n    correct_seven, res_seven = 0, 0\n    correct_eight, res_eight = 0, 0\n    correct_nine, res_nine = 0, 0\n\n\n    for each_file in res_lst:\n        # print(each_file)\n        try:\n            gt_file_lst.remove(each_file.result[-1])\n            if each_file.result[-1].replace('.xml', '.jpg') in gt_file_lst:\n                gt_file_lst.remove(each_file.result[-1].replace('.xml', '.jpg'))\n            correct_six += each_file.result[0].truePos\n            gt_num += each_file.result[0].gtTotal\n            res_six += each_file.result[0].resTotal\n            # print(\"{} {} {}\".format(each_file.result[0].truePos, each_file.result[0].gtTotal, each_file.result[0].resTotal))\n\n            correct_seven += each_file.result[1].truePos\n            res_seven += each_file.result[1].resTotal\n\n            correct_eight += each_file.result[2].truePos\n            res_eight += each_file.result[2].resTotal\n\n            correct_nine += each_file.result[3].truePos\n            res_nine += each_file.result[3].resTotal\n        except:\n            print(\"Error occur in processing result list.\")\n            print(each_file.result[-1])\n            break\n            # print(each_file.result[-1])\n            # print(each_file)\n\n    # for file in gt_file_lst:\n    #     if file.split(\".\") != \"xml\":\n    #         gt_file_lst.remove(file)\n    #     # print(gt_file_lst)\n\n    for i in range(len(gt_file_lst) - 1, -1, -1):\n        if gt_file_lst[i].split(\".\")[-1] != \"xml\":\n            del gt_file_lst[i]\n\n    if len(gt_file_lst) > 0:\n        print(\"\\nWarning: missing result annotations for file: {}\\n\".format(gt_file_lst))\n        gt_total = process_missing_files(track, gt_file_lst, gt_num)\n    else:\n        gt_total = gt_num\n\n\n    try:\n        # print(\"Evaluation of {}\".format(track.replace(\"-\", \"\")))\n        # iou @ 0.6\n        p_six = correct_six / res_six\n        r_six = correct_six / gt_total\n        f1_six = 2 * p_six * r_six / (p_six + r_six)\n        print(\"IOU @ 0.6 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_six, r_six, f1_six))\n        print(\"correct: {}, gt: {}, res: {}\\n\".format(correct_six, gt_total, res_six))\n\n        # iou @ 0.7\n        p_seven = correct_seven / res_seven\n        r_seven = correct_seven / gt_total\n        f1_seven = 2 * p_seven * r_seven / (p_seven + r_seven)\n        print(\"IOU @ 0.7 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_seven, r_seven, f1_seven))\n        print(\"correct: {}, gt: {}, res: {}\\n\".format(correct_seven, gt_total, res_seven))\n\n        # iou @ 0.8\n        p_eight = correct_eight / res_eight\n        r_eight = correct_eight / gt_total\n        f1_eight = 2 * p_eight * r_eight / (p_eight + r_eight)\n        print(\"IOU @ 0.8 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_eight, r_eight, f1_eight))\n        print(\"correct: {}, gt: {}, res: {}\\n\".format(correct_eight, gt_total, res_eight))\n\n        # iou @ 0.9\n        p_nine = correct_nine / res_nine\n        r_nine = correct_nine / gt_total\n        f1_nine = 2 * p_nine * r_nine / (p_nine + r_nine)\n        print(\"IOU @ 0.9 -\\nprecision: {}\\nrecall: {}\\nf1: {}\".format(p_nine, r_nine, f1_nine))\n        print(\"correct: {}, gt: {}, res: {}\".format(correct_nine, gt_total, res_nine))\n\n        F1 = [f1_six, f1_seven, f1_eight, f1_nine]\n        wF1 = calc(F1)\n\n        print(\"Average weight F1: {}\".format(wF1))\n\n        return {\n            'p_six':p_six * 100,\n            \"r_six\":r_six * 100,\n            \"f1_six\":f1_six * 100,\n            \"p_seven\":p_seven * 100,\n            \"r_seven\":r_seven * 100,\n            \"f1_seven\":f1_seven * 100,\n            \"p_eight\":p_eight * 100,\n            \"r_eight\":r_eight * 100,\n            \"f1_eight\":f1_eight * 100,\n            \"p_nine\":p_nine * 100,\n            \"r_nine\":r_nine * 100,\n            \"f1_nine\":f1_nine * 100,\n            \"wF1\":wF1 * 100\n        }\n    except ZeroDivisionError:\n        print(\n            \"Error: zero devision error found, (possible that no adjacency relations are found), please check the file input.\")\n        return {\"wF1\": 0}\n\n\nif __name__==\"__main__\":\n    pass\n"}
{"type": "source_file", "path": "modules/translate/base.py", "content": "from abc import ABC, abstractmethod\n\nclass TranslateBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n    @abstractmethod\n    def get_languages(self):\n        pass\n\n    def translate_all(self, layout, from_lang, to_lang):\n        for line in layout:\n            if line.text:\n                line.translated_text = self.translate(line.text, from_lang, to_lang)\n\n        return layout\n\n    @abstractmethod\n    def translate(self, text: str) -> str:\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTTrainer.py", "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates.\n\n\"\"\"\nThis file contains components with some default boilerplate logic user may need\nin training / testing. They will not work for everyone, but many users may find them useful.\n\nThe behavior of functions/classes in this file is subject to change,\nsince they are meant to represent the \"common default behavior\" people need in their projects.\n\"\"\"\n\nimport argparse\nimport logging\nimport os\nimport sys\nimport weakref\nfrom collections import OrderedDict\nfrom typing import Optional\nimport torch\nfrom fvcore.nn.precise_bn import get_bn_modules\nfrom omegaconf import OmegaConf\nfrom torch.nn.parallel import DistributedDataParallel\nimport numpy as np\n\nimport detectron2.data.transforms as T\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.config import CfgNode, LazyConfig\nfrom detectron2.data import (\n    MetadataCatalog,\n    build_detection_test_loader,\n    build_detection_train_loader,\n)\nfrom detectron2.evaluation import (\n    DatasetEvaluator,\n    inference_on_dataset,\n    print_csv_format,\n    verify_results,\n)\nfrom detectron2.modeling import build_model\nfrom detectron2.solver import build_lr_scheduler, build_optimizer\nfrom detectron2.utils import comm\nfrom detectron2.utils.collect_env import collect_env_info\nfrom detectron2.utils.env import seed_all_rng\nfrom detectron2.utils.events import CommonMetricPrinter, JSONWriter, TensorboardXWriter\nfrom detectron2.utils.file_io import PathManager\nfrom detectron2.utils.logger import setup_logger\n\nfrom detectron2.engine import hooks\nfrom detectron2.engine.train_loop import AMPTrainer, SimpleTrainer, TrainerBase\n\nfrom .VGTcheckpointer import MyDetectionCheckpointer\nfrom typing import Any, Dict, List, Set\nimport itertools\nfrom detectron2.solver.build import maybe_add_gradient_clipping\nfrom .dataset_mapper import DetrDatasetMapper\nfrom detectron2.evaluation import COCOEvaluator\n\nimport pickle\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.structures import (\n    BitMasks,\n    Boxes,\n    BoxMode,\n    Instances,\n    Keypoints,\n    PolygonMasks,\n    RotatedBoxes,\n    polygons_to_bitmask,\n)\n\n__all__ = [\n    \"create_ddp_model\",\n    \"default_argument_parser\",\n    \"default_setup\",\n    \"default_writers\",\n    \"DefaultPredictor\",\n    \"GridTextTrainer\",\n]\n\ndef torch_memory(device, tag=\"\"):\n    # Checks and prints GPU memory\n    print(tag, f'{torch.cuda.memory_allocated(device)/1024/1024:.2f} MB USED')\n    print(tag, f'{torch.cuda.memory_reserved(device)/1024/1024:.2f} MB RESERVED')\n    print(tag, f'{torch.cuda.max_memory_allocated(device)/1024/1024:.2f} MB USED MAX')\n    print(tag, f'{torch.cuda.max_memory_reserved(device)/1024/1024:.2f} MB RESERVED MAX')\n    print('')\n\ndef create_ddp_model(model, *, fp16_compression=False, **kwargs):\n    \"\"\"\n    Create a DistributedDataParallel model if there are >1 processes.\n\n    Args:\n        model: a torch.nn.Module\n        fp16_compression: add fp16 compression hooks to the ddp object.\n            See more at https://pytorch.org/docs/stable/ddp_comm_hooks.html#torch.distributed.algorithms.ddp_comm_hooks.default_hooks.fp16_compress_hook\n        kwargs: other arguments of :module:`torch.nn.parallel.DistributedDataParallel`.\n    \"\"\"  # noqa\n    if comm.get_world_size() == 1:\n        return model\n    if \"device_ids\" not in kwargs:\n        kwargs[\"device_ids\"] = [comm.get_local_rank()]\n    ddp = DistributedDataParallel(model, **kwargs)\n    if fp16_compression:\n        from torch.distributed.algorithms.ddp_comm_hooks import default as comm_hooks\n\n        ddp.register_comm_hook(state=None, hook=comm_hooks.fp16_compress_hook)\n    return ddp\n\n\ndef default_argument_parser(epilog=None):\n    \"\"\"\n    Create a parser with some common arguments used by detectron2 users.\n\n    Args:\n        epilog (str): epilog passed to ArgumentParser describing the usage.\n\n    Returns:\n        argparse.ArgumentParser:\n    \"\"\"\n    parser = argparse.ArgumentParser(\n        epilog=epilog\n        or f\"\"\"\nExamples:\n\nRun on single machine:\n    $ {sys.argv[0]} --num-gpus 8 --config-file cfg.yaml\n\nChange some config options:\n    $ {sys.argv[0]} --config-file cfg.yaml MODEL.WEIGHTS /path/to/weight.pth SOLVER.BASE_LR 0.001\n\nRun on multiple machines:\n    (machine0)$ {sys.argv[0]} --machine-rank 0 --num-machines 2 --dist-url <URL> [--other-flags]\n    (machine1)$ {sys.argv[0]} --machine-rank 1 --num-machines 2 --dist-url <URL> [--other-flags]\n\"\"\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n    )\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--resume\",\n        action=\"store_true\",\n        help=\"Whether to attempt to resume from the checkpoint directory. \"\n        \"See documentation of `MyTrainer.resume_or_load()` for what it means.\",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")\n    parser.add_argument(\"--num-gpus\", type=int, default=1, help=\"number of gpus *per machine*\")\n    parser.add_argument(\"--num-machines\", type=int, default=1, help=\"total number of machines\")\n    parser.add_argument(\n        \"--machine-rank\", type=int, default=0, help=\"the rank of this machine (unique per machine)\"\n    )\n\n    # PyTorch still may leave orphan processes in multi-gpu training.\n    # Therefore we use a deterministic way to obtain port,\n    # so that users are aware of orphan processes by seeing the port occupied.\n    port = 2 ** 15 + 2 ** 14 + hash(os.getuid() if sys.platform != \"win32\" else 1) % 2 ** 14\n    parser.add_argument(\n        \"--dist-url\",\n        default=\"tcp://127.0.0.1:{}\".format(port),\n        help=\"initialization URL for pytorch distributed backend. See \"\n        \"https://pytorch.org/docs/stable/distributed.html for details.\",\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"\"\"\nModify config options at the end of the command. For Yacs configs, use\nspace-separated \"PATH.KEY VALUE\" pairs.\nFor python-based LazyConfig, use \"path.key=value\".\n        \"\"\".strip(),\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    return parser\n\n\ndef _try_get_key(cfg, *keys, default=None):\n    \"\"\"\n    Try select keys from cfg until the first key that exists. Otherwise return default.\n    \"\"\"\n    if isinstance(cfg, CfgNode):\n        cfg = OmegaConf.create(cfg.dump())\n    for k in keys:\n        none = object()\n        p = OmegaConf.select(cfg, k, default=none)\n        if p is not none:\n            return p\n    return default\n\n\ndef _highlight(code, filename):\n    try:\n        import pygments\n    except ImportError:\n        return code\n\n    from pygments.lexers import Python3Lexer, YamlLexer\n    from pygments.formatters import Terminal256Formatter\n\n    lexer = Python3Lexer() if filename.endswith(\".py\") else YamlLexer()\n    code = pygments.highlight(code, lexer, Terminal256Formatter(style=\"monokai\"))\n    return code\n\n\ndef default_setup(cfg, args):\n    \"\"\"\n    Perform some basic common setups at the beginning of a job, including:\n\n    1. Set up the detectron2 logger\n    2. Log basic information about environment, cmdline arguments, and config\n    3. Backup the config to the output directory\n\n    Args:\n        cfg (CfgNode or omegaconf.DictConfig): the full config to be used\n        args (argparse.NameSpace): the command line arguments to be logged\n    \"\"\"\n    output_dir = _try_get_key(cfg, \"OUTPUT_DIR\", \"output_dir\", \"train.output_dir\")\n    if comm.is_main_process() and output_dir:\n        PathManager.mkdirs(output_dir)\n\n    rank = comm.get_rank()\n    setup_logger(output_dir, distributed_rank=rank, name=\"fvcore\")\n    logger = setup_logger(output_dir, distributed_rank=rank)\n\n    logger.info(\"Rank of current process: {}. World size: {}\".format(rank, comm.get_world_size()))\n    logger.info(\"Environment info:\\n\" + collect_env_info())\n\n    logger.info(\"Command line arguments: \" + str(args))\n    if hasattr(args, \"config_file\") and args.config_file != \"\":\n        logger.info(\n            \"Contents of args.config_file={}:\\n{}\".format(\n                args.config_file,\n                _highlight(PathManager.open(args.config_file, \"r\").read(), args.config_file),\n            )\n        )\n\n    if comm.is_main_process() and output_dir:\n        # Note: some of our scripts may expect the existence of\n        # config.yaml in output directory\n        path = os.path.join(output_dir, \"config.yaml\")\n        if isinstance(cfg, CfgNode):\n            logger.info(\"Running with full config:\\n{}\".format(_highlight(cfg.dump(), \".yaml\")))\n            with PathManager.open(path, \"w\") as f:\n                f.write(cfg.dump())\n        else:\n            LazyConfig.save(cfg, path)\n        logger.info(\"Full config saved to {}\".format(path))\n\n    # make sure each worker has a different, yet deterministic seed if specified\n    seed = _try_get_key(cfg, \"SEED\", \"train.seed\", default=-1)\n    seed_all_rng(None if seed < 0 else seed + rank)\n\n    # cudnn benchmark has large overhead. It shouldn't be used considering the small size of\n    # typical validation set.\n    if not (hasattr(args, \"eval_only\") and args.eval_only):\n        torch.backends.cudnn.benchmark = _try_get_key(\n            cfg, \"CUDNN_BENCHMARK\", \"train.cudnn_benchmark\", default=False\n        )\n\n\ndef default_writers(output_dir: str, max_iter: Optional[int] = None):\n    \"\"\"\n    Build a list of :class:`EventWriter` to be used.\n    It now consists of a :class:`CommonMetricPrinter`,\n    :class:`TensorboardXWriter` and :class:`JSONWriter`.\n\n    Args:\n        output_dir: directory to store JSON metrics and tensorboard events\n        max_iter: the total number of iterations\n\n    Returns:\n        list[EventWriter]: a list of :class:`EventWriter` objects.\n    \"\"\"\n    PathManager.mkdirs(output_dir)\n    return [\n        # It may not always print what you want to see, since it prints \"common\" metrics only.\n        CommonMetricPrinter(max_iter),\n        JSONWriter(os.path.join(output_dir, \"metrics.json\")),\n        TensorboardXWriter(output_dir),\n    ]\n\n\nclass DefaultPredictor:\n    \"\"\"\n    Create a simple end-to-end predictor with the given config that runs on\n    single device for a single input image.\n\n    Compared to using the model directly, this class does the following additions:\n\n    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.\n    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.\n    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.\n    4. Take one input image and produce a single output, instead of a batch.\n\n    This is meant for simple demo purposes, so it does the above steps automatically.\n    This is not meant for benchmarks or running complicated inference logic.\n    If you'd like to do anything more complicated, please refer to its source code as\n    examples to build and use the model manually.\n\n    Attributes:\n        metadata (Metadata): the metadata of the underlying dataset, obtained from\n            cfg.DATASETS.TEST.\n\n    Examples:\n    ::\n        pred = DefaultPredictor(cfg)\n        inputs = cv2.imread(\"input.jpg\")\n        outputs = pred(inputs)\n    \"\"\"\n    \n    def __init__(self, cfg):\n        self.cfg = cfg.clone()  # cfg can be modified by model\n        self.model = build_model(self.cfg)\n        self.model.eval()\n        if len(cfg.DATASETS.TEST):\n            self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n\n        checkpointer = DetectionCheckpointer(self.model)\n        checkpointer.load(cfg.MODEL.WEIGHTS)\n\n        self.aug = T.ResizeShortestEdge(\n            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n        )\n\n        self.input_format = cfg.INPUT.FORMAT\n        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n\n    def __call__(self, original_image, grid_path):\n        \"\"\"\n        Args:\n            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n\n        Returns:\n            predictions (dict):\n                the output of the model for one image only.\n                See :doc:`/tutorials/models` for details about the format.\n        \"\"\"\n        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n            # Apply pre-processing to image.\n            \n            # if self.input_format == \"RGB\":\n            #     # whether the model expects BGR inputs or RGB\n            #     import ipdb;ipdb.set_trace() \n            #     original_image = original_image[:, :, ::-1]\n            \n            height, width = original_image.shape[:2]\n            image, transforms = T.apply_transform_gens([self.aug], original_image)\n            \n            # add grid    \n            image_shape = image.shape[:2]  # h, w\n            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n            \n            with open(grid_path, \"rb\") as f:\n                sample_inputs = pickle.load(f)\n            input_ids = sample_inputs[\"input_ids\"]\n            bbox_subword_list = sample_inputs[\"bbox_subword_list\"]\n                \n            # word bbox\n            bbox = []\n            for bbox_per_subword in bbox_subword_list:\n                text_word = {}\n                text_word['bbox'] = bbox_per_subword.tolist()\n                text_word['bbox_mode'] = BoxMode.XYWH_ABS\n                utils.transform_instance_annotations(text_word, transforms, image_shape)\n                bbox.append(text_word['bbox'])\n                \n            dataset_dict = {}\n            dataset_dict[\"input_ids\"] = input_ids \n            dataset_dict[\"bbox\"] = bbox\n            dataset_dict[\"image\"] = image \n            dataset_dict[\"height\"] = height \n            dataset_dict[\"width\"] = width \n\n            predictions = self.model([dataset_dict])[0]\n            return predictions\n        \n\nclass VGTTrainer(TrainerBase):\n    \"\"\"\n    A trainer with default training logic. It does the following:\n\n    1. Create a :class:`SimpleTrainer` using model, optimizer, dataloader\n       defined by the given config. Create a LR scheduler defined by the config.\n    2. Load the last checkpoint or `cfg.MODEL.WEIGHTS`, if exists, when\n       `resume_or_load` is called.\n    3. Register a few common hooks defined by the config.\n\n    It is created to simplify the **standard model training workflow** and reduce code boilerplate\n    for users who only need the standard training workflow, with standard features.\n    It means this class makes *many assumptions* about your training logic that\n    may easily become invalid in a new research. In fact, any assumptions beyond those made in the\n    :class:`SimpleTrainer` are too much for research.\n\n    The code of this class has been annotated about restrictive assumptions it makes.\n    When they do not work for you, you're encouraged to:\n\n    1. Overwrite methods of this class, OR:\n    2. Use :class:`SimpleTrainer`, which only does minimal SGD training and\n       nothing else. You can then add your own hooks if needed. OR:\n    3. Write your own training loop similar to `tools/plain_train_net.py`.\n\n    See the :doc:`/tutorials/training` tutorials for more details.\n\n    Note that the behavior of this class, like other functions/classes in\n    this file, is not stable, since it is meant to represent the \"common default behavior\".\n    It is only guaranteed to work well with the standard models and training workflow in detectron2.\n    To obtain more stable behavior, write your own training logic with other public APIs.\n\n    Examples:\n    ::\n        trainer = MyTrainer(cfg)\n        trainer.resume_or_load()  # load last checkpoint or MODEL.WEIGHTS\n        trainer.train()\n\n    Attributes:\n        scheduler:\n        checkpointer (DetectionCheckpointer):\n        cfg (CfgNode):\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Args:\n            cfg (CfgNode):\n        \"\"\"\n        super().__init__()\n        logger = logging.getLogger(\"detectron2\")\n        if not logger.isEnabledFor(logging.INFO):  # setup_logger is not called for d2\n            setup_logger()\n        cfg = VGTTrainer.auto_scale_workers(cfg, comm.get_world_size())\n\n        self.cfg = cfg\n\n        # Assume these objects must be constructed in this order.\n        model = self.build_model(cfg)\n        optimizer = self.build_optimizer(cfg, model)\n        data_loader = self.build_train_loader(cfg)\n\n        model = create_ddp_model(model, broadcast_buffers=False)\n        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n            model, data_loader, optimizer\n        )\n\n        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n        self.checkpointer = MyDetectionCheckpointer(\n            # Assume you want to save checkpoints together with logs/statistics\n            model,\n            cfg.OUTPUT_DIR,\n            trainer=weakref.proxy(self),\n        )\n        self.start_iter = 0\n        self.max_iter = cfg.SOLVER.MAX_ITER\n        self.cfg = cfg\n\n        self.register_hooks(self.build_hooks())\n\n    def resume_or_load(self, resume=True):\n        \"\"\"\n        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n        a `last_checkpoint` file), resume from the file. Resuming means loading all\n        available states (eg. optimizer and scheduler) and update iteration counter\n        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n\n        Otherwise, this is considered as an independent training. The method will load model\n        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n        from iteration 0.\n\n        Args:\n            resume (bool): whether to do resume or not\n        \"\"\"\n        self.checkpointer.resume_or_load(self.cfg.MODEL.WEIGHTS, resume=resume)\n        if resume and self.checkpointer.has_checkpoint():\n            # The checkpoint stores the training iteration that just finished, thus we start\n            # at the next iteration\n            self.start_iter = self.iter + 1\n\n    def build_hooks(self):\n        \"\"\"\n        Build a list of default hooks, including timing, evaluation,\n        checkpointing, lr scheduling, precise BN, writing events.\n\n        Returns:\n            list[HookBase]:\n        \"\"\"\n        cfg = self.cfg.clone()\n        cfg.defrost()\n        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n\n        ret = [\n            hooks.IterationTimer(),\n            hooks.LRScheduler(),\n            hooks.PreciseBN(\n                # Run at the same freq as (but before) evaluation.\n                cfg.TEST.EVAL_PERIOD,\n                self.model,\n                # Build a new data loader to not affect training\n                self.build_train_loader(cfg),\n                cfg.TEST.PRECISE_BN.NUM_ITER,\n            )\n            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n            else None,\n        ]\n\n        # Do PreciseBN before checkpointer, because it updates the model and need to\n        # be saved by checkpointer.\n        # This is not always the best: if checkpointing has a different frequency,\n        # some checkpoints may have more precise statistics than others.\n        if comm.is_main_process():\n            ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))\n\n        def test_and_save_results():\n            self._last_eval_results = self.test(self.cfg, self.model)\n            return self._last_eval_results\n\n        # Do evaluation after checkpointer, because then if it fails,\n        # we can use the saved checkpoint to debug.\n        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))\n\n        if comm.is_main_process():\n            # Here the default print/log frequency of each writer is used.\n            # run writers in the end, so that evaluation metrics are written\n            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n        return ret\n\n    def build_writers(self):\n        \"\"\"\n        Build a list of writers to be used using :func:`default_writers()`.\n        If you'd like a different list of writers, you can overwrite it in\n        your trainer.\n\n        Returns:\n            list[EventWriter]: a list of :class:`EventWriter` objects.\n        \"\"\"\n        return default_writers(self.cfg.OUTPUT_DIR, self.max_iter)\n\n    def train(self):\n        \"\"\"\n        Run training.\n\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        \"\"\"\n        super().train(self.start_iter, self.max_iter)\n        if len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\n            assert hasattr(\n                self, \"_last_eval_results\"\n            ), \"No evaluation results obtained during training!\"\n            verify_results(self.cfg, self._last_eval_results)\n            return self._last_eval_results\n\n    def run_step(self):\n        try:\n            self._trainer.iter = self.iter\n            self._trainer.run_step()\n        except RuntimeError as exception:\n            if \"out of memory\" in str(exception):\n                logger = logging.getLogger(\"detectron2\")\n                logger.warn(\"Out of memory\")\n                # import ipdb;ipdb.set_trace()\n                if hasattr(torch.cuda, 'empty_cache'):\n                    torch.cuda.empty_cache()\n            else:\n                raise exception\n\n    @classmethod\n    def build_model(cls, cfg):\n        \"\"\"\n        Returns:\n            torch.nn.Module:\n\n        It now calls :func:`detectron2.modeling.build_model`.\n        Overwrite it if you'd like a different model.\n        \"\"\"\n        model = build_model(cfg)\n        \n        def compute_para(model):\n            params_num = []\n            filtered_parameters = []\n            for p in filter(lambda p: p.requires_grad, model.parameters()):\n                filtered_parameters.append(p)\n                params_num.append(np.prod(p.size()))\n            total_params = int(sum(params_num))\n            total_params = f'Trainable network params num : {total_params:,}'\n            # print(total_params)\n            return total_params\n        \n        logger = logging.getLogger(\"detectron2\")\n        logger.info(\"Model: {}\".format(compute_para(model)))\n        return model\n\n    @classmethod\n    def build_optimizer(cls, cfg, model):\n        params: List[Dict[str, Any]] = []\n        memo: Set[torch.nn.parameter.Parameter] = set()\n        for key, value in model.named_parameters(recurse=True):\n            if not value.requires_grad:\n                continue\n            # Avoid duplicating parameters\n            if value in memo:\n                continue\n            memo.add(value)\n            lr = cfg.SOLVER.BASE_LR\n            weight_decay = cfg.SOLVER.WEIGHT_DECAY\n            if \"backbone\" in key:\n                lr = lr * cfg.SOLVER.BACKBONE_MULTIPLIER\n            params += [{\"params\": [value], \"lr\": lr, \"weight_decay\": weight_decay}]\n\n        def maybe_add_full_model_gradient_clipping(optim):  # optim: the optimizer class\n            # detectron2 doesn't have full model gradient clipping now\n            clip_norm_val = cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE\n            enable = (\n                    cfg.SOLVER.CLIP_GRADIENTS.ENABLED\n                    and cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\"\n                    and clip_norm_val > 0.0\n            )\n\n            class FullModelGradientClippingOptimizer(optim):\n                def step(self, closure=None):\n                    all_params = itertools.chain(*[x[\"params\"] for x in self.param_groups])\n                    torch.nn.utils.clip_grad_norm_(all_params, clip_norm_val)\n                    super().step(closure=closure)\n\n            return FullModelGradientClippingOptimizer if enable else optim\n\n        optimizer_type = cfg.SOLVER.OPTIMIZER\n        if optimizer_type == \"SGD\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.SGD)(\n                params, cfg.SOLVER.BASE_LR, momentum=cfg.SOLVER.MOMENTUM\n            )\n        elif optimizer_type == \"ADAMW\":\n            optimizer = maybe_add_full_model_gradient_clipping(torch.optim.AdamW)(\n                params, cfg.SOLVER.BASE_LR\n            )\n        else:\n            raise NotImplementedError(f\"no optimizer type {optimizer_type}\")\n        if not cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE == \"full_model\":\n            optimizer = maybe_add_gradient_clipping(cfg, optimizer)\n        return optimizer\n\n    @classmethod\n    def build_lr_scheduler(cls, cfg, optimizer):\n        \"\"\"\n        It now calls :func:`detectron2.solver.build_lr_scheduler`.\n        Overwrite it if you'd like a different scheduler.\n        \"\"\"\n        return build_lr_scheduler(cfg, optimizer)\n\n    @classmethod\n    def build_train_loader(cls, cfg):\n        if cfg.AUG.DETR:\n            mapper = DetrDatasetMapper(cfg, is_train=True)\n        else:\n            mapper = None\n        return build_detection_train_loader(cfg, mapper=mapper)\n\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \"\"\"\n        Returns:\n            iterable\n\n        It now calls :func:`detectron2.data.build_detection_test_loader`.\n        Overwrite it if you'd like a different data loader.\n        \"\"\"\n        mapper = DetrDatasetMapper(cfg, is_train=False)\n        return build_detection_test_loader(cfg, dataset_name, mapper=mapper)\n    \n        # return build_detection_test_loader(cfg, dataset_name)\n\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        if output_folder is None:\n            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n            return COCOEvaluator(dataset_name, output_dir=output_folder)\n        \n        # if 'icdar' not in dataset_name:\n        #     return COCOEvaluator(dataset_name, output_dir=output_folder)\n        # else:\n        #     return ICDAREvaluator(dataset_name, output_dir=output_folder)\n\n    @classmethod\n    def test(cls, cfg, model, evaluators=None):\n        \"\"\"\n        Evaluate the given model. The given model is expected to already contain\n        weights to evaluate.\n\n        Args:\n            cfg (CfgNode):\n            model (nn.Module):\n            evaluators (list[DatasetEvaluator] or None): if None, will call\n                :meth:`build_evaluator`. Otherwise, must have the same length as\n                ``cfg.DATASETS.TEST``.\n\n        Returns:\n            dict: a dict of result metrics\n        \"\"\"\n        logger = logging.getLogger(__name__)\n        if isinstance(evaluators, DatasetEvaluator):\n            evaluators = [evaluators]\n        if evaluators is not None:\n            assert len(cfg.DATASETS.TEST) == len(evaluators), \"{} != {}\".format(\n                len(cfg.DATASETS.TEST), len(evaluators)\n            )\n\n        results = OrderedDict()\n        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n            data_loader = cls.build_test_loader(cfg, dataset_name)\n            # When evaluators are passed in as arguments,\n            # implicitly assume that evaluators can be created before data_loader.\n            if evaluators is not None:\n                evaluator = evaluators[idx]\n            else:\n                try:\n                    evaluator = cls.build_evaluator(cfg, dataset_name)\n                except NotImplementedError:\n                    logger.warn(\n                        \"No evaluator found. Use `MyTrainer.test(evaluators=)`, \"\n                        \"or implement its `build_evaluator` method.\"\n                    )\n                    results[dataset_name] = {}\n                    continue\n            results_i = inference_on_dataset(model, data_loader, evaluator)\n            results[dataset_name] = results_i\n            if comm.is_main_process():\n                assert isinstance(\n                    results_i, dict\n                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n                    results_i\n                )\n                logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n                print_csv_format(results_i)\n\n        if len(results) == 1:\n            results = list(results.values())[0]\n        return results\n\n    @staticmethod\n    def auto_scale_workers(cfg, num_workers: int):\n        \"\"\"\n        When the config is defined for certain number of workers (according to\n        ``cfg.SOLVER.REFERENCE_WORLD_SIZE``) that's different from the number of\n        workers currently in use, returns a new cfg where the total batch size\n        is scaled so that the per-GPU batch size stays the same as the\n        original ``IMS_PER_BATCH // REFERENCE_WORLD_SIZE``.\n\n        Other config options are also scaled accordingly:\n        * training steps and warmup steps are scaled inverse proportionally.\n        * learning rate are scaled proportionally, following :paper:`ImageNet in 1h`.\n\n        For example, with the original config like the following:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 16\n            BASE_LR: 0.1\n            REFERENCE_WORLD_SIZE: 8\n            MAX_ITER: 5000\n            STEPS: (4000,)\n            CHECKPOINT_PERIOD: 1000\n\n        When this config is used on 16 GPUs instead of the reference number 8,\n        calling this method will return a new config with:\n\n        .. code-block:: yaml\n\n            IMS_PER_BATCH: 32\n            BASE_LR: 0.2\n            REFERENCE_WORLD_SIZE: 16\n            MAX_ITER: 2500\n            STEPS: (2000,)\n            CHECKPOINT_PERIOD: 500\n\n        Note that both the original config and this new config can be trained on 16 GPUs.\n        It's up to user whether to enable this feature (by setting ``REFERENCE_WORLD_SIZE``).\n\n        Returns:\n            CfgNode: a new config. Same as original if ``cfg.SOLVER.REFERENCE_WORLD_SIZE==0``.\n        \"\"\"\n        old_world_size = cfg.SOLVER.REFERENCE_WORLD_SIZE\n        if old_world_size == 0 or old_world_size == num_workers:\n            return cfg\n        cfg = cfg.clone()\n        frozen = cfg.is_frozen()\n        cfg.defrost()\n\n        assert (\n            cfg.SOLVER.IMS_PER_BATCH % old_world_size == 0\n        ), \"Invalid REFERENCE_WORLD_SIZE in config!\"\n        scale = num_workers / old_world_size\n        bs = cfg.SOLVER.IMS_PER_BATCH = int(round(cfg.SOLVER.IMS_PER_BATCH * scale))\n        lr = cfg.SOLVER.BASE_LR = cfg.SOLVER.BASE_LR * scale\n        max_iter = cfg.SOLVER.MAX_ITER = int(round(cfg.SOLVER.MAX_ITER / scale))\n        warmup_iter = cfg.SOLVER.WARMUP_ITERS = int(round(cfg.SOLVER.WARMUP_ITERS / scale))\n        cfg.SOLVER.STEPS = tuple(int(round(s / scale)) for s in cfg.SOLVER.STEPS)\n        cfg.TEST.EVAL_PERIOD = int(round(cfg.TEST.EVAL_PERIOD / scale))\n        cfg.SOLVER.CHECKPOINT_PERIOD = int(round(cfg.SOLVER.CHECKPOINT_PERIOD / scale))\n        cfg.SOLVER.REFERENCE_WORLD_SIZE = num_workers  # maintain invariant\n        logger = logging.getLogger(__name__)\n        logger.info(\n            f\"Auto-scaling the config to batch_size={bs}, learning_rate={lr}, \"\n            f\"max_iter={max_iter}, warmup={warmup_iter}.\"\n        )\n\n        if frozen:\n            cfg.freeze()\n        return cfg\n\n\n# Access basic attributes from the underlying trainer\nfor _attr in [\"model\", \"data_loader\", \"optimizer\"]:\n    setattr(\n        VGTTrainer,\n        _attr,\n        property(\n            # getter\n            lambda self, x=_attr: getattr(self._trainer, x),\n            # setter\n            lambda self, value, x=_attr: setattr(self._trainer, x, value),\n        ),\n    )\n"}
{"type": "source_file", "path": "utils/__init__.py", "content": "import os\nfrom .textwrap_local import fw_fill, fw_wrap\nfrom .ocr_model import OCRModel\nfrom .layout_model import LayoutAnalyzer\nfrom .gui import create_gradio_app\n\nimport yaml\n\n__all__ = [\"fw_fill\", \"fw_wrap\", \"OCRModel\", \"LayoutAnalyzer\"]\n\ndef load_config(base_config_path, override_config_path):\n    with open(base_config_path, 'r') as base_file:\n        base_config = yaml.safe_load(base_file)\n    \n    final_config = base_config\n\n    if os.path.exists(override_config_path):\n        with open(override_config_path, 'r') as override_file:\n            override_config = yaml.safe_load(override_file)\n            final_config = update(base_config, override_config)\n    \n    # Update the base config with the override config\n    # This recursively updates nested dictionaries\n    def update(d, u):\n        for k, v in u.items():\n            if isinstance(v, dict):\n                d[k] = update(d.get(k, {}), v)\n            else:\n                d[k] = v\n        return d\n\n    return final_config\n\n\ndef draw_text(draw, processed_text, current_fnt, font_size, width, ygain):\n    y = 0\n    first = len(processed_text.split(\"\\n\")) > 1\n    for l in processed_text.split(\"\\n\"):\n        words = l.split(\" \")\n        words_length = sum(draw.textlength(w, font=current_fnt) for w in words)\n        if first: words_length += 40\n        space_length = (width - words_length) / (len(words))\n        if (space_length > 40): space_length = font_size/2.4\n        x = 0\n        if first: x+= 40\n        for word in words:\n            draw.text((x, y), word, font=current_fnt, fill=\"black\")\n            x += draw.textlength(word, font=current_fnt) + space_length\n        y += ygain\n        first = False"}
{"type": "source_file", "path": "modules/font/simple.py", "content": "import re\nimport numpy as np\nfrom tqdm import tqdm\nfrom .base import FontBase \n\nclass SimpleFont(FontBase):\n    FONT_SIZE = 29\n\n    def init(self, cfg: dict):\n        self.cfg = cfg\n \n    def get_all_fonts(self, layout):\n\n        for i, line in tqdm(enumerate(layout)):\n            if line.type in [\"text\", \"list\", \"title\"]:\n                # update this so image is created from images and layout bbox info\n                image = line.image\n                family, size, ygain = self.get_font_info(image, line.line_cnt)\n                line.font = { \"family\": family, \"size\": size, \"ygain\": ygain }\n\n\n        return layout\n    \n    def get_font_info(self, image: np.ndarray, line_cnt: int = 1):\n        if image.ndim == 3:  # If the image has channels (e.g., RGB)\n            height, width, _ = image.shape\n        else:  # For a 2D image (grayscale)\n            height, width = image.shape\n\n        font_size = height / line_cnt\n\n        print(f\"width: {width}, height: {height}, fs: {font_size}\")\n        if font_size > 46: \n            font_size = self.FONT_SIZE + 6\n            ygain = 40\n        elif font_size > 31: \n            font_size = self.FONT_SIZE\n            ygain = 33\n        elif font_size > 28.5: \n            font_size = self.FONT_SIZE - 3\n            ygain = 30\n        else:\n            font_size = self.FONT_SIZE - 6\n            ygain = 22\n\n        return 'TimesNewRoman.ttf', font_size, ygain\n                "}
{"type": "source_file", "path": "utils/ditod_vgt/__init__.py", "content": "# --------------------------------------------------------------------------------\n# MPViT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n\nfrom .config import add_vit_config\nfrom .VGTbackbone import build_VGT_fpn_backbone\nfrom .dataset_mapper import DetrDatasetMapper\nfrom .VGTTrainer import VGTTrainer\nfrom .VGT import VGT\n\nfrom .utils import eval_and_show, load_gt_from_json, pub_load_gt_from_json\n"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTbeit.py", "content": "\"\"\" Vision Transformer (ViT) in PyTorch\n\nA PyTorch implement of Vision Transformers as described in\n'An Image Is Worth 16 x 16 Words: Transformers for Image Recognition at Scale' - https://arxiv.org/abs/2010.11929\n\nThe official jax code is released and available at https://github.com/google-research/vision_transformer\n\nStatus/TODO:\n* Models updated to be compatible with official impl. Args added to support backward compat for old PyTorch weights.\n* Weights ported from official jax impl for 384x384 base and small models, 16x16 and 32x32 patches.\n* Trained (supervised on ImageNet-1k) my custom 'small' patch model to 77.9, 'base' to 79.4 top-1 with this code.\n* Hopefully find time and GPUs for SSL or unsupervised pretraining on OpenImages w/ ImageNet fine-tune in future.\n\nAcknowledgments:\n* The paper authors for releasing code and weights, thanks!\n* I fixed my class token impl based on Phil Wang's https://github.com/lucidrains/vit-pytorch ... check it out\nfor some einops/einsum fun\n* Simple transformer style inspired by Andrej Karpathy's https://github.com/karpathy/minGPT\n* Bert reference code checks against Huggingface Transformers and Tensorflow Bert\n\nHacked together by / Copyright 2020 Ross Wightman\n\"\"\"\nimport warnings\nimport math\nimport torch\nfrom functools import partial\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\n\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }\n\ndef torch_memory(device, tag=\"\"):\n    # Checks and prints GPU memory\n    print(tag, f'{torch.cuda.memory_allocated(device)/1024/1024:.2f} MB USED')\n    print(tag, f'{torch.cuda.memory_reserved(device)/1024/1024:.2f} MB RESERVED')\n    print(tag, f'{torch.cuda.max_memory_allocated(device)/1024/1024:.2f} MB USED MAX')\n    print(tag, f'{torch.cuda.max_memory_reserved(device)/1024/1024:.2f} MB RESERVED MAX')\n    print('')\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass CrossAttention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.q = nn.Linear(dim, all_head_dim, bias=False)\n        self.kv = nn.Linear(dim, all_head_dim * 2, bias=False)\n\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, y):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            kv_bias = torch.cat((torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        kv = F.linear(input=y, weight=self.kv.weight, bias=kv_bias)\n        kv = kv.reshape(B, N, 2, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        k, v = kv[0], kv[1]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = F.linear(input=x, weight=self.q.weight, bias=self.q_bias)\n        q = q.reshape(B, N, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4)[0]\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\nclass CrossBlock(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm_vis = norm_layer(dim)\n        self.norm_grid = norm_layer(dim)\n        self.vis_attn = CrossAttention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.grid_attn = CrossAttention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2_vis = norm_layer(dim)\n        self.norm2_grid = norm_layer(dim)\n\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.vis_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n        self.grid_mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        self.self_block = CrossSelfBlock(dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale, drop=drop, attn_drop=attn_drop, drop_path=drop_path, init_values=init_values, act_layer=act_layer, norm_layer=norm_layer, window_size=window_size, attn_head_dim=attn_head_dim)\n\n        if init_values is not None:\n            self.gamma_vis = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_grid = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_vis, self.gamma_grid, self.gamma_1, self.gamma_2 = None, None, None, None\n\n    def cross_att(self, vis_input, grid_input):\n        # Cross Attention\n        if self.gamma_vis is None:\n            vis_att_output = vis_input + self.drop_path(self.vis_attn(self.norm_vis(vis_input), self.norm_grid(grid_input)))\n            grid_att_output = grid_input + self.drop_path(self.grid_attn(self.norm_grid(grid_input), self.norm_vis(vis_input)))\n        else:\n            vis_att_output = vis_input + self.drop_path(self.gamma_vis * self.vis_attn(self.norm_vis(vis_input), self.norm_grid(grid_input)))\n            grid_att_output = grid_input + self.drop_path(self.gamma_grid * self.grid_attn(self.norm_grid(grid_input), self.norm_vis(vis_input)))\n        return vis_att_output, grid_att_output\n\n    def forward(self, vis_input, grid_input):\n        vis_att_output, grid_att_output = self.cross_att(vis_input, grid_input)\n        vis_output, grid_output = self.self_block(vis_att_output, grid_att_output)\n\n        if self.gamma_1 is None:\n            vis_output = vis_output + self.drop_path(self.vis_mlp(self.norm2_vis(vis_output)))\n            grid_output = grid_output + self.drop_path(self.grid_mlp(self.norm2_grid(grid_output)))\n        else:\n            vis_output = vis_output + self.drop_path(self.gamma_1 * self.vis_mlp(self.norm2_vis(vis_output)))\n            grid_output = grid_output + self.drop_path(self.gamma_2 * self.grid_mlp(self.norm2_grid(grid_output)))\n\n        return vis_output, grid_output\n\nclass CrossSelfBlock(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm_vis = norm_layer(dim)\n        self.norm_grid = norm_layer(dim)\n        self.vis_attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        self.grid_attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        mlp_hidden_dim = int(dim * mlp_ratio)\n\n        if init_values is not None:\n            self.gamma_vis = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_grid = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_vis, self.gamma_grid = None, None\n\n    def self_att(self, vis_input, grid_input):\n        # Cross Attention\n        if self.gamma_vis is None:\n            vis_att_output = vis_input + self.drop_path(self.vis_attn(self.norm_vis(vis_input)))\n            grid_att_output = grid_input + self.drop_path(self.grid_attn(self.norm_grid(grid_input)))\n        else:\n            vis_att_output = vis_input + self.drop_path(self.gamma_vis * self.vis_attn(self.norm_vis(vis_input)))\n            grid_att_output = grid_input + self.drop_path(self.gamma_grid * self.grid_attn(self.norm_grid(grid_input)))\n        return vis_att_output, grid_att_output\n\n    def forward(self, vis_input, grid_input):\n        vis_att_output, grid_att_output = self.self_att(vis_input, grid_input)\n\n        return vis_att_output, grid_att_output\n    \nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n\n            # trunc_normal_(self.relative_position_bias_table, std=.0)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            if training_window_size == self.window_size:\n                relative_position_bias = \\\n                    self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                        self.window_size[0] * self.window_size[1] + 1,\n                        self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n            else:\n                training_window_size = tuple(training_window_size.tolist())\n                new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n                # new_num_relative_dis  cls-clstok-clscls-tok\n                new_relative_position_bias_table = F.interpolate(\n                    self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                                 2 * self.window_size[0] - 1,\n                                                                                 2 * self.window_size[1] - 1),\n                    size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                    align_corners=False)\n                new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                         new_num_relative_distance - 3).permute(\n                    1, 0)\n                new_relative_position_bias_table = torch.cat(\n                    [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n                # get pair-wise relative position index for each token inside the window\n                coords_h = torch.arange(training_window_size[0])\n                coords_w = torch.arange(training_window_size[1])\n                coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n                coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n                relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n                relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n                relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n                relative_coords[:, :, 1] += training_window_size[1] - 1\n                relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n                relative_position_index = \\\n                    torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                                dtype=relative_coords.dtype)\n                relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n                relative_position_index[0, 0:] = new_num_relative_distance - 3\n                relative_position_index[0:, 0] = new_num_relative_distance - 2\n                relative_position_index[0, 0] = new_num_relative_distance - 1\n\n                relative_position_bias = \\\n                    new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                        training_window_size[0] * training_window_size[1] + 1,\n                        training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n                relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n                attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)), requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None, training_window_size=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(\n                self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias, training_window_size=training_window_size))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias,\n                                                            training_window_size=training_window_size))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n\n    def __init__(self, img_size=[224, 224], patch_size=16, in_chans=3, embed_dim=768, bias=True):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.num_patches_w = self.patch_shape[0]\n        self.num_patches_h = self.patch_shape[1]\n        # the so-called patch_shape is the patch shape during pre-training\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size, bias=bias)\n\n    def forward(self, x, position_embedding=None, **kwargs):\n        # FIXME look at relaxing size constraints\n        # assert H == self.img_size[0] and W == self.img_size[1], \\\n        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n\n        if position_embedding is not None:\n            # interpolate the position embedding to the corresponding size\n            position_embedding = position_embedding.view(1, self.patch_shape[0], self.patch_shape[1], -1).permute(0, 3,\n                                                                                                                  1, 2)\n            position_embedding = F.interpolate(position_embedding, size=(Hp, Wp), mode='bicubic')\n            x = x + position_embedding\n\n        x = x.flatten(2).transpose(1, 2)\n        return x, (Hp, Wp)\n    \n\nclass HybridEmbed(nn.Module):\n    \"\"\" CNN Feature Map Embedding\n    Extract feature map from CNN, flatten, project to embedding dim.\n    \"\"\"\n\n    def __init__(self, backbone, img_size=[224, 224], feature_size=None, in_chans=3, embed_dim=768):\n        super().__init__()\n        assert isinstance(backbone, nn.Module)\n        img_size = to_2tuple(img_size)\n        self.img_size = img_size\n        self.backbone = backbone\n        if feature_size is None:\n            with torch.no_grad():\n                # FIXME this is hacky, but most reliable way of determining the exact dim of the output feature\n                # map for all networks, the feature metadata has reliable channel and stride info, but using\n                # stride to calc feature dim requires info about padding of each stage that isn't captured.\n                training = backbone.training\n                if training:\n                    backbone.eval()\n                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))[-1]\n                feature_size = o.shape[-2:]\n                feature_dim = o.shape[1]\n                backbone.train(training)\n        else:\n            feature_size = to_2tuple(feature_size)\n            feature_dim = self.backbone.feature_info.channels()[-1]\n        self.num_patches = feature_size[0] * feature_size[1]\n        self.proj = nn.Linear(feature_dim, embed_dim)\n\n    def forward(self, x):\n        x = self.backbone(x)[-1]\n        x = x.flatten(2).transpose(1, 2)\n        x = self.proj(x)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        # trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def forward(self, training_window_size):\n        if training_window_size == self.window_size:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n        else:\n            training_window_size = tuple(training_window_size.tolist())\n            new_num_relative_distance = (2 * training_window_size[0] - 1) * (2 * training_window_size[1] - 1) + 3\n            # new_num_relative_dis  cls-clstok-clscls-tok\n            new_relative_position_bias_table = F.interpolate(\n                self.relative_position_bias_table[:-3, :].permute(1, 0).view(1, self.num_heads,\n                                                                             2 * self.window_size[0] - 1,\n                                                                             2 * self.window_size[1] - 1),\n                size=(2 * training_window_size[0] - 1, 2 * training_window_size[1] - 1), mode='bicubic',\n                align_corners=False)\n            new_relative_position_bias_table = new_relative_position_bias_table.view(self.num_heads,\n                                                                                     new_num_relative_distance - 3).permute(\n                1, 0)\n            new_relative_position_bias_table = torch.cat(\n                [new_relative_position_bias_table, self.relative_position_bias_table[-3::]], dim=0)\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(training_window_size[0])\n            coords_w = torch.arange(training_window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += training_window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += training_window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * training_window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(training_window_size[0] * training_window_size[1] + 1,) * 2,\n                            dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = new_num_relative_distance - 3\n            relative_position_index[0:, 0] = new_num_relative_distance - 2\n            relative_position_index[0, 0] = new_num_relative_distance - 1\n\n            relative_position_bias = \\\n                new_relative_position_bias_table[relative_position_index.view(-1)].view(\n                    training_window_size[0] * training_window_size[1] + 1,\n                    training_window_size[0] * training_window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n        return relative_position_bias\n\n\nclass BEiT(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n\n    def __init__(self,\n                 img_size=[224, 224],\n                 patch_size=16,\n                 in_chans=3,\n                 grid_chans=64,\n                 num_classes=80,\n                 embed_dim=768,\n                 self_depth=7,\n                 cross_depth=5,\n                 num_heads=12,\n                 mlp_ratio=4.,\n                 qkv_bias=False,\n                 qk_scale=None,\n                 drop_rate=0.,\n                 attn_drop_rate=0.,\n                 drop_path_rate=0.,\n                 hybrid_backbone=None,\n                 norm_layer=None,\n                 init_values=None,\n                 use_abs_pos_emb=False,\n                 use_rel_pos_bias=False,\n                 use_shared_rel_pos_bias=False,\n                 use_checkpoint=True,\n                 pretrained=None,\n                 out_features=None,\n                 ):\n\n        super(BEiT, self).__init__()\n\n        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n        self.use_checkpoint = use_checkpoint\n\n        if hybrid_backbone is not None:\n            self.patch_embed = HybridEmbed(\n                hybrid_backbone, img_size=img_size, in_chans=in_chans, embed_dim=embed_dim)\n        else:\n            self.patch_embed = PatchEmbed(\n                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n            self.grid_patch_embed = PatchEmbed(\n                img_size=img_size, patch_size=patch_size, in_chans=grid_chans, embed_dim=embed_dim, bias=True)\n        num_patches = self.patch_embed.num_patches\n        self.out_features = out_features\n        self.out_indices = [int(name[5:]) for name in out_features]\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.grid_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n            self.grid_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n            self.grid_pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        self.use_shared_rel_pos_bias = use_shared_rel_pos_bias\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, self_depth + cross_depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(self_depth)])\n        \n        self.grid_blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(self_depth)])\n        \n        self.cross_blocks = nn.ModuleList([\n            CrossBlock(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i + self_depth], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(cross_depth)])\n\n        # trunc_normal_(self.mask_token, std=.02)\n\n        if patch_size == 16:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                # nn.SyncBatchNorm(embed_dim),\n                nn.BatchNorm2d(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.fpn3 = nn.Identity()\n            self.fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n            \n            \n            self.grid_fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n                # nn.SyncBatchNorm(embed_dim),\n                nn.BatchNorm2d(embed_dim),\n                nn.GELU(),\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.grid_fpn2 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.grid_fpn3 = nn.Identity()\n            self.grid_fpn4 = nn.MaxPool2d(kernel_size=2, stride=2)\n            \n            \n        elif patch_size == 8:\n            self.fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.fpn2 = nn.Identity()\n            self.fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n            self.fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n            \n            self.grid_fpn1 = nn.Sequential(\n                nn.ConvTranspose2d(embed_dim, embed_dim, kernel_size=2, stride=2),\n            )\n            self.grid_fpn2 = nn.Identity()\n            self.grid_fpn3 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n            )\n            self.grid_fpn4 = nn.Sequential(\n                nn.MaxPool2d(kernel_size=4, stride=4),\n            )\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n            trunc_normal_(self.grid_pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        trunc_normal_(self.grid_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n        \n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    '''\n    def init_weights(self):\n        \"\"\"Initialize the weights in backbone.\n\n        Args:\n            pretrained (str, optional): Path to pre-trained weights.\n                Defaults to None.\n        \"\"\"\n        logger = get_root_logger()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n\n        if self.init_cfg is None:\n            logger.warn(f'No pre-trained weights for '\n                        f'{self.__class__.__name__}, '\n                        f'training start from scratch')\n        else:\n            assert 'checkpoint' in self.init_cfg, f'Only support ' \\\n                                                  f'specify `Pretrained` in ' \\\n                                                  f'`init_cfg` in ' \\\n                                                  f'{self.__class__.__name__} '\n            logger.info(f\"Will load ckpt from {self.init_cfg['checkpoint']}\")\n            load_checkpoint(self,\n                            filename=self.init_cfg['checkpoint'],\n                            strict=False,\n                            logger=logger,\n                            beit_spec_expand_rel_pos = self.use_rel_pos_bias,\n                            )\n    '''\n\n    def get_num_layers(self):\n        return len(self.blocks)\n\n    @torch.jit.ignore\n    def no_weight_decay(self):\n        return {'pos_embed', 'cls_token'}\n\n    def forward_features(self, x, grid):\n        B, C, H, W = x.shape\n        vis_x, (Hp, Wp) = self.patch_embed(x, self.pos_embed[:, 1:, :] if self.pos_embed is not None else None)\n        grid_x, (grid_Hp, grid_Wp) = self.grid_patch_embed(grid, self.grid_pos_embed[:, 1:, :] if self.grid_pos_embed is not None else None)\n        \n        # Hp, Wp are HW for patches\n        batch_size, seq_len, _ = grid_x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        grid_tokens = self.grid_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        if self.pos_embed is not None:\n            cls_tokens = cls_tokens + self.pos_embed[:, :1, :]\n            grid_tokens = grid_tokens + self.grid_pos_embed[:, :1, :]\n        vis_x = torch.cat((cls_tokens, vis_x), dim=1)\n        vis_x = self.pos_drop(vis_x)\n        \n        grid_x = torch.cat((grid_tokens, grid_x), dim=1)\n        grid_x = self.pos_drop(grid_x)\n\n        features = []\n        grid_features = []\n        training_window_size = torch.tensor([Hp, Wp])\n        grid_training_window_size = torch.tensor([grid_Hp, grid_Wp])\n\n        rel_pos_bias = self.rel_pos_bias(training_window_size) if self.rel_pos_bias is not None else None\n        \n        for i, blk in enumerate(self.blocks):\n            if self.use_checkpoint:\n                vis_x = checkpoint.checkpoint(blk, vis_x, rel_pos_bias, training_window_size)\n            else:\n                vis_x = blk(vis_x, rel_pos_bias=rel_pos_bias, training_window_size=training_window_size)\n            if i in self.out_indices:\n                xp = vis_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n        \n        for i, grid_blk in enumerate(self.grid_blocks):\n            if self.use_checkpoint:\n                grid_x = checkpoint.checkpoint(grid_blk, grid_x, rel_pos_bias, grid_training_window_size)\n            else:\n                grid_x = grid_blk(grid_x, rel_pos_bias=rel_pos_bias, training_window_size=grid_training_window_size)\n            if i in self.out_indices:\n                gp = grid_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, grid_Hp, grid_Wp)\n                grid_features.append(gp.contiguous())\n        \n        # import ipdb;ipdb.set_trace()\n        for i, cross_blk in enumerate(self.cross_blocks):\n            if self.use_checkpoint:\n                vis_x, grid_x = checkpoint.checkpoint(cross_blk, vis_x, grid_x)\n            else:\n                vis_x, grid_x = cross_blk(vis_input = vis_x, grid_input = grid_x)\n                \n            if 1:\n                xp = vis_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, Hp, Wp)\n                features.append(xp.contiguous())\n                \n                gp = grid_x[:, 1:, :].permute(0, 2, 1).reshape(B, -1, grid_Hp, grid_Wp)\n                grid_features.append(gp.contiguous())\n                \n        ops = [self.fpn1, self.fpn2, self.fpn3, self.fpn4]\n        grid_ops = [self.grid_fpn1, self.grid_fpn2, self.grid_fpn3, self.grid_fpn4]\n        \n        for i in range(len(features)):\n            features[i] = ops[i](features[i])\n        \n        for i in range(len(grid_features)):\n            grid_features[i] = grid_ops[i](grid_features[i])   \n\n        feat_out = {}\n        grid_feat_out = {}\n\n        for name, vis_value, grid_value in zip(self.out_features, features, grid_features):\n            feat_out[name] = vis_value\n            grid_feat_out[name] = grid_value\n\n        return feat_out, grid_feat_out\n\n    def forward(self, x, grid):\n        x,y = self.forward_features(x, grid)\n        return x,y\n\n\ndef beit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef beit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=None,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef VGT_dit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        self_depth=12,\n        cross_depth=0,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.1,\n        in_chans=3,\n        grid_chans=64,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_base_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.1,\n        in_chans=3,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\ndef dit_large_patch16(pretrained=False, **kwargs):\n    model = BEiT(\n        patch_size=16,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        qkv_bias=True,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=1e-5,\n        **kwargs)\n    model.default_cfg = _cfg()\n    return model\n\nif __name__ == '__main__':\n    model = BEiT(use_checkpoint=True, use_shared_rel_pos_bias=True)\n    model = model.to(\"cuda:0\")\n    input1 = torch.rand(2, 3, 512, 762).to(\"cuda:0\")\n    input2 = torch.rand(2, 3, 800, 1200).to(\"cuda:0\")\n    input3 = torch.rand(2, 3, 720, 1000).to(\"cuda:0\")\n    output1 = model(input1)\n    output2 = model(input2)\n    output3 = model(input3)\n    print(\"all done\")\n"}
{"type": "source_file", "path": "utils/ditod_vgt/VGT.py", "content": "# -*- coding: utf-8 -*-\n# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\nimport logging\nimport numpy as np\nfrom typing import Dict, List, Optional, Tuple\nimport torch\nfrom torch import nn\n\nfrom detectron2.config import configurable\nfrom detectron2.data.detection_utils import convert_image_to_rgb\nfrom detectron2.structures import ImageList, Instances\nfrom detectron2.utils.events import get_event_storage\nfrom detectron2.utils.logger import log_first_n\n\nfrom detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\nfrom detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n\nfrom .Wordnn_embedding import WordnnEmbedding\n\n__all__ = [\"VGT\"]\n\ndef torch_memory(device, tag=\"\"):\n    # Checks and prints GPU memory\n    print(tag, f'{torch.cuda.memory_allocated(device)/1024/1024:.2f} MB USED')\n    print(tag, f'{torch.cuda.memory_reserved(device)/1024/1024:.2f} MB RESERVED')\n    print(tag, f'{torch.cuda.max_memory_allocated(device)/1024/1024:.2f} MB USED MAX')\n    print(tag, f'{torch.cuda.max_memory_reserved(device)/1024/1024:.2f} MB RESERVED MAX')\n    print('') \n                    \n@META_ARCH_REGISTRY.register()\nclass VGT(GeneralizedRCNN):\n    \n    @configurable\n    def __init__(\n        self,\n        *,\n        vocab_size: int = 30552,\n        hidden_size: int = 768,\n        embedding_dim: int = 64,\n        bros_embedding_path: str = '',\n        use_pretrain_weight: bool = True,\n        use_UNK_text: bool = False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.Wordgrid_embedding = WordnnEmbedding(vocab_size, hidden_size, embedding_dim, \\\n                                                    bros_embedding_path, use_pretrain_weight, use_UNK_text)\n    @classmethod\n    def from_config(cls, cfg):\n        ret = super().from_config(cfg)\n        ret.update(\n            {\n                \"vocab_size\": cfg.MODEL.WORDGRID.VOCAB_SIZE,\n                \"hidden_size\": cfg.MODEL.WORDGRID.HIDDEN_SIZE,\n                \"embedding_dim\": cfg.MODEL.WORDGRID.EMBEDDING_DIM,\n                \"bros_embedding_path\": cfg.MODEL.WORDGRID.MODEL_PATH,\n                \"use_pretrain_weight\": cfg.MODEL.WORDGRID.USE_PRETRAIN_WEIGHT,\n                \"use_UNK_text\": cfg.MODEL.WORDGRID.USE_UNK_TEXT,\n            }\n        )\n        return ret\n    \n    def forward(self, batched_inputs: List[Dict[str, torch.Tensor]]):\n        \"\"\"\n        Args:\n            batched_inputs: a list, batched outputs of :class:`DatasetMapper` .\n                Each item in the list contains the inputs for one image.\n                For now, each item in the list is a dict that contains:\n\n                * image: Tensor, image in (C, H, W) format.\n                * instances (optional): groundtruth :class:`Instances`\n                * proposals (optional): :class:`Instances`, precomputed proposals.\n\n                Other information that's included in the original dicts, such as:\n\n                * \"height\", \"width\" (int): the output resolution of the model, used in inference.\n                  See :meth:`postprocess` for details.\n\n        Returns:\n            list[dict]:\n                Each dict is the output for one input image.\n                The dict contains one key \"instances\" whose value is a :class:`Instances`.\n                The :class:`Instances` object has the following keys:\n                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\", \"pred_keypoints\"\n        \"\"\"\n        if not self.training:\n            return self.inference(batched_inputs)\n\n        images = self.preprocess_image(batched_inputs)\n        \n        if \"instances\" in batched_inputs[0]:\n            gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n        else:\n            gt_instances = None\n\n        chargrid = self.Wordgrid_embedding(images.tensor, batched_inputs)\n        features = self.backbone(images.tensor, chargrid)\n\n        if self.proposal_generator is not None:\n            proposals, proposal_losses = self.proposal_generator(images, features, gt_instances)\n        else:\n            assert \"proposals\" in batched_inputs[0]\n            proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n            proposal_losses = {}\n            \n        _, detector_losses = self.roi_heads(images, features, proposals, gt_instances)\n        if self.vis_period > 0:\n            storage = get_event_storage()\n            if storage.iter % self.vis_period == 0:\n                self.visualize_training(batched_inputs, proposals)\n\n        losses = {}\n        losses.update(detector_losses)\n        losses.update(proposal_losses)\n        \n        return losses\n\n    \n    def inference(\n        self,\n        batched_inputs: List[Dict[str, torch.Tensor]],\n        detected_instances: Optional[List[Instances]] = None,\n        do_postprocess: bool = True,\n    ):\n        \"\"\"\n        Run inference on the given inputs.\n\n        Args:\n            batched_inputs (list[dict]): same as in :meth:`forward`\n            detected_instances (None or list[Instances]): if not None, it\n                contains an `Instances` object per image. The `Instances`\n                object contains \"pred_boxes\" and \"pred_classes\" which are\n                known boxes in the image.\n                The inference will then skip the detection of bounding boxes,\n                and only predict other per-ROI outputs.\n            do_postprocess (bool): whether to apply post-processing on the outputs.\n\n        Returns:\n            When do_postprocess=True, same as in :meth:`forward`.\n            Otherwise, a list[Instances] containing raw network outputs.\n        \"\"\"\n        assert not self.training\n\n        images = self.preprocess_image(batched_inputs)\n        \n        chargrid = self.Wordgrid_embedding(images.tensor, batched_inputs)\n        features = self.backbone(images.tensor, chargrid)\n\n        if detected_instances is None:\n            if self.proposal_generator is not None:\n                proposals, _ = self.proposal_generator(images, features, None)\n            else:\n                assert \"proposals\" in batched_inputs[0]\n                proposals = [x[\"proposals\"].to(self.device) for x in batched_inputs]\n\n            results, _ = self.roi_heads(images, features, proposals, None)\n        else:\n            detected_instances = [x.to(self.device) for x in detected_instances]\n            results = self.roi_heads.forward_with_given_boxes(features, detected_instances)\n        \n        if do_postprocess:\n            assert not torch.jit.is_scripting(), \"Scripting is not supported for postprocess.\"\n            return GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)\n        else:\n            return results"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTcheckpointer.py", "content": "from detectron2.checkpoint import DetectionCheckpointer\n\nfrom typing import Any\nimport torch\nimport torch.nn as nn\nfrom fvcore.common.checkpoint import _IncompatibleKeys, _strip_prefix_if_present, TORCH_VERSION, quantization, \\\n    ObserverBase, FakeQuantizeBase\nfrom torch import distributed as dist\nfrom scipy import interpolate\nimport numpy as np\nimport torch.nn.functional as F\n\n\ndef append_prefix(k):\n    prefix = 'backbone.'\n    if \"Wordgrid_embedding\" in k:\n        return k[10:]\n    elif \"myFPN\" in k:\n        return prefix + k[16:]\n    else:\n        return prefix + k if not k.startswith(prefix) else k\n    \ndef DiT_append_prefix(k):\n    prefix = 'backbone.bottom_up.backbone.'\n    return prefix + k if not k.startswith(prefix) else k\n\ndef modify_ckpt_state(model, state_dict, logger=None):\n    # reshape absolute position embedding for Swin\n    if state_dict.get(append_prefix('absolute_pos_embed')) is not None:\n        absolute_pos_embed = state_dict[append_prefix('absolute_pos_embed')]\n        N1, L, C1 = absolute_pos_embed.size()\n        N2, C2, H, W = model.backbone.bottom_up.backbone.absolute_pos_embed.size()\n        if N1 != N2 or C1 != C2 or L != H * W:\n            logger.warning(\"Error in loading absolute_pos_embed, pass\")\n        else:\n            state_dict[append_prefix('absolute_pos_embed')] = absolute_pos_embed.view(N2, H, W, C2).permute(0, 3, 1, 2)\n\n    def get_dist_info():\n        if dist.is_available() and dist.is_initialized():\n            rank = dist.get_rank()\n            world_size = dist.get_world_size()\n        else:\n            rank = 0\n            world_size = 1\n        return rank, world_size\n\n    rank, _ = get_dist_info()\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            if key not in model.state_dict():\n                continue\n            dst_num_pos, _ = model.state_dict()[key].size()\n            dst_patch_shape = model.backbone.bottom_up.backbone.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                if rank == 0:\n                    print(\"Position interpolate for %s from %dx%d to %dx%d\" % (\n                        key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r ** n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.13492:\n                #     q = 1.13492\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n                if rank == 0:\n                    print(\"x = {}\".format(x))\n                    print(\"dx = {}\".format(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = interpolate.interp2d(x, y, z, kind='cubic')\n                    all_rel_pos_bias.append(\n                        torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    if append_prefix('pos_embed') in state_dict:\n        pos_embed_checkpoint = state_dict[append_prefix('pos_embed')]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.backbone.bottom_up.backbone.patch_embed.num_patches\n        num_extra_tokens = model.backbone.bottom_up.backbone.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        # new_size = int(num_patches ** 0.5)\n        new_size_w = model.backbone.bottom_up.backbone.patch_embed.num_patches_w\n        new_size_h = model.backbone.bottom_up.backbone.patch_embed.num_patches_h\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size_h or orig_size != new_size_w:\n            if rank == 0:\n                print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size_w, new_size_h))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size_w, new_size_h), mode='bicubic', align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[append_prefix('pos_embed')] = new_pos_embed\n\n    # interpolate position bias table if needed\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for table_key in relative_position_bias_table_keys:\n        table_pretrained = state_dict[table_key]\n        if table_key not in model.state_dict():\n            continue\n        table_current = model.state_dict()[table_key]\n        L1, nH1 = table_pretrained.size()\n        L2, nH2 = table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {table_key}, pass\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                table_pretrained_resized = F.interpolate(\n                    table_pretrained.permute(1, 0).view(1, nH1, S1, S1),\n                    size=(S2, S2), mode='bicubic')\n                state_dict[table_key] = table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    if append_prefix('rel_pos_bias.relative_position_bias_table') in state_dict and \\\n            model.backbone.bottom_up.backbone.use_rel_pos_bias and \\\n            not model.backbone.bottom_up.backbone.use_shared_rel_pos_bias and \\\n            append_prefix('blocks.0.attn.relative_position_bias_table') not in state_dict:\n        logger.info(\"[BEIT] Expand the shared relative position embedding to each transformer block. \")\n        num_layers = model.backbone.bottom_up.backbone.get_num_layers()\n        rel_pos_bias = state_dict[append_prefix(\"rel_pos_bias.relative_position_bias_table\")]\n        for i in range(num_layers):\n            state_dict[\"blocks.%d.attn.relative_position_bias_table\" % i] = rel_pos_bias.clone()\n        state_dict.pop(append_prefix(\"rel_pos_bias.relative_position_bias_table\"))\n\n    return state_dict\n\n\nclass MyDetectionCheckpointer(DetectionCheckpointer):\n    def _load_model(self, checkpoint: Any) -> _IncompatibleKeys:\n        \"\"\"\n        Load weights from a checkpoint.\n\n        Args:\n            checkpoint (Any): checkpoint contains the weights.\n\n        Returns:\n            ``NamedTuple`` with ``missing_keys``, ``unexpected_keys``,\n                and ``incorrect_shapes`` fields:\n                * **missing_keys** is a list of str containing the missing keys\n                * **unexpected_keys** is a list of str containing the unexpected keys\n                * **incorrect_shapes** is a list of (key, shape in checkpoint, shape in model)\n\n            This is just like the return value of\n            :func:`torch.nn.Module.load_state_dict`, but with extra support\n            for ``incorrect_shapes``.\n        \"\"\"\n        DiT_checkpoint_state_dict = torch.load(\"/path/dit-base-224-p16-500k-62d53a.pth\", map_location=torch.device(\"cpu\"))[\"model\"]\n        checkpoint_state_dict = checkpoint.pop(\"model\")\n        # import ipdb;ipdb.set_trace()\n        self._convert_ndarray_to_tensor(checkpoint_state_dict)\n\n        # if the state_dict comes from a model that was wrapped in a\n        # DataParallel or DistributedDataParallel during serialization,\n        # remove the \"module\" prefix before performing the matching.\n        _strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n        # workaround https://github.com/pytorch/pytorch/issues/24139\n        model_state_dict = self.model.state_dict()\n        incorrect_shapes = []\n        \n        new_checkpoint_state_dict = {}\n        for k in checkpoint_state_dict.keys():\n            new_checkpoint_state_dict[append_prefix(k)] = checkpoint_state_dict[k]\n\n        for k in DiT_checkpoint_state_dict.keys():\n            new_checkpoint_state_dict[DiT_append_prefix(k)] = DiT_checkpoint_state_dict[k]\n            \n        checkpoint_state_dict = new_checkpoint_state_dict\n        \n        for k in list(checkpoint_state_dict.keys()):\n            if k in model_state_dict:\n                model_param = model_state_dict[k]\n                # Allow mismatch for uninitialized parameters\n                if TORCH_VERSION >= (1, 8) and isinstance(\n                        model_param, nn.parameter.UninitializedParameter\n                ):\n                    continue\n                shape_model = tuple(model_param.shape)\n                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n                if shape_model != shape_checkpoint:\n\n                    has_observer_base_classes = (\n                            TORCH_VERSION >= (1, 8)\n                            and hasattr(quantization, \"ObserverBase\")\n                            and hasattr(quantization, \"FakeQuantizeBase\")\n                    )\n                    if has_observer_base_classes:\n                        # Handle the special case of quantization per channel observers,\n                        # where buffer shape mismatches are expected.\n                        def _get_module_for_key(\n                                model: torch.nn.Module, key: str\n                        ) -> torch.nn.Module:\n                            # foo.bar.param_or_buffer_name -> [foo, bar]\n                            key_parts = key.split(\".\")[:-1]\n                            cur_module = model\n                            for key_part in key_parts:\n                                cur_module = getattr(cur_module, key_part)\n                            return cur_module\n\n                        cls_to_skip = (\n                            ObserverBase,\n                            FakeQuantizeBase,\n                        )\n                        target_module = _get_module_for_key(self.model, k)\n                        if isinstance(target_module, cls_to_skip):\n                            # Do not remove modules with expected shape mismatches\n                            # them from the state_dict loading. They have special logic\n                            # in _load_from_state_dict to handle the mismatches.\n                            continue\n\n                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                    checkpoint_state_dict.pop(k)\n        incompatible = self.model.load_state_dict(checkpoint_state_dict, strict=False)\n        return _IncompatibleKeys(\n            missing_keys=incompatible.missing_keys,\n            unexpected_keys=incompatible.unexpected_keys,\n            incorrect_shapes=incorrect_shapes,\n        )\n"}
{"type": "source_file", "path": "utils/ditod/table_evaluation/data_structure.py", "content": "\"\"\"\nData structures used by the evaluation process.\nYu Fang - March 2019\n\"\"\"\n\nfrom collections.abc import Iterable\n\nimport numpy as np\nfrom shapely.geometry import Polygon\n\n\n# helper functions\ndef flatten(lis):\n    for item in lis:\n        if isinstance(item, Iterable) and not isinstance(item, str):\n            for x in flatten(item):\n                yield x\n        else:\n            yield item\n\n# derived from https://blog.csdn.net/u012433049/article/details/82909484\ndef compute_poly_iou(list1, list2):\n    a1 = np.array(list1, dtype=int).reshape(-1, 2)\n    poly1 = Polygon(a1)\n    poly1_clean = poly1.buffer(0)\n\n    a2 = np.array(list2, dtype=int).reshape(-1, 2)\n    poly2 = Polygon(a2)\n    poly2_clean = poly2.buffer(0)\n\n    try:\n        # iou = poly1.intersection(poly2).area / poly1.union(poly2).area\n        iou = poly1_clean.intersection(poly2_clean).area / poly1_clean.union(poly2_clean).area\n    except ZeroDivisionError:\n        iou = 0\n    return iou\n\n\nclass Cell(object):\n    # @:param start_row : start row index of the Cell\n    # @:param start_col : start column index of the Cell\n    # @:param end-row : end row index of the Cell\n    # @:param end-col : end column index of the Cell\n    # @:param cell_box: bounding-box of the Cell (coordinates are saved as a string)\n    # @:param content_box: bounding-box of the text content within Cell (unused variable)\n    # @:param cell_id: unique id of the Cell\n\n    def __init__(self, table_id, start_row, start_col, cell_box, end_row, end_col, content_box=\"\"):\n        self._start_row = int(start_row)\n        self._start_col = int(start_col)\n        self._cell_box = cell_box\n        self._content_box = content_box\n        self._table_id = table_id    # the table_id this cell belongs to\n        # self._cell_name = cell_id    # specify the cell using passed-in cell_id\n        self._cell_id = id(self)\n        # self._region = region\n\n        # check for end-row and end-col special case\n        if end_row == -1:\n            self._end_row = self.start_row\n        else:\n            self._end_row = int(end_row)\n        if end_col == -1:\n            self._end_col = self._start_col\n        else:\n            self._end_col = int(end_col)\n\n    @property\n    def start_row(self):\n        return self._start_row\n\n    @property\n    def start_col(self):\n        return self._start_col\n\n    @property\n    def end_row(self):\n        return self._end_row\n\n    @property\n    def end_col(self):\n        return self._end_col\n\n    @property\n    def cell_box(self):\n        return self._cell_box\n\n    @property\n    def content_box(self):\n        return self._content_box\n\n    @property\n    def cell_id(self):\n        return self._cell_id\n\n    @property\n    def table_id(self):\n        return self._table_id\n\n    def __str__(self):\n        return \"CELL row=[%d, %d] col=[%d, %d] (coords=%s)\" %(self.start_row, self.end_row\n                                                              , self.start_col, self.end_col\n                                                              , self.cell_box)\n\n    # return the IoU value of two cell blocks\n    def compute_cell_iou(self, another_cell):\n        cell_box_1_temp = []\n        for el in self.cell_box.split():\n            cell_box_1_temp.append((el.split(\",\")))\n        cell_box_1 = list(flatten(cell_box_1_temp))\n        cell_box_1 = [int(x) for x in cell_box_1]\n\n        cell_box_2_temp = []\n        for el in another_cell.cell_box.split():\n            cell_box_2_temp.append((el.split(\",\")))\n        cell_box_2 = list(flatten(cell_box_2_temp))\n        cell_box_2 = [int(x) for x in cell_box_2]\n\n        return compute_poly_iou(cell_box_1, cell_box_2)\n\n    # check if the two cell object denotes same cell area in table\n    def check_same(self, another_cell):\n        return self._start_row == another_cell.start_row and self._end_row == another_cell.end_row and \\\n               self._start_col == another_cell.start_col and self._end_col == another_cell.end_col\n\n\n# Note: currently save the relation with two cell object involved,\n# can be replaced by cell_id in follow-up memory clean up\nclass AdjRelation:\n\n    DIR_HORIZ = 1\n    DIR_VERT = 2\n\n    def __init__(self, fromText, toText, direction):\n        # @param: fromText, toText are Cell objects may be changed to cell-ID for further development\n        self._fromText = fromText\n        self._toText = toText\n        self._direction = direction\n\n    @property\n    def fromText(self):\n        return self._fromText\n\n    @property\n    def toText(self):\n        return self._toText\n\n    @property\n    def direction(self):\n        return self._direction\n\n    def __str__(self):\n        if self.direction == self.DIR_VERT:\n            dir = \"vertical\"\n        else:\n            dir = \"horizontal\"\n        return 'ADJ_RELATION: ' + str(self._fromText) + '  ' + str(self._toText) + '    ' + dir\n\n    def isEqual(self, otherRelation):\n        return self.fromText.cell_id == otherRelation.fromText.cell_id and \\\n               self.toText.cell_id == otherRelation.toText.cell_id and self.direction == otherRelation.direction\n\n\nclass Table:\n\n    def __init__(self, tableNode):\n        self._root = tableNode\n        self._id = id(self)\n        self._table_coords = \"\"\n        self._maxRow = 0    # PS: indexing from 0\n        self._maxCol = 0\n        self._cells = []    # save a table as list of <Cell>s\n        self.adj_relations = []    # save the adj_relations for the table\n        self.parsed = False\n        self.found = False    # check if the find_adj_relations() has been called once\n\n        self.parse_table()\n\n    def __str__(self):\n        return \"TABLE object - {} row x {} col\".format(self._maxRow+1, self._maxCol+1)\n\n    @property\n    def id(self):\n        return self._id\n\n    @property\n    def table_coords(self):\n        return self._table_coords\n\n    @property\n    def table_cells(self):\n        return self._cells\n\n    # parse input xml to cell lists\n    def parse_table(self):\n        # get the table bbox\n        self._table_coords = str(self._root.getElementsByTagName(\"Coords\")[0].getAttribute(\"points\"))\n\n        # get info for each cell\n        cells = self._root.getElementsByTagName(\"cell\")\n        max_row = max_col = 0\n        for cell in cells:\n            sr = cell.getAttribute(\"start-row\")\n            sc = cell.getAttribute(\"start-col\")\n            cell_id = cell.getAttribute(\"id\")\n            b_points = str(cell.getElementsByTagName(\"Coords\")[0].getAttribute(\"points\"))\n            # try:\n            #     try:\n            #         text = cell.getElementsByTagName(\"content\")[0].firstChild.nodeValue\n            #     except AttributeError:\n            #         text = \"\"\n            # except IndexError:\n            #     text = \"initialized cell as no content\"\n            er = cell.getAttribute(\"end-row\") if cell.hasAttribute(\"end-row\") else -1\n            ec = cell.getAttribute(\"end-col\") if cell.hasAttribute(\"end-col\") else -1\n            new_cell = Cell(table_id=str(self.id), start_row=sr, start_col=sc, cell_box=b_points,\n                            end_row=er, end_col=ec)\n            max_row = max(max_row, int(sr), int(er))\n            max_col = max(max_col, int(sc), int(ec))\n            self._cells.append(new_cell)\n        self._maxCol = max_col\n        self._maxRow = max_row\n        self.parsed = True\n\n    # generate a table-like structure for finding adj_relations\n    def convert_2d(self):\n        table = [[0 for x in range(self._maxCol+1)] for y in range(self._maxRow+1)]    # init blank cell with int 0\n        for cell in self._cells:\n            cur_row = cell.start_row\n            while cur_row <= cell.end_row:\n                cur_col = cell.start_col\n                while cur_col <= cell.end_col:\n                    temp = table[cur_row][cur_col]\n                    if temp == 0:\n                        table[cur_row][cur_col] = cell\n                    elif type(temp) == list:\n                        temp.append(cell)\n                        table[cur_row][cur_col] = temp\n                    else:\n                        table[cur_row][cur_col] = [temp, cell]\n                    cur_col += 1\n                cur_row += 1\n\n        return table\n\n    def find_adj_relations(self):\n        if self.found:\n            return self.adj_relations\n        else:\n            # if len(self._cells) == 0:\n            if self.parsed == False:\n                # fix: cases where there's no cell in table?\n                print(\"table is not parsed for further steps.\")\n                self.parse_table()\n                self.find_adj_relations()\n            else:\n                retVal = []\n                tab = self.convert_2d()\n\n                # find horizontal relations\n                for r in range(self._maxRow+1):\n                    for c_from in range(self._maxCol):\n                        temp_pos = tab[r][c_from]\n                        if temp_pos == 0:\n                            continue\n                        elif type(temp_pos) == list:\n                            for cell in temp_pos:\n                                c_to = c_from + 1\n                                if tab[r][c_to] != 0:\n                                    # find relation between two adjacent cells\n                                    if type(tab[r][c_to]) == list:\n                                        for cell_to in tab[r][c_to]:\n                                            if cell != cell_to and (not cell.check_same(cell_to)):\n                                                adj_relation = AdjRelation(cell, cell_to, AdjRelation.DIR_HORIZ)\n                                                retVal.append(adj_relation)\n                                    else:\n                                        if cell != tab[r][c_to]:\n                                            adj_relation = AdjRelation(cell, tab[r][c_to], AdjRelation.DIR_HORIZ)\n                                            retVal.append(adj_relation)\n                                else:\n                                    # find the next non-blank cell, if exists\n                                    for temp in range(c_from + 1, self._maxCol + 1):\n                                        if tab[r][temp] != 0:\n                                            if type(tab[r][temp]) == list:\n                                                for cell_to in tab[r][temp]:\n                                                    adj_relation = AdjRelation(cell, cell_to,\n                                                                               AdjRelation.DIR_HORIZ)\n                                                    retVal.append(adj_relation)\n                                            else:\n                                                adj_relation = AdjRelation(cell, tab[r][temp],\n                                                                           AdjRelation.DIR_HORIZ)\n                                                retVal.append(adj_relation)\n                                            break\n                        else:\n                            c_to = c_from + 1\n                            if tab[r][c_to] != 0:\n                                # find relation between two adjacent cells\n                                if type(tab[r][c_to]) == list:\n                                    for cell_to in tab[r][c_to]:\n                                        if temp_pos != cell_to:\n                                            adj_relation = AdjRelation(temp_pos, cell_to, AdjRelation.DIR_HORIZ)\n                                            retVal.append(adj_relation)\n                                else:\n                                    if temp_pos != tab[r][c_to]:\n                                        adj_relation = AdjRelation(temp_pos, tab[r][c_to], AdjRelation.DIR_HORIZ)\n                                        retVal.append(adj_relation)\n                            else:\n                                # find the next non-blank cell, if exists\n                                for temp in range(c_from + 1, self._maxCol + 1):\n                                    if tab[r][temp] != 0:\n                                        if type(tab[r][temp]) == list:\n                                            for cell_to in tab[r][temp]:\n                                                adj_relation = AdjRelation(temp_pos, cell_to,\n                                                                           AdjRelation.DIR_HORIZ)\n                                                retVal.append(adj_relation)\n                                        else:\n                                            adj_relation = AdjRelation(temp_pos, tab[r][temp], AdjRelation.DIR_HORIZ)\n                                            retVal.append(adj_relation)\n                                        break\n\n                # find vertical relations\n                for c in range(self._maxCol+1):\n                    for r_from in range(self._maxRow):\n                        temp_pos = tab[r_from][c]\n                        if temp_pos == 0:\n                            continue\n                        elif type(temp_pos) == list:\n                            for cell in temp_pos:\n                                r_to = r_from + 1\n                                if tab[r_to][c] != 0:\n                                    # find relation between two adjacent cells\n                                    if type(tab[r_to][c]) == list:\n                                        for cell_to in tab[r_to][c]:\n                                            if cell != cell_to and (not cell.check_same(cell_to)):\n                                                adj_relation = AdjRelation(cell, cell_to, AdjRelation.DIR_VERT)\n                                                retVal.append(adj_relation)\n                                    else:\n                                        if cell != tab[r_to][c]:\n                                            adj_relation = AdjRelation(cell, tab[r_to][c], AdjRelation.DIR_VERT)\n                                            retVal.append(adj_relation)\n                                else:\n                                    # find the next non-blank cell, if exists\n                                    for temp in range(r_from + 1, self._maxRow + 1):\n                                        if tab[temp][c] != 0:\n                                            if type(tab[temp][c]) == list:\n                                                for cell_to in tab[temp][c]:\n                                                    adj_relation = AdjRelation(cell, cell_to,\n                                                                               AdjRelation.DIR_VERT)\n                                                    retVal.append(adj_relation)\n                                            else:\n                                                adj_relation = AdjRelation(cell, tab[temp][c],\n                                                                           AdjRelation.DIR_VERT)\n                                                retVal.append(adj_relation)\n                                            break\n                        else:\n                            r_to = r_from + 1\n                            if tab[r_to][c] != 0:\n                                # find relation between two adjacent cells\n                                if type(tab[r_to][c]) == list:\n                                    for cell_to in tab[r_to][c]:\n                                        if temp_pos != cell_to:\n                                            adj_relation = AdjRelation(temp_pos, cell_to, AdjRelation.DIR_VERT)\n                                            retVal.append(adj_relation)\n                                else:\n                                    if temp_pos != tab[r_to][c]:\n                                        adj_relation = AdjRelation(temp_pos, tab[r_to][c], AdjRelation.DIR_VERT)\n                                        retVal.append(adj_relation)\n                            else:\n                                # find the next non-blank cell, if exists\n                                for temp in range(r_from + 1, self._maxRow + 1):\n                                    if tab[temp][c] != 0:\n                                        if type(tab[temp][c]) == list:\n                                            for cell_to in tab[temp][c]:\n                                                adj_relation = AdjRelation(temp_pos, cell_to, AdjRelation.DIR_VERT)\n                                                retVal.append(adj_relation)\n                                        else:\n                                            adj_relation = AdjRelation(temp_pos, tab[temp][c], AdjRelation.DIR_VERT)\n                                            retVal.append(adj_relation)\n                                        break\n\n                # eliminate duplicates\n                repeat = True\n                while repeat:\n                    repeat = False\n                    duplicates = []\n\n                    for ar1 in retVal:\n                        for ar2 in retVal:\n                            if ar1 != ar2:\n                                if ar1.direction == ar2.direction and ar1.fromText == ar2.fromText and\\\n                                        ar1.toText == ar2.toText:\n                                    duplicates.append(ar2)\n                                    break\n                        else:\n                            continue\n                        break\n\n                    if len(duplicates) > 0:\n                        repeat = True\n                        retVal.remove(duplicates[0])\n\n                self.found = True\n                self.adj_relations = retVal\n            return self.adj_relations\n\n    # compute the IOU of table, pass-in var is another Table object\n    def compute_table_iou(self, another_table):\n        table_box_1_temp = []\n        for el in self.table_coords.split():\n            table_box_1_temp.append((el.split(\",\")))\n        table_box_1 = list(flatten(table_box_1_temp))\n        table_box_1 = [int(x) for x in table_box_1]\n\n        table_box_2_temp = []\n        for el in another_table.table_coords.split():\n            table_box_2_temp.append((el.split(\",\")))\n        table_box_2 = list(flatten(table_box_2_temp))\n        table_box_2 = [int(x) for x in table_box_2]\n\n        return compute_poly_iou(table_box_1, table_box_2)\n\n    # find the cell mapping of tables as dictionary, pass-in var is another table and the desired IOU value\n    def find_cell_mapping(self, target_table, iou_value):\n        mapped_cell = []    # store the matches as tuples - (gt, result) mind the order of table when passing in\n        for cell_1 in self.table_cells:\n            for cell_2 in target_table.table_cells:\n                if cell_1.compute_cell_iou(cell_2) >= iou_value:\n                    mapped_cell.append((cell_1, cell_2))\n                    break\n        ret = dict(mapped_cell)\n        # print(ret)\n        return ret\n\n    # to print a table cell mapping\n    @classmethod\n    def printCellMapping(cls, dMappedCell):\n        print(\"-\"*25)\n        for cell1, cell2 in dMappedCell.items():\n            print(\"  \", cell1, \" --> \", cell2)\n\n    # to print a table set of adjacency relations\n    @classmethod\n    def printAdjacencyRelationList(cls, lAdjRel, title=\"\"):\n        print(\"--- %s \"%title + \"-\"*25)\n        for adj in lAdjRel:\n            print(adj)\n\n\nclass ResultStructure:\n\n    def __init__(self, truePos, gtTotal, resTotal):\n        self._truePos = truePos\n        self._gtTotal = gtTotal\n        self._resTotal = resTotal\n\n    @property\n    def truePos(self):\n        return self._truePos\n\n    @property\n    def gtTotal(self):\n        return self._gtTotal\n\n    @property\n    def resTotal(self):\n        return self._resTotal\n\n    def __str__(self):\n        return \"true: {}, gt: {}, res: {}\".format(self._truePos, self._gtTotal, self._resTotal)"}
{"type": "source_file", "path": "utils/ditod/mycheckpointer.py", "content": "from detectron2.checkpoint import DetectionCheckpointer\n\nfrom typing import Any\nimport torch\nimport torch.nn as nn\nfrom fvcore.common.checkpoint import _IncompatibleKeys, _strip_prefix_if_present, TORCH_VERSION, quantization, \\\n    ObserverBase, FakeQuantizeBase\nfrom torch import distributed as dist\nfrom scipy import interpolate\nimport numpy as np\nimport torch.nn.functional as F\n\n\ndef append_prefix(k):\n    prefix = 'backbone.bottom_up.backbone.'\n    return prefix + k if not k.startswith(prefix) else k\n\n\ndef modify_ckpt_state(model, state_dict, logger=None):\n    # reshape absolute position embedding for Swin\n    if state_dict.get(append_prefix('absolute_pos_embed')) is not None:\n        absolute_pos_embed = state_dict[append_prefix('absolute_pos_embed')]\n        N1, L, C1 = absolute_pos_embed.size()\n        N2, C2, H, W = model.backbone.bottom_up.backbone.absolute_pos_embed.size()\n        if N1 != N2 or C1 != C2 or L != H * W:\n            logger.warning(\"Error in loading absolute_pos_embed, pass\")\n        else:\n            state_dict[append_prefix('absolute_pos_embed')] = absolute_pos_embed.view(N2, H, W, C2).permute(0, 3, 1, 2)\n\n    def get_dist_info():\n        if dist.is_available() and dist.is_initialized():\n            rank = dist.get_rank()\n            world_size = dist.get_world_size()\n        else:\n            rank = 0\n            world_size = 1\n        return rank, world_size\n\n    rank, _ = get_dist_info()\n    all_keys = list(state_dict.keys())\n    for key in all_keys:\n        if \"relative_position_index\" in key:\n            state_dict.pop(key)\n\n        if \"relative_position_bias_table\" in key:\n            rel_pos_bias = state_dict[key]\n            src_num_pos, num_attn_heads = rel_pos_bias.size()\n            if key not in model.state_dict():\n                continue\n            dst_num_pos, _ = model.state_dict()[key].size()\n            dst_patch_shape = model.backbone.bottom_up.backbone.patch_embed.patch_shape\n            if dst_patch_shape[0] != dst_patch_shape[1]:\n                raise NotImplementedError()\n            num_extra_tokens = dst_num_pos - (dst_patch_shape[0] * 2 - 1) * (dst_patch_shape[1] * 2 - 1)\n            src_size = int((src_num_pos - num_extra_tokens) ** 0.5)\n            dst_size = int((dst_num_pos - num_extra_tokens) ** 0.5)\n            if src_size != dst_size:\n                if rank == 0:\n                    print(\"Position interpolate for %s from %dx%d to %dx%d\" % (\n                        key, src_size, src_size, dst_size, dst_size))\n                extra_tokens = rel_pos_bias[-num_extra_tokens:, :]\n                rel_pos_bias = rel_pos_bias[:-num_extra_tokens, :]\n\n                def geometric_progression(a, r, n):\n                    return a * (1.0 - r ** n) / (1.0 - r)\n\n                left, right = 1.01, 1.5\n                while right - left > 1e-6:\n                    q = (left + right) / 2.0\n                    gp = geometric_progression(1, q, src_size // 2)\n                    if gp > dst_size // 2:\n                        right = q\n                    else:\n                        left = q\n\n                # if q > 1.13492:\n                #     q = 1.13492\n\n                dis = []\n                cur = 1\n                for i in range(src_size // 2):\n                    dis.append(cur)\n                    cur += q ** (i + 1)\n\n                r_ids = [-_ for _ in reversed(dis)]\n\n                x = r_ids + [0] + dis\n                y = r_ids + [0] + dis\n\n                t = dst_size // 2.0\n                dx = np.arange(-t, t + 0.1, 1.0)\n                dy = np.arange(-t, t + 0.1, 1.0)\n                if rank == 0:\n                    print(\"x = {}\".format(x))\n                    print(\"dx = {}\".format(dx))\n\n                all_rel_pos_bias = []\n\n                for i in range(num_attn_heads):\n                    z = rel_pos_bias[:, i].view(src_size, src_size).float().numpy()\n                    f = interpolate.interp2d(x, y, z, kind='cubic')\n                    all_rel_pos_bias.append(\n                        torch.Tensor(f(dx, dy)).contiguous().view(-1, 1).to(rel_pos_bias.device))\n\n                rel_pos_bias = torch.cat(all_rel_pos_bias, dim=-1)\n                new_rel_pos_bias = torch.cat((rel_pos_bias, extra_tokens), dim=0)\n                state_dict[key] = new_rel_pos_bias\n\n    if append_prefix('pos_embed') in state_dict:\n        pos_embed_checkpoint = state_dict[append_prefix('pos_embed')]\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.backbone.bottom_up.backbone.patch_embed.num_patches\n        num_extra_tokens = model.backbone.bottom_up.backbone.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        # new_size = int(num_patches ** 0.5)\n        new_size_w = model.backbone.bottom_up.backbone.patch_embed.num_patches_w\n        new_size_h = model.backbone.bottom_up.backbone.patch_embed.num_patches_h\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size_h or orig_size != new_size_w:\n            if rank == 0:\n                print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size_w, new_size_h))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size_w, new_size_h), mode='bicubic', align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            state_dict[append_prefix('pos_embed')] = new_pos_embed\n\n    # interpolate position bias table if needed\n    relative_position_bias_table_keys = [k for k in state_dict.keys() if \"relative_position_bias_table\" in k]\n    for table_key in relative_position_bias_table_keys:\n        table_pretrained = state_dict[table_key]\n        if table_key not in model.state_dict():\n            continue\n        table_current = model.state_dict()[table_key]\n        L1, nH1 = table_pretrained.size()\n        L2, nH2 = table_current.size()\n        if nH1 != nH2:\n            logger.warning(f\"Error in loading {table_key}, pass\")\n        else:\n            if L1 != L2:\n                S1 = int(L1 ** 0.5)\n                S2 = int(L2 ** 0.5)\n                table_pretrained_resized = F.interpolate(\n                    table_pretrained.permute(1, 0).view(1, nH1, S1, S1),\n                    size=(S2, S2), mode='bicubic')\n                state_dict[table_key] = table_pretrained_resized.view(nH2, L2).permute(1, 0)\n\n    if append_prefix('rel_pos_bias.relative_position_bias_table') in state_dict and \\\n            model.backbone.bottom_up.backbone.use_rel_pos_bias and \\\n            not model.backbone.bottom_up.backbone.use_shared_rel_pos_bias and \\\n            append_prefix('blocks.0.attn.relative_position_bias_table') not in state_dict:\n        logger.info(\"[BEIT] Expand the shared relative position embedding to each transformer block. \")\n        num_layers = model.backbone.bottom_up.backbone.get_num_layers()\n        rel_pos_bias = state_dict[append_prefix(\"rel_pos_bias.relative_position_bias_table\")]\n        for i in range(num_layers):\n            state_dict[\"blocks.%d.attn.relative_position_bias_table\" % i] = rel_pos_bias.clone()\n        state_dict.pop(append_prefix(\"rel_pos_bias.relative_position_bias_table\"))\n\n    return state_dict\n\n\nclass MyDetectionCheckpointer(DetectionCheckpointer):\n    def _load_model(self, checkpoint: Any) -> _IncompatibleKeys:\n        \"\"\"\n        Load weights from a checkpoint.\n\n        Args:\n            checkpoint (Any): checkpoint contains the weights.\n\n        Returns:\n            ``NamedTuple`` with ``missing_keys``, ``unexpected_keys``,\n                and ``incorrect_shapes`` fields:\n                * **missing_keys** is a list of str containing the missing keys\n                * **unexpected_keys** is a list of str containing the unexpected keys\n                * **incorrect_shapes** is a list of (key, shape in checkpoint, shape in model)\n\n            This is just like the return value of\n            :func:`torch.nn.Module.load_state_dict`, but with extra support\n            for ``incorrect_shapes``.\n        \"\"\"\n        checkpoint_state_dict = checkpoint.pop(\"model\")\n        self._convert_ndarray_to_tensor(checkpoint_state_dict)\n\n        # if the state_dict comes from a model that was wrapped in a\n        # DataParallel or DistributedDataParallel during serialization,\n        # remove the \"module\" prefix before performing the matching.\n        _strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n\n        # workaround https://github.com/pytorch/pytorch/issues/24139\n        model_state_dict = self.model.state_dict()\n        incorrect_shapes = []\n\n        # rename the para in checkpoint_state_dict\n        # some bug here, do not support re load\n\n        checkpoint_state_dict = {\n            append_prefix(k): checkpoint_state_dict[k]\n            for k in checkpoint_state_dict.keys()\n        }\n\n        checkpoint_state_dict = modify_ckpt_state(self.model, checkpoint_state_dict, logger=self.logger)\n\n        for k in list(checkpoint_state_dict.keys()):\n            if k in model_state_dict:\n                model_param = model_state_dict[k]\n                # Allow mismatch for uninitialized parameters\n                if TORCH_VERSION >= (1, 8) and isinstance(\n                        model_param, nn.parameter.UninitializedParameter\n                ):\n                    continue\n                shape_model = tuple(model_param.shape)\n                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n                if shape_model != shape_checkpoint:\n\n                    has_observer_base_classes = (\n                            TORCH_VERSION >= (1, 8)\n                            and hasattr(quantization, \"ObserverBase\")\n                            and hasattr(quantization, \"FakeQuantizeBase\")\n                    )\n                    if has_observer_base_classes:\n                        # Handle the special case of quantization per channel observers,\n                        # where buffer shape mismatches are expected.\n                        def _get_module_for_key(\n                                model: torch.nn.Module, key: str\n                        ) -> torch.nn.Module:\n                            # foo.bar.param_or_buffer_name -> [foo, bar]\n                            key_parts = key.split(\".\")[:-1]\n                            cur_module = model\n                            for key_part in key_parts:\n                                cur_module = getattr(cur_module, key_part)\n                            return cur_module\n\n                        cls_to_skip = (\n                            ObserverBase,\n                            FakeQuantizeBase,\n                        )\n                        target_module = _get_module_for_key(self.model, k)\n                        if isinstance(target_module, cls_to_skip):\n                            # Do not remove modules with expected shape mismatches\n                            # them from the state_dict loading. They have special logic\n                            # in _load_from_state_dict to handle the mismatches.\n                            continue\n\n                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n                    checkpoint_state_dict.pop(k)\n        incompatible = self.model.load_state_dict(checkpoint_state_dict, strict=False)\n        return _IncompatibleKeys(\n            missing_keys=incompatible.missing_keys,\n            unexpected_keys=incompatible.unexpected_keys,\n            incorrect_shapes=incorrect_shapes,\n        )\n"}
{"type": "source_file", "path": "utils/ditod/__init__.py", "content": "# --------------------------------------------------------------------------------\n# MPViT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n\nfrom .config import add_vit_config\nfrom .backbone import build_vit_fpn_backbone\nfrom .dataset_mapper import DetrDatasetMapper\nfrom .mycheckpointer import MyDetectionCheckpointer\nfrom .icdar_evaluation import ICDAREvaluator\nfrom .mytrainer import MyTrainer\nfrom .table_evaluation import calc_table_score"}
{"type": "source_file", "path": "utils/ditod/backbone.py", "content": "# --------------------------------------------------------------------------------\n# VIT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n# References:\n# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# CoaT: https://github.com/mlpc-ucsd/CoaT\n# --------------------------------------------------------------------------------\n\n\nimport torch\n\nfrom detectron2.layers import (\n    ShapeSpec,\n)\nfrom detectron2.modeling import Backbone, BACKBONE_REGISTRY, FPN\nfrom detectron2.modeling.backbone.fpn import LastLevelP6P7, LastLevelMaxPool\n\nfrom .beit import beit_base_patch16, dit_base_patch16, dit_large_patch16, beit_large_patch16\nfrom .deit import deit_base_patch16, mae_base_patch16\n\n__all__ = [\n    \"build_vit_fpn_backbone\",\n]\n\n\nclass VIT_Backbone(Backbone):\n    \"\"\"\n    Implement VIT backbone.\n    \"\"\"\n\n    def __init__(self, name, out_features, drop_path, img_size, pos_type, model_kwargs):\n        super().__init__()\n        self._out_features = out_features\n        if 'base' in name:\n            self._out_feature_strides = {\"layer3\": 4, \"layer5\": 8, \"layer7\": 16, \"layer11\": 32}\n        else:\n            self._out_feature_strides = {\"layer7\": 4, \"layer11\": 8, \"layer15\": 16, \"layer23\": 32}\n\n        if name == 'beit_base_patch16':\n            model_func = beit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == 'dit_base_patch16':\n            model_func = dit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"deit_base_patch16\":\n            model_func = deit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"mae_base_patch16\":\n            model_func = mae_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"dit_large_patch16\":\n            model_func = dit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        elif name == \"beit_large_patch16\":\n            model_func = beit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        else:\n            raise ValueError(\"Unsupported VIT name yet.\")\n\n        if 'beit' in name or 'dit' in name:\n            if pos_type == \"abs\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_abs_pos_emb=True,\n                                           **model_kwargs)\n            elif pos_type == \"shared_rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_shared_rel_pos_bias=True,\n                                           **model_kwargs)\n            elif pos_type == \"rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_rel_pos_bias=True,\n                                           **model_kwargs)\n            else:\n                raise ValueError()\n        else:\n            self.backbone = model_func(img_size=img_size,\n                                       out_features=out_features,\n                                       drop_path_rate=drop_path,\n                                       **model_kwargs)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n\n        Returns:\n            dict[str->Tensor]: names and the corresponding features\n        \"\"\"\n        assert x.dim() == 4, f\"VIT takes an input of shape (N, C, H, W). Got {x.shape} instead!\"\n        return self.backbone.forward_features(x)\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }\n\n\ndef build_VIT_backbone(cfg):\n    \"\"\"\n    Create a VIT instance from config.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        A VIT backbone instance.\n    \"\"\"\n    # fmt: off\n    name = cfg.MODEL.VIT.NAME\n    out_features = cfg.MODEL.VIT.OUT_FEATURES\n    drop_path = cfg.MODEL.VIT.DROP_PATH\n    img_size = cfg.MODEL.VIT.IMG_SIZE\n    pos_type = cfg.MODEL.VIT.POS_TYPE\n\n    model_kwargs = eval(str(cfg.MODEL.VIT.MODEL_KWARGS).replace(\"`\", \"\"))\n\n    return VIT_Backbone(name, out_features, drop_path, img_size, pos_type, model_kwargs)\n\n\n@BACKBONE_REGISTRY.register()\ndef build_vit_fpn_backbone(cfg, input_shape: ShapeSpec):\n    \"\"\"\n    Create a VIT w/ FPN backbone.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"\n    bottom_up = build_VIT_backbone(cfg)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = FPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n"}
{"type": "source_file", "path": "utils/ditod_vgt/FeatureMerge.py", "content": "import torch\nfrom torch import nn\n\nclass FeatureMerge(nn.Module):\n    \"\"\"Multimodal feature fusion used in VSR.\"\"\"\n    def __init__(self,\n                 feature_names,\n                 visual_dim,\n                 semantic_dim,\n                 merge_type='Sum',\n                 dropout_ratio=0.1,\n                 with_extra_fc=True,\n                 shortcut=False\n                 ):\n        \"\"\"Multimodal feature merge used in VSR.\n        Args:\n            visual_dim (list): the dim of visual features, e.g. [256]\n            semantic_dim (list): the dim of semantic features, e.g. [256]\n            merge_type (str): fusion type, e.g. 'Sum', 'Concat', 'Weighted'\n            dropout_ratio (float): dropout ratio of fusion features\n            with_extra_fc (bool): whether add extra fc layers for adaptation\n            shortcut (bool): whether add shortcut connection\n        \"\"\"\n        super().__init__()\n\n        # merge param\n        self.feature_names = feature_names\n        self.merge_type = merge_type\n        self.visual_dim = visual_dim\n        self.textual_dim = semantic_dim\n        self.with_extra_fc = with_extra_fc\n        self.shortcut = shortcut\n        self.relu = nn.ReLU(inplace=True)\n        \n        if self.merge_type == 'Sum':\n            assert len(self.visual_dim) == len(self.textual_dim)\n        elif self.merge_type == 'Concat':\n            assert len(self.visual_dim) == len(self.textual_dim)\n            # self.concat_proj = nn.ModuleList()\n            \n            self.vis_proj = nn.ModuleList()\n            self.text_proj = nn.ModuleList()\n            self.alpha_proj = nn.ModuleList()\n            \n            for idx in range(len(self.visual_dim)):\n                # self.concat_proj.append(nn.Conv2d(self.visual_dim[idx] + self.textual_dim[idx], self.visual_dim[idx], kernel_size = (1,1), stride=1))\n                if self.with_extra_fc:\n                    self.vis_proj.append(nn.Linear(self.visual_dim[idx], self.visual_dim[idx]))\n                    self.text_proj.append(nn.Linear(self.textual_dim[idx], self.textual_dim[idx]))\n                self.alpha_proj.append(nn.Linear(self.visual_dim[idx] + self.textual_dim[idx], self.visual_dim[idx]))\n            \n        elif self.merge_type == 'Weighted':\n            assert len(self.visual_dim) == len(self.textual_dim)\n            self.total_num = len(self.visual_dim)\n\n            # vis projection\n            self.vis_proj = nn.ModuleList()\n            self.vis_proj_relu = nn.ModuleList()\n\n            # text projection\n            self.text_proj = nn.ModuleList()\n            self.text_proj_relu = nn.ModuleList()\n\n            self.alpha_proj = nn.ModuleList()\n            for idx in range(self.total_num):\n                if self.with_extra_fc:\n                    self.vis_proj.append(nn.Linear(self.visual_dim[idx], self.visual_dim[idx]))\n                    self.text_proj.append(nn.Linear(self.textual_dim[idx], self.textual_dim[idx]))\n                self.alpha_proj.append(nn.Linear(self.visual_dim[idx] + self.textual_dim[idx], self.visual_dim[idx]))\n\n        else:\n            raise \"Unknown merge type {}\".format(self.merge_type)\n\n        self.dropout = nn.Dropout(dropout_ratio)\n\n        # visual context\n        # self.visual_ap = nn.AdaptiveAvgPool2d((1, 1))\n\n\n    def forward(self, visual_feat=None, textual_feat=None):\n        \"\"\" Forward computation\n        Args:\n            visual_feat (list(Tensor)): visual feature maps, in shape of [L x C x H x W] x B\n            textual_feat (Tensor): textual feature maps, in shape of B x L x C\n        Returns:\n            Tensor: fused feature maps, in shape of [B x L x C]\n        \"\"\"\n        assert len(visual_feat) == len(textual_feat)\n\n        # feature merge\n        merged_feat = {}\n        if self.merge_type == 'Sum':\n            for name in self.feature_names:\n                merged_feat[name] = visual_feat[name] + textual_feat[name]\n        elif self.merge_type == 'Concat':\n            for idx, name in enumerate(self.feature_names):\n                # merged_feat[name] = self.concat_proj[idx](torch.cat((visual_feat[name],textual_feat[name]),1))\n                per_vis = visual_feat[name].permute(0, 2, 3, 1)\n                per_text = textual_feat[name].permute(0, 2, 3, 1)\n                if self.with_extra_fc:\n                    per_vis = self.relu(self.vis_proj[idx](per_vis))\n                    per_text = self.relu(self.text_proj[idx](per_text))\n                x_sentence = self.alpha_proj[idx](torch.cat((per_vis, per_text), -1))\n                x_sentence = x_sentence.permute(0,3,1,2).contiguous()\n                merged_feat[name] = x_sentence\n        else:\n            assert self.total_num == len(visual_feat) or self.total_num == 1\n            # for per_vis, per_text in zip(visual_feat, textual_feat):\n            for idx, name in enumerate(self.feature_names):\n                per_vis = visual_feat[name].permute(0, 2, 3, 1)\n                per_text = textual_feat[name].permute(0, 2, 3, 1)\n                if self.with_extra_fc:\n                    per_vis = self.relu(self.vis_proj[idx](per_vis))\n                    per_text = self.relu(self.text_proj[idx](per_text))\n\n                alpha = torch.sigmoid(self.alpha_proj[idx](torch.cat((per_vis, per_text), -1)))\n                if self.shortcut:\n                    # shortcut\n                    x_sentence = per_vis + alpha * per_text\n                else:\n                    # selection\n                    x_sentence = alpha * per_vis + (1 - alpha) * per_text\n\n                x_sentence = x_sentence.permute(0,3,1,2).contiguous()\n                merged_feat[name] = x_sentence\n\n        return merged_feat"}
{"type": "source_file", "path": "utils/ditod_vgt/Wordnn_embedding.py", "content": "import numpy as np\nimport torch\nfrom torch import nn\nfrom .tokenization_bros import BrosTokenizer\n\ndef _init_weights(m):\n    if isinstance(m, nn.Linear):\n        # we use xavier_uniform following official JAX ViT:\n        torch.nn.init.xavier_uniform_(m.weight)\n        if isinstance(m, nn.Linear) and m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n    elif isinstance(m, nn.LayerNorm):\n        nn.init.constant_(m.bias, 0)\n        nn.init.constant_(m.weight, 1.0)\n        \nclass WordnnEmbedding(nn.Module):\n    \"\"\"Generate chargrid embedding feature map.\n    \"\"\"\n    def __init__(self,\n                 vocab_size=30552,\n                 hidden_size=768,\n                 embedding_dim=64,\n                 bros_embedding_path=\"/bros-base-uncased/\",\n                 use_pretrain_weight=True,\n                 use_UNK_text=False):\n        \"\"\"\n        Args\n            vocab_size (int): size of vocabulary.\n            embedding_dim (int): dim of input features\n        \"\"\"\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, hidden_size)\n        self.embedding_proj = nn.Linear(hidden_size, embedding_dim, bias=False)\n        # self.tokenizer = BrosTokenizer.from_pretrained(bros_embedding_path)\n        self.use_pretrain_weight = use_pretrain_weight\n        self.use_UNK_text = use_UNK_text\n        \n        self.init_weights(bros_embedding_path)\n        self.apply(_init_weights)\n\n    def init_weights(self, bros_embedding_path):\n        if self.use_pretrain_weight:\n            state_dict = torch.load(bros_embedding_path + \"pytorch_model.bin\", map_location='cpu')\n            if 'bert' in bros_embedding_path:\n                word_embs = state_dict[\"bert.embeddings.word_embeddings.weight\"]\n            elif 'bros' in bros_embedding_path:\n                word_embs = state_dict[\"embeddings.word_embeddings.weight\"]\n            elif 'layoutlm' in bros_embedding_path:\n                word_embs = state_dict[\"layoutlm.embeddings.word_embeddings.weight\"]\n            else:\n                print(\"Wrong bros_embedding_path!\")\n            self.embedding = nn.Embedding.from_pretrained(word_embs)\n            print(\"use_pretrain_weight: load model from:\", bros_embedding_path)\n        \n    def forward(self, img, batched_inputs, stride = 1):\n        \"\"\" Forward computation\n        Args:\n            img (Tensor): in shape of [B x 3 x H x W]\n            batched_inputs (list[dict]): \n        Returns:\n            Tensor: in shape of [B x N x L x D], where D is the embedding_dim.\n        \"\"\"\n        device = img.device\n        batch_b, _, batch_h, batch_w = img.size()\n\n        chargrid_map = torch.zeros((batch_b, batch_h // stride, batch_w // stride ), dtype=torch.int64).to(device)\n        \n        for iter_b in range(batch_b):\n            per_input_ids = batched_inputs[iter_b][\"input_ids\"]   \n            per_input_bbox = batched_inputs[iter_b][\"bbox\"]\n            \n            short_length_w = min(len(per_input_ids), len(per_input_bbox)) \n            \n            if short_length_w > 0 : \n                for word_idx in range(short_length_w): \n                    per_id = per_input_ids[word_idx]\n                    \n                    bbox = per_input_bbox[word_idx] / stride\n                    w_start, h_start, w_end, h_end = bbox.round().astype(np.int).tolist()\n                            \n                    if self.use_UNK_text:\n                        chargrid_map[iter_b, h_start:h_end, w_start: w_end] = 100\n                    else:\n                        chargrid_map[iter_b, h_start:h_end, w_start: w_end] = per_id\n\n        chargrid_map = self.embedding(chargrid_map)\n        chargrid_map = self.embedding_proj(chargrid_map)\n        \n        return chargrid_map.permute(0, 3, 1, 2).contiguous()\n        \n    "}
{"type": "source_file", "path": "modules/layout/base.py", "content": "from abc import ABC, abstractmethod\n\nclass LayoutBase(ABC):\n    @abstractmethod\n    def init(self, cfg: dict):\n        pass\n\n\n    @abstractmethod\n    def get_layout(self, text: str):\n        \"\"\"\n        Translates a given string into another language.\n\n        Parameters:\n        - text (str): The text to be translated.\n\n        Returns:\n        - str: The translated text.\n\n        This method needs to be implemented by subclasses.\n        \"\"\"\n        pass\n"}
{"type": "source_file", "path": "utils/ditod_vgt/VGTbackbone.py", "content": "# --------------------------------------------------------------------------------\n# VIT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# This source code is licensed(Dual License(GPL3.0 & Commercial)) under the license found in the\n# LICENSE file in the root directory of this source tree.\n# --------------------------------------------------------------------------------\n# References:\n# timm: https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# CoaT: https://github.com/mlpc-ucsd/CoaT\n# --------------------------------------------------------------------------------\n\n\nimport torch\nimport torch.nn.functional as F\nimport logging\n\nfrom detectron2.layers import (\n    ShapeSpec,\n)\nfrom detectron2.modeling import Backbone, BACKBONE_REGISTRY, FPN\nfrom detectron2.modeling.backbone.fpn import LastLevelP6P7, LastLevelMaxPool\n\nfrom .VGTbeit import beit_base_patch16, dit_base_patch16, dit_large_patch16, beit_large_patch16, VGT_dit_base_patch16\nfrom .FeatureMerge import FeatureMerge\n\n__all__ = [\n    \"build_VGT_fpn_backbone\",\n]\n\n\nclass PTM_VIT_Backbone(Backbone):\n    \"\"\"\n    Implement VIT backbone.\n    \"\"\"\n\n    def __init__(self, name, out_features, drop_path, img_size, pos_type, merge_type, model_kwargs):\n        super().__init__()\n        self._out_features = out_features\n        if 'base' in name:\n            self._out_feature_strides = {\"layer3\": 4, \"layer5\": 8, \"layer7\": 16, \"layer11\": 32}\n        else:\n            self._out_feature_strides = {\"layer7\": 4, \"layer11\": 8, \"layer15\": 16, \"layer23\": 32}\n\n        if name == 'beit_base_patch16':\n            model_func = beit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == 'dit_base_patch16':\n            model_func = dit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"deit_base_patch16\":\n            model_func = deit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == 'VGT_dit_base_patch16':\n            model_func = VGT_dit_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"mae_base_patch16\":\n            model_func = mae_base_patch16\n            self._out_feature_channels = {\"layer3\": 768, \"layer5\": 768, \"layer7\": 768, \"layer11\": 768}\n        elif name == \"dit_large_patch16\":\n            model_func = dit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        elif name == \"beit_large_patch16\":\n            model_func = beit_large_patch16\n            self._out_feature_channels = {\"layer7\": 1024, \"layer11\": 1024, \"layer15\": 1024, \"layer23\": 1024}\n        else:\n            raise ValueError(\"Unsupported VIT name yet.\")\n\n        if 'beit' in name or 'dit' in name:\n            if pos_type == \"abs\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_abs_pos_emb=True,\n                                           **model_kwargs)\n            elif pos_type == \"shared_rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_shared_rel_pos_bias=True,\n                                           **model_kwargs)\n            elif pos_type == \"rel\":\n                self.backbone = model_func(img_size=img_size,\n                                           out_features=out_features,\n                                           drop_path_rate=drop_path,\n                                           use_rel_pos_bias=True,\n                                           **model_kwargs)\n            else:\n                raise ValueError()\n        else:\n            self.backbone = model_func(img_size=img_size,\n                                       out_features=out_features,\n                                       drop_path_rate=drop_path,\n                                       **model_kwargs)\n        \n        logger = logging.getLogger(\"detectron2\")\n        logger.info(\"Merge using: {}\".format(merge_type))\n        self.FeatureMerge = FeatureMerge(feature_names = self._out_features, visual_dim=[768,768,768,768], semantic_dim=[768,768,768,768], merge_type = merge_type)\n\n    def forward(self, x, grid):\n        \"\"\"\n        Args:\n            x: Tensor of shape (N,C,H,W). H, W must be a multiple of ``self.size_divisibility``.\n\n        Returns:\n            dict[str->Tensor]: names and the corresponding features\n        \"\"\"\n        assert x.dim() == 4, f\"VIT takes an input of shape (N, C, H, W). Got {x.shape} instead!\"\n        \n        vis_feat_out, grid_feat_out = self.backbone.forward_features(x, grid)\n        return self.FeatureMerge.forward(vis_feat_out, grid_feat_out)\n        # return self.backbone.forward_features(x)\n\n    def output_shape(self):\n        return {\n            name: ShapeSpec(\n                channels=self._out_feature_channels[name], stride=self._out_feature_strides[name]\n            )\n            for name in self._out_features\n        }\n\n\nclass GridFPN(FPN):\n    def forward(self, x, grid):\n        \"\"\"\n        Args:\n            input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\n                feature map tensor for each feature level in high to low resolution order.\n        Returns:\n            dict[str->Tensor]:\n                mapping from feature map name to FPN feature map tensor\n                in high to low resolution order. Returned feature names follow the FPN\n                paper convention: \"p<stage>\", where stage has stride = 2 ** stage e.g.,\n                [\"p2\", \"p3\", ..., \"p6\"].\n        \"\"\"\n        bottom_up_features = self.bottom_up(x, grid)\n        results = []\n        prev_features = self.lateral_convs[0](bottom_up_features[self.in_features[-1]])\n        results.append(self.output_convs[0](prev_features))\n\n        # Reverse feature maps into top-down order (from low to high resolution)\n        for idx, (lateral_conv, output_conv) in enumerate(\n            zip(self.lateral_convs, self.output_convs)\n        ):\n            # Slicing of ModuleList is not supported https://github.com/pytorch/pytorch/issues/47336\n            # Therefore we loop over all modules but skip the first one\n            if idx > 0:\n                features = self.in_features[-idx - 1]\n                features = bottom_up_features[features]\n                top_down_features = F.interpolate(prev_features, scale_factor=2.0, mode=\"nearest\")\n                lateral_features = lateral_conv(features)\n                prev_features = lateral_features + top_down_features\n                if self._fuse_type == \"avg\":\n                    prev_features /= 2\n                results.insert(0, output_conv(prev_features))\n\n        if self.top_block is not None:\n            if self.top_block.in_feature in bottom_up_features:\n                top_block_in_feature = bottom_up_features[self.top_block.in_feature]\n            else:\n                top_block_in_feature = results[self._out_features.index(self.top_block.in_feature)]\n            results.extend(self.top_block(top_block_in_feature))\n        assert len(self._out_features) == len(results)\n        return {f: res for f, res in zip(self._out_features, results)}\n    \ndef build_PTM_VIT_Backbone(cfg):\n    \"\"\"\n    Create a VIT instance from config.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        A VIT backbone instance.\n    \"\"\"\n    # fmt: off\n    name = cfg.MODEL.VIT.NAME\n    out_features = cfg.MODEL.VIT.OUT_FEATURES\n    drop_path = cfg.MODEL.VIT.DROP_PATH\n    img_size = cfg.MODEL.VIT.IMG_SIZE\n    pos_type = cfg.MODEL.VIT.POS_TYPE\n    merge_type = cfg.MODEL.VIT.MERGE_TYPE\n    \n    model_kwargs = eval(str(cfg.MODEL.VIT.MODEL_KWARGS).replace(\"`\", \"\"))\n\n    return PTM_VIT_Backbone(name, out_features, drop_path, img_size, pos_type, merge_type, model_kwargs)\n\n\n@BACKBONE_REGISTRY.register()\ndef build_VGT_fpn_backbone(cfg, input_shape: ShapeSpec):\n    \"\"\"\n    Create a VIT w/ FPN backbone.\n\n    Args:\n        cfg: a detectron2 CfgNode\n\n    Returns:\n        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n    \"\"\"\n    bottom_up = build_PTM_VIT_Backbone(cfg)\n    in_features = cfg.MODEL.FPN.IN_FEATURES\n    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n    backbone = GridFPN(\n        bottom_up=bottom_up,\n        in_features=in_features,\n        out_channels=out_channels,\n        norm=cfg.MODEL.FPN.NORM,\n        top_block=LastLevelMaxPool(),\n        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n    )\n    return backbone\n"}
{"type": "source_file", "path": "utils/ditod/dataset_mapper.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# from https://github.com/facebookresearch/detr/blob/main/d2/detr/dataset_mapper.py\n\n\nimport copy\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.data import transforms as T\n\n__all__ = [\"DetrDatasetMapper\"]\n\n\ndef build_transform_gen(cfg, is_train):\n    \"\"\"\n    Create a list of :class:`TransformGen` from config.\n    Returns:\n        list[TransformGen]\n    \"\"\"\n    if is_train:\n        min_size = cfg.INPUT.MIN_SIZE_TRAIN\n        max_size = cfg.INPUT.MAX_SIZE_TRAIN\n        sample_style = cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING\n    else:\n        min_size = cfg.INPUT.MIN_SIZE_TEST\n        max_size = cfg.INPUT.MAX_SIZE_TEST\n        sample_style = \"choice\"\n    if sample_style == \"range\":\n        assert len(min_size) == 2, \"more than 2 ({}) min_size(s) are provided for ranges\".format(len(min_size))\n\n    logger = logging.getLogger(__name__)\n    tfm_gens = []\n    if is_train:\n        tfm_gens.append(T.RandomFlip())\n    tfm_gens.append(T.ResizeShortestEdge(min_size, max_size, sample_style))\n    if is_train:\n        logger.info(\"TransformGens used in training: \" + str(tfm_gens))\n    return tfm_gens\n\n\nclass DetrDatasetMapper:\n    \"\"\"\n    A callable which takes a dataset dict in Detectron2 Dataset format,\n    and map it into a format used by DETR.\n\n    The callable currently does the following:\n\n    1. Read the image from \"file_name\"\n    2. Applies geometric transforms to the image and annotation\n    3. Find and applies suitable cropping to the image and annotation\n    4. Prepare image and annotation to Tensors\n    \"\"\"\n\n    def __init__(self, cfg, is_train=True):\n        if cfg.INPUT.CROP.ENABLED and is_train:\n            self.crop_gen = [\n                T.ResizeShortestEdge([400, 500, 600], sample_style=\"choice\"),\n                T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE),\n            ]\n        else:\n            self.crop_gen = None\n\n        self.mask_on = cfg.MODEL.MASK_ON\n        self.tfm_gens = build_transform_gen(cfg, is_train)\n        logging.getLogger(__name__).info(\n            \"Full TransformGens used in training: {}, crop: {}\".format(str(self.tfm_gens), str(self.crop_gen))\n        )\n\n        self.img_format = cfg.INPUT.FORMAT\n        self.is_train = is_train\n\n    def __call__(self, dataset_dict):\n        \"\"\"\n        Args:\n            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n\n        Returns:\n            dict: a format that builtin models in detectron2 accept\n        \"\"\"\n        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n        utils.check_image_size(dataset_dict, image)\n\n        if self.crop_gen is None:\n            image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n        else:\n            if np.random.rand() > 0.5:\n                image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n            else:\n                image, transforms = T.apply_transform_gens(\n                    self.tfm_gens[:-1] + self.crop_gen + self.tfm_gens[-1:], image\n                )\n\n        image_shape = image.shape[:2]  # h, w\n\n        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n\n        if not self.is_train:\n            # USER: Modify this if you want to keep them for some reason.\n            dataset_dict.pop(\"annotations\", None)\n            return dataset_dict\n\n        if \"annotations\" in dataset_dict:\n            # USER: Modify this if you want to keep them for some reason.\n            for anno in dataset_dict[\"annotations\"]:\n                if not self.mask_on:\n                    anno.pop(\"segmentation\", None)\n                anno.pop(\"keypoints\", None)\n\n            # USER: Implement additional transformations if you have other types of data\n            annos = [\n                utils.transform_instance_annotations(obj, transforms, image_shape)\n                for obj in dataset_dict.pop(\"annotations\")\n                if obj.get(\"iscrowd\", 0) == 0\n            ]\n            instances = utils.annotations_to_instances(annos, image_shape)\n            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n        return dataset_dict"}
{"type": "source_file", "path": "server.py", "content": "import tempfile\nfrom pathlib import Path\nfrom typing import List, Tuple, Union\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PyPDF2\nimport uvicorn\nfrom fastapi import FastAPI, File, Form, UploadFile\nfrom fastapi.responses import FileResponse\n#from starlette.middleware.wsgi import WSGIMiddleware\nfrom pdf2image import convert_from_bytes, convert_from_path\nfrom PIL import Image, ImageDraw, ImageFont\nfrom pydantic import BaseModel, Field\nfrom tqdm import tqdm\nimport gradio as gr\n\n\nfrom utils import fw_fill, create_gradio_app, load_config, draw_text\nfrom modules import load_translator, load_layout_engine, load_ocr_engine, load_font_engine\n\n\n\n\ncfg = load_config('config.yaml', 'config.dev.yaml')\ntranslator = load_translator(cfg['translator'])\nlayout_engine = load_layout_engine(cfg['layout'])\nocr_engine = load_ocr_engine(cfg['ocr'])\nfont_engine = load_font_engine(cfg['font'])\n\n\n\nclass InputPdf(BaseModel):\n    \"\"\"Input PDF file.\"\"\"\n\n    input_pdf: UploadFile = Field(..., title=\"Input PDF file\")\n\n\nclass TranslateApi:\n    \"\"\"Translator API class.\n\n    Attributes\n    ----------\n    app: FastAPI\n        FastAPI instance\n    temp_dir: tempfile.TemporaryDirectory\n        Temporary directory for storing translated PDF files\n    temp_dir_name: Path\n        Path to the temporary directory\n    font: ImageFont\n        Font for drawing text on the image\n    layout_model: PPStructure\n        Layout model for detecting text blocks\n    ocr_model: PaddleOCR\n        OCR model for detecting text in the text blocks\n    translate_model: MarianMTModel\n        Translation model for translating text\n    translate_tokenizer: MarianTokenizer\n        Tokenizer for the translation model\n    \"\"\"\n\n    DPI = 200\n\n    def __init__(self, model_root_dir: Path = Path(\"/app/models/\")):\n        self.app = FastAPI()\n        self.app.add_api_route(\n            \"/translate_pdf/\",\n            self.translate_pdf,\n            methods=[\"POST\"],\n            response_class=FileResponse,\n        )\n        self.app.add_api_route(\n            \"/clear_temp_dir/\",\n            self.clear_temp_dir,\n            methods=[\"GET\"],\n        )\n\n        gradioapp = create_gradio_app(translator.get_languages())\n        gr.mount_gradio_app(self.app, gradioapp, '/')\n\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.temp_dir_name = Path(self.temp_dir.name)\n\n    \n    def run(self):\n        \"\"\"Run the API server\"\"\"\n        uvicorn.run(self.app, host=\"0.0.0.0\", port=8765)\n\n    async def translate_pdf(self, input_pdf: UploadFile = File(...), from_lang: str = Form(...), to_lang: str = Form(...), p_from: int = Form(...), p_to: int = Form(...), side_by_side: bool = Form(...) ) -> FileResponse:\n        \"\"\"API endpoint for translating PDF files.\"\"\"\n        input_pdf_data = await input_pdf.read()\n        self._translate_pdf(input_pdf_data, self.temp_dir_name, from_lang, to_lang, p_from, p_to, side_by_side)\n\n        return FileResponse(\n            self.temp_dir_name / \"translated.pdf\", media_type=\"application/pdf\"\n        )\n\n    async def clear_temp_dir(self):\n        \"\"\"API endpoint for clearing the temporary directory.\"\"\"\n        self.temp_dir.cleanup()\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.temp_dir_name = Path(self.temp_dir.name)\n        return {\"message\": \"temp dir cleared\"}\n\n    def _translate_pdf(\n        self, pdf_path_or_bytes: Union[Path, bytes], output_dir: Path, from_lang, to_lang, p_from, p_to, side_by_side\n    ) -> None:\n        \"\"\"Backend function for translating PDF files.\n\n        Translation is performed in the following steps:\n            1. Convert the PDF file to images\n            2. Detect text blocks in the images (layout detection)\n            3. For each text block, detect text (ocr)\n            4. translate the text\n            5. detect font properties\n            6. draw the new text        \n            7. Merge all PDF files into one PDF file\n\n        At 3, this function does not translate the text after\n        the references section. Instead, saves the image as it is.\n\n        Parameters\n        ----------\n        pdf_path_or_bytes: Union[Path, bytes]\n            Path to the input PDF file or bytes of the input PDF file\n        output_dir: Path\n            Path to the output directory\n        \"\"\"\n\n        if isinstance(pdf_path_or_bytes, Path):\n            pdf_images = convert_from_path(pdf_path_or_bytes, dpi=self.DPI)\n        else:\n            pdf_images = convert_from_bytes(pdf_path_or_bytes, dpi=self.DPI)\n        \n        pdf_files = []\n        \n        reached_references = False\n        for i, image in tqdm(enumerate(pdf_images)):\n            if i < p_from: continue\n            if i > p_to and p_to != 0: break\n            result = layout_engine.get_single_layout(image)\n            result = ocr_engine.get_all_text(result)\n            result = translator.translate_all(result, from_lang, to_lang)\n            result = font_engine.get_all_fonts(result)\n\n\n            output_path = output_dir / f\"{i:03}.pdf\"\n            if not reached_references:\n                # this function does steps 2-6\n                img, reached_references = self.__translate_one_page(\n                    image=image,\n                    result = result,\n                    reached_references=reached_references,\n                )\n                if side_by_side:\n                    fig, ax = plt.subplots(1, 2, figsize=(20, 14))\n                    ax[0].imshow(image)\n                    ax[1].imshow(img)\n                    ax[0].axis(\"off\")\n                    ax[1].axis(\"off\")\n                else:\n                    fig, ax = plt.subplots(1, 1, figsize=(10, 14))\n                    ax.imshow(img)\n                    ax.axis(\"off\")\n                plt.tight_layout()\n                plt.savefig(output_path, format=\"pdf\", dpi=self.DPI)\n                plt.close(fig)\n            else:\n                (\n                    image.convert(\"RGB\")\n                    .resize((int(1400 / image.size[1] * image.size[0]), 1400))\n                    .save(output_path, format=\"pdf\")\n                )\n\n            pdf_files.append(str(output_path))\n\n\n        # 7. merge\n        self.__merge_pdfs(pdf_files)\n\n\n    def __translate_one_page(\n        self,\n        image,\n        result,\n        reached_references: bool,\n    ) -> Tuple[np.ndarray, np.ndarray, bool]:\n        \"\"\"Translate one page of the PDF file.\"\"\"\n        img = np.array(image, dtype=np.uint8)\n        for line in result:\n            if line.type in [\"text\", \"list\"]:\n                if line.text:\n\n                    height = line.bbox[3] - line.bbox[1]\n                    width = line.bbox[2] - line.bbox[0]\n                    \n                    # calculate text wrapping\n                    processed_text = fw_fill(\n                        line.translated_text,\n                        width=int((width) / ((line.font['size'])/2.4))\n                        - 1,\n                    )\n\n                    fnt = ImageFont.truetype('fonts/' + line.font['family'], line.font['size']) \n                    \n                    # create new image block with new text\n                    new_block = Image.new(\"RGB\", ( width, height ), color=(255, 255, 255))\n                    draw = ImageDraw.Draw(new_block)\n                    draw_text(draw, processed_text, fnt, line.font['size'], width, line.font['ygain'])\n\n                    # copy over original image\n                    \n                    new_block = np.array(new_block)\n                    img[\n                        int(line.bbox[1]) : int(line.bbox[3]),\n                        int(line.bbox[0]) : int(line.bbox[2]),\n                    ] = new_block\n            \n            elif line.type == \"title\":\n                title = line.text\n                if title.lower() == \"references\" or title.lower() == \"reference\":\n                    reached_references = True\n\n            else:\n                # TODO: add list, table and image translation support\n                print(f\"\\n\\n\\nunknown: {line.type}\")\n\n        return img, reached_references\n\n    def __merge_pdfs(self, pdf_files: List[str]) -> None:\n        \"\"\"Merge translated PDF files into one file.\n\n        Merged file will be stored in the temp directory\n        as \"translated.pdf\".\n\n        Parameters\n        ----------\n        pdf_files: List[str]\n            List of paths to translated PDF files stored in\n            the temp directory.\n        \"\"\"\n        pdf_merger = PyPDF2.PdfMerger()\n\n        for pdf_file in sorted(pdf_files):\n            pdf_merger.append(pdf_file)\n        pdf_merger.write(self.temp_dir_name / \"translated.pdf\")\n\n\nif __name__ == \"__main__\":\n    translate_api = TranslateApi()\n    translate_api.run()\n"}
{"type": "source_file", "path": "utils/ditod_vgt/config.py", "content": "from detectron2.config import CfgNode as CN\n\n\ndef add_vit_config(cfg):\n    \"\"\"\n    Add config for VIT.\n    \"\"\"\n    _C = cfg\n\n    _C.MODEL.VIT = CN()\n\n    # CoaT model name.\n    _C.MODEL.VIT.NAME = \"\"\n\n    # Output features from CoaT backbone.\n    _C.MODEL.VIT.OUT_FEATURES = [\"layer3\", \"layer5\", \"layer7\", \"layer11\"]\n\n    _C.MODEL.VIT.IMG_SIZE = [224, 224]\n\n    _C.MODEL.VIT.POS_TYPE = \"shared_rel\"\n    \n    _C.MODEL.VIT.MERGE_TYPE = \"Sum\"\n\n    _C.MODEL.VIT.DROP_PATH = 0.\n\n    _C.MODEL.VIT.MODEL_KWARGS = \"{}\"\n\n    _C.SOLVER.OPTIMIZER = \"ADAMW\"\n\n    _C.SOLVER.BACKBONE_MULTIPLIER = 1.0\n\n    _C.AUG = CN()\n\n    _C.AUG.DETR = False\n    \n    _C.MODEL.WORDGRID = CN()\n    \n    _C.MODEL.WORDGRID.VOCAB_SIZE = 30552\n    \n    _C.MODEL.WORDGRID.EMBEDDING_DIM = 64 \n    \n    _C.MODEL.WORDGRID.MODEL_PATH = ''\n    \n    _C.MODEL.WORDGRID.HIDDEN_SIZE = 768\n    \n    _C.MODEL.WORDGRID.USE_PRETRAIN_WEIGHT = True\n    \n    _C.MODEL.WORDGRID.USE_UNK_TEXT = False\n"}
{"type": "source_file", "path": "utils/ditod_vgt/utils.py", "content": "import json\nimport os\nimport sys\n\nimport cv2\nimport numpy as np\nfrom shapely.geometry import Polygon\nfrom tabulate import tabulate\n\n\ndef get_image_path(image_dir, file_name_wo_ext):\n    ext_list = [\"\", '.jpg', '.JPG', '.png', '.PNG', \".jpeg\"]\n    image_path = None\n    for ext in ext_list:\n        image_path_tmp = os.path.join(image_dir, file_name_wo_ext + ext)\n        if os.path.exists(image_path_tmp):\n            image_path = image_path_tmp\n            break\n    return image_path\n\n\ndef visual_badcase(image_path, pred_list, label_list, output_dir=\"visual_badcase\", info=None, prefix=''):\n    \"\"\"\n    \"\"\"\n    img = cv2.imread(image_path) if os.path.exists(image_path) is not None else None\n    if img is None:\n        print(\"--> Warning: skip, given iamge NOT exists: {}\".format(image_path))\n        return None\n\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    for label in label_list:\n        points, class_id = label[\"poly\"], label[\"category_id\"]\n        pts = np.array(points).reshape((1, -1, 2)).astype(np.int32)\n        cv2.polylines(img, pts, isClosed=True, color=(0, 255, 0), thickness=3)\n        cv2.putText(img, \"gt:\" + str(class_id), tuple(pts[0][0].tolist()), font, 1, (0, 255, 0), 2)\n\n    for label in pred_list:\n        points, class_id = label[\"poly\"], label[\"category_id\"]\n        pts = np.array(points).reshape((1, -1, 2)).astype(np.int32)\n        cv2.polylines(img, pts, isClosed=True, color=(255, 0, 0), thickness=3)\n        cv2.putText(img, \"pred:\" + str(class_id), tuple(pts[0][-1].tolist()), font, 1, (255, 0, 0), 2)\n\n    if info is not None:\n        cv2.putText(img, str(info), (40, 40), font, 1, (0, 0, 255), 2)\n    output_path = os.path.join(output_dir, prefix + os.path.basename(image_path) + \"_vis.jpg\")\n    cv2.imwrite(output_path, img)\n    return output_path\n\ndef pub_load_gt_from_json(json_path):\n    \"\"\"\n    \"\"\"\n    with open(json_path) as f:\n        gt_info = json.load(f)\n    gt_image_list = gt_info[\"images\"]\n    gt_anno_list = gt_info[\"annotations\"]\n\n    id_to_image_info = {}\n    for image_item in gt_image_list:\n        id_to_image_info[image_item['id']] = {\n            \"file_name\": image_item['file_name'],\n            \"group_name\": image_item.get(\"group_name\", \"huntie\")\n        }\n\n    group_info = {}\n    for annotation_item in gt_anno_list:\n        image_info = id_to_image_info[annotation_item['image_id']]\n        image_name, group_name = image_info[\"file_name\"], image_info[\"group_name\"]\n        \n        # import ipdb;ipdb.set_trace()\n        if image_name == '15_103.tar_1705.05489.gz_main_12_ori.jpg':\n            print(image_info[\"file_name\"], annotation_item['image_id'])\n            # import ipdb;ipdb.set_trace()\n\n        if group_name not in group_info:\n            group_info[group_name] = {}\n        if image_name not in group_info[group_name]:\n            group_info[group_name][image_name] = []\n            \n        box_xywh = annotation_item[\"bbox\"]\n        box_xyxy = [ box_xywh[0], box_xywh[1], box_xywh[0] + box_xywh[2], box_xywh[1] + box_xywh[3]]\n        pts = np.round([ box_xyxy[0], box_xyxy[1], box_xyxy[2], box_xyxy[1], box_xyxy[2], box_xyxy[3], box_xyxy[0], box_xyxy[3] ])\n        anno_info = {\n            \"category_id\": annotation_item[\"category_id\"],\n            \"poly\": pts,\n            \"secondary_id\": annotation_item.get(\"secondary_id\", -1),\n            \"direction_id\": annotation_item.get(\"direction_id\", -1)\n        }\n        group_info[group_name][image_name].append(anno_info)\n\n    group_info_str = \", \".join([\"{}[{}]\".format(k, len(v)) for k, v in group_info.items()])\n    print(\"--> load {} groups: {}\".format(len(group_info.keys()), group_info_str))\n    return group_info\n\ndef load_gt_from_json(json_path):\n    \"\"\"\n    \"\"\"\n    with open(json_path) as f:\n        gt_info = json.load(f)\n    gt_image_list = gt_info[\"images\"]\n    gt_anno_list = gt_info[\"annotations\"]\n\n    id_to_image_info = {}\n    for image_item in gt_image_list:\n        id_to_image_info[image_item['id']] = {\n            \"file_name\": image_item['file_name'],\n            \"group_name\": image_item.get(\"group_name\", \"huntie\")\n        }\n\n    group_info = {}\n    for annotation_item in gt_anno_list:\n        image_info = id_to_image_info[annotation_item['image_id']]\n        image_name, group_name = image_info[\"file_name\"], image_info[\"group_name\"]\n\n        if group_name not in group_info:\n            group_info[group_name] = {}\n        if image_name not in group_info[group_name]:\n            group_info[group_name][image_name] = []\n        anno_info = {\n            \"category_id\": annotation_item[\"category_id\"],\n            \"poly\": annotation_item[\"poly\"],\n            \"secondary_id\": annotation_item.get(\"secondary_id\", -1),\n            \"direction_id\": annotation_item.get(\"direction_id\", -1)\n        }\n        group_info[group_name][image_name].append(anno_info)\n\n    group_info_str = \", \".join([\"{}[{}]\".format(k, len(v)) for k, v in group_info.items()])\n    print(\"--> load {} groups: {}\".format(len(group_info.keys()), group_info_str))\n    return group_info\n\n\ndef calc_iou(label, detect):\n    label_box = []\n    detect_box = []\n\n    d_area = []\n    for i in range(0, len(detect)):\n        pred_poly = detect[i][\"poly\"]\n        box_det = []\n        for k in range(0, 4):\n            box_det.append([pred_poly[2 * k], pred_poly[2 * k + 1]])\n        detect_box.append(box_det)\n        try:\n            poly = Polygon(box_det)\n            d_area.append(poly.area)\n        except:\n            print('invalid detects', pred_poly)\n            exit(-1)\n\n    l_area = []\n    for i in range(0, len(label)):\n        gt_poly = label[i][\"poly\"]\n        box_gt = []\n        for k in range(4):\n            box_gt.append([gt_poly[2 * k], gt_poly[2 * k + 1]])\n        label_box.append(box_gt)\n        try:\n            poly = Polygon(box_gt)\n            l_area.append(poly.area)\n        except:\n            print('invalid detects', gt_poly)\n            exit(-1)\n\n    ol_areas = []\n    for i in range(0, len(detect_box)):\n        ol_areas.append([])\n        poly1 = Polygon(detect_box[i])\n        for j in range(0, len(label_box)):\n            poly2 = Polygon(label_box[j])\n            try:\n                ol_area = poly2.intersection(poly1).area\n            except:\n                print('invaild pair', detect_box[i], label_box[j])\n                ol_areas[i].append(0.0)\n            else:\n                ol_areas[i].append(ol_area)\n\n    d_ious = [0.0] * len(detect_box)\n    l_ious = [0.0] * len(label_box)\n    for i in range(0, len(detect_box)):\n        for j in range(0, len(label_box)):\n            if int(label[j][\"category_id\"]) == int(detect[i][\"category_id\"]):\n                iou = min(ol_areas[i][j] / (d_area[i] + 1e-10), ol_areas[i][j] / (l_area[j] + 1e-10))\n            else:\n                iou = 0\n            d_ious[i] = max(d_ious[i], iou)\n            l_ious[j] = max(l_ious[j], iou)\n    return l_ious, d_ious\n\n\ndef eval(instance_info):\n    img_name, label_info = instance_info\n    label = label_info['gt']\n    detect = label_info['det']\n    l_ious, d_ious = calc_iou(label, detect)\n    return [img_name, d_ious, l_ious, detect, label]\n\n\ndef static_with_class(rets, iou_thresh=0.7, is_verbose=True, map_info=None, src_image_dir=None, visualization_dir=None):\n    if is_verbose:\n        table_head = ['Class_id', 'Class_name', 'Pre_hit', 'Pre_num', 'GT_hit', 'GT_num', 'Precision', 'Recall', 'F-score']\n    else:\n        table_head = ['Class_id', 'Class_name', 'Precision', 'Recall', 'F-score']\n    table_body = []\n    class_dict = {}\n\n    for i in range(len(rets)):\n        img_name, d_ious, l_ious, detects, labels = rets[i]\n        item_lv, item_dv, item_dm, item_lm = 0, 0, 0, 0\n        for label in labels:\n            item_lv += 1\n            category_id = label[\"category_id\"]\n            if category_id not in class_dict:\n                class_dict[category_id] = {}\n                class_dict[category_id]['dm'] = 0\n                class_dict[category_id]['dv'] = 0\n                class_dict[category_id]['lm'] = 0\n                class_dict[category_id]['lv'] = 0\n            class_dict[category_id]['lv'] += 1\n\n        for det in detects:\n            item_dv += 1\n            category_id = det[\"category_id\"]\n            if category_id not in class_dict:\n                print(\"--> category_id not exists in gt: {}\".format(category_id))\n                continue\n            class_dict[category_id]['dv'] += 1\n\n        for idx, iou in enumerate(d_ious):\n            if iou >= iou_thresh:\n                item_dm += 1\n                class_dict[detects[idx][\"category_id\"]]['dm'] += 1\n        for idx, iou in enumerate(l_ious):\n            if iou >= iou_thresh:\n                item_lm += 1\n                class_dict[labels[idx][\"category_id\"]]['lm'] += 1\n        item_p = item_dm / (item_dv + 1e-6)\n        item_r = item_lm / (item_lv + 1e-6)\n        item_f = 2 * item_p * item_r / (item_p + item_r + 1e-6)\n\n        \n        if item_f < 0.97 and src_image_dir is not None:\n            image_path = get_image_path(src_image_dir, os.path.basename(img_name))\n            visualization_output = visualization_dir if visualization_dir is not None else \"./visualization_badcase\"\n            item_info = \"IOU{}, {}, {}, {}\".format(iou_thresh, item_r, item_p, item_f)\n            vis_path = visual_badcase(image_path, detects, labels, output_dir=visualization_output, info=item_info, prefix=\"{:02d}_\".format(int(item_f * 100)))\n            if is_verbose:\n                print(\"--> info: save visualization at: {}\".format(vis_path))\n\n    dm, dv, lm, lv = 0, 0, 0, 0\n    map_info = {} if map_info is None else map_info\n    for key in class_dict.keys():\n        dm += class_dict[key]['dm']\n        dv += class_dict[key]['dv']\n        lm += class_dict[key]['lm']\n        lv += class_dict[key]['lv']\n        p = class_dict[key]['dm'] / (class_dict[key]['dv'] + 1e-6)\n        r = class_dict[key]['lm'] / (class_dict[key]['lv'] + 1e-6)\n        fscore = 2 * p * r / (p + r + 1e-6)\n        if is_verbose:\n            table_body.append((key, map_info.get(\"primary_map\", {}).get(str(key), str(key)), class_dict[key]['dm'],\n                               class_dict[key]['dv'], class_dict[key]['lm'], class_dict[key]['lv'], p, r, fscore))\n        else:\n            table_body.append((key,  map_info.get(str(key), str(key)), p, r, fscore))\n\n    p = dm / (dv + 1e-6)\n    r = lm / (lv + 1e-6)\n    f = 2 * p * r / (p + r + 1e-6)\n\n    table_body_sorted = sorted(table_body, key=lambda x: int((x[0])))\n    if is_verbose:\n        table_body_sorted.append(('IOU_{}'.format(iou_thresh), 'average', dm, dv, lm, lv, p, r, f))\n    else:\n        table_body_sorted.append(('IOU_{}'.format(iou_thresh), 'average', p, r, f))\n    print(tabulate(table_body_sorted, headers=table_head, tablefmt='pipe'))\n    return [table_head] + table_body_sorted\n\n\ndef multiproc(func, task_list, proc_num=30, retv=True, progress_bar=False):\n    from multiprocessing import Pool\n    pool = Pool(proc_num)\n\n    rets = []\n    if progress_bar:\n        import tqdm\n        with tqdm.tqdm(total=len(task_list)) as t:\n            for ret in pool.imap(func, task_list):\n                rets.append(ret)\n                t.update(1)\n    else:\n        for ret in pool.imap(func, task_list):\n            rets.append(ret)\n\n    pool.close()\n    pool.join()\n\n    if retv:\n        return rets\n\n\ndef eval_and_show(label_dict, detect_dict, output_dir, iou_thresh=0.7, map_info=None,\n                  src_image_dir=None, visualization_dir=None):\n    \"\"\"\n    \"\"\"\n    evaluation_group_info = {}\n    for group_name, gt_info in label_dict.items():\n        group_pair_list = []\n        for file_name, value_list in gt_info.items():\n            if file_name not in detect_dict:\n                print(\"--> missing pred:\", file_name)\n                continue\n            group_pair_list.append([file_name, {'gt': gt_info[file_name], 'det': detect_dict[file_name]}])\n        evaluation_group_info[group_name] = group_pair_list\n\n    res_info_all = {}\n    for group_name, group_pair_list in evaluation_group_info.items():\n        print(\" ------- group name: {} -----------\".format(group_name))\n        rets = multiproc(eval, group_pair_list, proc_num=16)\n        group_name_map_info = map_info.get(group_name, None) if map_info is not None else None\n        res_info = static_with_class(rets, iou_thresh=iou_thresh, map_info=group_name_map_info,\n                                     src_image_dir=src_image_dir, visualization_dir=visualization_dir)\n        res_info_all[group_name] = res_info\n\n    evaluation_res_info_path = os.path.join(output_dir, \"results_val.json\")\n    with open(evaluation_res_info_path, \"w\") as f:\n        json.dump(res_info_all, f, ensure_ascii=False, indent=4)\n    print(\"--> info: evaluation result is saved at {}\".format(evaluation_res_info_path))\n\n\nif __name__ == \"__main__\":\n\n    if len(sys.argv) != 5:\n        print(\"Usage: python {} gt_json_path pred_json_path output_dir iou_thresh\".format(__file__))\n        exit(-1)\n    else:\n        print('--> info: {}'.format(sys.argv))\n        gt_json_path, pred_json_path, output_dir, iou_thresh = sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4]\n\n    label_dict = load_gt_from_json(gt_json_path)\n    with open(pred_json_path, \"r\") as f:\n        detect_dict = json.load(f)\n\n    src_image_dir = None\n    eval_and_show(label_dict, detect_dict, output_dir, iou_thresh=iou_thresh, map_info=None,\n                  src_image_dir=src_image_dir, visualization_dir=None)\n"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/postprocess/__init__.py", "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\n\n__all__ = ['build_post_process']\n\nfrom .db_postprocess import DBPostProcess, DistillationDBPostProcess\nfrom .east_postprocess import EASTPostProcess\nfrom .sast_postprocess import SASTPostProcess\nfrom .fce_postprocess import FCEPostProcess\nfrom .rec_postprocess import CTCLabelDecode, AttnLabelDecode, SRNLabelDecode, \\\n    DistillationCTCLabelDecode, NRTRLabelDecode, SARLabelDecode, \\\n    SEEDLabelDecode, PRENLabelDecode, ViTSTRLabelDecode, ABINetLabelDecode, \\\n    SPINLabelDecode, VLLabelDecode, RFLLabelDecode\nfrom .cls_postprocess import ClsPostProcess\nfrom .pg_postprocess import PGPostProcess\nfrom .vqa_token_ser_layoutlm_postprocess import VQASerTokenLayoutLMPostProcess, DistillationSerPostProcess\nfrom .vqa_token_re_layoutlm_postprocess import VQAReTokenLayoutLMPostProcess, DistillationRePostProcess\nfrom .table_postprocess import TableMasterLabelDecode, TableLabelDecode\nfrom .picodet_postprocess import PicoDetPostProcess\nfrom .ct_postprocess import CTPostProcess\nfrom .drrg_postprocess import DRRGPostprocess\nfrom .rec_postprocess import CANLabelDecode\n\n\ndef build_post_process(config, global_config=None):\n    support_dict = [\n        'DBPostProcess', 'EASTPostProcess', 'SASTPostProcess', 'FCEPostProcess',\n        'CTCLabelDecode', 'AttnLabelDecode', 'ClsPostProcess', 'SRNLabelDecode',\n        'PGPostProcess', 'DistillationCTCLabelDecode', 'TableLabelDecode',\n        'DistillationDBPostProcess', 'NRTRLabelDecode', 'SARLabelDecode',\n        'SEEDLabelDecode', 'VQASerTokenLayoutLMPostProcess',\n        'VQAReTokenLayoutLMPostProcess', 'PRENLabelDecode',\n        'DistillationSARLabelDecode', 'ViTSTRLabelDecode', 'ABINetLabelDecode',\n        'TableMasterLabelDecode', 'SPINLabelDecode',\n        'DistillationSerPostProcess', 'DistillationRePostProcess',\n        'VLLabelDecode', 'PicoDetPostProcess', 'CTPostProcess',\n        'RFLLabelDecode', 'DRRGPostprocess', 'CANLabelDecode'\n    ]\n\n    if config['name'] == 'PSEPostProcess':\n        from .pse_postprocess import PSEPostProcess\n        support_dict.append('PSEPostProcess')\n\n    config = copy.deepcopy(config)\n    module_name = config.pop('name')\n    if module_name == \"None\":\n        return\n    if global_config is not None:\n        config.update(global_config)\n    assert module_name in support_dict, Exception(\n        'post process only support {}'.format(support_dict))\n    module_class = eval(module_name)(**config)\n    return module_class\n"}
{"type": "source_file", "path": "utils/ocr_model/__init__.py", "content": "from .ocr_model import OCRModel\n\n__all__ = [\"OCRModel\"]\n"}
{"type": "source_file", "path": "utils/object_detection/inference.py", "content": "import argparse\n\nimport cv2\n\nfrom ditod import add_vit_config\n\nimport torch\n\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import ColorMode, Visualizer\nfrom detectron2.data import MetadataCatalog\nfrom ditod.VGTTrainer import DefaultPredictor\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Detectron2 inference script\")\n    parser.add_argument(\n        \"--image_root\",\n        help=\"Path to input image\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--grid_root\",\n        help=\"Path to input image\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--image_name\",\n        help=\"Path to input image\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--output_root\",\n        help=\"Name of the output visualization file.\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--dataset\",\n        help=\"Path to input image\",\n        type=str,\n        required=True,\n    )\n    parser.add_argument(\n        \"--config-file\",\n        default=\"configs/quick_schedules/mask_rcnn_R_50_FPN_inference_acc_test.yaml\",\n        metavar=\"FILE\",\n        help=\"path to config file\",\n    )\n    parser.add_argument(\n        \"--opts\",\n        help=\"Modify config options using the command-line 'KEY VALUE' pairs\",\n        default=[],\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n    \n    if args.dataset in ('D4LA', 'doclaynet'):\n        image_path = args.image_root + args.image_name + \".png\"\n    else:\n        image_path = args.image_root + args.image_name + \".jpg\"\n    \n    if args.dataset == 'publaynet':\n        grid_path = args.grid_root + args.image_name + \".pdf.pkl\"\n    elif args.dataset == 'docbank':\n        grid_path = args.grid_root + args.image_name + \".pkl\"\n    elif args.dataset == 'D4LA':\n        grid_path = args.grid_root + args.image_name + \".pkl\"\n    elif args.dataset == 'doclaynet':\n        grid_path = args.grid_root + args.image_name + \".pdf.pkl\"\n        \n    output_file_name = args.output_root + args.image_name + \".jpg\"\n    \n    # Step 1: instantiate config\n    cfg = get_cfg()\n    add_vit_config(cfg)\n    cfg.merge_from_file(args.config_file)\n    \n    # Step 2: add model weights URL to config\n    cfg.merge_from_list(args.opts)\n    \n    # Step 3: set device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    cfg.MODEL.DEVICE = device\n\n    # Step 4: define model\n    predictor = DefaultPredictor(cfg)\n    \n    # Step 5: run inference\n    img = cv2.imread(image_path)\n    \n    md = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n    if args.dataset == 'publaynet':\n        md.set(thing_classes=[\"text\",\"title\",\"list\",\"table\",\"figure\"])\n    elif args.dataset == 'docbank':\n        md.set(thing_classes=[\"abstract\",\"author\",\"caption\",\"date\",\"equation\", \"figure\", \"footer\", \"list\", \"paragraph\", \"reference\", \"section\", \"table\", \"title\"])\n    elif args.dataset == 'D4LA':\n        md.set(thing_classes=[\"DocTitle\",\"ParaTitle\",\"ParaText\",\"ListText\",\"RegionTitle\", \"Date\", \"LetterHead\", \"LetterDear\", \"LetterSign\", \"Question\", \"OtherText\", \"RegionKV\", \"Regionlist\", \"Abstract\", \"Author\", \"TableName\", \"Table\", \"Figure\", \"FigureName\", \"Equation\", \"Reference\", \"Footnote\", \"PageHeader\", \"PageFooter\", \"Number\", \"Catalog\", \"PageNumber\"])\n    elif args.dataset == 'doclaynet':\n        md.set(thing_classes=[\"Caption\",\"Footnote\",\"Formula\",\"List-item\",\"Page-footer\", \"Page-header\", \"Picture\", \"Section-header\", \"Table\", \"Text\", \"Title\"])\n\n    output = predictor(img, grid_path)[\"instances\"]\n    \n    # import ipdb;ipdb.set_trace()\n    v = Visualizer(img[:, :, ::-1],\n                    md,\n                    scale=1.0,\n                    instance_mode=ColorMode.SEGMENTATION)\n    result = v.draw_instance_predictions(output.to(\"cpu\"))\n    result_image = result.get_image()[:, :, ::-1]\n\n    # step 6: save\n    cv2.imwrite(output_file_name, result_image)\n\nif __name__ == '__main__':\n    main()\n\n"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/postprocess/cls_postprocess.py", "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nclass ClsPostProcess(object):\n    \"\"\" Convert between text-label and text-index \"\"\"\n    def __init__(self, label_list=None, key=None, **kwargs):\n        super(ClsPostProcess, self).__init__()\n        self.label_list = label_list\n        self.key = key\n\n    def __call__(self, preds, label=None, *args, **kwargs):\n        if self.key is not None:\n            preds = preds[self.key]\n\n        label_list = self.label_list\n        if label_list is None:\n            label_list = {idx: idx for idx in range(preds.shape[-1])}\n\n        if isinstance(preds, paddle.Tensor):\n            preds = preds.numpy()\n\n        pred_idxs = preds.argmax(axis=1)\n        decode_out = [(label_list[idx], preds[i, idx])\n                      for i, idx in enumerate(pred_idxs)]\n        if label is None:\n            return decode_out\n        label = [(label_list[idx], 1.0) for idx in label]\n        return decode_out, label\n"}
{"type": "source_file", "path": "utils/layout_model.py", "content": "import sys\nfrom pathlib import Path\nimport cv2\nimport numpy as np\nfrom typing import Literal, Optional\nfrom dataclasses import asdict, dataclass, field\n\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import CfgNode as CN\nfrom utils.ditod.config import add_vit_config\n\n#from ditod import add_vit_config\n#from ditod.VGTTrainer import DefaultPredictor\n\n@dataclass\nclass Layout:\n    type: Literal[\"text\", \"title\", \"list\", \"table\", \"figure\"]\n    bbox: tuple[int, ...]\n    score: float\n    image: np.ndarray = field(init=False)\n    text: Optional[str] = None\n    translated_text: Optional[str] = None\n    line_cnt: Optional[int] = None\n    font: Optional[dict] = None\n    \n    def to_dict(self):\n        # Convert the dataclass to a dict\n        d = asdict(self)\n        # Handle the numpy array; here we're converting it to a list for simplicity\n        # Another option is to convert to a base64 string if you need to save binary data\n        d['image'] = 'test'  # For simplicity\n        d['bbox'] = d['bbox'].tolist()\n        d['score'] = float(d['score'])\n        return d\n\n\nclass LayoutAnalyzer:\n    def __init__(self, model_root_dir: Path, device: str = \"cuda\") -> None:\n        self.predictor = self._load_model(model_root_dir, device=device)\n\n    def __call__(self, image: np.ndarray) -> list[Layout]:\n        #grid_path = args.grid_root + args.image_name + \".pdf.pkl\"\n        output = self.predictor(image)[\"instances\"].to(\"cpu\")\n\n        layouts = []\n        for class_id, box, score in zip(\n            output.pred_classes.numpy(),\n            output.pred_boxes.tensor.numpy().astype(int),\n            output.scores.numpy(),\n        ):\n            if score > 0.8:\n                layouts.append(\n                    Layout(\n                        type=self._id_to_class_names[class_id], bbox=box, score=score\n                    )\n                )\n\n        #layouts = self._remove_overlapping_layouts(layouts)\n\n        for layout in layouts:\n            layout.image = self._get_image(image, layout.bbox)\n\n        return layouts\n\n    def _remove_overlapping_layouts(self, layouts: list[Layout]) -> list[Layout]:\n        if not layouts:\n            return []\n\n        # Sort layouts by confidence score (high to low)\n        layouts.sort(key=lambda x: x.score, reverse=True)\n\n        non_overlapping_layouts = []\n        while layouts:\n            # Take the layout with the highest score\n            current_layout = layouts.pop(0)\n            non_overlapping = True\n\n            for other_layout in non_overlapping_layouts:\n                # If IoU is above a threshold (e.g., 0.5), consider it as overlapping\n                if self._calculate_iou(current_layout.bbox, other_layout.bbox) > 0.5:\n                    non_overlapping = False\n                    break\n\n            if non_overlapping:\n                non_overlapping_layouts.append(current_layout)\n\n        return non_overlapping_layouts\n\n    def _get_image(self, image: np.ndarray, bbox: tuple[float, ...]) -> np.ndarray:\n        x1, y1, x2, y2 = bbox\n        return image[int(y1) : int(y2), int(x1) : int(x2)]\n\n    @property\n    def _id_to_class_names(self) -> dict[int, str]:\n        return {\n            0: \"text\",\n            1: \"title\",\n            2: \"list\",\n            3: \"table\",\n            4: \"figure\",\n        }\n\n    def _load_model(\n        self, model_root_dir: Path, device: str = \"cuda\"\n    ) -> DefaultPredictor:\n        cfg = get_cfg()\n        add_vit_config(cfg)\n        cfg.merge_from_file(str(model_root_dir / \"config/cascade_dit_base.yml\"))\n        cfg.MODEL.WEIGHTS = str(model_root_dir / \"publaynet_dit-b_cascade.pth\")\n        cfg.MODEL.DEVICE = device\n        return DefaultPredictor(cfg)\n\n    def _calculate_iou(self, box1: tuple[float, ...], box2: tuple[float, ...]) -> float:\n        x1, y1, x2, y2 = box1\n        x1_, y1_, x2_, y2_ = box2\n\n        # calculate the intersection coordinates\n        xi1 = max(x1, x1_)\n        yi1 = max(y1, y1_)\n        xi2 = min(x2, x2_)\n        yi2 = min(y2, y2_)\n        inter_area = max(xi2 - xi1, 0) * max(yi2 - yi1, 0)\n\n        # calculate each box area\n        box1_area = (x2 - x1) * (y2 - y1)\n        box2_area = (x2_ - x1_) * (y2_ - y1_)\n\n        # calculate union area\n        union_area = box1_area + box2_area - inter_area\n\n        # compute the IoU\n        return inter_area / union_area if union_area != 0 else 0\n\n\nif __name__ == \"__main__\":\n    layout_analyzer = LayoutAnalyzer(Path(\"models/\"))\n    image = cv2.imread(\"assets/sample1.png\")\n    print(layout_analyzer(image))\n"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/__init__.py", "content": "# Copyright (c) 2019 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"}
{"type": "source_file", "path": "utils/gui.py", "content": "from pathlib import Path\nfrom typing import Any\n\nimport gradio as gr\nimport requests\n\nTRANSLATE_URL = \"http://localhost:8765/translate_pdf/\"\nCLEAR_TEMP_URL = \"http://localhost:8765/clear_temp_dir/\"\n\n\ndef translate_request(file: Any, from_lang: Any, to_lang: Any, from_page: int, to_page: int, both: bool) -> tuple[Path]:\n    \"\"\"Sends a POST request to the translator server to translate a PDF.\n\n    Parameters\n    ----------\n    file : Any\n        the PDF to be translated.\n\n    Returns\n    -------\n    tuple[Path, list[Image.Image]]\n        Path to the translated PDF and a list of images of the\n        translated PDF.\n    \"\"\"\n    response = requests.post(TRANSLATE_URL, files={\"input_pdf\": open(file.name, \"rb\")}, data={\n        \"from_lang\": from_lang, \"to_lang\": to_lang, \"p_from\": from_page, \"p_to\": to_page, \"side_by_side\": both\n    })\n\n    if response.status_code == 200:\n        with open(\"temp/translated.pdf\", \"wb\") as f:\n            f.write(response.content)\n\n        return str(\"temp/translated.pdf\")\n    else:\n        print(f\"An error occurred: {response.status_code}\")\n\n\ndef create_gradio_app(langs):\n    with gr.Blocks(theme=\"Soft\") as demo:\n        with gr.Column():\n            title = gr.Markdown(\"## PDF Translator\")\n            file = gr.File(label=\"select file\")\n\n            \n            from_lang = gr.Dropdown(label='from language', choices=langs, value=\"English\")\n            to_lang = gr.Dropdown(label='to language', choices=langs, value=\"Slovenian\")\n            from_page = gr.Number(label='from page')\n            to_page = gr.Number(label='to page')\n            both = gr.Checkbox(label='render side by side', value=True)\n\n            btn = gr.Button(value=\"convert\")\n            translated_file = gr.File(label=\"translated fie\", file_types=[\".pdf\"])\n\n            btn.click(\n                translate_request,\n                inputs=[file, from_lang, to_lang, from_page, to_page, both],\n                outputs=[translated_file],\n            )\n\n        return demo\n\nif __name__ == \"__main__\":\n    app = create_gradio_app()\n    app.launch(share=True)"}
{"type": "source_file", "path": "utils/object_detection/train_VGT.py", "content": "#!/usr/bin/env python\n# --------------------------------------------------------------------------------\n# MPViT: Multi-Path Vision Transformer for Dense Prediction\n# Copyright (c) 2022 Electronics and Telecommunications Research Institute (ETRI).\n# All Rights Reserved.\n# Written by Youngwan Lee\n# --------------------------------------------------------------------------------\n\n\"\"\"\nDetection Training Script for MPViT.\n\"\"\"\n\nimport os\nimport itertools\n\nimport torch\n\nfrom typing import Any, Dict, List, Set\n\nfrom detectron2.data import build_detection_train_loader\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.config import get_cfg\nfrom detectron2.engine import DefaultTrainer, default_argument_parser, default_setup, launch\nfrom detectron2.evaluation import COCOEvaluator\nfrom detectron2.solver.build import maybe_add_gradient_clipping\n\nfrom ditod import add_vit_config\nfrom ditod import DetrDatasetMapper\n\nfrom detectron2.data.datasets import register_coco_instances\nimport logging\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.utils import comm\nfrom detectron2.engine.defaults import create_ddp_model\nimport weakref\nfrom detectron2.engine.train_loop import AMPTrainer, SimpleTrainer\nfrom ditod import VGTTrainer as MyTrainer\nimport numpy as np\n\ndef setup(args):\n    \"\"\"\n    Create configs and perform basic setups.\n    \"\"\"\n    cfg = get_cfg()\n    # add_coat_config(cfg)\n    add_vit_config(cfg)\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n    default_setup(cfg, args)\n    return cfg\n\n\ndef main(args):\n    \"\"\"\n    register publaynet first\n    \"\"\"\n    # add docbank data\n    register_coco_instances(\n        \"docbank_train\",\n        {},\n        \"/DocBank_root_path/DocBank/500K_train_VGT.json\",\n        \"/DocBank_root_path/DocBank/DocBank_500K_ori_img\"\n    )\n\n    register_coco_instances(\n        \"docbank_val\",\n        {},\n        \"/DocBank_root_path/DocBank/500K_valid_VGT.json\",\n        \"/DocBank_root_path/DocBank/DocBank_500K_ori_img\"\n    )\n    \n    \n    # add publaynet data\n    register_coco_instances(\n        \"publaynet_train\",\n        {},\n        \"/publaynet_root_path/publaynet/train.json\",\n        \"/publaynet_root_path/publaynet/train\"\n    )\n\n    register_coco_instances(\n        \"publaynet_val\",\n        {},\n        \"/publaynet_root_path/publaynet/val.json\",\n        \"/publaynet_root_path/publaynet/val\"\n    )\n    \n\n    # add D4LA data\n    register_coco_instances(\n        \"D4LA_train\",\n        {},\n        \"/D4LA_root_path/D4LA/json/train.json\",\n        \"/D4LA_root_path/D4LA/train_images\"\n    )\n\n    register_coco_instances(\n        \"D4LA_val\",\n        {},\n        \"/D4LA_root_path/D4LA/json/test.json\",\n        \"/D4LA_root_path/D4LA/test_images\"\n    )\n\n    \n    # add doclaynet data\n    register_coco_instances(\n        \"doclayent_train\",\n        {},\n        \"/doclayent_root_path/DocLayNet/COCO/train.json\",\n        \"/doclayent_root_path/DocLayNet/PNG\"\n    )\n\n    register_coco_instances(\n        \"doclayent_val\",\n        {},\n        \"/doclayent_root_path/DocLayNet/COCO/val.json\",\n        \"/doclayent_root_path/DocLayNet/PNG\"\n    )\n    \n\n    cfg = setup(args)\n    if args.eval_only:\n        model = MyTrainer.build_model(cfg)\n        DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n            cfg.MODEL.WEIGHTS, resume=args.resume\n        )\n        res = MyTrainer.test(cfg, model)\n        return res\n    \n    trainer = MyTrainer(cfg)\n    trainer.resume_or_load(resume=args.resume)\n    return trainer.train()\n\n\nif __name__ == \"__main__\":\n    parser = default_argument_parser()\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"enable debug mode\")\n    args = parser.parse_args()\n    print(\"Command Line Args:\", args)\n    \n    import os\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    os.environ[\n        \"TORCH_DISTRIBUTED_DEBUG\"\n    ] = \"DETAIL\"\n\n    if args.debug:\n        import debugpy\n\n        print(\"Enabling attach starts.\")\n        debugpy.listen(address=('0.0.0.0', 9310))\n        debugpy.wait_for_client()\n        print(\"Enabling attach ends.\")\n\n    launch(\n        main,\n        args.num_gpus,\n        num_machines=args.num_machines,\n        machine_rank=args.machine_rank,\n        dist_url=args.dist_url,\n        args=(args,),\n    )\n"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/data/imaug/__init__.py", "content": "# copyright (c) 2020 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom .operators import *\n\n\ndef transform(data, ops=None):\n    \"\"\" transform \"\"\"\n    if ops is None:\n        ops = []\n    for op in ops:\n        data = op(data)\n        if data is None:\n            return None\n    return data\n\n\ndef create_operators(op_param_list, global_config=None):\n    \"\"\"\n    create operators based on the config\n\n    Args:\n        params(list): a dict list, used to create some operators\n    \"\"\"\n    assert isinstance(op_param_list,\n                      list), ('operator config should be a list')\n    ops = []\n    for operator in op_param_list:\n        assert isinstance(operator,\n                          dict) and len(operator) == 1, \"yaml format error\"\n        op_name = list(operator)[0]\n        param = {} if operator[op_name] is None else operator[op_name]\n        if global_config is not None:\n            param.update(global_config)\n        op = eval(op_name)(**param)\n        ops.append(op)\n    return ops\n"}
{"type": "source_file", "path": "utils/ocr_model/ocr_model.py", "content": "from pathlib import Path\n\nimport numpy as np\n\nfrom .ppocr_onnx.ppocr_onnx import PaddleOcrONNX\n\n\nclass _DictDotNotation(dict):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\nclass OCRModel:\n    def __init__(self, model_root_dir: Path, device: str = \"cuda\") -> None:\n        \"\"\"\n        Initialize OCR model.\n\n        Parameters\n        ----------\n        model_root_dir : Path\n            Path to the PaddleOCR model root directory.\n        device : str, optional\n            Device to use, by default \"cuda\"\n\n        Raises\n        ------\n        FileNotFoundError\n            If the model directory is not found.\n        \"\"\"\n        self.paddleocr_parameters = self.__get_paddleocr_parameters(\n            model_root_dir, device\n        )\n        self.paddleocr = PaddleOcrONNX(self.paddleocr_parameters)\n\n    def __call__(self, image: np.ndarray) -> str:\n        \"\"\"\n        Perform OCR on the image.\n\n        Parameters\n        ----------\n        image : np.ndarray\n            RGB image data\n\n        Returns\n        -------\n        str\n            OCR result\n        \"\"\"\n        return self.paddleocr(image)\n\n    def __get_paddleocr_parameters(\n        self, model_root_dir: Path, device: str\n    ) -> _DictDotNotation:\n        \"\"\"\n        Get parameters for PaddleOCR.\n\n        Parameters\n        ----------\n        model_root_dir : Path\n            Path to the PaddleOCR model root directory.\n        device : str\n            Device to use.\n\n        Returns\n        -------\n        _DictDotNotation\n            PaddleOCR parameters.\n        \"\"\"\n        paddleocr_parameters = _DictDotNotation()\n\n        # params for prediction engine\n        paddleocr_parameters.use_gpu = True if device == \"cuda\" else False\n\n        # params for text detector\n        paddleocr_parameters.det_algorithm = \"DB\"\n        paddleocr_parameters.det_model_dir = str(\n            model_root_dir / \"en_PP-OCRv3_det_infer.onnx\"\n        )\n        paddleocr_parameters.det_limit_side_len = 960\n        paddleocr_parameters.det_limit_type = \"max\"\n        paddleocr_parameters.det_box_type = \"quad\"\n\n        # DB parmas\n        paddleocr_parameters.det_db_thresh = 0.3\n        paddleocr_parameters.det_db_box_thresh = 0.6\n        paddleocr_parameters.det_db_unclip_ratio = 1.5\n        paddleocr_parameters.max_batch_size = 10\n        paddleocr_parameters.use_dilation = False\n        paddleocr_parameters.det_db_score_mode = \"fast\"\n\n        # params for text recognizer\n        paddleocr_parameters.rec_algorithm = \"SVTR_LCNet\"\n        paddleocr_parameters.rec_model_dir = str(\n            model_root_dir / \"en_PP-OCRv3_rec_infer.onnx\"\n        )\n        paddleocr_parameters.rec_image_shape = \"3, 48, 320\"\n        paddleocr_parameters.rec_batch_num = 6\n        paddleocr_parameters.rec_char_dict_path = str(model_root_dir / \"en_dict.txt\")\n        paddleocr_parameters.use_space_char = True\n        paddleocr_parameters.drop_score = 0.5\n\n        # params for text classifier\n        paddleocr_parameters.use_angle_cls = False\n        paddleocr_parameters.cls_model_dir = str(\n            model_root_dir / \"ch_ppocr_mobile_v2.0_cls_infer.onnx\"\n        )\n        paddleocr_parameters.cls_image_shape = \"3, 48, 192\"\n        paddleocr_parameters.label_list = [\"0\", \"180\"]\n        paddleocr_parameters.cls_batch_num = 6\n        paddleocr_parameters.cls_thresh = 0.9\n\n        paddleocr_parameters.save_crop_res = False\n\n        return paddleocr_parameters\n"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/postprocess/ct_postprocess.py", "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refered from:\nhttps://github.com/shengtao96/CentripetalText/blob/main/test.py\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\n\n\nclass CTPostProcess(object):\n    \"\"\"\n    The post process for Centripetal Text (CT).\n    \"\"\"\n    def __init__(self, min_score=0.88, min_area=16, box_type='poly', **kwargs):\n        self.min_score = min_score\n        self.min_area = min_area\n        self.box_type = box_type\n\n        self.coord = np.zeros((2, 300, 300), dtype=np.int32)\n        for i in range(300):\n            for j in range(300):\n                self.coord[0, i, j] = j\n                self.coord[1, i, j] = i\n\n    def __call__(self, preds, batch):\n        outs = preds['maps']\n        out_scores = preds['score']\n\n        if isinstance(outs, paddle.Tensor):\n            outs = outs.numpy()\n        if isinstance(out_scores, paddle.Tensor):\n            out_scores = out_scores.numpy()\n\n        batch_size = outs.shape[0]\n        boxes_batch = []\n        for idx in range(batch_size):\n            bboxes = []\n            scores = []\n\n            img_shape = batch[idx]\n\n            org_img_size = img_shape[:3]\n            img_shape = img_shape[3:]\n            img_size = img_shape[:2]\n\n            out = np.expand_dims(outs[idx], axis=0)\n            outputs = dict()\n\n            score = np.expand_dims(out_scores[idx], axis=0)\n\n            kernel = out[:, 0, :, :] > 0.2\n            loc = out[:, 1:, :, :].astype(\"float32\")\n\n            score = score[0].astype(np.float32)\n            kernel = kernel[0].astype(np.uint8)\n            loc = loc[0].astype(np.float32)\n\n            label_num, label_kernel = cv2.connectedComponents(kernel,\n                                                              connectivity=4)\n\n            for i in range(1, label_num):\n                ind = (label_kernel == i)\n                if ind.sum(\n                ) < 10:  # pixel number less than 10, treated as background\n                    label_kernel[ind] = 0\n\n            label = np.zeros_like(label_kernel)\n            h, w = label_kernel.shape\n            pixels = self.coord[:, :h, :w].reshape(2, -1)\n            points = pixels.transpose([1, 0]).astype(np.float32)\n\n            off_points = (points +\n                          10. / 4. * loc[:, pixels[1], pixels[0]].T).astype(\n                              np.int32)\n            off_points[:, 0] = np.clip(off_points[:, 0], 0, label.shape[1] - 1)\n            off_points[:, 1] = np.clip(off_points[:, 1], 0, label.shape[0] - 1)\n\n            label[pixels[1], pixels[0]] = label_kernel[off_points[:, 1],\n                                                       off_points[:, 0]]\n            label[label_kernel > 0] = label_kernel[label_kernel > 0]\n\n            score_pocket = [0.0]\n            for i in range(1, label_num):\n                ind = (label_kernel == i)\n                if ind.sum() == 0:\n                    score_pocket.append(0.0)\n                    continue\n                score_i = np.mean(score[ind])\n                score_pocket.append(score_i)\n\n            label_num = np.max(label) + 1\n            label = cv2.resize(label, (img_size[1], img_size[0]),\n                               interpolation=cv2.INTER_NEAREST)\n\n            scale = (float(org_img_size[1]) / float(img_size[1]),\n                     float(org_img_size[0]) / float(img_size[0]))\n\n            for i in range(1, label_num):\n                ind = (label == i)\n                points = np.array(np.where(ind)).transpose((1, 0))\n\n                if points.shape[0] < self.min_area:\n                    continue\n\n                score_i = score_pocket[i]\n                if score_i < self.min_score:\n                    continue\n\n                if self.box_type == 'rect':\n                    rect = cv2.minAreaRect(points[:, ::-1])\n                    bbox = cv2.boxPoints(rect) * scale\n                    z = bbox.mean(0)\n                    bbox = z + (bbox - z) * 0.85\n                elif self.box_type == 'poly':\n                    binary = np.zeros(label.shape, dtype='uint8')\n                    binary[ind] = 1\n                    try:\n                        _, contours, _ = cv2.findContours(\n                            binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n                    except BaseException:\n                        contours, _ = cv2.findContours(binary,\n                                                       cv2.RETR_EXTERNAL,\n                                                       cv2.CHAIN_APPROX_SIMPLE)\n\n                    bbox = contours[0] * scale\n\n                bbox = bbox.astype('int32')\n                bboxes.append(bbox.reshape(-1, 2))\n                scores.append(score_i)\n\n            boxes_batch.append({'points': bboxes})\n\n        return boxes_batch\n"}
{"type": "source_file", "path": "utils/ditod_vgt/dataset_mapper.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n# from https://github.com/facebookresearch/detr/blob/main/d2/detr/dataset_mapper.py\n\n\nimport copy\nimport logging\n\nimport numpy as np\nimport torch\n\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.data import transforms as T\n\nimport json\nimport pickle\n\nfrom detectron2.structures import (\n    BitMasks,\n    Boxes,\n    BoxMode,\n    Instances,\n    Keypoints,\n    PolygonMasks,\n    RotatedBoxes,\n    polygons_to_bitmask,\n)\n\n__all__ = [\"DetrDatasetMapper\"]\n\n\ndef build_transform_gen(cfg, is_train):\n    \"\"\"\n    Create a list of :class:`TransformGen` from config.\n    Returns:\n        list[TransformGen]\n    \"\"\"\n    if is_train:\n        min_size = cfg.INPUT.MIN_SIZE_TRAIN\n        max_size = cfg.INPUT.MAX_SIZE_TRAIN\n        sample_style = cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING\n    else:\n        min_size = cfg.INPUT.MIN_SIZE_TEST\n        max_size = cfg.INPUT.MAX_SIZE_TEST\n        sample_style = \"choice\"\n    if sample_style == \"range\":\n        assert len(min_size) == 2, \"more than 2 ({}) min_size(s) are provided for ranges\".format(len(min_size))\n\n    logger = logging.getLogger(__name__)\n    tfm_gens = []\n    # if is_train:\n    #     tfm_gens.append(T.RandomFlip())\n    tfm_gens.append(T.ResizeShortestEdge(min_size, max_size, sample_style))\n    if is_train:\n        logger.info(\"TransformGens used in training: \" + str(tfm_gens))\n    return tfm_gens\n\n\ndef build_transform_gen_w(cfg, is_train):\n    \"\"\"\n    Create a list of :class:`TransformGen` from config.\n    Returns:\n        list[TransformGen]\n    \"\"\"\n    if is_train:\n        min_size = cfg.INPUT.MIN_SIZE_TRAIN\n        max_size = 800\n        sample_style = cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING\n    else:\n        min_size = cfg.INPUT.MIN_SIZE_TEST\n        max_size = cfg.INPUT.MAX_SIZE_TEST\n        sample_style = \"choice\"\n    if sample_style == \"range\":\n        assert len(min_size) == 2, \"more than 2 ({}) min_size(s) are provided for ranges\".format(len(min_size))\n\n    logger = logging.getLogger(__name__)\n    tfm_gens = []\n    # if is_train:\n    #     tfm_gens.append(T.RandomFlip())\n    tfm_gens.append(T.ResizeShortestEdge(min_size, max_size, sample_style))\n    if is_train:\n        logger.info(\"TransformGens used in training: \" + str(tfm_gens))\n    return tfm_gens\n\n\nclass DetrDatasetMapper:\n    \"\"\"\n    A callable which takes a dataset dict in Detectron2 Dataset format,\n    and map it into a format used by DETR.\n\n    The callable currently does the following:\n\n    1. Read the image from \"file_name\"\n    2. Applies geometric transforms to the image and annotation\n    3. Find and applies suitable cropping to the image and annotation\n    4. Prepare image and annotation to Tensors\n    \"\"\"\n\n    def __init__(self, cfg, is_train=True):\n        if cfg.INPUT.CROP.ENABLED and is_train:\n            self.crop_gen = [\n                T.ResizeShortestEdge([400, 500, 600], sample_style=\"choice\"),\n                T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE),\n            ]\n        else:\n            self.crop_gen = None\n\n        self.mask_on = cfg.MODEL.MASK_ON\n        self.tfm_gens = build_transform_gen(cfg, is_train)\n        self.tfm_gens_w = build_transform_gen_w(cfg, is_train)\n        logging.getLogger(__name__).info(\n            \"Full TransformGens used in training: {}, crop: {}\".format(str(self.tfm_gens), str(self.crop_gen))\n        )\n\n        self.img_format = cfg.INPUT.FORMAT\n        self.is_train = is_train\n        self.cfg = cfg\n\n        logger = logging.getLogger(\"detectron2\")\n            \n    def __call__(self, dataset_dict):\n        \"\"\"\n        Args:\n            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n\n        Returns:\n            dict: a format that builtin models in detectron2 accept\n        \"\"\"\n        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n        utils.check_image_size(dataset_dict, image)\n        \n        try:\n            name = dataset_dict[\"file_name\"][0:-4].split('/') \n            if 'publaynet' in name:\n                root = '/'.join(name[:-2])\n                if name[-2] == 'val':\n                    name[-2] = 'dev'\n                pdf_name = '/'.join(['/VGT_publaynet_grid_pkl'] + name[-2:])\n                with open((root + pdf_name + '.pdf.pkl'), \"rb\") as f:\n                    sample_inputs = pickle.load(f)\n                input_ids = sample_inputs[\"input_ids\"]\n                bbox_subword_list = sample_inputs[\"bbox_subword_list\"]\n            elif 'DocBank' in name:\n                root = '/'.join(name[:-2])\n                pdf_name = '/'.join(['/VGT_docbank_grid_pkl'] + name[-1:])\n                with open((root + pdf_name + '.pkl'), \"rb\") as f:\n                    sample_inputs = pickle.load(f)\n                input_ids = sample_inputs[\"input_ids\"]\n                bbox_subword_list = sample_inputs[\"bbox_subword_list\"]\n            elif 'D4LA' in name:\n                root = '/'.join(name[:-2])\n                pdf_name = '/'.join(['/VGT_D4LA_grid_pkl'] + name[-1:])\n                with open((root + pdf_name + '.pkl'), \"rb\") as f:\n                    sample_inputs = pickle.load(f)\n                input_ids = sample_inputs[\"input_ids\"]\n                bbox_subword_list = sample_inputs[\"bbox_subword_list\"]\n            elif 'DocLayNet' in name:\n                root = '/'.join(name[:-2])\n                pdf_name = '/'.join(['/VGT_DocLayNet_grid_pkl'] + name[-1:])\n                with open((root + pdf_name + '.pdf.pkl'), \"rb\") as f:\n                    sample_inputs = pickle.load(f)\n                input_ids = sample_inputs[\"input_ids\"]\n                bbox_subword_list = sample_inputs[\"bbox_subword_list\"]\n            else:\n                input_ids = []\n                bbox_subword_list = []\n                print(\"no grid pkl\")\n        except:\n            print(\"Wrong bbox file:\", dataset_dict[\"file_name\"])\n            input_ids = []\n            bbox_subword_list = []\n\n        image_shape_ori = image.shape[:2]  # h, w\n\n        if self.crop_gen is None:\n            if image_shape_ori[0] > image_shape_ori[1]:\n                image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n            else:\n                image, transforms = T.apply_transform_gens(self.tfm_gens_w, image)\n        else:\n            if np.random.rand() > 0.5:\n                if image_shape_ori[0] > image_shape_ori[1]:\n                    image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n                else:\n                    image, transforms = T.apply_transform_gens(self.tfm_gens_w, image)\n            else:\n                image, transforms = T.apply_transform_gens(\n                    self.tfm_gens_w[:-1] + self.crop_gen + self.tfm_gens_w[-1:], image\n                )\n\n        image_shape = image.shape[:2]  # h, w\n        \n        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n        \n        ##  text grid \n        bbox = []\n        for bbox_per_subword in bbox_subword_list:\n            text_word = {}\n            text_word['bbox'] = bbox_per_subword.tolist()\n            text_word['bbox_mode'] = BoxMode.XYWH_ABS\n            utils.transform_instance_annotations(text_word, transforms, image_shape)\n            bbox.append(text_word['bbox'])\n                \n        dataset_dict[\"input_ids\"] = input_ids \n        dataset_dict[\"bbox\"] = bbox\n\n        if not self.is_train:\n            # USER: Modify this if you want to keep them for some reason.\n            dataset_dict.pop(\"annotations\", None)\n            return dataset_dict\n\n        if \"annotations\" in dataset_dict:\n            # USER: Modify this if you want to keep them for some reason.\n            for anno in dataset_dict[\"annotations\"]:\n                if not self.mask_on:\n                    anno.pop(\"segmentation\", None)\n                anno.pop(\"keypoints\", None)\n\n            # USER: Implement additional transformations if you have other types of data\n            annos = [\n                utils.transform_instance_annotations(obj, transforms, image_shape)\n                for obj in dataset_dict.pop(\"annotations\")\n                if obj.get(\"iscrowd\", 0) == 0\n            ]\n            instances = utils.annotations_to_instances(annos, image_shape)\n            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)           \n\n        return dataset_dict"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/data/imaug/operators.py", "content": "\"\"\"\n# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport sys\nimport cv2\nimport numpy as np\nimport math\nfrom PIL import Image\n\n\nclass DecodeImage(object):\n    \"\"\" decode image \"\"\"\n    def __init__(self,\n                 img_mode='RGB',\n                 channel_first=False,\n                 ignore_orientation=False,\n                 **kwargs):\n        self.img_mode = img_mode\n        self.channel_first = channel_first\n        self.ignore_orientation = ignore_orientation\n\n    def __call__(self, data):\n        img = data['image']\n        if six.PY2:\n            assert type(img) is str and len(\n                img) > 0, \"invalid input 'img' in DecodeImage\"\n        else:\n            assert type(img) is bytes and len(\n                img) > 0, \"invalid input 'img' in DecodeImage\"\n        img = np.frombuffer(img, dtype='uint8')\n        if self.ignore_orientation:\n            img = cv2.imdecode(img,\n                               cv2.IMREAD_IGNORE_ORIENTATION | cv2.IMREAD_COLOR)\n        else:\n            img = cv2.imdecode(img, 1)\n        if img is None:\n            return None\n        if self.img_mode == 'GRAY':\n            img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n        elif self.img_mode == 'RGB':\n            assert img.shape[2] == 3, 'invalid shape of image[%s]' % (img.shape)\n            img = img[:, :, ::-1]\n\n        if self.channel_first:\n            img = img.transpose((2, 0, 1))\n\n        data['image'] = img\n        return data\n\n\nclass NormalizeImage(object):\n    \"\"\" normalize image such as substract mean, divide std\n    \"\"\"\n    def __init__(self, scale=None, mean=None, std=None, order='chw', **kwargs):\n        if isinstance(scale, str):\n            scale = eval(scale)\n        self.scale = np.float32(scale if scale is not None else 1.0 / 255.0)\n        mean = mean if mean is not None else [0.485, 0.456, 0.406]\n        std = std if std is not None else [0.229, 0.224, 0.225]\n\n        shape = (3, 1, 1) if order == 'chw' else (1, 1, 3)\n        self.mean = np.array(mean).reshape(shape).astype('float32')\n        self.std = np.array(std).reshape(shape).astype('float32')\n\n    def __call__(self, data):\n        img = data['image']\n        from PIL import Image\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        assert isinstance(img,\n                          np.ndarray), \"invalid input 'img' in NormalizeImage\"\n        data['image'] = (img.astype('float32') * self.scale -\n                         self.mean) / self.std\n        return data\n\n\nclass ToCHWImage(object):\n    \"\"\" convert hwc image to chw image\n    \"\"\"\n    def __init__(self, **kwargs):\n        pass\n\n    def __call__(self, data):\n        img = data['image']\n        from PIL import Image\n        if isinstance(img, Image.Image):\n            img = np.array(img)\n        data['image'] = img.transpose((2, 0, 1))\n        return data\n\n\nclass Fasttext(object):\n    def __init__(self, path=\"None\", **kwargs):\n        import fasttext\n        self.fast_model = fasttext.load_model(path)\n\n    def __call__(self, data):\n        label = data['label']\n        fast_label = self.fast_model[label]\n        data['fast_label'] = fast_label\n        return data\n\n\nclass KeepKeys(object):\n    def __init__(self, keep_keys, **kwargs):\n        self.keep_keys = keep_keys\n\n    def __call__(self, data):\n        data_list = []\n        for key in self.keep_keys:\n            data_list.append(data[key])\n        return data_list\n\n\nclass Pad(object):\n    def __init__(self, size=None, size_div=32, **kwargs):\n        if size is not None and not isinstance(size, (int, list, tuple)):\n            raise TypeError(\"Type of target_size is invalid. Now is {}\".format(\n                type(size)))\n        if isinstance(size, int):\n            size = [size, size]\n        self.size = size\n        self.size_div = size_div\n\n    def __call__(self, data):\n\n        img = data['image']\n        img_h, img_w = img.shape[0], img.shape[1]\n        if self.size:\n            resize_h2, resize_w2 = self.size\n            assert (\n                img_h < resize_h2 and img_w < resize_w2\n            ), '(h, w) of target size should be greater than (img_h, img_w)'\n        else:\n            resize_h2 = max(\n                int(math.ceil(img.shape[0] / self.size_div) * self.size_div),\n                self.size_div)\n            resize_w2 = max(\n                int(math.ceil(img.shape[1] / self.size_div) * self.size_div),\n                self.size_div)\n        img = cv2.copyMakeBorder(img,\n                                 0,\n                                 resize_h2 - img_h,\n                                 0,\n                                 resize_w2 - img_w,\n                                 cv2.BORDER_CONSTANT,\n                                 value=0)\n        data['image'] = img\n        return data\n\n\nclass Resize(object):\n    def __init__(self, size=(640, 640), **kwargs):\n        self.size = size\n\n    def resize_image(self, img):\n        resize_h, resize_w = self.size\n        ori_h, ori_w = img.shape[:2]  # (h, w, c)\n        ratio_h = float(resize_h) / ori_h\n        ratio_w = float(resize_w) / ori_w\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        return img, [ratio_h, ratio_w]\n\n    def __call__(self, data):\n        img = data['image']\n        if 'polys' in data:\n            text_polys = data['polys']\n\n        img_resize, [ratio_h, ratio_w] = self.resize_image(img)\n        if 'polys' in data:\n            new_boxes = []\n            for box in text_polys:\n                new_box = []\n                for cord in box:\n                    new_box.append([cord[0] * ratio_w, cord[1] * ratio_h])\n                new_boxes.append(new_box)\n            data['polys'] = np.array(new_boxes, dtype=np.float32)\n        data['image'] = img_resize\n        return data\n\n\nclass DetResizeForTest(object):\n    def __init__(self, **kwargs):\n        super(DetResizeForTest, self).__init__()\n        self.resize_type = 0\n        self.keep_ratio = False\n        if 'image_shape' in kwargs:\n            self.image_shape = kwargs['image_shape']\n            self.resize_type = 1\n            if 'keep_ratio' in kwargs:\n                self.keep_ratio = kwargs['keep_ratio']\n        elif 'limit_side_len' in kwargs:\n            self.limit_side_len = kwargs['limit_side_len']\n            self.limit_type = kwargs.get('limit_type', 'min')\n        elif 'resize_long' in kwargs:\n            self.resize_type = 2\n            self.resize_long = kwargs.get('resize_long', 960)\n        else:\n            self.limit_side_len = 736\n            self.limit_type = 'min'\n\n    def __call__(self, data):\n        img = data['image']\n        src_h, src_w, _ = img.shape\n        if sum([src_h, src_w]) < 64:\n            img = self.image_padding(img)\n\n        if self.resize_type == 0:\n            # img, shape = self.resize_image_type0(img)\n            img, [ratio_h, ratio_w] = self.resize_image_type0(img)\n        elif self.resize_type == 2:\n            img, [ratio_h, ratio_w] = self.resize_image_type2(img)\n        else:\n            # img, shape = self.resize_image_type1(img)\n            img, [ratio_h, ratio_w] = self.resize_image_type1(img)\n        data['image'] = img\n        data['shape'] = np.array([src_h, src_w, ratio_h, ratio_w])\n        return data\n\n    def image_padding(self, im, value=0):\n        h, w, c = im.shape\n        im_pad = np.zeros((max(32, h), max(32, w), c), np.uint8) + value\n        im_pad[:h, :w, :] = im\n        return im_pad\n\n    def resize_image_type1(self, img):\n        resize_h, resize_w = self.image_shape\n        ori_h, ori_w = img.shape[:2]  # (h, w, c)\n        if self.keep_ratio is True:\n            resize_w = ori_w * resize_h / ori_h\n            N = math.ceil(resize_w / 32)\n            resize_w = N * 32\n        ratio_h = float(resize_h) / ori_h\n        ratio_w = float(resize_w) / ori_w\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        # return img, np.array([ori_h, ori_w])\n        return img, [ratio_h, ratio_w]\n\n    def resize_image_type0(self, img):\n        \"\"\"\n        resize image to a size multiple of 32 which is required by the network\n        args:\n            img(array): array with shape [h, w, c]\n        return(tuple):\n            img, (ratio_h, ratio_w)\n        \"\"\"\n        limit_side_len = self.limit_side_len\n        h, w, c = img.shape\n\n        # limit the max side\n        if self.limit_type == 'max':\n            if max(h, w) > limit_side_len:\n                if h > w:\n                    ratio = float(limit_side_len) / h\n                else:\n                    ratio = float(limit_side_len) / w\n            else:\n                ratio = 1.\n        elif self.limit_type == 'min':\n            if min(h, w) < limit_side_len:\n                if h < w:\n                    ratio = float(limit_side_len) / h\n                else:\n                    ratio = float(limit_side_len) / w\n            else:\n                ratio = 1.\n        elif self.limit_type == 'resize_long':\n            ratio = float(limit_side_len) / max(h, w)\n        else:\n            raise Exception('not support limit type, image ')\n        resize_h = int(h * ratio)\n        resize_w = int(w * ratio)\n\n        resize_h = max(int(round(resize_h / 32) * 32), 32)\n        resize_w = max(int(round(resize_w / 32) * 32), 32)\n\n        try:\n            if int(resize_w) <= 0 or int(resize_h) <= 0:\n                return None, (None, None)\n            img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        except:\n            print(img.shape, resize_w, resize_h)\n            sys.exit(0)\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n        return img, [ratio_h, ratio_w]\n\n    def resize_image_type2(self, img):\n        h, w, _ = img.shape\n\n        resize_w = w\n        resize_h = h\n\n        if resize_h > resize_w:\n            ratio = float(self.resize_long) / resize_h\n        else:\n            ratio = float(self.resize_long) / resize_w\n\n        resize_h = int(resize_h * ratio)\n        resize_w = int(resize_w * ratio)\n\n        max_stride = 128\n        resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n        resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n        img = cv2.resize(img, (int(resize_w), int(resize_h)))\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n\n        return img, [ratio_h, ratio_w]\n\n\nclass E2EResizeForTest(object):\n    def __init__(self, **kwargs):\n        super(E2EResizeForTest, self).__init__()\n        self.max_side_len = kwargs['max_side_len']\n        self.valid_set = kwargs['valid_set']\n\n    def __call__(self, data):\n        img = data['image']\n        src_h, src_w, _ = img.shape\n        if self.valid_set == 'totaltext':\n            im_resized, [ratio_h, ratio_w] = self.resize_image_for_totaltext(\n                img, max_side_len=self.max_side_len)\n        else:\n            im_resized, (ratio_h, ratio_w) = self.resize_image(\n                img, max_side_len=self.max_side_len)\n        data['image'] = im_resized\n        data['shape'] = np.array([src_h, src_w, ratio_h, ratio_w])\n        return data\n\n    def resize_image_for_totaltext(self, im, max_side_len=512):\n\n        h, w, _ = im.shape\n        resize_w = w\n        resize_h = h\n        ratio = 1.25\n        if h * ratio > max_side_len:\n            ratio = float(max_side_len) / resize_h\n        resize_h = int(resize_h * ratio)\n        resize_w = int(resize_w * ratio)\n\n        max_stride = 128\n        resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n        resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n        im = cv2.resize(im, (int(resize_w), int(resize_h)))\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n        return im, (ratio_h, ratio_w)\n\n    def resize_image(self, im, max_side_len=512):\n        \"\"\"\n        resize image to a size multiple of max_stride which is required by the network\n        :param im: the resized image\n        :param max_side_len: limit of max image size to avoid out of memory in gpu\n        :return: the resized image and the resize ratio\n        \"\"\"\n        h, w, _ = im.shape\n\n        resize_w = w\n        resize_h = h\n\n        # Fix the longer side\n        if resize_h > resize_w:\n            ratio = float(max_side_len) / resize_h\n        else:\n            ratio = float(max_side_len) / resize_w\n\n        resize_h = int(resize_h * ratio)\n        resize_w = int(resize_w * ratio)\n\n        max_stride = 128\n        resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n        resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n        im = cv2.resize(im, (int(resize_w), int(resize_h)))\n        ratio_h = resize_h / float(h)\n        ratio_w = resize_w / float(w)\n\n        return im, (ratio_h, ratio_w)\n\n\nclass KieResize(object):\n    def __init__(self, **kwargs):\n        super(KieResize, self).__init__()\n        self.max_side, self.min_side = kwargs['img_scale'][0], kwargs[\n            'img_scale'][1]\n\n    def __call__(self, data):\n        img = data['image']\n        points = data['points']\n        src_h, src_w, _ = img.shape\n        im_resized, scale_factor, [ratio_h,\n                                   ratio_w], [new_h,\n                                              new_w] = self.resize_image(img)\n        resize_points = self.resize_boxes(img, points, scale_factor)\n        data['ori_image'] = img\n        data['ori_boxes'] = points\n        data['points'] = resize_points\n        data['image'] = im_resized\n        data['shape'] = np.array([new_h, new_w])\n        return data\n\n    def resize_image(self, img):\n        norm_img = np.zeros([1024, 1024, 3], dtype='float32')\n        scale = [512, 1024]\n        h, w = img.shape[:2]\n        max_long_edge = max(scale)\n        max_short_edge = min(scale)\n        scale_factor = min(max_long_edge / max(h, w),\n                           max_short_edge / min(h, w))\n        resize_w, resize_h = int(w * float(scale_factor) +\n                                 0.5), int(h * float(scale_factor) + 0.5)\n        max_stride = 32\n        resize_h = (resize_h + max_stride - 1) // max_stride * max_stride\n        resize_w = (resize_w + max_stride - 1) // max_stride * max_stride\n        im = cv2.resize(img, (resize_w, resize_h))\n        new_h, new_w = im.shape[:2]\n        w_scale = new_w / w\n        h_scale = new_h / h\n        scale_factor = np.array([w_scale, h_scale, w_scale, h_scale],\n                                dtype=np.float32)\n        norm_img[:new_h, :new_w, :] = im\n        return norm_img, scale_factor, [h_scale, w_scale], [new_h, new_w]\n\n    def resize_boxes(self, im, points, scale_factor):\n        points = points * scale_factor\n        img_shape = im.shape[:2]\n        points[:, 0::2] = np.clip(points[:, 0::2], 0, img_shape[1])\n        points[:, 1::2] = np.clip(points[:, 1::2], 0, img_shape[0])\n        return points\n\n\nclass SRResize(object):\n    def __init__(self,\n                 imgH=32,\n                 imgW=128,\n                 down_sample_scale=4,\n                 keep_ratio=False,\n                 min_ratio=1,\n                 mask=False,\n                 infer_mode=False,\n                 **kwargs):\n        self.imgH = imgH\n        self.imgW = imgW\n        self.keep_ratio = keep_ratio\n        self.min_ratio = min_ratio\n        self.down_sample_scale = down_sample_scale\n        self.mask = mask\n        self.infer_mode = infer_mode\n\n    def __call__(self, data):\n        imgH = self.imgH\n        imgW = self.imgW\n        images_lr = data[\"image_lr\"]\n        transform2 = ResizeNormalize(\n            (imgW // self.down_sample_scale, imgH // self.down_sample_scale))\n        images_lr = transform2(images_lr)\n        data[\"img_lr\"] = images_lr\n        if self.infer_mode:\n            return data\n\n        images_HR = data[\"image_hr\"]\n        label_strs = data[\"label\"]\n        transform = ResizeNormalize((imgW, imgH))\n        images_HR = transform(images_HR)\n        data[\"img_hr\"] = images_HR\n        return data\n\n\nclass ResizeNormalize(object):\n    def __init__(self, size, interpolation=Image.BICUBIC):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        img = img.resize(self.size, self.interpolation)\n        img_numpy = np.array(img).astype(\"float32\")\n        img_numpy = img_numpy.transpose((2, 0, 1)) / 255\n        return img_numpy\n\n\nclass GrayImageChannelFormat(object):\n    \"\"\"\n    format gray scale image's channel: (3,h,w) -> (1,h,w)\n    Args:\n        inverse: inverse gray image \n    \"\"\"\n    def __init__(self, inverse=False, **kwargs):\n        self.inverse = inverse\n\n    def __call__(self, data):\n        img = data['image']\n        img_single_channel = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        img_expanded = np.expand_dims(img_single_channel, 0)\n\n        if self.inverse:\n            data['image'] = np.abs(img_expanded - 1)\n        else:\n            data['image'] = img_expanded\n\n        data['src_image'] = img\n        return data"}
{"type": "source_file", "path": "utils/ditod_vgt/tokenization_bros.py", "content": "# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes.\"\"\"\n\n\nimport collections\n\nfrom transformers.models.bert.tokenization_bert import BertTokenizer\nfrom transformers.utils import logging\n\nlogger = logging.get_logger(__name__)\n\nVOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"naver-clova-ocr/bros-base-uncased\": \"https://huggingface.co/naver-clova-ocr/bros-base-uncased/resolve/main/vocab.txt\",\n        \"naver-clova-ocr/bros-large-uncased\": \"https://huggingface.co/naver-clova-ocr/bros-large-uncased/resolve/main/vocab.txt\",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \"naver-clova-ocr/bros-base-uncased\": 512,\n    \"naver-clova-ocr/bros-large-uncased\": 512,\n}\n\nPRETRAINED_INIT_CONFIGURATION = {\n    \"naver-clova-ocr/bros-base-uncased\": {\"do_lower_case\": True},\n    \"naver-clova-ocr/bros-large-uncased\": {\"do_lower_case\": True},\n}\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\"\\n\")\n        vocab[token] = index\n    return vocab\n\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(\"utf-8\", \"ignore\")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BrosTokenizer(BertTokenizer):\n    r\"\"\"\n    Construct a BERT tokenizer. Based on WordPiece.\n\n    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.\n    Users should refer to this superclass for more information regarding those methods.\n\n    Args:\n        vocab_file (:obj:`str`):\n            File containing the vocabulary.\n        do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to lowercase the input when tokenizing.\n        do_basic_tokenize (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to do basic tokenization before WordPiece.\n        never_split (:obj:`Iterable`, `optional`):\n            Collection of tokens which will never be split during tokenization. Only has an effect when\n            :obj:`do_basic_tokenize=True`\n        unk_token (:obj:`str`, `optional`, defaults to :obj:`\"[UNK]\"`):\n            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n            token instead.\n        sep_token (:obj:`str`, `optional`, defaults to :obj:`\"[SEP]\"`):\n            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n            sequence classification or for a text and a question for question answering. It is also used as the last\n            token of a sequence built with special tokens.\n        pad_token (:obj:`str`, `optional`, defaults to :obj:`\"[PAD]\"`):\n            The token used for padding, for example when batching sequences of different lengths.\n        cls_token (:obj:`str`, `optional`, defaults to :obj:`\"[CLS]\"`):\n            The classifier token which is used when doing sequence classification (classification of the whole sequence\n            instead of per-token classification). It is the first token of the sequence when built with special tokens.\n        mask_token (:obj:`str`, `optional`, defaults to :obj:`\"[MASK]\"`):\n            The token used for masking values. This is the token used when training this model with masked language\n            modeling. This is the token which the model will try to predict.\n        tokenize_chinese_chars (:obj:`bool`, `optional`, defaults to :obj:`True`):\n            Whether or not to tokenize Chinese characters.\n\n            This should likely be deactivated for Japanese (see this `issue\n            <https://github.com/huggingface/transformers/issues/328>`__).\n        strip_accents: (:obj:`bool`, `optional`):\n            Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n            value for :obj:`lowercase` (as in the original BERT).\n    \"\"\"\n\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/postprocess/db_postprocess.py", "content": "# Copyright (c) 2020 PaddlePaddle Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refered from:\nhttps://github.com/WenmuZhou/DBNet.pytorch/blob/master/post_processing/seg_detector_representer.py\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport cv2\nfrom shapely.geometry import Polygon\nimport pyclipper\n\n\nclass DBPostProcess(object):\n    \"\"\"\n    The post process for Differentiable Binarization (DB).\n    \"\"\"\n    def __init__(self,\n                 thresh=0.3,\n                 box_thresh=0.7,\n                 max_candidates=1000,\n                 unclip_ratio=2.0,\n                 use_dilation=False,\n                 score_mode=\"fast\",\n                 box_type='quad',\n                 **kwargs):\n        self.thresh = thresh\n        self.box_thresh = box_thresh\n        self.max_candidates = max_candidates\n        self.unclip_ratio = unclip_ratio\n        self.min_size = 3\n        self.score_mode = score_mode\n        self.box_type = box_type\n        assert score_mode in [\n            \"slow\", \"fast\"\n        ], \"Score mode must be in [slow, fast] but got: {}\".format(score_mode)\n\n        self.dilation_kernel = None if not use_dilation else np.array([[1, 1],\n                                                                       [1, 1]])\n\n    def polygons_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        '''\n        _bitmap: single map with shape (1, H, W),\n            whose values are binarized as {0, 1}\n        '''\n\n        bitmap = _bitmap\n        height, width = bitmap.shape\n\n        boxes = []\n        scores = []\n\n        contours, _ = cv2.findContours((bitmap * 255).astype(np.uint8),\n                                       cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours[:self.max_candidates]:\n            epsilon = 0.002 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            points = approx.reshape((-1, 2))\n            if points.shape[0] < 4:\n                continue\n\n            score = self.box_score_fast(pred, points.reshape(-1, 2))\n            if self.box_thresh > score:\n                continue\n\n            if points.shape[0] > 2:\n                box = self.unclip(points, self.unclip_ratio)\n                if len(box) > 1:\n                    continue\n            else:\n                continue\n            box = box.reshape(-1, 2)\n\n            _, sside = self.get_mini_boxes(box.reshape((-1, 1, 2)))\n            if sside < self.min_size + 2:\n                continue\n\n            box = np.array(box)\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0,\n                                dest_width)\n            box[:, 1] = np.clip(np.round(box[:, 1] / height * dest_height), 0,\n                                dest_height)\n            boxes.append(box.tolist())\n            scores.append(score)\n        return boxes, scores\n\n    def boxes_from_bitmap(self, pred, _bitmap, dest_width, dest_height):\n        '''\n        _bitmap: single map with shape (1, H, W),\n                whose values are binarized as {0, 1}\n        '''\n\n        bitmap = _bitmap\n        height, width = bitmap.shape\n\n        outs = cv2.findContours((bitmap * 255).astype(np.uint8), cv2.RETR_LIST,\n                                cv2.CHAIN_APPROX_SIMPLE)\n        if len(outs) == 3:\n            img, contours, _ = outs[0], outs[1], outs[2]\n        elif len(outs) == 2:\n            contours, _ = outs[0], outs[1]\n\n        num_contours = min(len(contours), self.max_candidates)\n\n        boxes = []\n        scores = []\n        for index in range(num_contours):\n            contour = contours[index]\n            points, sside = self.get_mini_boxes(contour)\n            if sside < self.min_size:\n                continue\n            points = np.array(points)\n            if self.score_mode == \"fast\":\n                score = self.box_score_fast(pred, points.reshape(-1, 2))\n            else:\n                score = self.box_score_slow(pred, contour)\n            if self.box_thresh > score:\n                continue\n\n            box = self.unclip(points, self.unclip_ratio).reshape(-1, 1, 2)\n            box, sside = self.get_mini_boxes(box)\n            if sside < self.min_size + 2:\n                continue\n            box = np.array(box)\n\n            box[:, 0] = np.clip(np.round(box[:, 0] / width * dest_width), 0,\n                                dest_width)\n            box[:, 1] = np.clip(np.round(box[:, 1] / height * dest_height), 0,\n                                dest_height)\n            boxes.append(box.astype(\"int32\"))\n            scores.append(score)\n        return np.array(boxes, dtype=\"int32\"), scores\n\n    def unclip(self, box, unclip_ratio):\n        poly = Polygon(box)\n        distance = poly.area * unclip_ratio / poly.length\n        offset = pyclipper.PyclipperOffset()\n        offset.AddPath(box, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n        expanded = np.array(offset.Execute(distance))\n        return expanded\n\n    def get_mini_boxes(self, contour):\n        bounding_box = cv2.minAreaRect(contour)\n        points = sorted(list(cv2.boxPoints(bounding_box)), key=lambda x: x[0])\n\n        index_1, index_2, index_3, index_4 = 0, 1, 2, 3\n        if points[1][1] > points[0][1]:\n            index_1 = 0\n            index_4 = 1\n        else:\n            index_1 = 1\n            index_4 = 0\n        if points[3][1] > points[2][1]:\n            index_2 = 2\n            index_3 = 3\n        else:\n            index_2 = 3\n            index_3 = 2\n\n        box = [\n            points[index_1], points[index_2], points[index_3], points[index_4]\n        ]\n        return box, min(bounding_box[1])\n\n    def box_score_fast(self, bitmap, _box):\n        '''\n        box_score_fast: use bbox mean score as the mean score\n        '''\n        h, w = bitmap.shape[:2]\n        box = _box.copy()\n        xmin = np.clip(np.floor(box[:, 0].min()).astype(\"int32\"), 0, w - 1)\n        xmax = np.clip(np.ceil(box[:, 0].max()).astype(\"int32\"), 0, w - 1)\n        ymin = np.clip(np.floor(box[:, 1].min()).astype(\"int32\"), 0, h - 1)\n        ymax = np.clip(np.ceil(box[:, 1].max()).astype(\"int32\"), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n        box[:, 0] = box[:, 0] - xmin\n        box[:, 1] = box[:, 1] - ymin\n        cv2.fillPoly(mask, box.reshape(1, -1, 2).astype(\"int32\"), 1)\n        return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0]\n\n    def box_score_slow(self, bitmap, contour):\n        '''\n        box_score_slow: use polyon mean score as the mean score\n        '''\n        h, w = bitmap.shape[:2]\n        contour = contour.copy()\n        contour = np.reshape(contour, (-1, 2))\n\n        xmin = np.clip(np.min(contour[:, 0]), 0, w - 1)\n        xmax = np.clip(np.max(contour[:, 0]), 0, w - 1)\n        ymin = np.clip(np.min(contour[:, 1]), 0, h - 1)\n        ymax = np.clip(np.max(contour[:, 1]), 0, h - 1)\n\n        mask = np.zeros((ymax - ymin + 1, xmax - xmin + 1), dtype=np.uint8)\n\n        contour[:, 0] = contour[:, 0] - xmin\n        contour[:, 1] = contour[:, 1] - ymin\n\n        cv2.fillPoly(mask, contour.reshape(1, -1, 2).astype(\"int32\"), 1)\n        return cv2.mean(bitmap[ymin:ymax + 1, xmin:xmax + 1], mask)[0]\n\n    def __call__(self, outs_dict, shape_list):\n        pred = outs_dict['maps']\n        # pred = pred.numpy()\n        pred = pred[:, 0, :, :]\n        segmentation = pred > self.thresh\n\n        boxes_batch = []\n        for batch_index in range(pred.shape[0]):\n            src_h, src_w, ratio_h, ratio_w = shape_list[batch_index]\n            if self.dilation_kernel is not None:\n                mask = cv2.dilate(\n                    np.array(segmentation[batch_index]).astype(np.uint8),\n                    self.dilation_kernel)\n            else:\n                mask = segmentation[batch_index]\n            if self.box_type == 'poly':\n                boxes, scores = self.polygons_from_bitmap(\n                    pred[batch_index], mask, src_w, src_h)\n            elif self.box_type == 'quad':\n                boxes, scores = self.boxes_from_bitmap(pred[batch_index], mask,\n                                                       src_w, src_h)\n            else:\n                raise ValueError(\"box_type can only be one of ['quad', 'poly']\")\n\n            boxes_batch.append({'points': boxes})\n        return boxes_batch\n\n\nclass DistillationDBPostProcess(object):\n    def __init__(self,\n                 model_name=[\"student\"],\n                 key=None,\n                 thresh=0.3,\n                 box_thresh=0.6,\n                 max_candidates=1000,\n                 unclip_ratio=1.5,\n                 use_dilation=False,\n                 score_mode=\"fast\",\n                 box_type='quad',\n                 **kwargs):\n        self.model_name = model_name\n        self.key = key\n        self.post_process = DBPostProcess(thresh=thresh,\n                                          box_thresh=box_thresh,\n                                          max_candidates=max_candidates,\n                                          unclip_ratio=unclip_ratio,\n                                          use_dilation=use_dilation,\n                                          score_mode=score_mode,\n                                          box_type=box_type)\n\n    def __call__(self, predicts, shape_list):\n        results = {}\n        for k in self.model_name:\n            results[k] = self.post_process(predicts[k], shape_list=shape_list)\n        return results\n"}
{"type": "source_file", "path": "utils/ocr_model/ppocr_onnx/ppocr/postprocess/drrg_postprocess.py", "content": "# copyright (c) 2022 PaddlePaddle Authors. All Rights Reserve.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nThis code is refer from:\nhttps://github.com/open-mmlab/mmocr/blob/main/mmocr/models/textdet/postprocess/drrg_postprocessor.py\n\"\"\"\n\nimport functools\nimport operator\n\nimport numpy as np\nfrom numpy.linalg import norm\nimport cv2\n\n\nclass Node:\n    def __init__(self, ind):\n        self.__ind = ind\n        self.__links = set()\n\n    @property\n    def ind(self):\n        return self.__ind\n\n    @property\n    def links(self):\n        return set(self.__links)\n\n    def add_link(self, link_node):\n        self.__links.add(link_node)\n        link_node.__links.add(self)\n\n\ndef graph_propagation(edges, scores, text_comps, edge_len_thr=50.):\n    assert edges.ndim == 2\n    assert edges.shape[1] == 2\n    assert edges.shape[0] == scores.shape[0]\n    assert text_comps.ndim == 2\n    assert isinstance(edge_len_thr, float)\n\n    edges = np.sort(edges, axis=1)\n    score_dict = {}\n    for i, edge in enumerate(edges):\n        if text_comps is not None:\n            box1 = text_comps[edge[0], :8].reshape(4, 2)\n            box2 = text_comps[edge[1], :8].reshape(4, 2)\n            center1 = np.mean(box1, axis=0)\n            center2 = np.mean(box2, axis=0)\n            distance = norm(center1 - center2)\n            if distance > edge_len_thr:\n                scores[i] = 0\n        if (edge[0], edge[1]) in score_dict:\n            score_dict[edge[0], edge[1]] = 0.5 * (score_dict[edge[0], edge[1]] +\n                                                  scores[i])\n        else:\n            score_dict[edge[0], edge[1]] = scores[i]\n\n    nodes = np.sort(np.unique(edges.flatten()))\n    mapping = -1 * np.ones((np.max(nodes) + 1), dtype=np.int)\n    mapping[nodes] = np.arange(nodes.shape[0])\n    order_inds = mapping[edges]\n    vertices = [Node(node) for node in nodes]\n    for ind in order_inds:\n        vertices[ind[0]].add_link(vertices[ind[1]])\n\n    return vertices, score_dict\n\n\ndef connected_components(nodes, score_dict, link_thr):\n    assert isinstance(nodes, list)\n    assert all([isinstance(node, Node) for node in nodes])\n    assert isinstance(score_dict, dict)\n    assert isinstance(link_thr, float)\n\n    clusters = []\n    nodes = set(nodes)\n    while nodes:\n        node = nodes.pop()\n        cluster = {node}\n        node_queue = [node]\n        while node_queue:\n            node = node_queue.pop(0)\n            neighbors = set([\n                neighbor for neighbor in node.links if\n                score_dict[tuple(sorted([node.ind, neighbor.ind]))] >= link_thr\n            ])\n            neighbors.difference_update(cluster)\n            nodes.difference_update(neighbors)\n            cluster.update(neighbors)\n            node_queue.extend(neighbors)\n        clusters.append(list(cluster))\n    return clusters\n\n\ndef clusters2labels(clusters, num_nodes):\n    assert isinstance(clusters, list)\n    assert all([isinstance(cluster, list) for cluster in clusters])\n    assert all(\n        [isinstance(node, Node) for cluster in clusters for node in cluster])\n    assert isinstance(num_nodes, int)\n\n    node_labels = np.zeros(num_nodes)\n    for cluster_ind, cluster in enumerate(clusters):\n        for node in cluster:\n            node_labels[node.ind] = cluster_ind\n    return node_labels\n\n\ndef remove_single(text_comps, comp_pred_labels):\n    assert text_comps.ndim == 2\n    assert text_comps.shape[0] == comp_pred_labels.shape[0]\n\n    single_flags = np.zeros_like(comp_pred_labels)\n    pred_labels = np.unique(comp_pred_labels)\n    for label in pred_labels:\n        current_label_flag = (comp_pred_labels == label)\n        if np.sum(current_label_flag) == 1:\n            single_flags[np.where(current_label_flag)[0][0]] = 1\n    keep_ind = [i for i in range(len(comp_pred_labels)) if not single_flags[i]]\n    filtered_text_comps = text_comps[keep_ind, :]\n    filtered_labels = comp_pred_labels[keep_ind]\n\n    return filtered_text_comps, filtered_labels\n\n\ndef norm2(point1, point2):\n    return ((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)**0.5\n\n\ndef min_connect_path(points):\n    assert isinstance(points, list)\n    assert all([isinstance(point, list) for point in points])\n    assert all([isinstance(coord, int) for point in points for coord in point])\n\n    points_queue = points.copy()\n    shortest_path = []\n    current_edge = [[], []]\n\n    edge_dict0 = {}\n    edge_dict1 = {}\n    current_edge[0] = points_queue[0]\n    current_edge[1] = points_queue[0]\n    points_queue.remove(points_queue[0])\n    while points_queue:\n        for point in points_queue:\n            length0 = norm2(point, current_edge[0])\n            edge_dict0[length0] = [point, current_edge[0]]\n            length1 = norm2(current_edge[1], point)\n            edge_dict1[length1] = [current_edge[1], point]\n        key0 = min(edge_dict0.keys())\n        key1 = min(edge_dict1.keys())\n\n        if key0 <= key1:\n            start = edge_dict0[key0][0]\n            end = edge_dict0[key0][1]\n            shortest_path.insert(0, [points.index(start), points.index(end)])\n            points_queue.remove(start)\n            current_edge[0] = start\n        else:\n            start = edge_dict1[key1][0]\n            end = edge_dict1[key1][1]\n            shortest_path.append([points.index(start), points.index(end)])\n            points_queue.remove(end)\n            current_edge[1] = end\n\n        edge_dict0 = {}\n        edge_dict1 = {}\n\n    shortest_path = functools.reduce(operator.concat, shortest_path)\n    shortest_path = sorted(set(shortest_path), key=shortest_path.index)\n\n    return shortest_path\n\n\ndef in_contour(cont, point):\n    x, y = point\n    is_inner = cv2.pointPolygonTest(cont, (int(x), int(y)), False) > 0.5\n    return is_inner\n\n\ndef fix_corner(top_line, bot_line, start_box, end_box):\n    assert isinstance(top_line, list)\n    assert all(isinstance(point, list) for point in top_line)\n    assert isinstance(bot_line, list)\n    assert all(isinstance(point, list) for point in bot_line)\n    assert start_box.shape == end_box.shape == (4, 2)\n\n    contour = np.array(top_line + bot_line[::-1])\n    start_left_mid = (start_box[0] + start_box[3]) / 2\n    start_right_mid = (start_box[1] + start_box[2]) / 2\n    end_left_mid = (end_box[0] + end_box[3]) / 2\n    end_right_mid = (end_box[1] + end_box[2]) / 2\n    if not in_contour(contour, start_left_mid):\n        top_line.insert(0, start_box[0].tolist())\n        bot_line.insert(0, start_box[3].tolist())\n    elif not in_contour(contour, start_right_mid):\n        top_line.insert(0, start_box[1].tolist())\n        bot_line.insert(0, start_box[2].tolist())\n    if not in_contour(contour, end_left_mid):\n        top_line.append(end_box[0].tolist())\n        bot_line.append(end_box[3].tolist())\n    elif not in_contour(contour, end_right_mid):\n        top_line.append(end_box[1].tolist())\n        bot_line.append(end_box[2].tolist())\n    return top_line, bot_line\n\n\ndef comps2boundaries(text_comps, comp_pred_labels):\n    assert text_comps.ndim == 2\n    assert len(text_comps) == len(comp_pred_labels)\n    boundaries = []\n    if len(text_comps) < 1:\n        return boundaries\n    for cluster_ind in range(0, int(np.max(comp_pred_labels)) + 1):\n        cluster_comp_inds = np.where(comp_pred_labels == cluster_ind)\n        text_comp_boxes = text_comps[cluster_comp_inds, :8].reshape(\n            (-1, 4, 2)).astype(np.int32)\n        score = np.mean(text_comps[cluster_comp_inds, -1])\n\n        if text_comp_boxes.shape[0] < 1:\n            continue\n\n        elif text_comp_boxes.shape[0] > 1:\n            centers = np.mean(text_comp_boxes, axis=1).astype(np.int32).tolist()\n            shortest_path = min_connect_path(centers)\n            text_comp_boxes = text_comp_boxes[shortest_path]\n            top_line = np.mean(text_comp_boxes[:, 0:2, :],\n                               axis=1).astype(np.int32).tolist()\n            bot_line = np.mean(text_comp_boxes[:, 2:4, :],\n                               axis=1).astype(np.int32).tolist()\n            top_line, bot_line = fix_corner(top_line, bot_line,\n                                            text_comp_boxes[0],\n                                            text_comp_boxes[-1])\n            boundary_points = top_line + bot_line[::-1]\n\n        else:\n            top_line = text_comp_boxes[0, 0:2, :].astype(np.int32).tolist()\n            bot_line = text_comp_boxes[0, 2:4:-1, :].astype(np.int32).tolist()\n            boundary_points = top_line + bot_line\n\n        boundary = [p for coord in boundary_points for p in coord] + [score]\n        boundaries.append(boundary)\n\n    return boundaries\n\n\nclass DRRGPostprocess(object):\n    \"\"\"Merge text components and construct boundaries of text instances.\n\n    Args:\n        link_thr (float): The edge score threshold.\n    \"\"\"\n    def __init__(self, link_thr, **kwargs):\n        assert isinstance(link_thr, float)\n        self.link_thr = link_thr\n\n    def __call__(self, preds, shape_list):\n        \"\"\"\n        Args:\n            edges (ndarray): The edge array of shape N * 2, each row is a node\n                index pair that makes up an edge in graph.\n            scores (ndarray): The edge score array of shape (N,).\n            text_comps (ndarray): The text components.\n\n        Returns:\n            List[list[float]]: The predicted boundaries of text instances.\n        \"\"\"\n        edges, scores, text_comps = preds\n        if edges is not None:\n            if isinstance(edges, paddle.Tensor):\n                edges = edges.numpy()\n            if isinstance(scores, paddle.Tensor):\n                scores = scores.numpy()\n            if isinstance(text_comps, paddle.Tensor):\n                text_comps = text_comps.numpy()\n            assert len(edges) == len(scores)\n            assert text_comps.ndim == 2\n            assert text_comps.shape[1] == 9\n\n            vertices, score_dict = graph_propagation(edges, scores, text_comps)\n            clusters = connected_components(vertices, score_dict, self.link_thr)\n            pred_labels = clusters2labels(clusters, text_comps.shape[0])\n            text_comps, pred_labels = remove_single(text_comps, pred_labels)\n            boundaries = comps2boundaries(text_comps, pred_labels)\n        else:\n            boundaries = []\n\n        boundaries, scores = self.resize_boundary(\n            boundaries, (1 / shape_list[0, 2:]).tolist()[::-1])\n        boxes_batch = [dict(points=boundaries, scores=scores)]\n        return boxes_batch\n\n    def resize_boundary(self, boundaries, scale_factor):\n        \"\"\"Rescale boundaries via scale_factor.\n\n        Args:\n            boundaries (list[list[float]]): The boundary list. Each boundary\n            with size 2k+1 with k>=4.\n            scale_factor(ndarray): The scale factor of size (4,).\n\n        Returns:\n            boundaries (list[list[float]]): The scaled boundaries.\n        \"\"\"\n        boxes = []\n        scores = []\n        for b in boundaries:\n            sz = len(b)\n            scores.append(b[-1])\n            b = (np.array(b[:sz - 1]) *\n                 (np.tile(scale_factor[:2], int(\n                     (sz - 1) / 2)).reshape(1, sz - 1))).flatten().tolist()\n            boxes.append(np.array(b).reshape([-1, 2]))\n        return boxes, scores\n"}
