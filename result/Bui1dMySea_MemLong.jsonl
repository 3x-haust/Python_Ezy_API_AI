{"repo_info": {"repo_name": "MemLong", "repo_owner": "Bui1dMySea", "repo_url": "https://github.com/Bui1dMySea/MemLong"}}
{"type": "source_file", "path": "eval/icl/MemLong/align_memory.py", "content": "import numpy as np\nimport faiss\nimport faiss.contrib.torch_utils\nimport torch\nfrom typing import Literal, Tuple, List, Optional, Union\nfrom .utils import MemCache\nfrom dataclasses import dataclass\n\n\n\"\"\"\n记忆与存储机制\n** Memory **\n- 维护一个固定大小的记忆单元，用于存储历史信息\n- 利用 save 模块来更新记忆\n- 每次传入的记忆单元是一个 mem_update , 其包含了长度为不固定的 SigMemCache 对象\n- save 模块需要完成的操作包括：将SigMemCache对象拆解为文本，检索嵌入，Key-Value-Mask三元组；利用拆解对象，维护一个MemCache用于之后的检索\n** Retrieval **\n- 利用 retrieve 模块来检索记忆\n- 记忆长度分析：如果 bsz 间长度不一致，则用 PAD 填充\n\"\"\"\n\n@dataclass\nclass MemRecoder:\n    counters:List[List[int]]\n    dstore_idx:List[int]\n    length:List[List[int]]\n\nclass ChunkMemory:\n    def __init__(self, model_config, toolkit_config, device=None):\n        # llama config\n        self.hidden_size = model_config.hidden_size\n        self.num_heads = model_config.num_attention_heads\n        self.head_dim = int(self.hidden_size / self.num_heads)\n        self.memory_size = model_config.memory_size\n        self.pooling_tokens = model_config.pooling_tokens\n        self.dtype = model_config.mem_dtype\n        boundary = model_config.update_boundary\n        # toolkit config\n        self.task = toolkit_config.task\n        self.ret_embeddings_dim = toolkit_config.ret_embeddings_dim\n        # other config\n        if boundary is not None:\n            self.lower, self.upper = list(map(int, boundary.split(\",\")))\n        else:\n            self.lower , self.upper = 0 , 100\n        assert self.lower < self.upper and self.lower >= 0 and self.upper <= 100\n        # init\n        self.device = device\n        self.mem_caches = MemCache()\n        self.mem_caches.texts = []\n        self.mem_caches.embeddings = []\n        self.res = None\n        if device != torch.device(\"cpu\"):\n            self.res = faiss.StandardGpuResources()\n            \n        self.mem_recoder = MemRecoder(\n            counters = [],\n            dstore_idx = [],\n            length = []\n        )\n        \n        self.log_time = 1\n\n    def to(self,device):\n        old_device = self.device\n        self.device = device\n        if self.device != torch.device(\"cpu\"):\n            if self.res == None :\n                self.res = faiss.StandardGpuResources() \n            \n            for i,index in enumerate(self.mem_caches.embeddings):\n                print(f\"put index from {old_device} to {self.device}\")\n                if index == torch.device(\"cpu\"):\n                    index = faiss.index_cpu_to_gpu(self.res, self.device.index, index)\n                else:\n                    temp = faiss.index_gpu_to_cpu(index)\n                    index = faiss.index_cpu_to_gpu(self.res, self.device.index, temp)\n                    \n                self.mem_caches.embeddings[i] = index\n        \n        if self.mem_caches.keys is not None:\n            self.mem_caches.keys = self.mem_caches.keys.to(device)\n            self.mem_caches.values = self.mem_caches.values.to(device)\n            self.mem_caches.masks = self.mem_caches.masks.to(device)\n        \n    \n    def reset(self):\n        temp = self.mem_caches.embeddings\n        self.mem_caches = MemCache()\n        self.mem_caches.texts = []\n        for index in temp:\n            index.reset()\n        self.mem_caches.embeddings = temp\n        self.mem_recoder = MemRecoder(\n            counters = [],\n            dstore_idx = [],\n            length = []\n    )\n        \n    #DELETE\n    def _fill_and_update(self, kv_list) -> List:\n        device = self.device\n        pooling_list = []\n        for i, kv in enumerate(kv_list):\n            if kv is None:\n                pooling_list.append(None)\n                continue\n            k = kv[\"k\"]\n            v = kv[\"v\"]\n            pad_len = 0\n            if k.shape[0] % self.mem_granularity != 0:\n                pad_len = self.mem_granularity - k.shape[0] % self.mem_granularity\n                # fill\n                k = torch.cat((k,torch.zeros((pad_len, self.num_heads, self.head_dim),device=device,),),dim=0,)\n                v = torch.cat((v,torch.zeros((pad_len, self.num_heads, self.head_dim),device=device,),),dim=0,)\n\n            update_nums = k.shape[0] // self.mem_granularity\n            k = k.view(update_nums, self.mem_granularity, self.num_heads, self.head_dim)\n            v = v.view(update_nums, self.mem_granularity, self.num_heads, self.head_dim)\n            pooling_keys = [torch.zeros((update_nums, self.capacity, self.head_dim),device=device,)for _ in range(self.num_heads)]\n            pooling_values = [torch.zeros(\n(update_nums, self.capacity, self.head_dim),\n                    device=device,\n                )\n                for _ in range(self.num_heads)\n            ]\n            for i in range(self.num_heads):\n                # TODO: optimize\n                k_norm = [\n                    torch.mean(\n                        k[\n                            : update_nums - 1,\n                            num * self.pooling_tokens : (num + 1) * self.pooling_tokens,\n                            i,\n                            :,\n                        ],\n                        dim=1,\n                    )\n                    for num in range(self.capacity)\n                ]\n                k_chg = [\n                    (\n                        torch.mean(\n                            k[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                        if num != self.capacity\n                        else torch.mean(\n                            k[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens\n                                - pad_len,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                    )\n                    for num in range(self.capacity)\n                ]\n                v_norm = [\n                    torch.mean(\n                        v[\n                            : update_nums - 1,\n                            num * self.pooling_tokens : (num + 1) * self.pooling_tokens,\n                            i,\n                            :,\n                        ],\n                        dim=1,\n                    )\n                    for num in range(self.capacity)\n                ]\n                v_chg = [\n                    (\n                        torch.mean(\n                            v[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                        if num != self.capacity\n                        else torch.mean(\n                            v[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens\n                                - pad_len,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                    )\n                    for num in range(self.capacity)\n                ]\n\n                merged_k = [\n                    torch.cat((item1, item2.unsqueeze(0)), dim=0)\n                    for item1, item2 in zip(k_norm, k_chg)\n                ]\n                merged_v = [\n                    torch.cat((item1, item2.unsqueeze(0)), dim=0)\n                    for item1, item2 in zip(v_norm, v_chg)\n                ]\n                pooling_keys[i] = torch.stack(\n                    merged_k,\n                    dim=1,\n                )\n                pooling_values[i] = torch.stack(\n                    merged_v,\n                    dim=1,\n                )\n                # add\n            pooling_list.append(\n                {\n                    \"pooling_keys\": torch.stack(pooling_keys, dim=-2),\n                    \"pooling_values\": torch.stack(pooling_values, dim=-2),\n                }\n            )\n        return pooling_list\n\n    def get_key_embeddings(self):\n        return self.ret_embeddings\n\n    def achieve_condition(self, condition=5):\n        for idx in self.dstore_idx:\n            if idx < condition:\n                return False\n        return True\n    \n    def get_min_number(self):\n        return min(self.mem_recoder.dstore_idx) if self.mem_recoder.dstore_idx != [] else 0\n    \n    def save(self,mem_update):\n        if self.log_time>0:\n            print('use mem!!!')\n            self.log_time -= 1\n        # 1. Text\n        texts = mem_update.texts\n        # 2. embeddings\n        embeddings = mem_update.embeddings\n        # 3. key-value-mask\n        keys , values, masks = mem_update.keys , mem_update.values , mem_update.masks\n        \n        if len(keys.shape) != 4 or len(values.shape) != 4 or len(masks.shape) != 4:\n            raise ValueError(f\"Memory cache content should be consistent in shape got {keys.shape} {values.shape} {masks.shape}\")\n        \n        # No pooling\n        if self.pooling_tokens is None:\n            # check overflow\n            if self.mem_caches.keys is not None and self.mem_caches.keys.shape[-2] + keys.shape[-2] > self.memory_size:\n                # print(\"Memory is full, update memory\")\n                self.mem_caches.keys = self.mem_caches.keys[...,-self.memory_size//2:,:]\n                self.mem_caches.values = self.mem_caches.values[...,-self.memory_size//2:,:]\n                self.mem_caches.masks = self.mem_caches.masks[...,-self.memory_size//2:,:]\n\n                for i in range(len(self.mem_caches.keys)):\n                    if self.device != torch.device(\"cpu\"):\n                        if self.log_time == 0:\n                            print(\"remove index to cpu\")\n                            self.log_time -= 1\n                        temp = faiss.index_gpu_to_cpu(self.mem_caches.embeddings[i])\n                    else:\n                        if self.log_time == 0:\n                            print(\"remove in cpu\")\n                            self.log_time -= 1\n                        temp = self.mem_caches.embeddings[i]\n                    assert temp.ntotal == len(self.mem_caches.texts[i]) == len(self.mem_recoder.length[i]) == self.mem_recoder.dstore_idx[i]\n                    rm_indices = torch.arange(0,temp.ntotal//2)\n                    rm_total = temp.ntotal // 2\n                    temp.remove_ids(rm_indices.cpu().numpy())\n                    if self.device != torch.device(\"cpu\"):\n                        self.mem_caches.embeddings[i] = faiss.index_cpu_to_gpu(self.res, self.device.index, temp)\n                    else:\n                        self.mem_caches.embeddings[i] = temp\n                    self.mem_caches.texts[i] = self.mem_caches.texts[i][-rm_total:]\n                    self.mem_recoder.length[i] = self.mem_recoder.length[i][-rm_total:]\n                    self.mem_recoder.dstore_idx[i] = self.mem_recoder.dstore_idx[i]//2\n\n            self.mem_caches.keys = torch.cat((self.mem_caches.keys, keys), dim=-2) if self.mem_caches.keys is not None else keys\n            self.mem_caches.values = torch.cat((self.mem_caches.values, values), dim=-2) if self.mem_caches.values is not None else values\n            self.mem_caches.masks = torch.cat((self.mem_caches.masks, masks), dim=-2) if self.mem_caches.masks is not None else masks\n            \n            bsz = len(texts)\n            for i in range(bsz):\n                if len(self.mem_caches.texts) < i + 1:\n                    # init\n                    if len(self.mem_caches.embeddings) < i + 1:\n                        index = faiss.IndexFlatIP(self.ret_embeddings_dim)\n                        if self.device != torch.device(\"cpu\"):\n                            print(f\"put index {i} from cpu to gpu {self.device}\")\n                            index = faiss.index_cpu_to_gpu(self.res, self.device.index, index)\n                        self.mem_caches.embeddings.append(index)\n                    self.mem_caches.embeddings[i].add(embeddings[i].view(1,-1).to(torch.float32))\n                    self.mem_caches.texts.append(texts[i])\n                    self.mem_recoder.length.append([keys.shape[-2]])\n                    self.mem_recoder.dstore_idx.append(1)\n                else:    \n                    self.mem_caches.texts[i].append(texts[i])\n                    self.mem_caches.embeddings[i].add(embeddings[i].view(1,-1).to(torch.float32))\n                    self.mem_recoder.length[i].append(keys.shape[-2])\n                    self.mem_recoder.dstore_idx[i] += 1\n                \n        else:\n            # TODO: Pooling\n            pass\n    \"\"\"\n    def save(self, memory):\n        ret_embeddings_list = memory[\"embeddings_list\"]\n        examples_list = memory[\"examples_list\"]\n        # [bsz,seq_len,num_heads,head_dim]\n        kv_list = memory[\"kv_list\"]\n        pooling_list = self._fill_and_update(kv_list)\n        # update_nums是seq_len // pooling_tokens\n        # ret_embeddings的数量是 seq_len // csz\n        # 两个之间的映射关系其实是 csz // pooling_tokens\n\n        for i, (pooling_kv, ret_embeddings, examples_list) in enumerate(\n            zip(pooling_list, ret_embeddings_list, examples_list)\n        ):\n            if ret_embeddings is None:\n                continue\n            pooling_k = pooling_kv[\"pooling_keys\"]\n            pooling_v = pooling_kv[\"pooling_values\"]\n            update_nums = len(ret_embeddings)\n            if self.dstore_idx[i] + update_nums > self.memory_size:\n                print(f\"batch{i} : memory is full, update memory\")\n                del_num = 0\n                mask = torch.ones(\n                    self.dstore_idx[i], dtype=torch.bool, device=self.device\n                )\n                before_num = int(self.dstore_idx[i] * self.lower / 100)\n                upper_num = int(self.dstore_idx[i] * self.upper / 100)\n                # 1. 删除前10%\n                del_num += before_num\n                mask[:before_num] = False\n                # 2. 保留后10%\n                ret_counters_mid = self.ret_counters[i][before_num:upper_num]\n                # 3. 动态删除中间部分\n                counters, _ = torch.sort(torch.unique(ret_counters_mid))\n                count = 0\n                while del_num < self.dstore_idx[i] // 2:\n                    mid_mask = ret_counters_mid == counters[count]\n                    count += 1\n                    mid_del = torch.sum(mid_mask)\n                    del_num += mid_del\n                    mask[before_num:upper_num] = mask[before_num:upper_num].masked_fill(\n                        mid_mask, False\n                    )\n                # 4. 拼接\n                save_indices = torch.nonzero(mask).flatten()\n                rm_indices = torch.nonzero(~mask).flatten()\n                try:\n                    self.examples_list = [self.examples_list[i] for i in save_indices]\n                except IndexError:\n                    breakpoint()\n                cat_num = del_num + self.memory_size - self.dstore_idx[i]\n                self.keys[i] = torch.cat(\n                    (\n                        self.keys[\n                            torch.Tensor([i]).type_as(save_indices), save_indices\n                        ],\n                        torch.zeros(\n                            cat_num, self.capacity, self.num_heads, self.head_dim\n                        ).type_as(self.keys),\n                    ),\n                    dim=0,\n                )\n\n                self.values[i] = torch.cat(\n                    (\n                        self.values[\n                            torch.Tensor([i]).type_as(save_indices), save_indices\n                        ],\n                        torch.zeros(\n                            cat_num, self.capacity, self.num_heads, self.head_dim\n                        ).type_as(self.values),\n                    ),\n                    dim=0,\n                )\n                if self.device != torch.device(\"cpu\"):\n                    temp = faiss.index_gpu_to_cpu(self.index_list[i])\n                else:\n                    temp = self.index_list[i]\n                remove_n_total = temp.remove_ids(rm_indices.cpu().numpy())\n                print(f\"Removing {remove_n_total} key-values from index\")\n                self.ret_counters[i] = torch.cat(\n                    (\n                        torch.masked_select(\n                            self.ret_counters[i][: self.dstore_idx[i]], mask\n                        ),\n                        torch.zeros(\n                            (cat_num),\n                            dtype=torch.int,\n                            device=self.ret_counters.device,\n                        ),\n                    ),\n                    dim=0,\n                )\n                self.dstore_idx[i] -= del_num\n                if self.device != torch.device(\"cpu\"):\n                    self.index_list[i] = faiss.index_cpu_to_gpu(\n                        self.res, self.device.index, temp\n                    )\n\n            # 更新\n            self.keys[i][\n                self.dstore_idx[i] : self.dstore_idx[i] + update_nums\n            ] = pooling_k\n            self.values[i][\n                self.dstore_idx[i] : self.dstore_idx[i] + update_nums\n            ] = pooling_v\n            self.index_list[i].add(ret_embeddings)\n            self.examples_list[i].append(examples_list[i])\n            self.dstore_idx[i] += update_nums\n    \"\"\"\n    def _expand_index_tensor(self, x):\n        return torch.cat(list(map(lambda x: torch.arange(x * self.div, (x + 1) * self.div), x)))\n\n    def retrieve_index(self, query_embeddings_list, k):\n        return [self.mem_caches.embeddings[i].search(query_embeddings_list[i].to(torch.float32), k) for i in range(len(query_embeddings_list))]    \n        \n    def expand_elements_2d(self,length_list,indices_list):\n        expand_indices = []\n        for length , indices in zip(length_list,indices_list):\n            indices = indices.tolist()[0]\n            tmp = []\n            for i in indices:\n                if i == 0:\n                    tmp.extend(list(range(length[i])))\n                else:\n                    total = sum(length[:i])\n                    tmp.extend(list(range(total,length[i] + total)))\n            expand_indices.append((tmp))\n        return expand_indices\n        \n    def get(self,indices_list:Optional[List[torch.Tensor]]) -> MemCache:\n        indices_list = [torch.sort(indices,dim=1)[0] for indices in indices_list]\n        mem_caches = MemCache()\n        expand_indices = self.expand_elements_2d(self.mem_recoder.length,indices_list)\n        expand_indices = torch.LongTensor(expand_indices).to(self.device)\n        kv_expand_indices = expand_indices.unsqueeze(1).unsqueeze(-1).expand(-1,self.num_heads,-1,self.head_dim)\n        mask_expand_indices = expand_indices.unsqueeze(1).unsqueeze(-1)\n        mem_caches.keys =  torch.gather(self.mem_caches.keys,2,kv_expand_indices)\n        mem_caches.values = torch.gather(self.mem_caches.values,2,kv_expand_indices)\n        mem_caches.masks = torch.gather(self.mem_caches.masks,2,mask_expand_indices)\n        \n        # TODO: ret_counters\n        \n        return mem_caches\n        \n\n"}
{"type": "source_file", "path": "eval/icl/MemLong/__init__.py", "content": ""}
{"type": "source_file", "path": "__init__.py", "content": ""}
{"type": "source_file", "path": "data/text_processing.py", "content": "import os\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer\nfrom itertools import chain\nfrom functools import partial\nfrom tqdm import tqdm\nimport argparse\nimport math \n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Process data for MemLong\")\n    parser.add_argument(\"--chunk_size\", type=int, default=1024, help=\"Chunk size for input_ids\")\n    parser.add_argument(\"--llama2_tokenizer\", type=str, default=\"NousResearch/Llama-2-7b-chat-hf\", help=\"Tokenizer for llama2\")\n    parser.add_argument(\"--hf_tokenizer\", type=str,required=True, help=\"Tokenizer for huggingface\")\n    parser.add_argument(\"--slimpajama_name_or_path\", type=str,required=True, help=\"Dataset name or path for slimpajama\")\n    parser.add_argument(\"--target_token\", type=int, default=500000000,help=\"Target token for processing\")\n\n    return parser.parse_args()\n    \ndef data_process(\n    chunk_size: int,  \n    llama2_tokenizer: AutoTokenizer,\n    hf_tokenizer: AutoTokenizer,\n    slimpajama_name_or_path: str,\n    target_token: int = 500000000,\n):\n    if not os.path.exists(f\"./processed/{target_token/1000000000:.1f}B_{chunk_size}\"):\n        os.makedirs(f\"./processed/{target_token/1000000000:.1f}B_{chunk_size}\")\n        \n    if hf_tokenizer.model_max_length < 131072:\n        hf_tokenizer.model_max_length = 1310720    \n    \n    slimpajama_dataset = load_dataset(slimpajama_name_or_path)\n    target_index = target_token // 131072 + 1\n    partial_split = slimpajama_dataset[\"train\"].select(range(target_index))\n    column_names = list(partial_split.features)\n    \n    def _group_input_ids(examples, **kwargs):\n        texts = llama2_tokenizer.batch_decode(examples[\"input_ids\"])\n        examples = hf_tokenizer(texts, add_special_tokens=False)\n        concatenated_examples = {\"input_ids\": list(chain(*examples[\"input_ids\"]))}\n        total_length = len(concatenated_examples[\"input_ids\"])\n        total_length = (total_length // chunk_size) * chunk_size\n        result = {\n            k: [text[i:i + chunk_size] for i in range(0, total_length, chunk_size)]\n            for k, text in concatenated_examples.items()\n        }\n        return result\n    \n    processed_chunks_set = partial_split.map(\n        _group_input_ids,\n        batch_size=4,\n        batched=True,\n        num_proc=64,\n        remove_columns=column_names\n    )\n    processed_chunks_set = processed_chunks_set.train_test_split(train_size=0.95,seed=42,shuffle=False)\n    processed_chunks_set[\"validation\"] = processed_chunks_set['test']\n    processed_chunks_set.pop(\"test\")\n    processed_chunks_set.save_to_disk(f\"./processed/{target_token/1000000000:.1f}B_{chunk_size}\")\n    print(\"Downloaded and processed data\")\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    data_process(\n        chunk_size=args.chunk_size, \n        llama2_tokenizer=AutoTokenizer.from_pretrained(args.llama2_tokenizer),\n        hf_tokenizer=AutoTokenizer.from_pretrained(args.hf_tokenizer),\n        slimpajama_name_or_path=args.slimpajama_name_or_path,\n        target_token=args.target_token,\n    )\n\n        \n        \n        "}
{"type": "source_file", "path": "eval/icl/MemLong/cache_utils.py", "content": "from dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\n\n@dataclass\nclass Cache:\n    \"\"\"\n    Base, abstract class for all caches. The actual data structure is specific to each subclass.\n    \"\"\"\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. These are specific to each subclass and allow new types of\n                cache to be created.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        raise NotImplementedError(\"Make sure to implement `update` in a subclass.\")\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states, if there is any.\"\"\"\n        raise NotImplementedError(\"Make sure to implement `get_max_length` in a subclass.\")\n\n    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Given the sequence length of the new inputs, returns the usable length of the cache.\"\"\"\n        # Cache without size limit -> all cache is usable\n        # Cache with size limit -> if the length cache plus the length of the new inputs is larger the maximum cache\n        #   length, we will need to evict part of the cache (and thus not all cache is usable)\n        max_length = self.get_max_length()\n        previous_seq_length = self.get_seq_length(layer_idx)\n        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n            return max_length - new_seq_length\n        return previous_seq_length\n\n    def reorder_cache(self, beam_idx: torch.LongTensor):\n        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n        for layer_idx in range(len(self.key_cache)):\n            device = self.key_cache[layer_idx].device\n            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n            device = self.value_cache[layer_idx].device\n            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n\n    @property\n    def seen_tokens(self):\n        logger.warning_once(\n            \"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` \"\n            \"model input instead.\"\n        )\n        if hasattr(self, \"_seen_tokens\"):\n            return self._seen_tokens\n        else:\n            return None\n\n\nclass DynamicCache(Cache):\n    \"\"\"\n    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        self.position_ids_cache: List[torch.Tensor] = []\n        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n\n    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n        \"\"\"\n        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n        sequence length.\n        \"\"\"\n        if layer_idx < len(self):\n            return (self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx])\n        else:\n            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n\n    def __iter__(self):\n        \"\"\"\n        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n        keys and values\n        \"\"\"\n        for layer_idx in range(len(self)):\n            yield (self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx])\n\n    def __len__(self):\n        \"\"\"\n        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n        to the number of layers in the model.\n        \"\"\"\n        return len(self.key_cache)\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        position_ids: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the cache\n        if len(self.key_cache) <= layer_idx:\n            self.key_cache.append(key_states)\n            self.value_cache.append(value_states)\n            self.position_ids_cache.append(position_ids)\n        else:\n            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n            self.position_ids_cache[layer_idx] = torch.cat([self.position_ids_cache[layer_idx], position_ids], dim=-1)\n            \n        return self.key_cache[layer_idx], self.value_cache[layer_idx] , self.position_ids_cache[layer_idx]\n\n    def drop_and_update(self,\n        keys_states : torch.Tensor,\n        value_states : torch.Tensor,\n        position_ids : torch.Tensor,\n        num_elems_to_drop : int,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        \n        assert keys_states.shape[-2] == value_states.shape[-2], \"The number of keys and values to drop must be the same\"\n        assert self.key_cache[layer_idx].shape[-2] >= num_elems_to_drop, \"The number of keys to drop must be less than the number of keys in the cache\"\n        \n        self.key_cache[layer_idx] = torch.concat((self.key_cache[layer_idx][:, :, num_elems_to_drop:],keys_states), dim=-2)\n        self.value_cache[layer_idx] = torch.concat((self.value_cache[layer_idx][:, :, num_elems_to_drop:], value_states), dim=-2)\n        self.position_ids_cache[layer_idx] = torch.concat((self.position_ids_cache[layer_idx][:, num_elems_to_drop:], position_ids), dim=-1)\n                        \n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens -= num_elems_to_drop\n            self._seen_tokens += keys_states.shape[-2]\n        \n        return self.key_cache[layer_idx], self.value_cache[layer_idx] , self.value_cache[layer_idx]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        if len(self.key_cache) <= layer_idx:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n        return None\n\n    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n        \"\"\"Converts the `DynamicCache` instance into the its equivalent in the legacy cache format.\"\"\"\n        legacy_cache = ()\n        for layer_idx in range(len(self)):\n            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx]),)\n        return legacy_cache\n\n    @classmethod\n    def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n        \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`.\"\"\"\n        cache = cls()\n        if past_key_values is not None:\n            for layer_idx in range(len(past_key_values)):\n                key_states, value_states,position_ids= past_key_values[layer_idx]\n                cache.update(key_states, value_states, position_ids,layer_idx)\n        return cache\n\n\nclass SinkCache(Cache):\n    \"\"\"\n    A cache that as described in the [Attention Sinks paper](https://arxiv.org/abs/2309.17453). It allows the model to\n    generate beyond the length of its context window, without losing fluency in the conversation. As it discards past\n    tokens, the model will lose the ability to generate tokens that depend on the context that was discarded.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n\n    Parameters:\n        window_length (`int`):\n            The length of the context window.\n        num_sink_tokens (`int`):\n            The number of sink tokens. See the original paper for more information.\n    \"\"\"\n\n    def __init__(self, window_length: int, num_sink_tokens: int) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        self.window_length = window_length\n        self.num_sink_tokens = num_sink_tokens\n        self.cos_sin_rerotation_cache = {}\n        self._cos_cache = None\n        self._sin_cache = None\n        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n\n    @staticmethod\n    def _rotate_half(x):\n        x1 = x[..., : x.shape[-1] // 2]\n        x2 = x[..., x.shape[-1] // 2 :]\n        return torch.cat((-x2, x1), dim=-1)\n\n    def _apply_key_rotary_pos_emb(\n        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n    ) -> torch.Tensor:\n        rotated_key_states = (key_states * cos) + (self._rotate_half(key_states) * sin)\n        return rotated_key_states\n\n    def _get_rerotation_cos_sin(\n        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if key_states.shape[-2] not in self.cos_sin_rerotation_cache:\n            # Upcast to float32 temporarily for better accuracy\n            cos = cos.to(torch.float32)\n            sin = sin.to(torch.float32)\n\n            # Compute the cos and sin required for back- and forward-rotating to one position earlier in the sequence\n            original_cos = cos[self.num_sink_tokens + key_states.shape[-2] :]\n            shifted_cos = cos[self.num_sink_tokens : -key_states.shape[-2]]\n            original_sin = sin[self.num_sink_tokens + key_states.shape[-2] :]\n            shifted_sin = sin[self.num_sink_tokens : -key_states.shape[-2]]\n            rerotation_cos = original_cos * shifted_cos + original_sin * shifted_sin\n            rerotation_sin = -original_sin * shifted_cos + original_cos * shifted_sin\n\n            self.cos_sin_rerotation_cache[key_states.shape[-2]] = (\n                rerotation_cos.to(key_states.dtype).unsqueeze(0),\n                rerotation_sin.to(key_states.dtype).unsqueeze(0),\n            )\n        return self.cos_sin_rerotation_cache[key_states.shape[-2]]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        # Workaround to make 'key_states.shape[-2] + past_key_value.get_seq_length(self.layer_idx)' <= window_length\n        if len(self.key_cache) <= layer_idx:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n        return self.window_length\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. The following arguments can be used in `SinkCache`: `sin`,\n                `cos` and `partial_rotation_size`. These arguments are used with models using RoPE, to recompute the\n                rotation as the tokens are shifted.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # Optional kwargs for `SinkCache` -- needed on models using RoPE. `partial_rotation_size` is used on models\n        # with partially rotated position embeddings, like Phi or Persimmon.\n        sin = cache_kwargs.get(\"sin\")\n        cos = cache_kwargs.get(\"cos\")\n        partial_rotation_size = cache_kwargs.get(\"partial_rotation_size\")\n        using_rope = cos is not None and sin is not None\n\n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the sin/cos cache, which holds sin/cos values for all possible positions\n        if using_rope and layer_idx == 0:\n            # BC: some models still pass `sin`/`cos` with 2 dims. In those models, they are the full sin/cos. Remove\n            # after all RoPE models have a llama-like cache utilization.\n            if cos.dim() == 2:\n                self._cos_cache = cos\n                self._sin_cache = sin\n            else:\n                if self._cos_cache is None:\n                    self._cos_cache = cos[0, ...]\n                    self._sin_cache = sin[0, ...]\n                elif self._cos_cache.shape[0] < self.window_length:\n                    self._cos_cache = torch.cat([self._cos_cache, cos[0, ...]], dim=0)\n                    self._sin_cache = torch.cat([self._sin_cache, sin[0, ...]], dim=0)\n\n        # [bsz, num_heads, seq_len, head_dim]\n        if len(self.key_cache) <= layer_idx:\n            # Empty cache\n            self.key_cache.append(key_states)\n            self.value_cache.append(value_states)\n\n        elif key_states.shape[-2] + self.get_seq_length(layer_idx) < self.window_length:\n            # Growing cache\n            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n\n        else:\n            # Shifting cache\n            keys_to_keep = self.key_cache[layer_idx][\n                :, :, -self.window_length + self.num_sink_tokens + key_states.shape[-2] :\n            ]\n\n            # On RoPE models, we need to recompute the Key rotation as the tokens are shifted\n            if using_rope:\n                rerotation_cos, rerotation_sin = self._get_rerotation_cos_sin(\n                    key_states, self._cos_cache[: self.window_length], self._sin_cache[: self.window_length]\n                )\n                if partial_rotation_size is not None:\n                    keys_to_keep, keys_pass = (\n                        keys_to_keep[..., :partial_rotation_size],\n                        keys_to_keep[..., partial_rotation_size:],\n                    )\n                keys_to_keep = self._apply_key_rotary_pos_emb(keys_to_keep, rerotation_cos, rerotation_sin)\n                if partial_rotation_size is not None:\n                    keys_to_keep = torch.cat((keys_to_keep, keys_pass), dim=-1)\n\n            # Concatenate sink tokens, shifted & rotated tokens (if needed), and new tokens\n            sink_keys = self.key_cache[layer_idx][:, :, : self.num_sink_tokens]\n            self.key_cache[layer_idx] = torch.cat([sink_keys, keys_to_keep, key_states], dim=-2)\n\n            sink_values = self.value_cache[layer_idx][:, :, : self.num_sink_tokens]\n            values_to_keep = self.value_cache[layer_idx][\n                :, :, -self.window_length + self.num_sink_tokens + value_states.shape[-2] :\n            ]\n            self.value_cache[layer_idx] = torch.cat([sink_values, values_to_keep, value_states], dim=-2)\n\n        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n\n\nclass StaticCache(Cache):\n    \"\"\"\n    Static Cache class to be used with `torch.compile(model)`.\n\n    Parameters:\n        config (`PretrainedConfig):\n            The configuration file defining the shape-related attributes required to initialize the static cache.\n        max_batch_size (`int`):\n            The maximum batch size with which the model will be used.\n        max_cache_len (`int`):\n            The maximum sequence length with which the model will be used.\n        device (`torch.device`):\n            The device on which the cache should be initialized. Should be the same as the layer.\n        dtype (*optional*, defaults to `torch.float32`):\n            The default `dtype` to use when initializing the layer.\n    \"\"\"\n\n    def __init__(self, config: PretrainedConfig, max_batch_size: int, max_cache_len: int, device, dtype=None) -> None:\n        super().__init__()\n        self.max_batch_size = max_batch_size\n        self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n        # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n        self.head_dim = (\n            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n        )\n\n        self.dtype = dtype if dtype is not None else torch.float32\n        self.num_key_value_heads = (\n            config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n        )\n\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        cache_shape = (max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n        for _ in range(config.num_hidden_layers):\n            # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n            # breaks when updating the cache.\n            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n            torch._dynamo.mark_static_address(new_layer_key_cache)\n            torch._dynamo.mark_static_address(new_layer_value_cache)\n            self.key_cache.append(new_layer_key_cache)\n            self.value_cache.append(new_layer_value_cache)\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n        It is VERY important to index using a tensor, otherwise you introduce a copy to the device.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. The `StaticCache` needs the `cache_position` input\n                to know how where to write in the cache.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        cache_position = cache_kwargs.get(\"cache_position\")\n        k_out = self.key_cache[layer_idx]\n        v_out = self.value_cache[layer_idx]\n\n        k_out[:, :, cache_position] = key_states\n        v_out[:, :, cache_position] = value_states\n\n        return k_out, v_out\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states that were seen by the model.\"\"\"\n        # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n        # limit the check to the first batch member and head dimension.\n        # TODO: deprecate this function in favor of `cache_position`\n        return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n        return self.max_cache_len\n\n    def reset(self):\n        \"\"\"Resets the cache values while preserving the objects\"\"\"\n        for layer_idx in range(len(self.key_cache)):\n            # In-place ops prevent breaking the static address\n            self.key_cache[layer_idx].zero_()\n            self.value_cache[layer_idx].zero_()\n"}
{"type": "source_file", "path": "eval/icl/MemLong/configuration_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" LLaMA model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\nimport torch\n\nlogger = logging.get_logger(__name__)\n\n\nclass LlamaConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the LLaMA-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`LlamaModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer decoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer decoder.\n        num_key_value_heads (`int`, *optional*):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n            `num_attention_heads`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n            Llama 2 up to 4096, CodeLlama up to 16384.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        pad_token_id (`int`, *optional*):\n            Padding token id.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            Beginning of stream token id.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            End of stream token id.\n        pretraining_tp (`int`, *optional*, defaults to 1):\n            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n            issue](https://github.com/pytorch/pytorch/issues/76232).\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        rope_scaling (`Dict`, *optional*):\n            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n            these scaling strategies behave:\n            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n            experimental feature, subject to breaking API changes in future versions.\n        attention_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        mlp_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n\n    ```python\n    >>> from transformers import LlamaModel, LlamaConfig\n\n    >>> # Initializing a LLaMA llama-7b style configuration\n    >>> configuration = LlamaConfig()\n\n    >>> # Initializing a model from the llama-7b style configuration\n    >>> model = LlamaModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"llama\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=11008,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=None,\n        hidden_act=\"silu\",\n        max_position_embeddings=2048,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=0,\n        bos_token_id=1,\n        eos_token_id=2,\n        pretraining_tp=1,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling=None,\n        attention_bias=False,\n        attention_dropout=0.0,\n        mlp_bias=False,\n        mem_layer=None,\n        ret_attn_layers = [],\n        memory_size = 32768,\n        last_context_length=1024,\n        ret_group_size=16,\n        pooling_tokens=None,\n        clear_memories_on_eos_token_id = False,\n        clear_memories_on_bos_token_id = False,\n        update_boundary = None,\n        use_gate = False,\n        position_type = \"Continual\",\n        mem_positionals=True,\n        mem_dtype= \"bfloat16\",\n        mem_group_size=32,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.pretraining_tp = pretraining_tp\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self._rope_scaling_validation()\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.mlp_bias = mlp_bias\n        \n        self.position_type = position_type\n        # RET\n        self.mem_layer = mem_layer\n        self.ret_attn_layers = ret_attn_layers\n        self.memory_size = memory_size\n        self.last_context_length = last_context_length\n        self.pooling_tokens = pooling_tokens\n        self.clear_memories_on_eos_token_id = clear_memories_on_eos_token_id\n        self.clear_memories_on_bos_token_id = clear_memories_on_bos_token_id\n        self.use_gate = use_gate\n        self.update_boundary = update_boundary\n        self.mem_group_size = mem_group_size\n        self.mem_dtype = mem_dtype\n        self.mem_positionals = mem_positionals\n        self.ret_group_size = ret_group_size\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def _rope_scaling_validation(self):\n        \"\"\"\n        Validate the `rope_scaling` configuration.\n        \"\"\"\n        if self.rope_scaling is None:\n            return\n\n        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n            raise ValueError(\n                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n            )\n        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n            raise ValueError(\n                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n            )\n        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"}
{"type": "source_file", "path": "eval/icl/MemLong/toolkit.py", "content": "from .ret_embedder import llm_embedder, st_embedder,bge_embedder\nfrom .align_memory import ChunkMemory\nfrom typing import List\nimport torch\n\n\nclass ToolKit:\n    def __init__(self, model_config, toolkit_config,device=None):\n        self.device = device\n        embedder_name = toolkit_config.embedder_name\n        if embedder_name == \"llm_embedder\":\n            self.embedder = llm_embedder(toolkit_config=toolkit_config,device=device)\n        elif embedder_name == \"st_embedder\":\n            self.embedder = st_embedder(toolkit_config=toolkit_config,device=device)\n        elif embedder_name ==\"bge_embedder\":\n            self.embedder = bge_embedder(toolkit_config=toolkit_config,device=device)\n        else:\n            raise NotImplementedError\n        self.max_ret_length = self.embedder.get_max_seq_length()\n        self.tokenizer = None\n        self.chunk_memory = ChunkMemory(model_config=model_config, toolkit_config=toolkit_config,device=device)\n\n    def set_tokenizer(self, tokenizer):\n        self.tokenizer = tokenizer\n        \n    def to(self, device):\n        self.device = device\n        self.embedder.to(device)\n        self.chunk_memory.to(device)\n    \n    def update(self, mem_update):\n        bsz_texts = mem_update.texts\n        embeddings = []\n        for i, texts in enumerate(bsz_texts):\n            if texts is None:\n                embeddings.append(None)\n            else:\n                embeddings.append(self.embedder.get_embeddings(examples=texts, mode=\"key\"))\n        mem_update.embeddings = embeddings\n        self.chunk_memory.save(mem_update)\n    \n    def get_texts(self , input_ids):\n        if self.tokenizer == None:\n            raise ValueError(\"Before using MemLong, you should set tokenizer by calling set_toolkit_tokenizer(tokenizer)\")\n        \n        texts_list = [[] for _ in range(input_ids.shape[0])]\n        for i in range(input_ids.shape[0]):\n            texts_list[i].append(self.tokenizer.decode(input_ids[i], skip_special_tokens=True))\n        return texts_list\n        \n    # 1. 如果不满足，则返回mem中所有的cache\n    # 2. 如果满足，进行检索\n\n    def reset(self):\n        self.chunk_memory.reset()\n    \n    def retrieve(self, queries: List[str], k: int = 5):\n        k = min(k, self.chunk_memory.get_min_number())\n        if k == 0:\n            return None\n        queries_list = [self.embedder.get_embeddings(examples=query, mode=\"query\").to(torch.float32) for query in queries]\n        result = self.chunk_memory.retrieve_index(query_embeddings_list=queries_list, k=k)\n        scores_list = list(map(lambda x: x[0], result))\n        indices_list = list(map(lambda x: x[1], result))\n        mem_caches = self.chunk_memory.get(indices_list=indices_list)\n        return mem_caches\n    \n    def get_max_seq_length(self):\n        return self.embedder.get_max_seq_length()\n\n    def reset_by_batch(self, batch_indices_to_clear):\n        self.chunk_memory.reset_by_batch(batch_indices_to_clear)\n\n    def reset(self):\n        self.chunk_memory.reset()\n"}
{"type": "source_file", "path": "eval/icl/MemLong/ret_embedder.py", "content": "from transformers import AutoTokenizer\nfrom transformers import AutoTokenizer, AutoModel\nfrom sentence_transformers import SentenceTransformer\nfrom FlagEmbedding import BGEM3FlagModel\nimport torch\nfrom typing import List, Literal\nfrom faiss import normalize_L2\n\nINSTRUCTIONS = {\n    \"qa\": {\n        \"query\": \"Represent this query for retrieving relevant documents: \",\n        \"key\": \"Represent this document for retrieval: \",\n    },\n    \"icl\": {\n        \"query\": \"Convert this example into vector to look for useful examples: \",\n        \"key\": \"Convert this example into vector for retrieval: \",\n    },\n    \"chat\": {\n        \"query\": \"Embed this dialogue to find useful historical dialogues: \",\n        \"key\": \"Embed this historical dialogue for retrieval: \",\n    },\n    \"lrlm\": {\n        \"query\": \"Embed this text chunk for finding useful historical chunks: \",\n        \"key\": \"Embed this historical text chunk for retrieval: \",\n    },\n    \"tool\": {\n        \"query\": \"Transform this user request for fetching helpful tool descriptions: \",\n        \"key\": \"Transform this tool description for retrieval: \",\n    },\n    \"convsearch\": {\n        \"query\": \"Encode this query and context for searching relevant passages: \",\n        \"key\": \"Encode this passage for retrieval: \",\n    },\n}\n\n\nclass llm_embedder:\n    def __init__(self, toolkit_config,device):\n        self.embedder_model = AutoModel.from_pretrained(toolkit_config.embedder_path).to(device)\n        self.embedder_tokenizer = AutoTokenizer.from_pretrained(toolkit_config.embedder_path)\n        self.instruction = INSTRUCTIONS[toolkit_config.task]\n    \n    def to(self,device):\n        print(f\"Move Retriever from {self.embedder_model.device} to {device}\")\n        self.embedder_model.to(device)\n\n    def get_embeddings(self, examples: List[str], mode: Literal[\"query\", \"key\"]):\n        examples = [self.instruction[mode] + example for example in examples]\n        inputs = self.embedder_tokenizer(examples, padding=True, truncation=True, return_tensors=\"pt\")\n        inputs = {name: tensor.to(self.embedder_model.device) for name, tensor in inputs.items()}\n        with torch.no_grad():\n            outputs = self.embedder_model(**inputs)\n            embeddings = outputs.last_hidden_state[:, 0]\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n            # normalize_L2(embeddings)\n        return embeddings\n\n    def get_max_seq_length(self):\n        return 512\n\nclass st_embedder:\n    def __init__(self, toolkit_config):\n        self.embedder_model = SentenceTransformer(toolkit_config.embedder_path, device=toolkit_config.device)\n        self.max_seq_length = self.embedder_model.max_seq_length\n\n    def to(self,device):\n        print(f\"Move Retriever from {self.embedder_model.device} to {device}\")\n        self.embedder_model.to(device)\n    \n    def get_embeddings(self, examples: List[str], mode: Literal[\"query\", \"key\"]):\n        embeddings = self.embedder_model.encode(examples,convert_to_tensor=True,normalize_embeddings=True,show_progress_bar=False,)\n        # normalize_L2(embeddings)\n        return embeddings\n\n    def get_max_seq_length(self):\n        return self.max_seq_length\n\nclass bge_embedder:\n    def __init__(self,toolkit_config,device) -> None:\n        self.embedder_model = BGEM3FlagModel(toolkit_config.embedder_path, use_fp16=True,device=device)   \n        self.max_seq_length = 8192\n    \n    def to(self,device):\n        print(f\"Move Retriever from {self.embedder_model.device} to {device}\")\n        self.embedder_model.device = device\n        self.embedder_model.model.to(device)\n    \n    def get_embeddings(self, examples: List[str], mode=None):\n        embeddings = self.embedder_model.encode(examples,max_length=self.max_seq_length)['dense_vecs']\n        return torch.from_numpy(embeddings).to(self.embedder_model.device)\n    \n    def get_max_seq_length(self):\n        return self.max_seq_length\n        \n"}
{"type": "source_file", "path": "eval/icl/MemLong/utils.py", "content": "from dataclasses import dataclass\nimport faiss\nimport torch\nfrom torch.utils.data import Sampler\nfrom typing import Dict, Iterator, Literal, Iterable, Union, List , Optional\nfrom transformers import AutoTokenizer\nfrom peft import PeftModel, get_peft_model, LoraConfig\nfrom itertools import chain\nimport os\nimport transformers\nfrom .configuration_llama import LlamaConfig\n\nTASK_CHOICE = Literal[\"qa\", \"icl\", \"chat\", \"lrlm\", \"tool\", \"convsearch\"]\n\n\nclass BatchSequentialSampler(Sampler):\n    def __init__(self, data, batch_size, num_process=None) -> None:\n        self.total_len = len(data)\n        # self.data = data[: self.total_len - self.total_len % batch_size]\n        self.batch_size = batch_size\n        self.num_process = num_process\n\n    def __iter__(self) -> Iterator[int]:\n        if self.num_process is None:\n            per_batch_nums = self.total_len // self.batch_size\n            for i in range(per_batch_nums):\n                for j in range(self.batch_size):\n                    yield i + j * per_batch_nums\n        else:\n            per_batch_nums = self.total_len // self.batch_size // self.num_process\n            for i in range(per_batch_nums):\n                for j in range(self.batch_size):\n                    for k in range(self.num_process):\n                        yield i + j * per_batch_nums + k * per_batch_nums * self.batch_size\n\n    def __len__(self,) -> int:\n        return self.total_len\n    \ndef set_freeze_by_idxs(model, idxs: Union[int, List[int]], freeze: bool):\n    if not isinstance(idxs, Iterable):\n        idxs = [idxs]\n\n    if \"llama\" in type(model).__name__.lower():\n        iter_model = list(list(model.children())[0].children())[1]\n        num_child = len(iter_model)\n    elif \"peft\" in type(model).__name__.lower():\n        iter_model = list(list(list(list(model.children())[0].children())[0].children())[0].children())[-1]\n        num_child = len(iter_model)\n    else:\n        iter_model = list(model.children())\n        num_child = len(iter_model)\n    idxs = tuple(map(lambda idx: num_child + idx if idx < 0 else idx, idxs))\n    for idx, child in enumerate(iter_model):\n        if idx not in idxs:\n            continue\n        for name, param in child.named_parameters():\n            if \"peft\" in type(model).__name__.lower():\n                if \"lora\" in name or \"modules_to_save\" in name:\n                    param.requires_grad = not freeze\n                    \ndef group_texts(block_size, examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\ndef tokenize_fn(tokenizer, seq_len, examples):\n    tokenizer.padding_side = \"right\"\n    text = list(map(lambda x: x + \" \" + tokenizer.eos_token,examples[\"raw_content\"],))\n    outputs = tokenizer(\n        text,\n        truncation=True,\n        max_length=seq_len,\n        return_overflowing_tokens=True,\n        padding=\"max_length\",\n        return_length=True,\n    )\n    input_batch = []\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n        if length == seq_len:\n            input_batch.append(input_ids)\n\n    return {\"input_ids\": input_batch}\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True\n        )\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True\n        )\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n        \ndef convert_to_lora(\n    model,\n    trainable_params: List[str] = None,\n    targets: List[str] = None,\n    r=64,\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n):\n    # Check the model type\n    lora_config = LoraConfig(\n        r=r,\n        lora_alpha=lora_alpha,\n        target_modules=targets,\n        lora_dropout=lora_dropout,\n        bias=bias,\n        modules_to_save=trainable_params,\n        task_type=task_type,\n    )\n    model = get_peft_model(model, lora_config)\n    return model\n\ndef save_config(config, output_dir):\n    config.save_pretrained(output_dir)\n\ndef load_config(config_path):\n    if os.path.isdir(config_path):\n        config_path = os.path.join(config_path, \"config.json\")\n    return LlamaConfig.from_pretrained(config_path)\n\n@dataclass\nclass ToolkitConfig:\n    task: TASK_CHOICE\n    embedder_name: str\n    embedder_path: str\n    ret_embeddings_dim: int\n    dtype: torch.dtype = torch.bfloat16\n    \n@dataclass\nclass MemConfig:\n    positionals: bool\n    use_gate: bool\n    cache_dtype: torch.dtype\n    \n@dataclass\nclass MemCache:\n    texts: List[Optional[List[str]]] = None\n    embeddings:List[Union[faiss.IndexFlatIP,List[torch.Tensor]]] = None\n    keys: torch.Tensor = None\n    values: torch.Tensor = None\n    masks: torch.Tensor = None\n\n@dataclass\nclass SigMemCache:\n    texts: List[str]\n    embeddings: List[torch.Tensor]\n    keys: torch.Tensor\n    values: torch.Tensor \n    masks: torch.Tensor\n    length: int\n"}
{"type": "source_file", "path": "eval/icl/eval_icl.py", "content": "import os, sys\nimport torch\nimport argparse\nimport json\nimport faiss\nfrom tqdm import tqdm\nimport random\nfrom argparse import Namespace\nimport numpy as np\nimport itertools\nfrom math import *\nfrom transformers import AutoTokenizer,AutoModelForCausalLM,AutoConfig\nfrom peft import PeftModel,AutoPeftModelForCausalLM,PeftConfig\nfrom time import time\nfrom datetime import datetime\nfrom MemLong import modeling_llama_position  \nfrom MemLong import configuration_llama\nfrom MemLong.utils import ToolkitConfig\nfrom longllama.configuration_longllama import LongLlamaConfig\nfrom longllama.modeling_longllama import LongLlamaForCausalLM\n\n\ndef parse_config():\n    parser = argparse.ArgumentParser(description=\"Arguments for evaluating in context learning(ICL)\")\n    parser.add_argument(\"--loc_data_dir\",type=str,help=\"The path to the data directory\")\n    parser.add_argument(\"--model_provider\",type=str,choices=['OpenLLaMA','LongLLaMA','MemLong',\"RagOpenLLaMA\"])\n    parser.add_argument(\"--model_path_or_name\",type=str,help=\"The path to the model checkpoint or the model name\")\n    parser.add_argument(\"--task\",type=str,choices=['mpqa','mr','sst-2','sst-5','subj'],help=\"The task for in-context learning\")\n    parser.add_argument(\"--peft_model\",type=str,default=None,help=\"The path to the PEFT model\")\n    parser.add_argument(\"--attn_implementation\",type=str,choices=['eager','sdpa','flash_attention_2'],default='eager')\n    parser.add_argument(\"--k\",type=int,default=20,help=\"The number of demonstration examples\")\n    parser.add_argument(\"--cache_k\",type=int,default=0,help=\"The number of cached examples in LongMem's memory\")\n    parser.add_argument(\"--subset\", type=str, default=\"test\", help=\"normally test set. But for SST-2, there is no testset, we use validation set instead\")\n    parser.add_argument(\"--last_context_length\",default=0,type=int,help=\"The last context length for LongLLaMA\")\n    parser.add_argument(\"--memory_size\",type=int,help=\"The memory size for MemLong\")\n    parser.add_argument(\"--embedder_name\",type=str,choices=[\"llm_embedder\",'st_embedder','bge_embedder'],help=\"The embedder name for MemLong\")\n    parser.add_argument(\"--output_dir\",type=str,default=\"result\",help=\"The path to the output directory\")\n    args = parser.parse_args()\n    return args\n    \ndef load_data(loc_data_dir,task):\n    data = {}\n    path = os.path.join(loc_data_dir, task)\n    if task == \"sst-2\":\n        for split in [\"train\", \"validation\",]:\n            data[split] = []\n            file_split = \"dev\" if split == \"validation\" else split\n            with open(os.path.join(path, \"{}.tsv\".format(file_split)), \"r\") as f:\n                for line in tqdm(f.readlines()):\n                    review, label = line.strip(\"\\n\").split(\"\\t\")\n                    if label == \"label\":\n                        continue\n                    label = \"positive\" if int(label) == 1 else \"negative\"\n                    data[split].append([review, label])\n    \n    if task == \"sst-5\":\n        mapping = {0: \"terrible\", 1: \"bad\", 2: \"okay\", 3: \"good\", 4: \"great\"}\n        for split in [\"train\", \"validation\", \"test\"]:\n            file_split = \"dev\" if split == \"validation\" else split\n            if os.path.exists(os.path.join(path, \"stsa.fine.{}\".format(file_split))):\n                data[split] = []\n                with open(os.path.join(path, \"stsa.fine.{}\".format(file_split)), \"r\") as f:\n                    for line in tqdm(f.readlines()):\n                        review, label = line[2:], line[0]                        \n                        label = mapping[int(label)]\n                        data[split].append([review.strip(\"\\n\"), label])\n\n    elif task in [\"mr\", \"mpqa\"]:\n        for split in [\"train\", \"validation\", \"test\"]:\n            if os.path.exists(os.path.join(path, \"{}.csv\".format(split))):\n                data[split] = []\n                with open(os.path.join(path, \"{}.csv\".format(split)), \"r\") as f:\n                    for line in tqdm(f.readlines()):\n                        review, label = line[2:], line[0]\n                        label = \"positive\" if int(label) == 1 else \"negative\"\n                        data[split].append([review.strip(\"\\n\"), label])\n        print(data['test'][0])\n\n    elif task in [\"subj\"]:\n        for split in [\"train\", \"test\"]:\n            if os.path.exists(os.path.join(path, \"{}.csv\".format(split))):\n                data[split] = []\n                with open(os.path.join(path, \"{}.csv\".format(split)), \"r\") as f:\n                    for line in tqdm(f.readlines()):\n                        review, label = line[2:], line[0]\n                        label = \"subjective\" if int(label) == 0 else \"objective\"\n                        data[split].append([review.strip(\"\\n\").strip('\"'), label])\n        print(data['test'][0])\n    \n    return data\n\n\ndef main():\n    args = parse_config()\n    task_template_dict = {\"sst-2\": \"Review: {} Sentiment: {}. \",\n                          \"sst-5\": \"Review: {} Sentiment: {}. \",\n                            \"mr\": \"Review: {} Sentiment: {}. \",\n                          \"mpqa\": \"Review: {} Sentiment: {}. \",\n                          \"subj\": \"Input: {} Type: {}. \"}\n    # Config\n    if args.model_provider in [\"LongLLaMA\"]:\n        config = LongLlamaConfig.from_pretrained(args.model_path_or_name)\n        config.last_context_length = args.last_context_length\n    \n    elif args.model_provider in [\"MemLong\"]:\n        if args.peft_model:\n            config = configuration_llama.LlamaConfig.from_pretrained(args.peft_model)\n        else:\n            config = configuration_llama.LlamaConfig.from_pretrained(args.model_path_or_name)\n        config.use_cache = False\n        if args.memory_size is not None:\n            config.memory_size = int(args.memory_size)\n        if args.last_context_length > 0:\n            config.last_context_length = args.last_context_length\n        else:\n            args.last_context_length = config.last_context_length\n        config.memory_size = 65536\n            \n    elif \"peft\" in args.model_provider:\n        config = PeftConfig.from_pretrained(args.peft_model)\n    else:\n        config = AutoConfig.from_pretrained(args.model_path_or_name)\n        \n    if \"OpenLLaMA\" in args.model_provider:\n        if args.peft_model:\n            tokenizer = AutoTokenizer.from_pretrained(args.peft_model)\n            model = AutoPeftModelForCausalLM.from_pretrained(\n                    args.peft_model,\n                    config=config,\n                    use_flash_attention_2=\"flash_attention_2\" if args.flash_attn else None,\n                    torch_dtype=torch.bfloat16,\n                    device_map=\"auto\",\n            ).eval()\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n            model = AutoModelForCausalLM.from_pretrained( \n                    args.model_path_or_name,  \n                    device_map=\"cuda\",  \n                    attn_implementation=args.attn_implementation,\n                    torch_dtype=\"auto\",\n                    trust_remote_code=True,  \n                )             \n        tokenzier_vocab_size = len(tokenizer)\n        model_vocab_size = model.get_input_embeddings().weight.size(0)\n        if model_vocab_size != tokenzier_vocab_size:\n            assert tokenzier_vocab_size > model_vocab_size\n            print(\"Resize model embeddings to fit tokenizer\")\n            model.resize_token_embeddings(tokenzier_vocab_size)\n            input_embeddings = model.get_input_embeddings().weight.data\n            output_embeddings = model.get_output_embeddings().weight.data\n            num_new_tokens = tokenzier_vocab_size - model_vocab_size\n            input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                dim=0, keepdim=True\n            )\n            output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                dim=0, keepdim=True\n            )\n            input_embeddings[-num_new_tokens:] = input_embeddings_avg\n            output_embeddings[-num_new_tokens:] = output_embeddings_avg\n        else:\n            tokenizer.pad_token_id = config.pad_token_id\n            \n    elif args.model_provider == \"LongLLaMA\":\n        tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n        model = LongLlamaForCausalLM.from_pretrained(args.model_path_or_name,torch_dtype=torch.bfloat16,device_map=\"auto\").eval()\n        tokenizer.pad_token_id = config.pad_token_id\n    elif args.model_provider == \"MemLong\":\n        tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n        if args.embedder_name == \"llm_embedder\":\n            toolkit_config = ToolkitConfig(\n                    task=\"lrlm\",\n                    embedder_name=\"llm_embedder\",\n                    embedder_path=\"/opt/data/private/lwj/acl2023/FineRange/llm-embedder\",\n                    ret_embeddings_dim=768,\n                )\n        elif args.embedder_name == \"bge_embedder\":\n            toolkit_config = ToolkitConfig(\n                task=\"lrlm\",\n                embedder_name=\"bge_embedder\",\n                embedder_path=\"/opt/data/private/lwj/emnlp2024/bge-m3\",\n                ret_embeddings_dim=1024,\n            )\n        elif args.embedder_name == \"st_embedder\":\n            toolkit_config = ToolkitConfig(\n                task=\"lrlm\",\n                embedder_name=\"st_embedder\",\n                embedder_path=\"/opt/data/private/lwj/emnlp2024/st-m3\",\n                ret_embeddings_dim=512,\n            )\n        else:\n            raise NotImplemented\n        model = modeling_llama_position.LlamaForCausalLM.from_pretrained(\n            args.model_path_or_name,\n            config=config,\n            toolkit_config=toolkit_config,\n            attn_implementation=args.attn_implementation,\n            torch_dtype=torch.bfloat16,\n            # device_map=\"auto\",\n        )\n        tokenzier_vocab_size = len(tokenizer)\n        model_vocab_size = model.get_input_embeddings().weight.size(0)\n        num_new_tokens = tokenzier_vocab_size - model_vocab_size\n        if num_new_tokens > 0:\n            print(\"Resize model embeddings to fit tokenizer\")\n            model = model.resize_token_embeddings(tokenzier_vocab_size)\n            input_embeddings = model.get_input_embeddings().weight.data\n            output_embeddings = model.get_output_embeddings().weight.data\n\n            input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                dim=0, keepdim=True\n            )\n            output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                dim=0, keepdim=True\n            )\n\n            input_embeddings[-num_new_tokens:] = input_embeddings_avg\n            output_embeddings[-num_new_tokens:] = output_embeddings_avg\n        else:\n            tokenizer.pad_token_id = config.pad_token_id\n\n        if args.peft_model:\n            print(\"Loading PEFT model\")\n            model = PeftModel.from_pretrained(\n                model,\n                args.peft_model,\n                #device_map=\"auto\",\n                torch_dtype=torch.bfloat16,\n            )\n            model = model.merge_and_unload()\n        model.set_toolkit_tokenizer(tokenizer)\n        model.to(\"cuda\").eval()\n    \n    data = load_data(args.loc_data_dir,args.task)\n    if args.task in ['sst-2', \"sst-5\"]:\n        args.subset = \"validation\"\n\n    task_template = task_template_dict[args.task]\n    context_length = 1024\n    model_list_acc = []\n    \n    model = model.eval()\n    model = model.cuda()\n    seeds = [1,2,3]\n    with tqdm(total=len(seeds)*len(data[args.subset])) as pbar:\n        for seed in seeds:\n            random.seed(seed)\n            original_demon_train_subset = random.sample(data['train'], args.k)\n            original_demon_train_subset = [task_template.format(s[0], s[1]) for s in original_demon_train_subset]\n            demonstration = \"\".join(original_demon_train_subset)\n            total_cnt = 0\n            acc_cnt = 0\n            print(\"Load {} examples into memory\".format(args.cache_k))\n            for item in data[args.subset]:\n                total_cnt += 1\n                mem_inputs = None\n                if args.model_provider in [\"LongLLaMA\",\"MemLong\"]:\n                    memory_set = [task_template.format(s[0], s[1]) for idx, s in enumerate(random.sample(data['train'], args.cache_k))]\n                    memory_set = \"\".join(memory_set)\n                    mem_inputs = tokenizer(memory_set, return_tensors='pt', padding=True)\n                    mem_input_ids = mem_inputs['input_ids'].to(model.device)\n                    mem_attention_mask = mem_inputs['attention_mask'].to(model.device)\n                elif args.model_provider == \"RagOpenLLaMA\" :\n                    memory_set = [task_template.format(s[0], s[1]) for idx, s in enumerate(random.sample(data['train'], args.cache_k))]\n                    memory_set = \"\".join(memory_set)\n\n                test_subset = original_demon_train_subset + [task_template[:-5].format(item[0])]\n                inputs = tokenizer(''.join(test_subset), return_tensors='pt', padding=True)\n                input_ids = inputs['input_ids'].to(model.device)\n                attention_mask = inputs['attention_mask'].to(model.device)\n\n                if args.model_provider in [\"LongLLaMA\",\"MemLong\"]:\n                    with torch.no_grad():\n                        if mem_inputs is not None:\n                            prediction = model(torch.concat((mem_input_ids,input_ids),dim=-1), attention_mask=torch.concat((mem_attention_mask,attention_mask),dim=-1), use_cache=False,return_dict=True,last_context_length=input_ids.shape[-1]).logits          \n                        else:\n                            prediction = model(input_ids, attention_mask=attention_mask, use_cache=False,last_context_length=input_ids.shape[-1]+1,return_dict=True).logits  \n                elif args.model_provider in [\"RagOpenLLaMA\"]:\n                    with torch.no_grad():\n                        if mem_inputs is not None:\n                            prediction = model(torch.concat((mem_input_ids,input_ids),dim=-1), attention_mask=torch.concat((mem_attention_mask,attention_mask),dim=-1), use_cache=False,return_dict=True).logits          \n                        else:\n                            prediction = model(input_ids, attention_mask=attention_mask, use_cache=False,return_dict=True).logits  \n                    \n                elif args.model_provider == \"OpenLLaMA\":\n                    if input_ids.shape[1] > 2048:\n                        input_ids = input_ids[:,-2048:]\n                        attention_mask = attention_mask[:,-2048:]\n                    with torch.no_grad():\n                        prediction = model(input_ids, attention_mask=attention_mask, use_cache=False,return_dict=True).logits\n                prediction = prediction[0, -1, :].softmax(dim=-1)\n                if  args.model_provider == \"MemLong\":\n                    model.reset_memory()\n                \n                prediction = tokenizer.decode(prediction.argmax(-1).item())\n                acc_cnt += (item[1].startswith(prediction.strip()) and prediction.strip() != \"\")\n                pbar.update(1)\n                \n            model_list_acc.append(acc_cnt / total_cnt)\n\n            try:\n                if model.decoder.external_memory:\n                    model.decoder.external_memory.reset()\n            except AttributeError:\n                pass\n\n            print(\"Acc for random seed {}: {}\".format(seed, acc_cnt / total_cnt))\n    model_list_acc = [np.mean(model_list_acc), np.std(model_list_acc)] \n    \n    print(\"Mean acc across 6 seeds: {:.4f}, std: {:.4f}\".format(model_list_acc[0], model_list_acc[1]))\n    \n    location = f\"{args.output_dir}/{args.model_provider}_icl.json\"\n    \n    if not os.path.exists(args.output_dir):\n        print(\"Creating output directory\", args.output_dir)\n        os.makedirs(args.output_dir)\n    \n    now = datetime.now()\n    # 格式化日期和时间\n    formatted_now = now.strftime(\"%Y-%m-%d %H:%M\")\n    info = {\n        \"k\": args.k,\n        \"cache_k\": args.cache_k,\n        \"task\": args.task,\n        \"acc\": model_list_acc[0],\n        \"std\": model_list_acc[1],\n        \"peft model\": args.peft_model,\n        \"time\": formatted_now,\n    }\n        \n    with open(f\"{location}\",'a+') as f:\n        f.write(json.dumps(info) + \"\\n\")\n\nif __name__==\"__main__\":\n    main()"}
{"type": "source_file", "path": "eval/icl/longllama/configuration_longllama.py", "content": "# coding=utf-8\n# Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" LongLLaMA model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nLONGLLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"syzymon/long_llama_3b\": \"https://huggingface.co/syzymon/long_llama_3b/resolve/main/config.json\",\n}\n\n\nclass LongLlamaConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`LongLlamaModel`]. It is used to instantiate an LongLLaMA\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the LongLLaMA-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the LongLLaMA model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`LongLlamaModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        tie_word_embeddings(`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        mem_layers (`List[int]`, defaults to `[]`):\n            Layers with memory\n        mem_positionals (`bool`, *optional*, defaults to `True`):\n            Whether to use positional embeddings in memory layers\n        mem_dtype (`str`, *optional*, defaults to `\"bfloat16\"`):\n            Type for keys and values stored in memory\n        mem_attention_grouping (`Tuple[int, int]`, *optional*, defaults to `None`):\n            One can trade speed for memory by performing attention\n            in memory layers sequentially.\n            When equal to `(4, 2048)` the memory layers will process at most 4 heads and 2048 queries from each head at once.\n            That is at most 4*2048 queries at once.\n        torch_attention (`bool`, *optional*, defaults to `False`):\n            Whether to use torch scaled_dot_product_attention\n        gradient_checkpoint_every_ith (`int`, *optional*, defaults to `1`):\n            When gradient checkpointing is enabled checkpoint every ith layer\n\n        Example:\n\n    ```python\n    >>> from transformers import LongLlamaModel, LongLlamaConfig\n\n    >>> # Initializing a LongLLaMA longllama-7b style configuration\n    >>> configuration = LongLlamaConfig()\n\n    >>> # Initializing a model from the longllama-7b style configuration\n    >>> model = LongLlamaModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"longllama\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=11008,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        hidden_act=\"silu\",\n        max_position_embeddings=2048,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=0,\n        bos_token_id=1,\n        eos_token_id=2,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling=None,\n        last_context_length=1024,\n        mem_layers=[],\n        mem_positionals=True,\n        mem_dtype=\"bfloat16\",\n        mem_attention_grouping=None,\n        torch_attention=False,\n        gradient_checkpoint_every_ith=1,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.last_context_length = last_context_length\n        self.mem_layers = mem_layers\n        self.mem_positionals = mem_positionals\n        self.mem_dtype = mem_dtype\n        self.mem_attention_grouping = mem_attention_grouping\n        self.torch_attention = torch_attention\n        self.gradient_checkpoint_every_ith = gradient_checkpoint_every_ith\n\n        self._rope_scaling_validation()\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def _rope_scaling_validation(self):\n        \"\"\"\n        Validate the `rope_scaling` configuration.\n        \"\"\"\n        if self.rope_scaling is not None:\n            raise ValueError(\"LongLLaMA does not use rope_scaling\")"}
{"type": "source_file", "path": "eval/icl/longllama/__init__.py", "content": ""}
{"type": "source_file", "path": "eval/icl/longllama/longllama_utils.py", "content": "from collections import namedtuple\nfrom dataclasses import dataclass\nimport torch\nfrom typing import Tuple, Optional\n\n\n@dataclass\nclass LongLlamaMemConfig:\n    \"\"\"\n    Class for configuring memory caches for LongLlama model.\n\n    Args:\n        positionals (`boolean`)\n            Whether to use positional embeddings in memory layer\n        cache_dtype (`torch.dtype`)\n            Specifies storing type for keys and values\n        attention_grouping (`Tuple[int, int]`, *optional*)\n            One can trade speed for memory by performing attention\n            in memory layers sequentially.\n            When equal to `(4, 128)` the memory layers will process at most 4 heads and 128 queries\n            from each head at once. That is at most 512 queries at once.\n    \"\"\"\n\n    positionals: bool = True\n    cache_dtype: torch.dtype = torch.bfloat16\n    attention_grouping: Optional[Tuple[int, int]] = None\n\n\n@dataclass\nclass LongLlamaMemCache:\n    \"\"\"\n    Class with LongLlama's memory cache\n\n    Args:\n        keys (`torch.FloatTensor` of shape `(batch_size, num_heads, mem_length, embed_size_per_head)`)\n        values (`torch.FloatTensor` of shape `(batch_size, num_heads, mem_length, embed_size_per_head)`)\n        masks (`torch.FloatTensor` of shape `(batch_size, 1, mem_length, 1)`)\n            For masking out parts of memory\n    \"\"\"\n\n    keys: torch.FloatTensor\n    values: torch.FloatTensor\n    masks: torch.FloatTensor\n\n\ndef mem_apply_update(\n    prev_mem_cache: LongLlamaMemCache, new_mem_content: LongLlamaMemCache, mem_config: LongLlamaMemConfig\n):\n    def update_one(prev, new):\n        if len(prev.shape) != 4 or len(new.shape) != 4:\n            raise ValueError(f\"Memory cache content should be consistent in shape got {prev.shape} {new.shape}\")\n\n        return torch.concat([prev, new], dim=-2)\n\n    insert_size = new_mem_content.keys.shape[-2]\n\n    if new_mem_content.values.shape[-2] != insert_size or new_mem_content.masks.shape[-2] != insert_size:\n        raise ValueError(f\"Inconsistent mem_length in new_mem_content\")\n\n    return LongLlamaMemCache(\n        keys=update_one(prev_mem_cache.keys, new_mem_content.keys),\n        values=update_one(prev_mem_cache.values, new_mem_content.values),\n        masks=update_one(prev_mem_cache.masks, new_mem_content.masks),\n    )"}
{"type": "source_file", "path": "eval/icl/longllama/modeling_longllama.py", "content": "# coding=utf-8\n# Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LongLLaMA model.\"\"\"\nfrom dataclasses import dataclass\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom .configuration_longllama import LongLlamaConfig\nfrom .longllama_utils import mem_apply_update, LongLlamaMemCache, LongLlamaMemConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LongLlamaConfig\"\n\n\n@dataclass\nclass LongLlamaModelOutputWithPast(BaseModelOutputWithPast):\n    \"\"\"\n    Based on BaseModelOutputWithPast\n\n    Args:\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n        mem_caches  (`tuple(LongLlamaMemCache))`, *optional*, returned for layers with memory cache enabled):\n            For the layers without memory None is returned\n    \"\"\"\n\n    mem_caches: Optional[LongLlamaMemCache] = None\n\n\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\n# Copied from transformers.models.bart.modeling_bart._expand_mask\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->LongLlama\nclass LongLlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LongLlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->LongLlama\nclass LongLlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        # Build here to make `torch.jit.trace` work.\n        self._set_cos_sin_cache(\n            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n        )\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        if seq_len > self.max_seq_len_cached:\n            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\n# Based on transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef rotate_one(x, cos, sin, position_ids):\n    if len(position_ids.shape) != 2 or x.shape[0] != position_ids.shape[0] or x.shape[-2] != position_ids.shape[1]:\n        raise ValueError(f\"Position ids shoud have shape [bsz, seq_len] got {position_ids.shape}\")\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    x_embed = (x * cos) + (rotate_half(x) * sin)\n    return x_embed\n\n\ndef rotate_as_if_first(x, rotary_emb):\n    # x: [bs, num_attention_heads, seq_len, head_size]\n    # apply rotary as if all elements were first in the sequence\n    cos, sin = rotary_emb(x, x.shape[-2])\n    return rotate_one(x, cos, sin, torch.zeros(x.shape[0], x.shape[-2], dtype=torch.long, device=cos.device))\n\n\n# Based on an 4.30 transformers.models.llama.modeling_llama.LlamaMLP with Llama->LongLlama\nclass LongLlamaMLP(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\n# Modified transformers.models.llama.modeling_llama.LlamaAttention\nclass LongLlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper with FoT modifications\"\"\"\n\n    def __init__(self, config: LongLlamaConfig, mem_config: Optional[LongLlamaMemConfig] = None):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.max_cache = 32\n        self.rope_theta = config.rope_theta\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self._init_rope()\n        self.mem_config = mem_config\n\n    def _init_rope(self):\n        assert self.config.rope_scaling is None\n        self.rotary_emb = LongLlamaRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        mem_cache: Optional[LongLlamaMemCache] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if attention_mask is None:\n            tgt_seq_len = hidden_states.shape[-2]\n            if past_key_value is not None:\n                src_seq_len = past_key_value[0].shape[-2] + tgt_seq_len\n            else:\n                src_seq_len = tgt_seq_len\n\n            attention_mask = torch.zeros(\n                hidden_states.shape[0],\n                1,\n                tgt_seq_len,\n                src_seq_len,\n                device=hidden_states.device,\n                dtype=hidden_states.dtype,\n            )\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        position_ids = position_ids[:, None, :, None]\n\n        if position_ids.shape != (key_states.shape[0], 1, key_states.shape[-2], 1):\n            raise ValueError(\"position_ids should match batch and seq_len of the input\")\n\n        mem_no_local_cache = self.mem_config is not None and past_key_value is None and (not use_cache)\n        mem_and_local_cache = self.mem_config is not None and use_cache\n        # positonal embeddings can be disabled for memory layers\n        use_positionals = self.mem_config is None or self.mem_config.positionals\n\n        if mem_no_local_cache or mem_and_local_cache:\n            pass\n        \n        if mem_no_local_cache:\n            # the whole context window will be moved to memory cache after the attention\n            if use_positionals:\n                # positionally embedd memory content as first token in the sequence\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n            # attention_mask [bsz, 1, tgt_seq_len, src_seq_len]\n            # we base the mask on the last token in the context window\n            mem_update = LongLlamaMemCache(\n                keys=rfst_key_states.to(self.mem_config.cache_dtype),\n                values=value_states.to(self.mem_config.cache_dtype),\n                masks=attention_mask[..., -1, :, None],\n            )\n\n        if past_key_value is not None:\n            past_local_cache_size = past_key_value[0].shape[-2]\n            key_states = torch.cat([past_key_value[0], key_states], dim=-2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=-2)\n            # FoT additionally stores position_ids to support long inputs\n            position_ids = torch.cat([past_key_value[2], position_ids], dim=-2)\n\n            if attention_mask.shape[-1] != key_states.shape[-2] and attention_mask.shape[-2] != query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            # local cache is maintained so that it is <= self.max_cache\n            # remaining elements are either dropped or go to memory cache\n            if key_states.shape[-2] > self.max_cache:\n                num_elems_to_drop = past_local_cache_size\n\n                if mem_and_local_cache:\n                    drop_keys = key_states[:, :, :num_elems_to_drop, :]\n                    drop_values = value_states[:, :, :num_elems_to_drop, :]\n                    # as memory mask use the masking of the last key in context\n                    # attention_mask [bsz, 1, tgt_seq_len, src_seq_len]\n                    drop_masks = attention_mask[..., -1, :, None]\n                    drop_masks = drop_masks[:, :, :num_elems_to_drop, :]\n\n                    if use_positionals:\n                        rfst_drop_keys = rotate_as_if_first(drop_keys, self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    mem_update = LongLlamaMemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype),\n                        values=drop_values.to(self.mem_config.cache_dtype),\n                        masks=drop_masks,\n                    )\n                    if mem_cache is None:\n                        mem_cache = mem_update\n                    else:\n                        mem_cache = mem_apply_update(\n                            prev_mem_cache=mem_cache, new_mem_content=mem_update, mem_config=self.mem_config\n                        )\n\n                key_states = key_states[:, :, num_elems_to_drop:, :]\n                value_states = value_states[:, :, num_elems_to_drop:, :]\n                position_ids = position_ids[:, :, num_elems_to_drop:, :]\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n\n        # FoT additionally stores position_ids to support long inputs\n        past_key_value = (key_states, value_states, position_ids) if use_cache else None\n\n        kv_seq_len = key_states.shape[-2]\n\n        if use_positionals:\n            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n            rel_pos_ids = position_ids - torch.min(position_ids, dim=-2, keepdim=True)[0]\n            rel_pos_ids = rel_pos_ids.squeeze(3).squeeze(1)\n            query_states = rotate_one(query_states, cos, sin, rel_pos_ids[:, -query_states.shape[-2] :])\n            key_states = rotate_one(key_states, cos, sin, rel_pos_ids)\n\n        if self.mem_config is not None and self.mem_config.attention_grouping is not None:\n            attn_grouping_h, attn_grouping_q = self.mem_config.attention_grouping\n            if attn_grouping_h <= 0 or attn_grouping_q <= 0:\n                raise ValueError(\"Attention grouping should be positive\")\n        else:\n            attn_grouping_h, attn_grouping_q = self.num_heads, q_len\n\n        attn_output_h = []\n        for beg_h in range(0, self.num_heads, attn_grouping_h):\n            end_h = min(beg_h + attn_grouping_h, self.num_heads)\n\n            attn_output_q = []\n            for beg_q in range(0, q_len, attn_grouping_q):\n                end_q = min(beg_q + attn_grouping_q, q_len)\n\n                if self.config.torch_attention:\n                    if mem_cache is not None:\n                        attn_keys = torch.concat(\n                            [key_states[:, beg_h:end_h], mem_cache.keys[:, beg_h:end_h].to(key_states.dtype)], dim=-2\n                        )\n                        attn_values = torch.concat(\n                            [value_states[:, beg_h:end_h], mem_cache.values[:, beg_h:end_h].to(value_states.dtype)],\n                            dim=-2,\n                        )\n                        mem_mask = mem_cache.masks.squeeze(-1).unsqueeze(-2)\n                        assert len(mem_mask.shape) == 4\n                        assert mem_mask.shape[2] == 1\n                        assert mem_mask.shape[3] == mem_cache.keys.shape[-2]\n                        mem_mask = torch.broadcast_to(\n                            mem_mask, (mem_mask.shape[0], mem_mask.shape[1], end_q - beg_q, mem_mask.shape[3])\n                        )\n                        attn_mask = torch.concat([attention_mask[:, :, beg_q:end_q], mem_mask], dim=-1)\n                        assert attn_mask.shape[-1] == attn_keys.shape[-2]\n                    else:\n                        attn_keys = key_states[:, beg_h:end_h]\n                        attn_values = value_states[:, beg_h:end_h]\n                        attn_mask = attention_mask[:, :, beg_q:end_q]\n\n                    attn_queries = query_states[:, beg_h:end_h, beg_q:end_q]\n\n                    attn_output = torch.nn.functional.scaled_dot_product_attention(\n                        query=attn_queries, key=attn_keys, value=attn_values, attn_mask=attn_mask\n                    )\n                    attn_output_q.append(attn_output)\n                else:\n                    attn_weights = torch.matmul(\n                        query_states[:, beg_h:end_h, beg_q:end_q], key_states[:, beg_h:end_h].transpose(2, 3)\n                    ) / math.sqrt(self.head_dim)\n\n                    if attn_weights.size() != (bsz, end_h - beg_h, end_q - beg_q, kv_seq_len):\n                        raise ValueError(\n                            f\"Attention weights should be of size {(bsz, end_h - beg_h, end_q - beg_q, kv_seq_len)}, but is\"\n                            f\" {attn_weights.size()}\"\n                        )\n\n                    if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                        raise ValueError(\n                            f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                        )\n                    attn_weights = attn_weights + attention_mask[:, :, beg_q:end_q]\n                    min_value = (\n                        torch.finfo(attn_weights.dtype).min\n                        if -1000000.0 < torch.finfo(attn_weights.dtype).min\n                        else -1000000.0\n                    )\n                    attn_weights = torch.max(\n                        attn_weights, torch.tensor(min_value, device=attn_weights.device, dtype=attn_weights.dtype)\n                    )\n\n                    if mem_cache is not None:\n                        mem_mask = mem_cache.masks.squeeze(-1).unsqueeze(-2)\n                        mem_attn_weights = torch.matmul(\n                            query_states[:, beg_h:end_h, beg_q:end_q],\n                            mem_cache.keys[:, beg_h:end_h].transpose(2, 3).to(key_states.dtype),\n                        ) / math.sqrt(self.head_dim)\n\n                        assert mem_mask.shape[2] == 1\n                        mem_attn_weights = mem_attn_weights + mem_mask\n                        min_value = (\n                            torch.finfo(mem_attn_weights.dtype).min\n                            if -1000000.0 < torch.finfo(mem_attn_weights.dtype).min\n                            else -1000000.0\n                        )\n                        mem_attn_weights = torch.max(\n                            mem_attn_weights,\n                            torch.tensor(min_value, device=mem_attn_weights.device, dtype=mem_attn_weights.dtype),\n                        )\n\n                        attn_weights = torch.concat([attn_weights, mem_attn_weights], dim=-1)\n                        combined_value_states = torch.concat(\n                            [value_states[:, beg_h:end_h], mem_cache.values[:, beg_h:end_h].to(value_states.dtype)],\n                            dim=-2,\n                        )\n                    else:\n                        combined_value_states = value_states[:, beg_h:end_h]\n                    # upcast attention to fp32\n                    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n                        query_states.dtype\n                    )\n                    attn_output = torch.matmul(attn_weights, combined_value_states)\n                    assert attn_output.shape[-2] == end_q - beg_q\n                    attn_output_q.append(attn_output)\n            attn_output_h.append(torch.concat(attn_output_q, dim=-2))\n\n        attn_output = torch.concat(attn_output_h, dim=-3)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        if mem_no_local_cache:\n            if mem_cache is not None:\n                mem_cache = mem_apply_update(\n                    prev_mem_cache=mem_cache, new_mem_content=mem_update, mem_config=self.mem_config\n                )\n            else:\n                mem_cache = mem_update\n\n        return attn_output, attn_weights, past_key_value, mem_cache\n\n\n# Modified transformers.models.llama.modeling_llama.LlamaDecoderLayer\nclass LongLlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LongLlamaConfig, mem_config: Optional[LongLlamaMemConfig] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LongLlamaAttention(config=config, mem_config=mem_config)\n        self.mlp = LongLlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LongLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LongLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        mem_cache: Optional[LongLlamaMemCache] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n                along with information about positions\n            mem_cache (`LongLlamaMemCache`, *optional*): memory cache for specific layers\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value, mem_cache = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            mem_cache=mem_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs + (mem_cache,)\n\n\nLONGLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LongLlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\nLONGLLAMA_MEML_DOCSTRING = r\"\"\"\n        mem_layers ([`int`], *optional*):\n            Indices of layers to be augmented with memory, if None then parameters from config will be used\n        mem_dtype (`str`, *optional*):\n            Keys and values will be casted to this type for storage.\n\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LongLLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LONGLLAMA_START_DOCSTRING,\n)\n# Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel with Llama->LongLlama\nclass LongLlamaPreTrainedModel(PreTrainedModel):\n    config_class = LongLlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LongLlamaDecoderLayer\"]\n    _skip_keys_device_placement = \"past_key_values\"\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LongLlamaModel):\n            module.gradient_checkpointing = value\n\n\nLONGLLAMA_COMMON_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`\n            or memory cache is enabled):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 1 additional tensor of shape\n            `(batch_size, 1, sequence_length, 1)`. For memory enriched layers it also contains content of memory cache.\n            It is padded with empty tensors so when returned it alwyas has 6 elements.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) \n            that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail. This is NOT supported in LongLlamaForCausalLM and LongLlamaForSequenceClassification\n            due to the specific input processing.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\nLONGLLAMA_MODEL_INPUTS_DOCSTRING = r\"\"\"\n        mem_caches (`tuple(LongLlamaMemCache)`, *optional*) \n            Memory caches for specified layers, None for others\n\"\"\"\n\nLONGLLAMA_ADD_INPUTS_DOCSTRING = r\"\"\"\n        last_context_length (`int`, *optional*) \n            Useful for generation, specifies number of tokens that won't be loaded to memory and \n            will be left for generation cache\n\"\"\"\n\n\ndef _prepare_pos_ids(past_key_values, batch_size, input_length, device):\n    if past_key_values is not None:\n        # take previous max pos_id + 1\n        if past_key_values[0][2].shape[0] != batch_size:\n            raise ValueError(\n                f\"first dimension of past_key_values should match batch size: {batch_size}\"\n                f\"but got {past_key_values[0][2].shape[0]}\"\n            )\n        next_pos = torch.max(past_key_values[0][2].view(batch_size, -1), dim=-1)[0] + 1\n        next_pos = next_pos.view(batch_size, 1)\n    else:\n        next_pos = torch.zeros(batch_size, 1, device=device, dtype=torch.long)\n\n    position_ids = torch.arange(0, input_length, dtype=torch.long, device=device).view(1, input_length)\n    position_ids = position_ids + next_pos\n    return position_ids\n\n\n@add_start_docstrings(\n    \"The bare LongLLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LONGLLAMA_START_DOCSTRING,\n    LONGLLAMA_MEML_DOCSTRING,\n)\n# Modified transformers.models.llama.modeling_llama.LlamaModel\nclass LongLlamaModel(LongLlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LongLlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LongLlamaConfig):\n        super().__init__(config)\n        self.mem_layers = config.mem_layers\n        self.mem_config = LongLlamaMemConfig(\n            positionals=config.mem_positionals,\n            cache_dtype=getattr(torch, config.mem_dtype),\n            attention_grouping=config.mem_attention_grouping,\n        )\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n\n        for mem_layer_id in self.mem_layers:\n            if mem_layer_id < 0 or mem_layer_id >= config.num_hidden_layers:\n                raise ValueError(\n                    f\"Memory layer ids should be between 0 and {config.num_hidden_layers}, got {mem_layer_id}\"\n                )\n\n        layers = []\n        for layer_id in range(config.num_hidden_layers):\n            if layer_id in self.mem_layers:\n                layer = LongLlamaDecoderLayer(config, mem_config=self.mem_config)\n            else:\n                layer = LongLlamaDecoderLayer(config, mem_config=None)\n            layers.append(layer)\n\n        self.layers = nn.ModuleList(layers)\n        self.norm = LongLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    @add_start_docstrings_to_model_forward(LONGLLAMA_COMMON_INPUTS_DOCSTRING, LONGLLAMA_MODEL_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        mem_caches: Optional[Tuple[Optional[LongLlamaMemCache]]] = None,\n    ) -> Union[Tuple, LongLlamaModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[-2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n    \n        if position_ids is None or position_ids.shape[1] == 0:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = _prepare_pos_ids(past_key_values, batch_size, seq_length, device)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = ()\n        next_mem_caches = ()\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n            mem_cache = mem_caches[idx] if mem_caches else None\n\n            if mem_cache is not None and idx not in self.mem_layers:\n                raise ValueError(\"Memory cache provided for a non-memory leayer\")\n\n            if (\n                self.gradient_checkpointing\n                and self.training\n                and mem_cache is None\n                and idx % self.config.gradient_checkpoint_every_ith == 0\n            ):\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, None, mem_cache=None)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    mem_cache=mem_cache,\n                )\n\n            new_mem_cache = layer_outputs[-1]\n            layer_outputs = layer_outputs[:-1]\n            next_mem_caches += (new_mem_cache,)\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n            else:\n                next_decoder_cache += (None,)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n\n        mem_cache_returned = False\n        for mem_cache in next_mem_caches:\n            if mem_cache is not None:\n                mem_cache_returned = True\n        next_mem_caches = next_mem_caches if mem_cache_returned else None\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, next_mem_caches]\n                if v is not None\n            )\n        return LongLlamaModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            mem_caches=next_mem_caches,\n        )\n\n\ndef _handle_output_of_past_key_values(outputs):\n    # merges local caches and memory caches into one single tuple of past_key_values\n    # in order to support generation\n    batch_size = outputs.last_hidden_state.shape[0]\n    if outputs.past_key_values is None and outputs.mem_caches is None:\n        return None\n\n    if outputs.past_key_values is None:\n        out_past_key_values = (None,) * len(outputs.mem_caches)\n    else:\n        out_past_key_values = outputs.past_key_values\n\n    if outputs.mem_caches is None:\n        out_mem_caches = (None,) * len(outputs.past_key_values)\n    else:\n        out_mem_caches = outputs.mem_caches\n\n    device = outputs.last_hidden_state.device\n    past_key_values = ()\n    for local_cache, mem_cache in zip(out_past_key_values, out_mem_caches):\n        layer = ()\n        if local_cache is not None:\n            assert len(local_cache) == 3\n            layer += local_cache\n        else:\n            layer += (torch.empty(batch_size, 0, 0, 0, device=device),) * 3\n\n        if mem_cache is not None:\n            layer += (mem_cache.keys, mem_cache.values, mem_cache.masks)\n        else:\n            layer += (torch.empty(batch_size, 0, 0, 0, device=device),) * 3\n\n        assert len(layer) == 6\n\n        past_key_values += (layer,)\n\n    return past_key_values\n\n\ndef _split_past_key_values(past_key_values):\n    # splits past_key_values to local cache and memory cache\n    local_cache_preset = False\n    mem_caches_present = False\n    if past_key_values is not None:\n        local_caches = ()\n        mem_caches = ()\n        for layer in past_key_values:\n            if len(layer) != 6:\n                raise ValueError(\n                    \"Expected elements of past_key_values to contain 6 elements.\"\n                    \"First 3 describing local cache and last 3 describing memory cache.\"\n                    f\"Instead got {len(layer)} elements\"\n                )\n            else:\n                lk, lv, li, memk, memv, memm = layer\n                if lk.shape[-2] != 0:\n                    local_cache_preset = True\n                    local_caches += ((lk, lv, li),)\n                else:\n                    local_caches += (None,)\n\n                if memk.shape[-2] != 0:\n                    mem_caches_present = True\n                    mem_caches += (LongLlamaMemCache(keys=memk, values=memv, masks=memm),)\n                else:\n                    mem_caches += (None,)\n\n    local_caches = local_caches if local_cache_preset else None\n    mem_caches = mem_caches if mem_caches_present else None\n\n    return local_caches, mem_caches\n\n\ndef _handle_long_input(\n    model,\n    input_ids,\n    attention_mask,\n    position_ids,\n    past_key_values,\n    inputs_embeds,\n    use_cache,\n    output_attentions,\n    output_hidden_states,\n    return_dict,\n    context_window_length,\n    last_context_length,\n):\n    if output_attentions:\n        logger.warning(\n            f\"Outputing attentions is not supported in LongLlamaForCausalLM and LongLlamaForSequenceClassification. \"\n            f\"Attention of the last window will be returned\"\n        )\n\n    past_key_values, mem_caches = _split_past_key_values(past_key_values)\n\n    if past_key_values is not None and use_cache is False:\n        raise ValueError(\"past_key_values it not None should imply use_cache == True\")\n\n    if past_key_values is not None:\n        initial_past_key_values_length = past_key_values[0][0].shape[-2]\n    else:\n        initial_past_key_values_length = 0\n\n    if input_ids is not None:\n        batch_size, input_length = input_ids.shape\n    else:\n        batch_size, input_length, _ = inputs_embeds.shape\n\n    if position_ids is None:\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        position_ids = _prepare_pos_ids(past_key_values, batch_size, input_length, device)\n\n    if position_ids.shape != (batch_size, input_length):\n        raise ValueError(f\"Shape of position_ids [{position_ids}] should match [{batch_size, input_length}]\")\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[..., -(initial_past_key_values_length + input_length) :]\n        if attention_mask is not None and (\n            attention_mask.shape != (batch_size, initial_past_key_values_length + input_length)\n        ):\n            raise ValueError(\n                \"Attention mask should be provided for both the local cache and the input\",\n                f\"Expected shape {(batch_size, initial_past_key_values_length + input_length)},\"\n                f\"got {attention_mask.shape}.\",\n            )\n\n    # First we load prefix to memory cache\n    mem_input_length = max(input_length - last_context_length, 0)\n    outputs_list = []\n    attn_offset = initial_past_key_values_length\n    if mem_input_length > 0:\n        for i in range(0, mem_input_length, context_window_length):\n            beg, end = i, min(mem_input_length, i + context_window_length)\n\n            if attention_mask is not None:\n                if past_key_values is not None:\n                    local_cache_size = past_key_values[0][0].shape[-2]\n                else:\n                    local_cache_size = 0\n                attn_length = attention_mask.shape[-1]\n                attn_beg = beg + attn_offset - local_cache_size\n                attn_end = end + attn_offset\n                assert attn_end <= attn_length\n                assert attn_beg >= 0 and attn_end > attn_beg\n\n            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn, mem_caches)\n            outputs = model(\n                input_ids=input_ids[..., beg:end] if input_ids is not None else None,\n                attention_mask=attention_mask[..., attn_beg:attn_end] if attention_mask is not None else None,\n                position_ids=position_ids[..., beg:end],\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds[..., beg:end, :] if inputs_embeds is not None else None,\n                use_cache=False if past_key_values is None else use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=True,\n                mem_caches=mem_caches,\n            )\n            if i > 0:\n                if mem_caches is not None and past_key_values is None:\n                    for mc_layer in mem_caches:\n                        if mc_layer is not None:\n                            del mc_layer.keys\n                            del mc_layer.values\n                            del mc_layer.masks\n            \n            mem_caches = outputs.mem_caches\n            outputs.mem_caches = None\n            past_key_values = outputs.past_key_values\n            outputs.past_key_values = None\n            outputs_list.append(outputs)\n\n    remaining_input_length = input_length - mem_input_length\n    beg = mem_input_length\n    attn_length = remaining_input_length\n    if past_key_values is not None:\n        attn_length += past_key_values[0][0].shape[-2]\n    attention_mask = attention_mask[..., -attn_length:] if attention_mask is not None else None\n    if beg < input_ids.shape[1]:\n        outputs = model(\n            input_ids=input_ids[..., beg:] if input_ids is not None else None,\n            attention_mask=attention_mask,\n            position_ids=position_ids[..., beg:],\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds[..., beg:, :] if inputs_embeds is not None else None,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n            mem_caches=mem_caches,\n        )\n\n        outputs_list.append(outputs)\n\n        past_key_values = _handle_output_of_past_key_values(outputs_list[-1])\n\n    if output_hidden_states:\n        hidden_states = ()\n        for hd in zip(*[x.hidden_states for x in outputs_list]):\n            hidden_states += (torch.cat(hd, dim=-2),)\n    else:\n        hidden_states = None\n\n    outputs = BaseModelOutputWithPast(\n        last_hidden_state=torch.concat([x.last_hidden_state for x in outputs_list], dim=-2),\n        past_key_values=past_key_values,\n        hidden_states=hidden_states,\n        attentions=outputs_list[-1].attentions,\n    )\n\n    if not return_dict:\n        outputs = tuple(\n            v\n            for v in [outputs.last_hidden_state, outputs.past_key_values, outputs.hidden_states, outputs.attentions]\n            if v is not None\n        )\n    return outputs\n\n\n# Modified transformers.models.llama.modeling_llama.LlamaForCausalLM\nclass LongLlamaForCausalLM(LongLlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.context_window_length = config.max_position_embeddings\n\n        self.model = LongLlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    def _has_generation_cache(self, past_key_values):\n        if past_key_values is not None:\n            assert len(past_key_values[0]) == 6\n            return past_key_values[0][0].shape[-2] != 0\n\n        return False\n\n    @add_start_docstrings_to_model_forward(LONGLLAMA_COMMON_INPUTS_DOCSTRING, LONGLLAMA_ADD_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        last_context_length: Optional[int] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        last_context_length = (last_context_length if last_context_length is not None else self.config.last_context_length)\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = _handle_long_input(\n            model=self.model,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            context_window_length=self.context_window_length,\n            last_context_length=last_context_length,\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        last_context_length=None,\n        **kwargs,\n    ):\n        if self._has_generation_cache(past_key_values):\n            input_ids = input_ids[:, -1:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill(position_ids < 0, 0)\n            if self._has_generation_cache(past_key_values):\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"last_context_length\": last_context_length,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LongLLaMA Model transformer with a sequence classification head on top (linear layer).\n\n    [`LongLlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LONGLLAMA_START_DOCSTRING,\n    LONGLLAMA_MEML_DOCSTRING,\n)\n# Modified from transformers.models.llama.modeling_llama.LlamaForSequenceClassification\nclass LongLlamaForSequenceClassification(LongLlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.context_window_length = config.max_position_embeddings\n        self.model = LongLlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LONGLLAMA_COMMON_INPUTS_DOCSTRING, LONGLLAMA_ADD_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        last_context_length: Optional[int] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        last_context_length = (\n            last_context_length if last_context_length is not None else self.config.last_context_length\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        transformer_outputs = _handle_long_input(\n            model=self.model,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            context_window_length=self.context_window_length,\n            last_context_length=last_context_length,\n        )\n\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )"}
{"type": "source_file", "path": "eval/language_modeling/CEPE/__init__.py", "content": "from .modeling_llama_flash import LlamaForCausalContextLM"}
{"type": "source_file", "path": "eval/language_modeling/InfiniteTransformer/__init__.py", "content": "from .configuration_infini_gemma import GemmaConfig\nfrom .modeling_infini_gemma import GemmaForCausalLM\n"}
{"type": "source_file", "path": "eval/language_modeling/InfiniteTransformer/configuration_infini_gemma.py", "content": "from transformers import GemmaConfig as OriginalGemmaConfig\n\n\nclass GemmaConfig(OriginalGemmaConfig):\n    def __init__(\n        self,\n        vocab_size=256000,\n        hidden_size=3072,\n        intermediate_size=24576,\n        num_hidden_layers=28,\n        num_attention_heads=16,\n        num_key_value_heads=16,\n        head_dim=256,\n        hidden_act=\"gelu_pytorch_tanh\",\n        hidden_activation=None,\n        max_position_embeddings=32768,\n        initializer_range=0.02,\n        rms_norm_eps=0.000001,\n        use_cache=True,\n        pad_token_id=0,\n        eos_token_id=1,\n        bos_token_id=2,\n        tie_word_embeddings=True,\n        rope_theta=10000,\n        attention_bias=False,\n        attention_dropout=0,\n        segment_size=2048,\n        **kwargs\n    ):\n        super().__init__(\n            vocab_size,\n            hidden_size,\n            intermediate_size,\n            num_hidden_layers,\n            num_attention_heads,\n            num_key_value_heads,\n            head_dim,\n            hidden_act,\n            hidden_activation,\n            max_position_embeddings,\n            initializer_range,\n            rms_norm_eps,\n            use_cache,\n            pad_token_id,\n            eos_token_id,\n            bos_token_id,\n            tie_word_embeddings,\n            rope_theta,\n            attention_bias,\n            attention_dropout,\n            **kwargs\n        )\n        self.segment_size = segment_size\n"}
{"type": "source_file", "path": "eval/language_modeling/InfiniteTransformer/modeling_infini_gemma.py", "content": "# coding=utf-8\n# Copyright 2024 Google Inc. HuggingFace Inc. team. All rights reserved.\n#\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch Gemma model, with Infini-Attention.\"\"\"\n\nimport os\nimport math\nimport warnings\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import (\n    AttentionMaskConverter,\n    _prepare_4d_causal_attention_mask,\n)\nfrom transformers.modeling_outputs import (\n    ModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS, is_torch_greater_or_equal_than_1_13\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.utils.import_utils import is_torch_fx_available\n\nfrom dataclasses import dataclass\nfrom .configuration_infini_gemma import GemmaConfig\n\nDEBUG = os.environ.get(\"DEBUG\", False)\n\n\ndef debug_print(*args):\n    if DEBUG:\n        print(*args)\n\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\n# This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n# It means that the function will not be traced through and simply appear as a node in the graph.\nif is_torch_fx_available():\n    if not is_torch_greater_or_equal_than_1_13:\n        import torch.fx\n\n    _prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"GemmaConfig\"\n\n@dataclass\nclass InfiniBaseModelOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n\n    Args:\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n\n            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n            hidden_size)` is output.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n            encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n            input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    last_hidden_state: torch.FloatTensor = None\n    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n    memory: torch.FloatTensor = None\n    norm_term: torch.FloatTensor = None\n\n\n@dataclass\nclass InfiniCausalLMOutputWithPast(ModelOutput):\n    \"\"\"\n    Base class for causal language model (or autoregressive) outputs.\n\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Language modeling loss (for next-token prediction).\n        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n            `past_key_values` input) to speed up sequential decoding.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n    memory: torch.FloatTensor = None\n    norm_term: torch.FloatTensor = None\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(\n        torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0)\n    )\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\n\nclass GemmaRMSNorm(nn.Module):\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.zeros(dim))\n\n    def _norm(self, x):\n        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n\n    def forward(self, x):\n        output = self._norm(x.float())\n        # Llama does x.to(float16) * w whilst Gemma is (x * w).to(float16)\n        # See https://github.com/huggingface/transformers/pull/29402\n        output = output * (1.0 + self.weight.float())\n        return output.type_as(x)\n\n\nALL_LAYERNORM_LAYERS.append(GemmaRMSNorm)\n\n\nclass GemmaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        self.register_buffer(\"inv_freq\", None, persistent=False)\n\n    @torch.no_grad()\n    def forward(self, x, position_ids, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        if self.inv_freq is None:\n            self.inv_freq = 1.0 / (\n                self.base\n                ** (\n                    torch.arange(\n                        0, self.dim, 2, dtype=torch.int64, device=x.device\n                    ).float()\n                    / self.dim\n                )\n            )\n        inv_freq_expanded = (\n            self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        )\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = (\n            device_type\n            if isinstance(device_type, str) and device_type != \"mps\"\n            else \"cpu\"\n        )\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (\n                inv_freq_expanded.float() @ position_ids_expanded.float()\n            ).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\n# Copied from transformers.models.llama.modeling_llama.rotate_half\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\n# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass GemmaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        if config.hidden_activation is None:\n            logger.warning_once(\n                \"Gemma's activation function should be approximate GeLU and not exact GeLU.\\n\"\n                \"Changing the activation function to `gelu_pytorch_tanh`.\"\n                f\"if you want to use the legacy `{config.hidden_act}`, \"\n                f\"edit the `model.config` to set `hidden_activation={config.hidden_act}` \"\n                \"  instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\"\n            )\n            hidden_activation = \"gelu_pytorch_tanh\"\n        else:\n            hidden_activation = config.hidden_activation\n        self.act_fn = ACT2FN[hidden_activation]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\n# Copied from transformers.models.llama.modeling_llama.repeat_kv\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(\n        batch, num_key_value_heads, n_rep, slen, head_dim\n    )\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass GemmaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    # Ignore copy\n    def __init__(self, config: GemmaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = config.head_dim\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if self.hidden_size % self.num_heads != 0:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(\n            self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias\n        )\n        self.k_proj = nn.Linear(\n            self.hidden_size,\n            self.num_key_value_heads * self.head_dim,\n            bias=config.attention_bias,\n        )\n        self.v_proj = nn.Linear(\n            self.hidden_size,\n            self.num_key_value_heads * self.head_dim,\n            bias=config.attention_bias,\n        )\n        self.o_proj = nn.Linear(\n            self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias\n        )\n        self.rotary_emb = GemmaRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(\n            bsz, q_len, self.num_heads, self.head_dim\n        ).transpose(1, 2)\n        key_states = key_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n        query_states, key_states = apply_rotary_pos_emb(\n            query_states, key_states, cos, sin, None\n        )\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(\n                key_states, value_states, self.layer_idx, cache_kwargs\n            )\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(\n            query_states, key_states.transpose(2, 3)\n        ) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(\n            attn_weights, dim=-1, dtype=torch.float32\n        ).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.attention_dropout, training=self.training\n        )\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.view(bsz, q_len, -1)\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Gemma\nclass GemmaFlashAttention2(GemmaAttention):\n    \"\"\"\n    Gemma flash attention module. This module inherits from `GemmaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    # Ignore copy\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(\n            bsz, q_len, self.num_heads, self.head_dim\n        ).transpose(1, 2)\n        key_states = key_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n        query_states, key_states = apply_rotary_pos_emb(\n            query_states, key_states, cos, sin, None\n        )\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(\n                key_states, value_states, self.layer_idx, cache_kwargs\n            )\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (GemmaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(\n            query_states,\n            key_states,\n            value_states,\n            attention_mask,\n            q_len,\n            dropout=dropout_rate,\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self,\n        query_states,\n        key_states,\n        value_states,\n        attention_mask,\n        query_length,\n        dropout=0.0,\n        softmax_scale=None,\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in GemmaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            (\n                query_states,\n                key_states,\n                value_states,\n                indices_q,\n                cu_seq_lens,\n                max_seq_lens,\n            ) = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(\n                attn_output_unpad, indices_q, batch_size, query_length\n            )\n        else:\n            attn_output = flash_attn_func(\n                query_states,\n                key_states,\n                value_states,\n                dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n        return attn_output\n\n    def _upad_input(\n        self, query_layer, key_layer, value_layer, attention_mask, query_length\n    ):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim),\n            indices_k,\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim),\n            indices_k,\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim),\n                indices_k,\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(\n                query_layer, attention_mask\n            )\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with Llama->Gemma\nclass GemmaSdpaAttention(GemmaAttention):\n    \"\"\"\n    Gemma attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `GemmaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Ignore copy\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"GemmaModel is using GemmaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(\n            bsz, q_len, self.num_heads, self.head_dim\n        ).transpose(1, 2)\n        key_states = key_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n        value_states = value_states.view(\n            bsz, q_len, self.num_key_value_heads, self.head_dim\n        ).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n        query_states, key_states = apply_rotary_pos_emb(\n            query_states, key_states, cos, sin, None\n        )\n\n        past_key_value = getattr(self, \"past_key_value\", past_key_value)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(\n                key_states, value_states, self.layer_idx, cache_kwargs\n            )\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, -1)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\nclass GemmaInfiniGate(nn.Module):\n    def __init__(self,num_heads):\n        super().__init__()\n        self.gate =  nn.Parameter(torch.full((1, num_heads, 1, 1), -100.0))\n\n    def forward(self,memory_output,attn_output):\n        return F.sigmoid(self.gate)*memory_output + (1-F.sigmoid(self.gate))*attn_output\n\nclass GemmaInfiniAttention(GemmaAttention):\n    def __init__(\n        self,\n        config: GemmaConfig,\n        layer_idx: Optional[int] = None,\n    ):\n        super().__init__(config, layer_idx)\n\n        # Each head has its own gate\n        # init with -100 to make it close to 0 effect at the beginning\n        # TODO\n        self.gemma_gate = GemmaInfiniGate(self.num_heads)\n        self.segment_size = config.segment_size\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        memory: Optional[torch.Tensor] = None,\n        norm_term: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        self.memory = memory\n        self.norm_term = norm_term\n        \n        total_len = hidden_states.size(1)\n        debug_print(\"total_len\", total_len)\n        segments = torch.tensor_split(\n            hidden_states,\n            list(range(self.segment_size, total_len, self.segment_size)),\n            dim=1,\n        )\n        \n        # Pre-allocate tensor for all outputs\n        bsz, _, hidden_dim = hidden_states.size()\n        final_output = torch.empty(bsz, total_len, hidden_dim, device=hidden_states.device, dtype=hidden_states.dtype)\n\n        debug_print(\"len(segments):\", len(segments))\n\n        start_index = 0\n        for segment in segments:\n            # Process each segment\n            query_states = self.q_proj(segment)\n            key_states = self.k_proj(segment)\n            value_states = self.v_proj(segment)\n\n            # Assuming the presence of batch size and dimension handling as before\n            bsz, q_len, _ = segment.size()  # q_len == self.segment_size\n            query_states = query_states.view(\n                bsz, q_len, self.num_heads, self.head_dim\n            ).transpose(1, 2)\n            key_states = key_states.view(\n                bsz, q_len, self.num_key_value_heads, self.head_dim\n            ).transpose(1, 2)\n            value_states = value_states.view(\n                bsz, q_len, self.num_key_value_heads, self.head_dim\n            ).transpose(1, 2)\n\n            # Rotary embeddings, set seq_len to q_len as we are processing a segment\n            cos, sin = self.rotary_emb(value_states, position_ids, seq_len=q_len)\n\n            query_states, key_states = apply_rotary_pos_emb(\n                query_states,\n                key_states,\n                cos[:, : min(self.segment_size, q_len), :],\n                sin[:, : min(self.segment_size, q_len), :],\n                None,\n            )\n\n            # Basic cache\n            past_key_value = getattr(self, \"past_key_value\", past_key_value)\n            if past_key_value is not None:\n                # sin and cos are specific to RoPE models; cache_position needed for the static cache\n                cache_kwargs = {\n                    \"sin\": sin,\n                    \"cos\": cos,\n                    \"cache_position\": cache_position,\n                }\n                key_states, value_states = past_key_value.update(\n                    key_states, value_states, self.layer_idx, cache_kwargs\n                )\n\n            # GQA\n            # Memory retrieval and attention calculation per segment\n            memory_output = self._retrieve_from_memory(query_states)\n            debug_print(\"Memory Output Shape:\", memory_output.shape)\n            # Update memory with current segment's key and value states\n            self._update_memory(key_states, value_states)\n            key_states = repeat_kv(key_states, self.num_key_value_groups)\n            value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n            causal_mask = attention_mask\n            if attention_mask is not None:\n                causal_mask = causal_mask[\n                    :, :, : min(self.segment_size, q_len), : key_states.shape[-2]\n                ]  # FIXME: This is wrong, should be [:, :, :, :self.segment_size]\n\n            debug_print(\"causal_mask.shape\", causal_mask.shape)\n            debug_print(\"query_states.shape\", query_states.shape)\n\n            attn_output = torch.nn.functional.scaled_dot_product_attention(\n                query_states,\n                key_states,\n                value_states,\n                attn_mask=causal_mask,\n                dropout_p=self.attention_dropout if self.training else 0.0,\n            )\n\n            combined_output = self.gemma_gate(memory_output, attn_output)\n\n            # Prepare output for this segment\n            combined_output = combined_output.transpose(1, 2).contiguous()\n            combined_output = combined_output.view(bsz, q_len, self.hidden_size)\n            \n            segment_output = self.o_proj(combined_output)\n        \n            # Determine the segment size (important for the last segment which might be smaller)\n            current_segment_size = segment.size(1)\n\n            # Fill the corresponding part of the pre-allocated tensor\n            final_output[:, start_index:start_index + current_segment_size, :] = segment_output\n            start_index += current_segment_size\n            \n        return final_output, None, None, self.memory.detach(), self.norm_term.detach()\n    \n\n    def _retrieve_from_memory(self, query_states):\n        # query_states: [batch_size, num_heads, seq_len, head_dim]\n\n        # Check if memory is initialized\n        if self.memory is None or self.norm_term is None:\n            debug_print(\"[Retrieve] No memory or norm term found\")\n            return torch.zeros_like(query_states)\n\n        debug_print(\"[Retrieve] query_states.shape\", query_states.shape)\n        debug_print(\"[Retrieve] self.memory.shape\", self.memory.shape)\n        # Apply ELU activation\n        query_states = F.elu(query_states) + 1  # ELU activation + 1 for stability\n        memory_output = torch.matmul(query_states, self.memory)\n\n        debug_print(\"[Retrieve] memory_output.shape\", memory_output.shape)\n        debug_print(\"[Retrieve] self.norm_term.shape\", self.norm_term.shape)\n\n        # Broadcast norm_term to the shape of query_states, then sum across head_dim for normalization\n        norm_term_broadcastable = torch.matmul(\n            query_states,\n            self.norm_term.transpose(-2, -1),\n        )\n        debug_print(\n            \"[Broadcast] norm_term_broadcastable.shape\", norm_term_broadcastable.shape\n        )\n\n        # Perform division\n        memory_output = memory_output / norm_term_broadcastable\n        return memory_output\n\n    def _update_memory(self, key_states, value_states):\n        # key_states: [batch_size, num_heads, seq_len, head_dim]\n        # value_states: [batch_size, num_heads, seq_len, value_dim]\n\n        key_states = F.elu(key_states) + 1  # Apply ELU activation\n\n        if self.memory is not None:\n            self.memory = self.memory + torch.matmul(\n                key_states.transpose(-2, -1), value_states\n            )\n        else:\n            self.memory = torch.matmul(key_states.transpose(-2, -1), value_states)\n\n        if self.norm_term is not None:\n            self.norm_term = self.norm_term + key_states.sum(\n                dim=2, keepdim=True\n            )  # Update normalization term\n        else:\n            self.norm_term = key_states.sum(\n                dim=2, keepdim=True\n            )  # Initialize normalization term\n\n        debug_print(\"[Update] self.memory.shape\", self.memory.shape)\n        debug_print(\"[Update] self.norm_term.shape\", self.norm_term.shape)\n\n\n# GEMMA_ATTENTION_CLASSES = {\n#     \"eager\": GemmaInfiniAttention,  # GemmaAttention,\n#     \"flash_attention_2\": GemmaFlashAttention2,\n#     \"sdpa\": GemmaSdpaAttention,\n# }\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaDecoderLayer with LLAMA->GEMMA,Llama->Gemma\nclass GemmaDecoderLayer(nn.Module):\n    def __init__(self, config: GemmaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = GemmaInfiniAttention( #GEMMA_ATTENTION_CLASSES[config._attn_implementation](\n            config=config, layer_idx=layer_idx\n        )\n\n        self.mlp = GemmaMLP(config)\n        self.input_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = GemmaRMSNorm(\n            config.hidden_size, eps=config.rms_norm_eps\n        )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        memory: Optional[torch.Tensor] = None,\n        norm_term: Optional[torch.Tensor] = None,\n        **kwargs,\n    ) -> Tuple[\n        torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]\n    ]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        if \"padding_mask\" in kwargs:\n            warnings.warn(\n                \"Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`\"\n            )\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        _attended = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            memory=memory,\n            norm_term=norm_term,\n            **kwargs,\n        )\n        hidden_states, self_attn_weights, present_key_value, memory, norm_term = _attended\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        if memory is not None and norm_term is not None:\n            outputs += (memory, norm_term,)\n\n        return outputs\n\n\nGEMMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`GemmaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare Gemma Model outputting raw hidden-states without any specific head on top.\",\n    GEMMA_START_DOCSTRING,\n)\nclass GemmaPreTrainedModel(PreTrainedModel):\n    config_class = GemmaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _keep_in_fp32_modules = [\"inv_freq\", \"rotary_emb\", \"cos_cached\", \"sin_cached\"]\n    _no_split_modules = [\"GemmaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _setup_cache(\n        self, cache_cls, max_batch_size, max_cache_len: Optional[int] = None\n    ):\n        if (\n            self.config._attn_implementation == \"flash_attention_2\"\n            and cache_cls == StaticCache\n        ):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        for layer in self.model.layers:\n            weights = layer.self_attn.o_proj.weight\n            layer.self_attn.past_key_value = cache_cls(\n                self.config,\n                max_batch_size,\n                max_cache_len,\n                device=weights.device,\n                dtype=weights.dtype,\n            )\n\n    def _reset_cache(self):\n        for layer in self.model.layers:\n            layer.self_attn.past_key_value = None\n\n\nGEMMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare Gemma Model outputting raw hidden-states without any specific head on top.\",\n    GEMMA_START_DOCSTRING,\n)\n# Copied from transformers.models.llama.modeling_llama.LlamaModel with LLAMA->GEMMA,Llama->Gemma\nclass GemmaModel(GemmaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`GemmaDecoderLayer`]\n\n    Args:\n        config: GemmaConfig\n    \"\"\"\n\n    def __init__(self, config: GemmaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(\n            config.vocab_size, config.hidden_size, self.padding_idx\n        )\n        self.layers = nn.ModuleList(\n            [\n                GemmaDecoderLayer(config, layer_idx)\n                for layer_idx in range(config.num_hidden_layers)\n            ]\n        )\n        self.norm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n    # Ignore copy\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        memory: Optional[torch.Tensor] = None,\n        norm_term: Optional[torch.Tensor] = None,\n    ) -> Union[Tuple, InfiniBaseModelOutputWithPast]:\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        past_seen_tokens = 0\n        if use_cache:  # kept for BC (cache positions)\n            if not isinstance(past_key_values, StaticCache):\n                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n            past_seen_tokens = past_key_values.get_seq_length()\n\n        if cache_position is None:\n            cache_position = torch.arange(\n                past_seen_tokens,\n                past_seen_tokens + inputs_embeds.shape[1],\n                device=inputs_embeds.device,\n            )\n\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n\n        causal_mask = self._update_causal_mask(\n            attention_mask,\n            inputs_embeds,\n            cache_position,\n            past_seen_tokens + inputs_embeds.shape[1],\n        )\n\n        # embed positions\n        hidden_states = inputs_embeds\n\n        # normalized\n        # Gemma downcasts the below to float16, causing sqrt(3072)=55.4256 to become 55.5\n        # See https://github.com/huggingface/transformers/pull/29402\n        normalizer = torch.tensor(\n            self.config.hidden_size**0.5, dtype=hidden_states.dtype\n        )\n        hidden_states = hidden_states * normalizer\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                    memory, # FIXME?\n                    norm_term,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                    memory=memory,\n                    norm_term=norm_term,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n            memory = layer_outputs[-2]\n            norm_term = layer_outputs[-1]\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = None\n        if use_cache:\n            next_cache = (\n                next_decoder_cache.to_legacy_cache()\n                if isinstance(next_decoder_cache, Cache)\n                else next_decoder_cache\n            )\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns]\n                if v is not None\n            )\n        return InfiniBaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            memory=memory,\n            norm_term=norm_term,\n        )\n\n    # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n    # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n    # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n    # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n    def _update_causal_mask(\n        self, attention_mask, input_tensor, cache_position, current_length\n    ):\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        min_dtype = torch.finfo(dtype).min\n        sequence_length = input_tensor.shape[1]\n        if hasattr(\n            getattr(self.layers[0], \"self_attn\", {}), \"past_key_value\"\n        ):  # static cache\n            target_length = self.config.max_position_embeddings\n        else:  # dynamic cache\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else current_length + 1\n            )\n\n        causal_mask = torch.full(\n            (sequence_length, target_length),\n            fill_value=min_dtype,\n            dtype=dtype,\n            device=device,\n        )\n        if sequence_length != 1:\n            causal_mask = torch.triu(causal_mask, diagonal=1)\n        causal_mask *= torch.arange(\n            target_length, device=device\n        ) > cache_position.reshape(-1, 1)\n        causal_mask = causal_mask[None, None, :, :].expand(\n            input_tensor.shape[0], 1, -1, -1\n        )\n        if attention_mask is not None:\n            causal_mask = (\n                causal_mask.clone()\n            )  # copy to contiguous memory for in-place edit\n            if attention_mask.dim() == 2:\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[\n                    :, None, None, :\n                ].eq(0.0)\n                causal_mask[..., :mask_length] = causal_mask[\n                    ..., :mask_length\n                ].masked_fill(padding_mask, min_dtype)\n            elif attention_mask.dim() == 4:\n                # backwards compatibility: we allow passing a 4D attention mask shorter than the input length with\n                # cache. In that case, the 4D attention mask attends to the newest tokens only.\n                if attention_mask.shape[-2] < cache_position[0] + sequence_length:\n                    offset = cache_position[0]\n                else:\n                    offset = 0\n                mask_shape = attention_mask.shape\n                mask_slice = (attention_mask.eq(0.0)).to(dtype=dtype) * min_dtype\n                causal_mask[\n                    : mask_shape[0],\n                    : mask_shape[1],\n                    offset : mask_shape[2] + offset,\n                    : mask_shape[3],\n                ] = mask_slice\n\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            causal_mask = AttentionMaskConverter._unmask_unattended(\n                causal_mask, min_dtype\n            )\n\n        return causal_mask\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with LLAMA->GEMMA,Llama->Gemma,llama->gemma\nclass GemmaForCausalLM(GemmaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = GemmaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    # Ignore copy\n    @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(\n        output_type=InfiniCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC\n    )\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        memory: Optional[torch.Tensor] = None,\n        norm_term: Optional[torch.Tensor] = None,\n    ) -> Union[Tuple, InfiniCausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, transformers.,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, transformers., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, GemmaForCausalLM\n\n        >>> model = GemmaForCausalLM.from_pretrained(\"google/gemma-7b\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n\n        >>> prompt = \"What is your favorite condiment?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"What is your favorite condiment?\"\n        ```\"\"\"\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n            memory=memory,\n            norm_term=norm_term,\n        )\n\n        hidden_states = outputs[0]\n        memory = outputs.memory\n        norm_term = outputs.norm_term\n        logits = self.lm_head(hidden_states)\n        logits = logits.float()\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return InfiniCausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            memory=memory,\n            norm_term=norm_term,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        cache_position=None,\n        **kwargs,\n    ):\n        # With static cache, the `past_key_values` is None\n        # TODO joao: standardize interface for the different Cache classes and remove of this if\n        has_static_cache = False\n        if past_key_values is None:\n            past_key_values = getattr(\n                getattr(self.model.layers[0], \"self_attn\", {}), \"past_key_value\", None\n            )\n            has_static_cache = past_key_values is not None\n\n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                past_length = (\n                    cache_position[0]\n                    if cache_position is not None\n                    else past_key_values.get_seq_length()\n                )\n                max_cache_length = (\n                    torch.tensor(\n                        past_key_values.get_max_length(), device=input_ids.device\n                    )\n                    if past_key_values.get_max_length() is not None\n                    else None\n                )\n                cache_length = (\n                    past_length\n                    if max_cache_length is None\n                    else torch.min(max_cache_length, past_length)\n                )\n            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n            # input)\n            if (\n                attention_mask is not None\n                and attention_mask.shape[1] > input_ids.shape[1]\n            ):\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                input_ids = input_ids[:, past_length:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if (\n                max_cache_length is not None\n                and attention_mask is not None\n                and cache_length + input_ids.shape[1] > max_cache_length\n            ):\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        input_length = (\n            position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        )\n        if cache_position is None:\n            cache_position = torch.arange(\n                past_length, past_length + input_length, device=input_ids.device\n            )\n        else:\n            cache_position = cache_position[-input_length:]\n\n        if has_static_cache:\n            past_key_values = None\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(\n                    past_state.index_select(0, beam_idx.to(past_state.device))\n                    for past_state in layer_past\n                ),\n            )\n        return reordered_past\n\n# Remove: Classifiers"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/align_memory.py", "content": "import numpy as np\nimport faiss\nimport faiss.contrib.torch_utils\nimport torch\nfrom typing import Literal, Tuple, List, Optional, Union\nfrom .utils import MemCache\nfrom dataclasses import dataclass\n\n\n\"\"\"\n记忆与存储机制\n** Memory **\n- 维护一个固定大小的记忆单元，用于存储历史信息\n- 利用 save 模块来更新记忆\n- 每次传入的记忆单元是一个 mem_update , 其包含了长度为不固定的 SigMemCache 对象\n- save 模块需要完成的操作包括：将SigMemCache对象拆解为文本，检索嵌入，Key-Value-Mask三元组；利用拆解对象，维护一个MemCache用于之后的检索\n** Retrieval **\n- 利用 retrieve 模块来检索记忆\n- 记忆长度分析：如果 bsz 间长度不一致，则用 PAD 填充\n\"\"\"\n\n@dataclass\nclass MemRecoder:\n    counters:List[List[int]]\n    dstore_idx:List[int]\n    length:List[List[int]]\n\nclass ChunkMemory:\n    def __init__(self, model_config, toolkit_config, device=None):\n        # llama config\n        self.hidden_size = model_config.hidden_size\n        self.num_heads = model_config.num_attention_heads\n        self.head_dim = int(self.hidden_size / self.num_heads)\n        self.memory_size = model_config.memory_size\n        self.pooling_tokens = model_config.pooling_tokens\n        self.dtype = model_config.mem_dtype\n        boundary = model_config.update_boundary\n        # toolkit config\n        self.task = toolkit_config.task\n        self.ret_embeddings_dim = toolkit_config.ret_embeddings_dim\n        # other config\n        if boundary is not None:\n            self.lower, self.upper = list(map(int, boundary.split(\",\")))\n        else:\n            self.lower , self.upper = 0 , 100\n        assert self.lower < self.upper and self.lower >= 0 and self.upper <= 100\n        # init\n        self.device = device\n        self.mem_caches = MemCache()\n        self.mem_caches.texts = []\n        self.mem_caches.embeddings = []\n        self.res = None\n        if device != torch.device(\"cpu\"):\n            self.res = faiss.StandardGpuResources()\n            \n        self.mem_recoder = MemRecoder(\n            counters = [],\n            dstore_idx = [],\n            length = []\n        )\n        \n        self.log_time = 1\n\n    def to(self,device):\n        old_device = self.device\n        self.device = device\n        if self.device != torch.device(\"cpu\"):\n            if self.res == None :\n                self.res = faiss.StandardGpuResources() \n            \n            for i,index in enumerate(self.mem_caches.embeddings):\n                print(f\"put index from {old_device} to {self.device}\")\n                if index == torch.device(\"cpu\"):\n                    index = faiss.index_cpu_to_gpu(self.res, self.device.index, index)\n                else:\n                    temp = faiss.index_gpu_to_cpu(index)\n                    index = faiss.index_cpu_to_gpu(self.res, self.device.index, temp)\n                    \n                self.mem_caches.embeddings[i] = index\n        \n        if self.mem_caches.keys is not None:\n            self.mem_caches.keys = self.mem_caches.keys.to(device)\n            self.mem_caches.values = self.mem_caches.values.to(device)\n            self.mem_caches.masks = self.mem_caches.masks.to(device)\n        \n    \n    def reset(self):\n        temp = self.mem_caches.embeddings\n        self.mem_caches = MemCache()\n        self.mem_caches.texts = []\n        for index in temp:\n            index.reset()\n        self.mem_caches.embeddings = temp\n        self.mem_recoder = MemRecoder(\n            counters = [],\n            dstore_idx = [],\n            length = []\n    )\n        \n    #DELETE\n    def _fill_and_update(self, kv_list) -> List:\n        device = self.device\n        pooling_list = []\n        for i, kv in enumerate(kv_list):\n            if kv is None:\n                pooling_list.append(None)\n                continue\n            k = kv[\"k\"]\n            v = kv[\"v\"]\n            pad_len = 0\n            if k.shape[0] % self.mem_granularity != 0:\n                pad_len = self.mem_granularity - k.shape[0] % self.mem_granularity\n                # fill\n                k = torch.cat((k,torch.zeros((pad_len, self.num_heads, self.head_dim),device=device,),),dim=0,)\n                v = torch.cat((v,torch.zeros((pad_len, self.num_heads, self.head_dim),device=device,),),dim=0,)\n\n            update_nums = k.shape[0] // self.mem_granularity\n            k = k.view(update_nums, self.mem_granularity, self.num_heads, self.head_dim)\n            v = v.view(update_nums, self.mem_granularity, self.num_heads, self.head_dim)\n            pooling_keys = [torch.zeros((update_nums, self.capacity, self.head_dim),device=device,)for _ in range(self.num_heads)]\n            pooling_values = [torch.zeros(\n(update_nums, self.capacity, self.head_dim),\n                    device=device,\n                )\n                for _ in range(self.num_heads)\n            ]\n            for i in range(self.num_heads):\n                # TODO: optimize\n                k_norm = [\n                    torch.mean(\n                        k[\n                            : update_nums - 1,\n                            num * self.pooling_tokens : (num + 1) * self.pooling_tokens,\n                            i,\n                            :,\n                        ],\n                        dim=1,\n                    )\n                    for num in range(self.capacity)\n                ]\n                k_chg = [\n                    (\n                        torch.mean(\n                            k[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                        if num != self.capacity\n                        else torch.mean(\n                            k[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens\n                                - pad_len,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                    )\n                    for num in range(self.capacity)\n                ]\n                v_norm = [\n                    torch.mean(\n                        v[\n                            : update_nums - 1,\n                            num * self.pooling_tokens : (num + 1) * self.pooling_tokens,\n                            i,\n                            :,\n                        ],\n                        dim=1,\n                    )\n                    for num in range(self.capacity)\n                ]\n                v_chg = [\n                    (\n                        torch.mean(\n                            v[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                        if num != self.capacity\n                        else torch.mean(\n                            v[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens\n                                - pad_len,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                    )\n                    for num in range(self.capacity)\n                ]\n\n                merged_k = [\n                    torch.cat((item1, item2.unsqueeze(0)), dim=0)\n                    for item1, item2 in zip(k_norm, k_chg)\n                ]\n                merged_v = [\n                    torch.cat((item1, item2.unsqueeze(0)), dim=0)\n                    for item1, item2 in zip(v_norm, v_chg)\n                ]\n                pooling_keys[i] = torch.stack(\n                    merged_k,\n                    dim=1,\n                )\n                pooling_values[i] = torch.stack(\n                    merged_v,\n                    dim=1,\n                )\n                # add\n            pooling_list.append(\n                {\n                    \"pooling_keys\": torch.stack(pooling_keys, dim=-2),\n                    \"pooling_values\": torch.stack(pooling_values, dim=-2),\n                }\n            )\n        return pooling_list\n\n    def get_key_embeddings(self):\n        return self.ret_embeddings\n\n    def achieve_condition(self, condition=5):\n        for idx in self.dstore_idx:\n            if idx < condition:\n                return False\n        return True\n    \n    def get_min_number(self):\n        return min(self.mem_recoder.dstore_idx) if self.mem_recoder.dstore_idx != [] else 0\n    \n    def save(self,mem_update):\n        if self.log_time>0:\n            print('use mem!!!')\n            self.log_time -= 1\n        # 1. Text\n        texts = mem_update.texts\n        # 2. embeddings\n        embeddings = mem_update.embeddings\n        # 3. key-value-mask\n        keys , values, masks = mem_update.keys , mem_update.values , mem_update.masks\n        \n        if len(keys.shape) != 4 or len(values.shape) != 4 or len(masks.shape) != 4:\n            raise ValueError(f\"Memory cache content should be consistent in shape got {keys.shape} {values.shape} {masks.shape}\")\n        \n        # No pooling\n        if self.pooling_tokens is None:\n            # check overflow\n            if self.mem_caches.keys is not None and self.mem_caches.keys.shape[-2] + keys.shape[-2] > self.memory_size:\n                # print(\"Memory is full, update memory\")\n                self.mem_caches.keys = self.mem_caches.keys[...,-self.memory_size//2:,:]\n                self.mem_caches.values = self.mem_caches.values[...,-self.memory_size//2:,:]\n                self.mem_caches.masks = self.mem_caches.masks[...,-self.memory_size//2:,:]\n\n                for i in range(len(self.mem_caches.keys)):\n                    if self.device != torch.device(\"cpu\"):\n                        if self.log_time == 0:\n                            print(\"remove index to cpu\")\n                            self.log_time -= 1\n                        temp = faiss.index_gpu_to_cpu(self.mem_caches.embeddings[i])\n                    else:\n                        if self.log_time == 0:\n                            print(\"remove in cpu\")\n                            self.log_time -= 1\n                        temp = self.mem_caches.embeddings[i]\n                    assert temp.ntotal == len(self.mem_caches.texts[i]) == len(self.mem_recoder.length[i]) == self.mem_recoder.dstore_idx[i]\n                    rm_indices = torch.arange(0,temp.ntotal//2)\n                    rm_total = temp.ntotal // 2\n                    temp.remove_ids(rm_indices.cpu().numpy())\n                    if self.device != torch.device(\"cpu\"):\n                        self.mem_caches.embeddings[i] = faiss.index_cpu_to_gpu(self.res, self.device.index, temp)\n                    else:\n                        self.mem_caches.embeddings[i] = temp\n                    self.mem_caches.texts[i] = self.mem_caches.texts[i][-rm_total:]\n                    self.mem_recoder.length[i] = self.mem_recoder.length[i][-rm_total:]\n                    self.mem_recoder.dstore_idx[i] = self.mem_recoder.dstore_idx[i]//2\n\n            self.mem_caches.keys = torch.cat((self.mem_caches.keys, keys), dim=-2) if self.mem_caches.keys is not None else keys\n            self.mem_caches.values = torch.cat((self.mem_caches.values, values), dim=-2) if self.mem_caches.values is not None else values\n            self.mem_caches.masks = torch.cat((self.mem_caches.masks, masks), dim=-2) if self.mem_caches.masks is not None else masks\n            \n            bsz = len(texts)\n            for i in range(bsz):\n                if len(self.mem_caches.texts) < i + 1:\n                    # init\n                    if len(self.mem_caches.embeddings) < i + 1:\n                        index = faiss.IndexFlatIP(self.ret_embeddings_dim)\n                        if self.device != torch.device(\"cpu\"):\n                            print(f\"put index {i} from cpu to gpu {self.device}\")\n                            index = faiss.index_cpu_to_gpu(self.res, self.device.index, index)\n                        self.mem_caches.embeddings.append(index)\n                    self.mem_caches.embeddings[i].add(embeddings[i].view(1,-1).to(torch.float32))\n                    self.mem_caches.texts.append(texts[i])\n                    self.mem_recoder.length.append([keys.shape[-2]])\n                    self.mem_recoder.dstore_idx.append(1)\n                else:    \n                    self.mem_caches.texts[i].append(texts[i])\n                    self.mem_caches.embeddings[i].add(embeddings[i].view(1,-1).to(torch.float32))\n                    self.mem_recoder.length[i].append(keys.shape[-2])\n                    self.mem_recoder.dstore_idx[i] += 1\n                \n        else:\n            # TODO: Pooling\n            pass\n    \"\"\"\n    def save(self, memory):\n        ret_embeddings_list = memory[\"embeddings_list\"]\n        examples_list = memory[\"examples_list\"]\n        # [bsz,seq_len,num_heads,head_dim]\n        kv_list = memory[\"kv_list\"]\n        pooling_list = self._fill_and_update(kv_list)\n        # update_nums是seq_len // pooling_tokens\n        # ret_embeddings的数量是 seq_len // csz\n        # 两个之间的映射关系其实是 csz // pooling_tokens\n\n        for i, (pooling_kv, ret_embeddings, examples_list) in enumerate(\n            zip(pooling_list, ret_embeddings_list, examples_list)\n        ):\n            if ret_embeddings is None:\n                continue\n            pooling_k = pooling_kv[\"pooling_keys\"]\n            pooling_v = pooling_kv[\"pooling_values\"]\n            update_nums = len(ret_embeddings)\n            if self.dstore_idx[i] + update_nums > self.memory_size:\n                print(f\"batch{i} : memory is full, update memory\")\n                del_num = 0\n                mask = torch.ones(\n                    self.dstore_idx[i], dtype=torch.bool, device=self.device\n                )\n                before_num = int(self.dstore_idx[i] * self.lower / 100)\n                upper_num = int(self.dstore_idx[i] * self.upper / 100)\n                # 1. 删除前10%\n                del_num += before_num\n                mask[:before_num] = False\n                # 2. 保留后10%\n                ret_counters_mid = self.ret_counters[i][before_num:upper_num]\n                # 3. 动态删除中间部分\n                counters, _ = torch.sort(torch.unique(ret_counters_mid))\n                count = 0\n                while del_num < self.dstore_idx[i] // 2:\n                    mid_mask = ret_counters_mid == counters[count]\n                    count += 1\n                    mid_del = torch.sum(mid_mask)\n                    del_num += mid_del\n                    mask[before_num:upper_num] = mask[before_num:upper_num].masked_fill(\n                        mid_mask, False\n                    )\n                # 4. 拼接\n                save_indices = torch.nonzero(mask).flatten()\n                rm_indices = torch.nonzero(~mask).flatten()\n                try:\n                    self.examples_list = [self.examples_list[i] for i in save_indices]\n                except IndexError:\n                    breakpoint()\n                cat_num = del_num + self.memory_size - self.dstore_idx[i]\n                self.keys[i] = torch.cat(\n                    (\n                        self.keys[\n                            torch.Tensor([i]).type_as(save_indices), save_indices\n                        ],\n                        torch.zeros(\n                            cat_num, self.capacity, self.num_heads, self.head_dim\n                        ).type_as(self.keys),\n                    ),\n                    dim=0,\n                )\n\n                self.values[i] = torch.cat(\n                    (\n                        self.values[\n                            torch.Tensor([i]).type_as(save_indices), save_indices\n                        ],\n                        torch.zeros(\n                            cat_num, self.capacity, self.num_heads, self.head_dim\n                        ).type_as(self.values),\n                    ),\n                    dim=0,\n                )\n                if self.device != torch.device(\"cpu\"):\n                    temp = faiss.index_gpu_to_cpu(self.index_list[i])\n                else:\n                    temp = self.index_list[i]\n                remove_n_total = temp.remove_ids(rm_indices.cpu().numpy())\n                print(f\"Removing {remove_n_total} key-values from index\")\n                self.ret_counters[i] = torch.cat(\n                    (\n                        torch.masked_select(\n                            self.ret_counters[i][: self.dstore_idx[i]], mask\n                        ),\n                        torch.zeros(\n                            (cat_num),\n                            dtype=torch.int,\n                            device=self.ret_counters.device,\n                        ),\n                    ),\n                    dim=0,\n                )\n                self.dstore_idx[i] -= del_num\n                if self.device != torch.device(\"cpu\"):\n                    self.index_list[i] = faiss.index_cpu_to_gpu(\n                        self.res, self.device.index, temp\n                    )\n\n            # 更新\n            self.keys[i][\n                self.dstore_idx[i] : self.dstore_idx[i] + update_nums\n            ] = pooling_k\n            self.values[i][\n                self.dstore_idx[i] : self.dstore_idx[i] + update_nums\n            ] = pooling_v\n            self.index_list[i].add(ret_embeddings)\n            self.examples_list[i].append(examples_list[i])\n            self.dstore_idx[i] += update_nums\n    \"\"\"\n    def _expand_index_tensor(self, x):\n        return torch.cat(list(map(lambda x: torch.arange(x * self.div, (x + 1) * self.div), x)))\n\n    def retrieve_index(self, query_embeddings_list, k):\n        return [self.mem_caches.embeddings[i].search(query_embeddings_list[i].to(torch.float32), k) for i in range(len(query_embeddings_list))]    \n        \n    def expand_elements_2d(self,length_list,indices_list):\n        expand_indices = []\n        for length , indices in zip(length_list,indices_list):\n            indices = indices.tolist()[0]\n            tmp = []\n            for i in indices:\n                if i == 0:\n                    tmp.extend(list(range(length[i])))\n                else:\n                    total = sum(length[:i])\n                    tmp.extend(list(range(total,length[i] + total)))\n            expand_indices.append((tmp))\n        return expand_indices\n        \n    def get(self,indices_list:Optional[List[torch.Tensor]]) -> MemCache:\n        indices_list = [torch.sort(indices,dim=1)[0] for indices in indices_list]\n        mem_caches = MemCache()\n        expand_indices = self.expand_elements_2d(self.mem_recoder.length,indices_list)\n        expand_indices = torch.LongTensor(expand_indices).to(self.device)\n        kv_expand_indices = expand_indices.unsqueeze(1).unsqueeze(-1).expand(-1,self.num_heads,-1,self.head_dim)\n        mask_expand_indices = expand_indices.unsqueeze(1).unsqueeze(-1)\n        mem_caches.keys =  torch.gather(self.mem_caches.keys,2,kv_expand_indices)\n        mem_caches.values = torch.gather(self.mem_caches.values,2,kv_expand_indices)\n        mem_caches.masks = torch.gather(self.mem_caches.masks,2,mask_expand_indices)\n        \n        # TODO: ret_counters\n        \n        return mem_caches\n        \n\n"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/__init__.py", "content": ""}
{"type": "source_file", "path": "eval/icl/MemLong/modeling_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch LLaMA model.\"\"\"\n\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom .configuration_llama import LlamaConfig\n\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (\n                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n            )\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat(\n                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n            )\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [\n                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n            ]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        \n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n        if self.layer_idx == 0:\n            breakpoint()\n        attn_output = self._flash_attention_forward(\n            query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n        is_causal = True if causal_mask is None and q_len > 1 else False\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": LlamaAttention,\n    \"flash_attention_2\": LlamaFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n    _supports_static_cache = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        \n        \n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        return_legacy_cache = False\n        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n            return_legacy_cache = True\n            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n        causal_mask = self._update_causal_mask(\n            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n        )\n\n        # embed positions\n        hidden_states = inputs_embeds\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                \n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                )\n            \n            if input_ids[0][-1] == 32000:\n                pass\n                #breakpoint()\n                \n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n        \n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if return_legacy_cache:\n            next_cache = next_cache.to_legacy_cache()\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n        # to infer the attention mask.\n        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n        using_static_cache = isinstance(past_key_values, StaticCache)\n\n        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                attention_mask,\n                inputs_embeds=input_tensor,\n                past_key_values_length=past_seen_tokens,\n                is_training=self.training,\n            ):\n                return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        min_dtype = torch.finfo(dtype).min\n        sequence_length = input_tensor.shape[1]\n        if using_static_cache:\n            target_length = past_key_values.get_max_length()\n        else:\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else past_seen_tokens + sequence_length + 1\n            )\n\n        if attention_mask is not None and attention_mask.dim() == 4:\n            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n            if attention_mask.max() != 0:\n                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n            causal_mask = attention_mask\n        else:\n            causal_mask = torch.full(\n                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n            )\n            if sequence_length != 1:\n                causal_mask = torch.triu(causal_mask, diagonal=1)\n            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n            if attention_mask is not None:\n                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                padding_mask = padding_mask == 0\n                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                    padding_mask, min_dtype\n                )\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n            and not output_attentions\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        \n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n        \n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        cache_position=None,\n        use_cache=True,\n        **kwargs,\n    ):\n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\n                max_cache_length = (\n                    torch.tensor(past_key_values.get_max_length(), device=input_ids.device)\n                    if past_key_values.get_max_length() is not None\n                    else None\n                )\n                cache_length = past_length if max_cache_length is None else torch.min(max_cache_length, past_length)\n            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                input_ids = input_ids[:, past_length:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if (\n                max_cache_length is not None\n                and attention_mask is not None\n                and cache_length + input_ids.shape[1] > max_cache_length\n            ):\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        if cache_position is None:\n            cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        elif use_cache:\n            cache_position = cache_position[-input_length:]\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": use_cache,\n                \"attention_mask\": attention_mask,\n            }\n        )\n        \n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n                sequence_lengths = sequence_lengths.to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\nThe Llama Model transformer with a span classification head on top for extractive question-answering tasks like\nSQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForQuestionAnswering(LlamaPreTrainedModel):\n    base_model_prefix = \"transformer\"\n\n    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = LlamaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.transformer.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.transformer.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\n    The Llama Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n    output) e.g. for Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForTokenClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        if getattr(config, \"classifier_dropout\", None) is not None:\n            classifier_dropout = config.classifier_dropout\n        elif getattr(config, \"hidden_dropout\", None) is not None:\n            classifier_dropout = config.hidden_dropout\n        else:\n            classifier_dropout = 0.1\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.score = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.score(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )"}
{"type": "source_file", "path": "eval/language_modeling/LongLora/llama_attn_replace.py", "content": "# Modified based on https://github.com/lm-sys/FastChat\n\nimport warnings\nfrom typing import Optional, Tuple\n\nimport torch\nfrom torch import nn\nimport transformers\nfrom einops import rearrange\nfrom flash_attn import __version__ as flash_attn_version\nfrom flash_attn.bert_padding import pad_input, unpad_input\nfrom flash_attn.flash_attn_interface import (\n    flash_attn_func,\n    flash_attn_varlen_kvpacked_func,\n    flash_attn_varlen_qkvpacked_func\n)\nfrom transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv, rotate_half\nfrom flash_attn.bert_padding import unpad_input, pad_input\nimport math\n\ngroup_size_ratio = 1/4\ndef forward_flashattn(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n\n    attention_mask: [bsz, q_len]\n    \"\"\"\n    if not self.training:\n        warnings.warn(\"This function should be used just for training as it may exhibit reduced inference performance. For inference, please use forward_flashattn_inference.\")\n\n    if output_attentions:\n        warnings.warn(\n            \"Output attentions is not supported for patched `LlamaAttention`, returning `None` instead.\"\n        )\n\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n\n    # Past Key value support\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # repeat k/v heads if n_kv_heads < n_heads\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack(\n        [query_states, key_states, value_states], dim=2\n    )  # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n\n    key_padding_mask = attention_mask.repeat(2, 1)\n    nheads = qkv.shape[-2]\n    # shift\n\n    group_size = int(q_len * group_size_ratio)\n    if q_len % group_size > 0:\n        raise ValueError(\"q_len %d should be divisible by group size %d.\" % (q_len, group_size))\n\n    qkv = qkv.reshape(bsz, q_len, 3, 2, self.num_heads // 2, self.head_dim).permute(0, 3, 1, 2, 4, 5).reshape(bsz * 2,\n                                                                                                              q_len, 3,\n                                                                                                              self.num_heads // 2,\n                                                                                                              self.head_dim)\n    x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n    x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n    cu_q_len_tmp = torch.arange(0, max_s, group_size, device=key_padding_mask.device, dtype=cu_q_lens.dtype)\n    cu_q_len_tmp = torch.stack([cu_q_len_tmp, cu_q_len_tmp + group_size // 2]).repeat(bsz, 1) + cu_q_lens[:-1].unsqueeze(-1)\n    cu_q_lens = torch.cat([cu_q_len_tmp, cu_q_lens[1:].unsqueeze(-1)], dim=-1).view(-1)\n\n    x_unpad = rearrange(\n        x_unpad, \"nnz (three h d) -> nnz three h d\", three=3, h=nheads // 2\n    )\n    output_unpad = flash_attn_varlen_qkvpacked_func(\n        x_unpad, cu_q_lens, group_size, 0.0, softmax_scale=None, causal=True\n    )\n    output = rearrange(\n        pad_input(\n            rearrange(output_unpad, \"nnz h d -> nnz (h d)\"), indices, bsz * 2, q_len\n        ),\n        \"b s (h d) -> b s h d\",\n        h=nheads // 2,\n    )\n    output = output.reshape(bsz, 2, q_len, nheads // 2, self.head_dim).transpose(1, 2).reshape(bsz, q_len, nheads,\n                                                                                               self.head_dim)\n\n    return self.o_proj(rearrange(output, \"b s h d -> b s (h d)\")), None, past_key_value\n\ndef forward_flashattn_full(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    \"\"\"Input shape: Batch x Time x Channel\n\n    attention_mask: [bsz, q_len]\n    \"\"\"\n    if output_attentions:\n        warnings.warn(\n            \"Output attentions is not supported for patched `LlamaAttention`, returning `None` instead.\"\n        )\n\n    bsz, q_len, _ = hidden_states.size()\n\n    query_states = (\n        self.q_proj(hidden_states)\n        .view(bsz, q_len, self.num_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    key_states = (\n        self.k_proj(hidden_states)\n        .view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    value_states = (\n        self.v_proj(hidden_states)\n        .view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n        .transpose(1, 2)\n    )\n    # [bsz, q_len, nh, hd]\n    # [bsz, nh, q_len, hd]\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(\n        query_states, key_states, cos, sin, position_ids\n    )\n\n    # Past Key value support\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # repeat k/v heads if n_kv_heads < n_heads\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    # Flash attention codes from\n    # https://github.com/HazyResearch/flash-attention/blob/main/flash_attn/flash_attention.py\n\n    # transform the data into the format required by flash attention\n    qkv = torch.stack(\n        [query_states, key_states, value_states], dim=2\n    )  # [bsz, nh, 3, q_len, hd]\n    qkv = qkv.transpose(1, 3)  # [bsz, q_len, 3, nh, hd]\n\n    # We have disabled _prepare_decoder_attention_mask in LlamaModel\n    # the attention_mask should be the same as the key_padding_mask\n\n    key_padding_mask = attention_mask\n    nheads = qkv.shape[-2]\n    x = rearrange(qkv, \"b s three h d -> b s (three h d)\")\n    x_unpad, indices, cu_q_lens, max_s = unpad_input(x, key_padding_mask)\n    x_unpad = rearrange(\n        x_unpad, \"nnz (three h d) -> nnz three h d\", three=3, h=nheads\n    )\n    output_unpad = flash_attn_varlen_qkvpacked_func(\n        x_unpad, cu_q_lens, max_s, 0.0, softmax_scale=None, causal=True\n    )\n    output = rearrange(\n        pad_input(\n            rearrange(output_unpad, \"nnz h d -> nnz (h d)\"), indices, bsz, q_len\n        ),\n        \"b s (h d) -> b s h d\",\n        h=nheads,\n    )\n    output = output.reshape(bsz, q_len, self.num_heads, self.head_dim)\n\n    return self.o_proj(rearrange(output, \"b s h d -> b s (h d)\")), None, past_key_value\n\n\ndef forward_noflashattn(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.LongTensor] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    group_size = int(q_len * group_size_ratio)\n\n    if q_len % group_size > 0:\n        raise ValueError(\"q_len %d should be divisible by group size %d.\"%(q_len, group_size))\n    num_group = q_len // group_size\n\n    if self.config.pretraining_tp > 1:\n        key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n        query_slices = self.q_proj.weight.split(\n            (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n        )\n        key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n        value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n        query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n        query_states = torch.cat(query_states, dim=-1)\n\n        key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n        key_states = torch.cat(key_states, dim=-1)\n\n        value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n        value_states = torch.cat(value_states, dim=-1)\n\n    else:\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # repeat k/v heads if n_kv_heads < n_heads\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    # shift\n    def shift(qkv, bsz, q_len, group_size, num_heads, head_dim):\n        qkv[:, num_heads // 2:] = qkv[:, num_heads // 2:].roll(-group_size // 2, dims=2)\n        qkv = qkv.transpose(1, 2).reshape(bsz * (q_len // group_size), group_size, num_heads, head_dim).transpose(1, 2)\n        return qkv\n\n    query_states = shift(query_states, bsz, q_len, group_size, self.num_heads, self.head_dim)\n    key_states = shift(key_states, bsz, q_len, group_size, self.num_heads, self.head_dim)\n    value_states = shift(value_states, bsz, q_len, group_size, self.num_heads, self.head_dim)\n\n    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n    if attn_weights.size() != (bsz * num_group, self.num_heads, group_size, group_size):\n        raise ValueError(\n            f\"Attention weights should be of size {(bsz * num_group, self.num_heads, group_size, group_size)}, but is\"\n            f\" {attn_weights.size()}\"\n        )\n\n    attention_mask = attention_mask[:, :, :group_size, :group_size].repeat(num_group, 1, 1, 1)\n    if attention_mask is not None:\n        if attention_mask.size() != (bsz * num_group, 1, group_size, group_size):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz * num_group, 1, group_size, group_size)}, but is {attention_mask.size()}\"\n            )\n        attn_weights = attn_weights + attention_mask\n\n    # upcast attention to fp32\n    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n    attn_output = torch.matmul(attn_weights, value_states)\n\n    if attn_output.size() != (bsz * num_group, self.num_heads, group_size, self.head_dim):\n        raise ValueError(\n            f\"`attn_output` should be of size {(bsz * num_group, self.num_heads, group_size, self.head_dim)}, but is\"\n            f\" {attn_output.size()}\"\n        )\n    attn_output = attn_output.transpose(1, 2).contiguous()\n\n    attn_output = attn_output.reshape(bsz, q_len, self.num_heads, self.head_dim)\n\n    # shift back\n    attn_output[:, :, self.num_heads//2:] = attn_output[:, :, self.num_heads//2:].roll(group_size//2, dims=1)\n\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    if self.config.pretraining_tp > 1:\n        attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n        o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n        attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n    else:\n        attn_output = self.o_proj(attn_output)\n\n    if not output_attentions:\n        attn_weights = None\n\n    return attn_output, attn_weights, past_key_value\n\n# Disable the transformation of the attention mask in LlamaModel as the flash attention\n# requires the attention mask to be the same as the key_padding_mask\ndef _prepare_decoder_attention_mask(\n    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n):\n    # [bsz, seq_len]\n    return attention_mask\n\ndef apply_rotary_pos_emb_inference(q, k, cos_sin, position_ids):\n    gather_indices = position_ids[:, :, None, None]  # [bsz, seq_len, 1, 1]\n    gather_indices = gather_indices.repeat(\n        1, 1, cos_sin[0].shape[1], cos_sin[0].shape[3]\n    )\n    bsz = gather_indices.shape[0]\n    cos, sin = (\n        torch.gather(x.transpose(1, 2).repeat(bsz, 1, 1, 1), 1, gather_indices)\n        for x in cos_sin\n    )\n    q, k = ((x * cos) + (rotate_half(x) * sin) for x in (q, k))\n    return q, k\n\n\ndef forward_flashattn_inference(\n    self,\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.Tensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n    padding_mask: Optional[torch.Tensor] = None,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    if output_attentions:\n        warnings.warn(\n            \"Output attentions is not supported for patched `LlamaAttention`, returning `None` instead.\"\n        )\n\n    bsz, q_len, _ = hidden_states.size()\n    kv_heads = getattr(self, \"num_key_value_heads\", self.num_heads)\n\n    q, k, v = (\n        op(hidden_states).view(bsz, q_len, nh, self.head_dim)\n        for op, nh in (\n            (self.q_proj, self.num_heads),\n            (self.k_proj, kv_heads),\n            (self.v_proj, kv_heads),\n        )\n    )\n    # shape: (b, s, num_heads, head_dim)\n\n    kv_seq_len = k.shape[1]\n    past_kv_len = 0\n    if past_key_value is not None:\n        past_kv_len = past_key_value[0].shape[2]\n        kv_seq_len += past_kv_len\n\n    cos_sin = self.rotary_emb(v, seq_len=kv_seq_len)\n    q, k = apply_rotary_pos_emb_inference(q, k, cos_sin, position_ids)\n\n    if past_key_value is not None:\n        assert (\n            flash_attn_version >= \"2.1.0\"\n        ), \"past_key_value support requires flash-attn >= 2.1.0\"\n        # reuse k, v\n        k = torch.cat([past_key_value[0].transpose(1, 2), k], dim=1)\n        v = torch.cat([past_key_value[1].transpose(1, 2), v], dim=1)\n\n    past_key_value = (k.transpose(1, 2), v.transpose(1, 2)) if use_cache else None\n\n    if attention_mask is None:\n        output = flash_attn_func(q, k, v, 0.0, softmax_scale=None, causal=True).view(\n            bsz, q_len, -1\n        )\n    else:\n        q, indices, cu_q_lens, max_s = unpad_input(q, attention_mask[:, -q_len:])\n        # We can skip concat and call unpad twice but seems better to call unpad only once.\n        kv, _, cu_k_lens, max_k = unpad_input(\n            torch.stack((k, v), dim=2), attention_mask\n        )\n        output_unpad = flash_attn_varlen_kvpacked_func(\n            q,\n            kv,\n            cu_q_lens,\n            cu_k_lens,\n            max_s,\n            max_k,\n            0.0,\n            softmax_scale=None,\n            causal=True,\n        )\n        output_unpad = output_unpad.reshape(-1, self.num_heads * self.head_dim)\n        output = pad_input(output_unpad, indices, bsz, q_len)\n\n    return self.o_proj(output), None, past_key_value\n\ndef _prepare_decoder_attention_mask_inference(\n    self, attention_mask, input_shape, inputs_embeds, past_key_values_length\n):\n    # [bsz, seq_len]\n    if past_key_values_length > 0 and attention_mask is not None:\n        attention_mask = torch.cat(\n            (\n                torch.full(\n                    (input_shape[0], past_key_values_length),\n                    True,\n                    dtype=attention_mask.dtype,\n                    device=attention_mask.device,\n                ),\n                attention_mask,\n            ),\n            dim=-1,\n        )\n\n    if attention_mask is not None and torch.all(attention_mask):\n        return None  # This uses the faster call when training with full samples\n\n    return attention_mask\n\ndef replace_llama_attn(use_flash_attn=True, use_full=False, inference=False):\n    if use_flash_attn:\n        cuda_major, cuda_minor = torch.cuda.get_device_capability()\n        if cuda_major < 8:\n            warnings.warn(\n                \"Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.\"\n                \"ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593\"\n            )\n        if inference:\n            transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = _prepare_decoder_attention_mask_inference\n            transformers.models.llama.modeling_llama.LlamaAttention.forward = forward_flashattn_inference\n        else:\n            transformers.models.llama.modeling_llama.LlamaModel._prepare_decoder_attention_mask = (\n                _prepare_decoder_attention_mask\n            )\n            transformers.models.llama.modeling_llama.LlamaAttention.forward = forward_flashattn_full if use_full else forward_flashattn\n    else:\n        transformers.models.llama.modeling_llama.LlamaAttention.forward = forward_noflashattn"}
{"type": "source_file", "path": "eval/language_modeling/LongLora/__init__.py", "content": "from .llama_attn_replace import replace_llama_attn"}
{"type": "source_file", "path": "eval/icl/MemLong/modeling_llama_position.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch LLaMA model.\"\"\"\n\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom .cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\n\nfrom .configuration_llama import LlamaConfig\nfrom .utils import ToolkitConfig,MemConfig,MemCache\nfrom typing import Literal\nfrom dataclasses import dataclass\nfrom .toolkit import ToolKit\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\n\n@dataclass\nclass BaseModelOutputWithMem(BaseModelOutputWithPast):\n    mem_update: Optional[MemCache] = None\n\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (\n                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n            )\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\ndef apply_rotary_pos_emb_for_relative_query(q, cos, sin,unsqueeze_dim=1):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)  # q: [bs, nh, seq_len, dim]\n    return q_embed\n\ndef apply_rotary_pos_emb_for_relative_keys(k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return k_embed\n\n# Based on transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef rotate_one(x, cos, sin, position_ids):\n    if len(position_ids.shape) != 2 or x.shape[0] != position_ids.shape[0] or x.shape[-2] != position_ids.shape[1]:\n        raise ValueError(f\"Position ids shoud have shape [bsz, seq_len] got {position_ids.shape}\")\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    x_embed = (x * cos) + (rotate_half(x) * sin)\n    return x_embed\n\ndef rotate_as_if_first(x, rotary_emb):\n    # x: [bs, num_attention_heads, seq_len, head_size]\n    # apply rotary as if all elements were first in the sequence\n    cos, sin = rotary_emb(x, torch.arange(x.shape[-2]).view(1,-1).type_as(x))\n    return rotate_one(x, cos, sin, torch.zeros(x.shape[0], x.shape[-2], dtype=torch.long, device=cos.device))\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat(\n                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n            )\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [\n                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n            ]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\nclass RetGate(nn.Module):\n    def __init__(self, num_heads):\n        super().__init__()\n        self.output_gate = torch.nn.Parameter(torch.zeros(1, num_heads, 1, 1))\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, ret_score, attn_score):\n        # print(self.output_gate[0][0][0])\n        # print(self.sigmoid(self.output_gate[0][0][0]))\n        return torch.cat((self.sigmoid(self.output_gate) * ret_score,(1 - self.sigmoid(self.output_gate)) * attn_score,),dim=-1,)\n\n\nclass RetrievalCausalAttention(nn.Module):\n    \n    def __init__(self, config:LlamaConfig, mem_config: MemConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.max_local_cache = config.max_position_embeddings\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        \n        # control mem behavior\n        self.mem_config = mem_config\n        # begin of sentence\n        self.clear_memory_on_bos_token_id = getattr(mem_config,\"clear_memory_on_bos_token_id\",False)\n        # end of sentence\n        self.clear_memory_on_eos_token_id = getattr(mem_config,\"clear_memory_on_eos_token_id\",False)\n        # None;Zero;Continual\n        self.position_type = config.position_type\n        self.ret_gate = None\n        # Only for Retrieval\n        if getattr(mem_config,\"use_gate\",False):\n            self.ret_gate = RetGate(self.num_heads)\n        \n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n    \n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return (\n            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n            .contiguous()\n        )\n    \n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[MemCache] = None,\n        output_mem: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\n        bsz, q_len, _ = hidden_states.size()\n        if self.config.pretraining_tp > 1:\n            raise NotImplementedError(\"pretraining_tp > 1 not supported for RetrievalCausalAttention\")\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        \n        # Whether to use positional encoding when saving \n        use_positionals = self.mem_config is None or getattr(self.mem_config,\"positionals\",True)\n        # two conditions \n        # 1. use for saving or training \n        mem_no_local_cache = output_mem is True and past_key_value is None and (not use_cache)\n        # 2. use for generation\n        mem_and_local_cache = output_mem is True and use_cache\n        mem_update = None\n        if mem_no_local_cache:\n            if use_positionals and self.position_type == \"Zero\":\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n            \n            mem_update = MemCache(\n                keys=rfst_key_states.detach().to(self.mem_config.cache_dtype),\n                values=value_states.detach().to(self.mem_config.cache_dtype),\n                masks=attention_mask[...,-1,:,None].detach()\n            )   \n            \n        # Get local_seq_length\n        if use_cache and past_key_value is not None:\n            past_local_cache_size = past_key_value.get_seq_length(layer_idx=self.layer_idx)\n            local_seq_length = past_local_cache_size + hidden_states.shape[-2]\n        else:\n            local_seq_length = hidden_states.shape[-2]\n                \n        if past_key_value is not None:\n            if len(past_key_value) <= self.layer_idx:\n                merged_key_states = key_states\n                merged_value_states = value_states\n            else:\n                merged_key_states = torch.cat([past_key_value.key_cache[self.layer_idx], key_states], dim=-2)\n                merged_value_states = torch.cat([past_key_value.value_cache[self.layer_idx], value_states], dim=-2)\n            # merged_position_ids = torch.cat([past_key_value.position_ids_cache[self.layer_idx], loc_position_ids], dim=-1)\n            \n            if attention_mask.shape[-1] != merged_key_states.shape[-2] and attention_mask.shape[-2] == query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            assert local_seq_length == merged_key_states.shape[-2]\n            \n            if merged_key_states.shape[-2] > self.max_local_cache:\n                # We drop half of max_local_cache for Memory    \n                num_elems_to_drop = past_local_cache_size // 2\n                # key_states,value_states = past_key_value.drop_and_update(drop_keys,drop_values,key_states,value_states,self.layer_idx)\n                if mem_and_local_cache:\n                    drop_keys = merged_key_states[..., :num_elems_to_drop, :]\n                    drop_values = merged_value_states[...,:num_elems_to_drop,:]\n                    drop_masks = attention_mask[...,-1,:,None]\n                    drop_masks = drop_masks[:,:, :num_elems_to_drop, :]\n                    \n                    if use_positionals and self.position_type == \"Zero\":\n                        rfst_drop_keys = rotate_as_if_first(drop_keys,self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    \n                    mem_update = MemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype).detach(),\n                        values=drop_values.to(self.mem_config.cache_dtype).detach(),\n                        masks=drop_masks.to(self.mem_config.cache_dtype).detach(),\n                    )            \n                key_states, value_states , position_ids = past_key_value.drop_and_update(key_states,value_states,position_ids,num_elems_to_drop,self.layer_idx)\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n            else:\n                key_states, value_states , position_ids = past_key_value.update(key_states, value_states, position_ids , self.layer_idx)\n\n        kv_seq_len = key_states.shape[-2]\n        \n        # Get mem_caches_length\n        if mem_caches is not None:\n            mem_caches_length = mem_caches.keys.shape[-2]\n        else:\n            mem_caches_length = 0\n        \n        if use_positionals and self.position_type == \"Zero\":\n            loc_position_ids = torch.arange(1,kv_seq_len+1).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.zeros((1,mem_caches_length)).type_as(position_ids)\n        elif use_positionals and self.position_type == \"Continual\":\n            loc_position_ids = torch.arange(mem_caches_length,mem_caches_length+kv_seq_len).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.arange(mem_caches_length).view(1,-1).type_as(position_ids)\n        else:\n            # FIXME\n            raise NotImplementedError(\"For normal generation\")\n        \n        retrieval_upper =  mem_caches is not None and self.layer_idx in self.config.ret_attn_layers\n\n        # We rotate the mem firstly\n        if retrieval_upper and self.position_type==\"Continual\":\n            mem_cos , mem_sin = self.rotary_emb(value_states, mem_position_ids)\n            mem_caches.keys = apply_rotary_pos_emb_for_relative_keys(mem_caches.keys, mem_cos,mem_sin)\n            \n        loc_cos, loc_sin = self.rotary_emb(value_states, loc_position_ids)\n        query_states = apply_rotary_pos_emb_for_relative_query(query_states, loc_cos[:,-query_states.shape[-2]:], loc_sin[:,-query_states.shape[-2]:])\n        key_states = apply_rotary_pos_emb_for_relative_keys(key_states, loc_cos, loc_sin)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        loc_attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n            )\n        loc_attn_weights = loc_attn_weights + attention_mask\n\n        if retrieval_upper:\n            mem_mask = mem_caches.masks.squeeze(-1).unsqueeze(-2)\n            # TODO: ret granularity\n            mem_attn_weights = torch.matmul(query_states,mem_caches.keys.transpose(2, 3).to(key_states.dtype),) / math.sqrt(self.head_dim)\n            \n            assert mem_mask.shape[2] == 1\n            mem_attn_weights = mem_attn_weights + mem_mask\n            if self.ret_gate:\n                attn_weights = self.ret_gate(mem_attn_weights, loc_attn_weights)\n            else:\n                attn_weights = torch.concat((mem_attn_weights,loc_attn_weights),dim=-1)\n            combined_value_states = torch.concat([mem_caches.values.to(value_states.dtype),value_states],dim=-2,)\n        else:\n            attn_weights = loc_attn_weights\n            combined_value_states = value_states\n        \n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, combined_value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n        \n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value, mem_update\n\nclass RetrievalFlashAttention2(RetrievalCausalAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self.ret_gate = None\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[MemCache] = None,\n        output_mem: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n        \n        if attention_mask == None:\n            tgt_seq_len = hidden_states.shape[-2]\n            if past_key_value is not None:\n                src_seq_len = past_key_value.get_seq_length(self.layer_idx) + tgt_seq_len\n            else:\n                src_seq_len = tgt_seq_len\n            attention_mask = torch.ones(hidden_states.shape[0],tgt_seq_len).type_as(hidden_states)\n            rfst_attention_mask = self._gen_causal_mask(attention_mask,cache_position,src_seq_len,tgt_seq_len)\n        else:\n            rfst_attention_mask = attention_mask\n\n        output_attentions = False\n        bsz, q_len, _ = hidden_states.size()\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        # Whether to use positional encoding when saving \n        use_positionals = self.mem_config is None or getattr(self.mem_config,\"positionals\",True)\n        # two conditions \n        # 1. use for saving or training \n        mem_no_local_cache = output_mem is True and past_key_value is None and (not use_cache)\n        # 2. use for generation\n        mem_and_local_cache = output_mem is True and use_cache\n        mem_update = None\n        if mem_no_local_cache:\n            if use_positionals and self.position_type == \"Zero\":\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n                \n            mem_update = MemCache(\n                keys=rfst_key_states.detach().to(self.mem_config.cache_dtype),\n                values=value_states.detach().to(self.mem_config.cache_dtype),\n                masks=rfst_attention_mask[...,-1,:,None].detach()\n            )\n        # Get local_seq_length\n        if use_cache and past_key_value is not None:\n            past_local_cache_size = past_key_value.get_seq_length(layer_idx=self.layer_idx)\n            local_seq_length = past_local_cache_size + hidden_states.shape[-2]\n        else:\n            local_seq_length = hidden_states.shape[-2]\n        \n        if past_key_value is not None:\n            if len(past_key_value) <= self.layer_idx:\n                merged_key_states = key_states\n                merged_value_states = value_states\n            else:\n                merged_key_states = torch.cat([past_key_value.key_cache[self.layer_idx], key_states], dim=-2)\n                merged_value_states = torch.cat([past_key_value.value_cache[self.layer_idx], value_states], dim=-2)\n            if rfst_attention_mask.shape[-1] != merged_key_states.shape[-2] and rfst_attention_mask.shape[-2] == query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            assert local_seq_length == merged_key_states.shape[-2]\n            \n            if merged_key_states.shape[-2] > self.max_local_cache:\n                # We drop half of max_local_cache for Memory    \n                num_elems_to_drop = past_local_cache_size // 2\n                # key_states,value_states = past_key_value.drop_and_update(drop_keys,drop_values,key_states,value_states,self.layer_idx)\n                if mem_and_local_cache:\n                    drop_keys = merged_key_states[..., :num_elems_to_drop, :]\n                    drop_values = merged_value_states[...,:num_elems_to_drop,:]\n                    drop_masks = rfst_attention_mask[...,-1,:,None]\n                    drop_masks = drop_masks[:,:, :num_elems_to_drop, :]\n                    \n                    if use_positionals and self.position_type == \"Zero\":\n                        rfst_drop_keys = rotate_as_if_first(drop_keys,self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    \n                    mem_update = MemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype).detach(),\n                        values=drop_values.to(self.mem_config.cache_dtype).detach(),\n                        masks=drop_masks.to(self.mem_config.cache_dtype).detach(),\n                    )            \n                key_states, value_states , position_ids = past_key_value.drop_and_update(key_states,value_states,position_ids,num_elems_to_drop,self.layer_idx)\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n            else:\n                key_states, value_states , position_ids = past_key_value.update(key_states, value_states, position_ids , self.layer_idx)\n\n        kv_seq_len = key_states.shape[-2]\n        \n        # Get mem_caches_length\n        if mem_caches is not None:\n            mem_caches_length = mem_caches.keys.shape[-2]\n        else:\n            mem_caches_length = 0\n        \n        if use_positionals and self.position_type == \"Zero\":\n            loc_position_ids = torch.arange(1,kv_seq_len+1).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.zeros((1,mem_caches_length)).type_as(position_ids)\n        elif use_positionals and self.position_type == \"Continual\":\n            loc_position_ids = torch.arange(mem_caches_length,mem_caches_length+kv_seq_len).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.arange(mem_caches_length).view(1,-1).type_as(position_ids)\n        else:\n            # FIXME\n            raise NotImplementedError(\"For normal generation\")\n        \n        retrieval_upper =  mem_caches is not None and self.layer_idx in self.config.ret_attn_layers\n\n        # We rotate the mem firstly\n        if retrieval_upper and self.position_type==\"Continual\":\n            mem_cos , mem_sin = self.rotary_emb(value_states, mem_position_ids)\n            mem_caches.keys = apply_rotary_pos_emb_for_relative_keys(mem_caches.keys, mem_cos,mem_sin)\n            \n        loc_cos, loc_sin = self.rotary_emb(value_states, loc_position_ids)\n        query_states = apply_rotary_pos_emb_for_relative_query(query_states, loc_cos[:,-query_states.shape[-2]:], loc_sin[:,-query_states.shape[-2]:])\n        key_states = apply_rotary_pos_emb_for_relative_keys(key_states, loc_cos, loc_sin)\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n        \n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n            \n        loc_attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n        \n        if retrieval_upper:\n            input_dtype = query_states.dtype\n            if input_dtype == torch.float32:\n                if torch.is_autocast_enabled():\n                    target_dtype = torch.get_autocast_gpu_dtype()\n                # Handle the case where the model is quantized\n                elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                    target_dtype = self.config._pre_quantization_dtype\n                else:\n                    target_dtype = self.q_proj.weight.dtype\n\n                logger.warning_once(\n                    f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                    f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                    f\" {target_dtype}.\"\n                )\n                ret_key_states = mem_caches.keys.to(target_dtype)\n                ret_value_states = mem_caches.values.to(target_dtype)\n                mem_mask = mem_caches.masks.squeeze(-1).squeeze(-2)\n                mem_mask = (mem_mask == 0).to(target_dtype)\n            else:\n                ret_key_states = mem_caches.keys\n                ret_value_states = mem_caches.values\n                mem_mask = mem_caches.masks.squeeze(-1).squeeze(-2)\n                mem_mask = (mem_mask == 0).to(input_dtype)\n            ret_key_states = ret_key_states.transpose(1, 2)\n            ret_value_states = ret_value_states.transpose(1, 2)\n            ret_attn_output = self._flash_attention_forward(query_states, ret_key_states, ret_value_states, mem_mask, q_len, dropout=dropout_rate)\n            attn_output = loc_attn_output + ret_attn_output\n        else:\n            attn_output = loc_attn_output\n        \n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value, mem_update\n    \n    def _gen_causal_mask(self,attention_mask,cache_position,sequence_length,target_length):\n        dtype, device = attention_mask.dtype, attention_mask.device\n        min_dtype = torch.finfo(dtype).min\n        causal_mask = torch.full(\n                    (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n        )\n        if sequence_length != 1:\n            causal_mask = torch.triu(causal_mask, diagonal=1)\n        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n        causal_mask = causal_mask[None, None, :, :].expand(attention_mask.shape[0], 1, -1, -1)\n        if attention_mask is not None:\n            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n            mask_length = attention_mask.shape[-1]\n            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n            padding_mask = padding_mask == 0\n            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                padding_mask, min_dtype\n        )\n        return causal_mask\n    \n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        output_attentions = False\n        bsz, q_len, _ = hidden_states.size()\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(\n            query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        # We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\n        # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n        is_causal = True if causal_mask is None and q_len > 1 else False\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": RetrievalCausalAttention,\n    \"flash_attention_2\": RetrievalFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int,mem_config: Optional[MemConfig] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.mem_config = mem_config\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, mem_config=mem_config,layer_idx=layer_idx)\n        self.output_mem = layer_idx == config.mem_layer\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches=None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n        \n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value , mem_update= self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            mem_caches=mem_caches,\n            output_mem=self.output_mem,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        outputs += (mem_update,)\n        \n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n    _supports_static_cache = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n        if config.mem_layer < 0 or config.mem_layer >= config.num_hidden_layers:\n            raise ValueError(f\"mem_layer should be between 0 and {config.num_hidden_layers - 1}\")\n        assert isinstance(config.ret_attn_layers, list), \"ret_attn_layers should be a list\"\n        self.mem_config = MemConfig(\n            positionals=config.mem_positionals,\n            use_gate=config.use_gate,\n            cache_dtype=getattr(torch, config.mem_dtype),\n        )\n        self.ret_attn_layers = config.ret_attn_layers\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])        \n        layers = []\n        for layer_id in range(config.num_hidden_layers):\n            if layer_id in self.ret_attn_layers or layer_id == config.mem_layer:\n                layer = LlamaDecoderLayer(config, layer_id, mem_config=self.mem_config)\n            else:\n                layer = LlamaDecoderLayer(config, layer_id)\n            layers.append(layer)\n\n        self.layers = nn.ModuleList(layers)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[Tuple[Optional[MemCache]]] = None,\n    ) -> Union[Tuple, BaseModelOutputWithMem]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        return_legacy_cache = False\n        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n            return_legacy_cache = True\n            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n            \n        causal_mask = self._update_causal_mask(\n            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n        )\n        \n        # embed positions\n        hidden_states = inputs_embeds\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n        mem_update = None\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                    mem_caches=mem_caches,\n                )\n            if input_ids[0][-1] == 32000:\n                pass\n                #breakpoint()\n\n            hidden_states = layer_outputs[0]\n            \n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n            \n            if layer_outputs[-1] is not None:\n                mem_update = layer_outputs[-1]\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if return_legacy_cache:\n            next_cache = next_cache.to_legacy_cache()\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        \n        return BaseModelOutputWithMem(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            mem_update=mem_update,\n        )\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n        # to infer the attention mask.\n        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n        using_static_cache = isinstance(past_key_values, StaticCache)\n\n        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                attention_mask,\n                inputs_embeds=input_tensor,\n                past_key_values_length=past_seen_tokens,\n                is_training=self.training,\n            ):\n                return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        min_dtype = torch.finfo(dtype).min\n        sequence_length = input_tensor.shape[1]\n        if using_static_cache:\n            target_length = past_key_values.get_max_length()\n        else:\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else past_seen_tokens + sequence_length\n            )\n\n        if attention_mask is not None and attention_mask.dim() == 4:\n            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n            if attention_mask.max() != 0:\n                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n            causal_mask = attention_mask\n        else:\n            causal_mask = torch.full(\n                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n            )\n            if sequence_length != 1:\n                causal_mask = torch.triu(causal_mask, diagonal=1)\n            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n            if attention_mask is not None:\n                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                padding_mask = padding_mask == 0\n                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                    padding_mask, min_dtype\n                )\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n            and not output_attentions\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\ndef _split_past_key_values(past_key_values):\n    # splits past_key_values to local cache and memory cache\n    local_cache_present = False\n    mem_caches_present = False\n    if past_key_values is not None:\n        local_caches = ()\n        mem_caches = ()\n        for layer in past_key_values:\n            if len(layer) != 6:\n                raise ValueError(\n                    \"Expected elements of past_key_values to contain 6 elements.\"\n                    \"First 3 describing local cache and last 3 describing memory cache.\"\n                    f\"Instead got {len(layer)} elements\"\n                )\n            else:\n                lk, lv, li, memk, memv, memm = layer\n                if lk.shape[-2] != 0:\n                    local_cache_present = True\n                    local_caches += ((lk, lv, li),)\n                else:\n                    local_caches += (None,)\n\n                if memk.shape[-2] != 0:\n                    mem_caches_present = True\n                    #FIXME\n                    mem_caches += (MemCache(keys=memk, values=memv, masks=memm),)\n                else:\n                    mem_caches += (None,)\n\n    local_caches = local_caches if local_cache_present else None\n    mem_caches = mem_caches if mem_caches_present else None\n\n    return local_caches, mem_caches\n\ndef _clear_memory(input_ids, toolkit, target:Literal[\"bos\",\"eos\"]=None):\n    if target == \"bos\":\n        clear_memory = (input_ids == toolkit.bos_token_id).any(dim=-1)\n    else:\n        clear_memory = (input_ids == toolkit.eos_token_id).any(dim=-1)\n    indices = [i for i, x in enumerate(clear_memory) if x]\n    if indices != []:\n        print(f\"Meet special token. Clear memory at {indices}\")\n        toolkit.reset_by_batch(indices)\n    return indices\n\ndef _retrieve(\n    toolkit,\n    input_ids,\n    ret_group_size,\n):\n    if input_ids.shape[-1] > toolkit.get_max_seq_length():\n        input_ids = input_ids[..., -toolkit.get_max_seq_length():]\n        \n    # Retrieve the memory cache\n    queries = toolkit.get_texts(input_ids)\n    # [bsz,k*?,num_heads,head_dim] | k (bsz,?,num_heads,head_dim)\n    mem_caches = toolkit.retrieve(queries=queries,k=ret_group_size)\n    return mem_caches\n\ndef _update(\n    input_ids,\n    mem_update:MemCache,\n    context_window_length,\n    toolkit,\n    clear_memory=None,\n    clear_mode: Literal[\"bos\",\"eos\"] = None\n) -> None :\n    if mem_update == None:\n        return\n\n    mem_length = mem_update.keys.shape[-2]\n    assert mem_length <= input_ids.shape[-1]\n    re_input_ids = input_ids[..., :mem_length]\n    spc_token = None\n    if clear_mode == \"bos\":\n        spc_token = toolkit.bos_token_id\n    elif clear_mode == \"eos\":\n        spc_token = toolkit.eos_token_id    \n    for length in range(0,mem_length,context_window_length):\n        now_mem_update = MemCache()\n        now_mem_update.keys = mem_update.keys[...,length:length+context_window_length,:]\n        now_mem_update.values = mem_update.values[...,length:length+context_window_length,:]\n        now_mem_update.masks = mem_update.masks[...,length:length+context_window_length,:]\n        now_mem_update.texts = toolkit.get_texts(re_input_ids[...,length:length+context_window_length])\n        # FIXME: \n        if clear_memory == True:\n            positions = (re_input_ids == spc_token).nonzero(as_tuple=True)\n            batch_indices,seq_indices = positions\n            for bsz_idx in range(re_input_ids.shape[0]):\n                if bsz_idx in batch_indices:\n                    idxs = (batch_indices == bsz_idx).nonzero(as_tuple=True)[0]\n                    last_spc_position = seq_indices[idxs].max().item()\n                    if last_spc_position == re_input_ids.shape[1] - 1:\n                        textList.append(None)\n                        kvList.append(None)\n                    else:\n                        tokens = re_input_ids[bsz_idx, last_spc_position + 1 :]\n                        examples = toolkit.get_texts(\n                            [\n                                tokens[i : i + toolkit.mem_granularity]\n                                for i in range(0, len(tokens), toolkit.mem_granularity)\n                            ],\n                            skip_special_tokens=True,\n                        )\n                        textList.append(examples)\n                        kvList.append(\n                            {\n                                \"k\": mem_update[\"k\"][bsz_idx, last_spc_position + 1 :],\n                                \"v\": mem_update[\"v\"][bsz_idx, last_spc_position + 1 :],\n                            }\n                        )\n                else:\n                    tokens = re_input_ids[bsz_idx]\n                    examples = toolkit.get_texts(\n                        [\n                            tokens[i : i + toolkit.mem_granularity]\n                            for i in range(0, len(tokens), toolkit.mem_granularity)\n                        ],\n                        skip_special_tokens=True,\n                    )\n                    textList.append(examples)\n                    kvList.append(\n                        {\n                            \"k\": mem_update[\"k\"][bsz_idx],\n                            \"v\": mem_update[\"v\"][bsz_idx],\n                        }\n                    )\n        else:\n            toolkit.update(mem_update=now_mem_update)\n\ndef _prepare_pos_ids(past_key_values, batch_size, input_length, device):\n    if past_key_values is not None:\n        # take previous max pos_id + 1\n        if past_key_values[0][2].shape[0] != batch_size:\n            raise ValueError(\n                f\"first dimension of past_key_values should match batch size: {batch_size}\"\n                f\"but got {past_key_values[0][2].shape[0]}\"\n            )\n        next_pos = torch.max(past_key_values[0][2].view(batch_size, -1), dim=-1)[0] + 1\n        next_pos = next_pos.view(batch_size, 1)\n    else:\n        next_pos = torch.zeros(batch_size, 1, device=device, dtype=torch.long)\n\n    position_ids = torch.arange(0, input_length, dtype=torch.long, device=device).view(1, input_length)\n    position_ids = position_ids + next_pos\n    return position_ids\n\n# 如果position_type是Zero，那么position_ids之前就已经定义了\n# 如果position_type是Continual，那么position_ids需要在这里定义\n\ndef _handle_mem_caches(\n    mem_caches_list:Optional[List[List[Optional[MemCache]]]],\n):\n    if mem_caches_list == None:\n        return None\n    \n    def _concat(mem_caches_list:List[List[Optional[MemCache]]]) -> List[Optional[MemCache]]:\n        sigMemCaches = []\n        max_length = 0    \n        for i , mem_caches in enumerate(mem_caches_list):\n            if mem_caches is None:\n                sigMemCaches.append(None)\n                continue\n            mem_caches = [mem_cache[i] for mem_cache in mem_caches if mem_cache is not None]\n            if mem_caches == []:\n                sigMemCaches.append(None)\n                continue\n            texts = [mem_cache.text[i] for mem_cache in mem_caches]\n            embeddings = [mem_cache.embeddings[i] for mem_cache in mem_caches]\n            keys = torch.cat([mem_cache.keys[i] for mem_cache in mem_caches],dim=1)\n            values = torch.cat([mem_cache.values[i] for mem_cache in mem_caches],dim=1)\n            masks = torch.cat([mem_cache.masks[i] for mem_cache in mem_caches],dim=1)\n            assert keys.shape[0] == values.shape[0] == masks.shape[0]\n            max_length = max(max_length,keys.shape[0])\n            sigMemCaches.append(MemCache(texts=texts,embeddings=embeddings,keys=keys,values=values,masks=masks,length=keys.shape[0]))\n        return sigMemCaches , max_length\n    \n    # mem_caches_list 包含bsz 个列表，每个列表中包含 k 个 mem_caches\n    sigMemCaches , max_length = _concat(mem_caches_list=mem_caches_list)\n    # to one cache\n    mem_caches = MemCache()\n    # fill into max_length\n    for sigMemCache in sigMemCaches:\n        length = sigMemCache.length\n        if length < max_length:\n            pad_length = max_length - length\n            sigMemCache.keys = F.pad(sigMemCache.keys,(0,0,0,pad_length))\n            sigMemCache.values = F.pad(sigMemCache.values,(0,0,0,pad_length))\n            sigMemCache.masks = F.pad(sigMemCache.masks,(0,0,0,pad_length))\n    texts,embeddings, keys,values,masks = [], [] ,[], [] , []\n    for sigMemCache in sigMemCaches:\n        texts.append(sigMemCache.texts)\n        embeddings.append(sigMemCache.embeddings)\n        keys.append(sigMemCache.keys)\n        values.append(sigMemCache.values)\n        masks.append(sigMemCache.masks)\n    mem_caches.texts = texts\n    mem_caches.embeddings = embeddings\n    mem_caches.keys = torch.stack(keys,dim=0)\n    mem_caches.values = torch.stack(values,dim=0)\n    mem_caches.masks = torch.stack(masks,dim=0)\n    \n    return mem_caches\n\ndef _handle_long_input(\n    model,\n    toolkit,\n    ret_group_size,\n    last_context_length,\n    max_mem_size,\n    context_window_length,\n    clear_memories_on_bos_token_id,\n    clear_memories_on_eos_token_id,\n    input_ids,\n    past_input_ids,\n    attention_mask,\n    position_ids,\n    past_key_values,\n    inputs_embeds,\n    use_cache,\n    output_attentions,\n    output_hidden_states,\n    return_dict,\n    cache_position,\n):\n    \"\"\"\n    Handle input that is too long for the model by splitting it in chunks and then concatenating the result.\n    \"\"\"\n    \n    \n    # Split the input into context window and last context\n    if output_attentions:\n        logger.warning(\n            f\"Outputing attentions is not supported in MemLong\"\n            f\"Attention of the last window will be returned\"\n        )\n\n    if past_key_values is not None and use_cache is False:\n        raise ValueError(\"past_key_values it not None should imply use_cache == True\")\n\n    if past_key_values is not None:\n        initial_past_key_values_length = past_key_values[0][0].shape[-2]\n    else:\n        initial_past_key_values_length = 0\n    \n    if input_ids is not None:\n        batch_size , input_length = input_ids.shape\n    else:\n        batch_size , input_length , _ = inputs_embeds.shape \n\n    if position_ids is None:\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        position_ids = _prepare_pos_ids(past_key_values, batch_size, input_length , device)\n        \n    if position_ids.shape != (batch_size, input_length):\n        raise ValueError(f\"Shape of position_ids [{position_ids}] should match [{batch_size, input_length}]\")\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[..., -(initial_past_key_values_length + input_length) :]\n        if attention_mask is not None and (\n            attention_mask.shape != (batch_size, initial_past_key_values_length + input_length)\n        ):\n            raise ValueError(\n                \"Attention mask should be provided for both the local cache and the input\",\n                f\"Expected shape {(batch_size, initial_past_key_values_length + input_length)},\"\n                f\"got {attention_mask.shape}.\",\n            )\n\n    if toolkit == None:\n        outputs = model(\n            input_ids=input_ids if input_ids is not None else None,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds if inputs_embeds is not None else None,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n            mem_caches=None,\n        )\n        outputs_list = [outputs]\n    else:\n        MemForwardNum = max(input_length - last_context_length,0)\n        MemBankSize = min(max(MemForwardNum, 0), max_mem_size)\n        outputs_list = []\n        attn_offset = initial_past_key_values_length\n\n        if MemBankSize > 0:\n            for i in range(MemForwardNum-MemBankSize , MemForwardNum , context_window_length):\n                beg,end = i, min(MemForwardNum, i + context_window_length)    \n                if attention_mask is not None:\n                    if past_key_values is not None:\n                        local_cache_size = past_key_values[0][0].shape[-2]\n                    else:\n                        local_cache_size = 0\n                    attn_length = attention_mask.shape[-1]\n                    attn_beg = beg + attn_offset - local_cache_size\n                    attn_end = end + attn_offset\n\n                mem_caches = _retrieve(toolkit, input_ids[..., beg:end] if input_ids is not None else None,ret_group_size=ret_group_size)\n                with torch.no_grad():\n                    outputs = model(\n                        input_ids=input_ids[..., beg:end] if input_ids is not None else None,\n                        attention_mask=attention_mask[..., attn_beg:attn_end] if attention_mask is not None else None,\n                        position_ids=position_ids[..., beg:end],\n                        past_key_values=past_key_values,\n                        inputs_embeds=inputs_embeds[..., beg:end, :] if inputs_embeds is not None else None,\n                        use_cache=False if past_key_values is None else use_cache,\n                        output_attentions=output_attentions,\n                        output_hidden_states=output_hidden_states,\n                        return_dict=True,\n                        cache_position=cache_position[beg:end] if cache_position is not None else None,\n                        mem_caches=mem_caches,\n                    )\n                # TODO\n                mem_update = outputs.mem_update  \n                outputs.mem_update = None\n                past_key_values = outputs.past_key_values  \n                outputs.past_key_values = None\n                outputs_list.append(outputs)\n                clear_memory , clear_mode = False , None\n                if clear_memories_on_bos_token_id:\n                    _clear_memory(input_ids,token_id=\"bos\",toolkit=toolkit)\n                    clear_memory , clear_mode = True , \"bos\"\n                elif clear_memories_on_eos_token_id:\n                    _clear_memory(input_ids,token_id=\"eos\",toolkit=toolkit)\n                    clear_memory , clear_mode = True , \"eos\"\n                _update(input_ids[...,beg:end],mem_update,context_window_length,toolkit,clear_memory,clear_mode)\n        \n        remaining_input_length = input_length - MemForwardNum\n        beg = MemForwardNum\n        attn_length = remaining_input_length\n        if past_key_values is not None:\n            attn_length += past_key_values[0][0].shape[-2]\n        attention_mask = attention_mask[..., -attn_length:] if attention_mask is not None else None\n        if past_input_ids is None:\n            mem_caches = _retrieve(toolkit, input_ids[..., beg:] if input_ids is not None else None,ret_group_size=ret_group_size)\n        else:\n            mem_caches = _retrieve(toolkit, input_ids=torch.concat((past_input_ids,input_ids),dim=-1) if input_ids is not None else None,ret_group_size=ret_group_size)\n        outputs = model(\n            input_ids=input_ids[..., beg:] if input_ids is not None else None,\n            attention_mask=attention_mask,\n            position_ids=position_ids[..., beg:],\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds[..., beg:, :] if inputs_embeds is not None else None,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n            mem_caches=mem_caches,\n        )\n        outputs_list.append(outputs) \n        clear_memory , clear_mode = False , None\n        if clear_memories_on_bos_token_id:\n            _clear_memory(input_ids,token_id=\"bos\",toolkit=toolkit)\n            clear_memory , clear_mode = True , \"bos\"\n        elif clear_memories_on_eos_token_id:\n            _clear_memory(input_ids,token_id=\"eos\",toolkit=toolkit)\n            clear_memory , clear_mode = True , \"eos\"\n        _update(past_input_ids if past_input_ids is not None else input_ids[...,beg:],outputs.mem_update,context_window_length, toolkit,clear_memory,clear_mode)\n\n    if output_hidden_states:\n        hidden_states = ()\n        for hd in zip(*[x.hidden_states for x in outputs_list]):\n            hidden_states += (torch.cat(hd, dim=-2))\n    else:\n        hidden_states = None\n            \n    past_key_values = outputs_list[-1].past_key_values\n    outputs = BaseModelOutputWithPast(\n        last_hidden_state=torch.concat([x.last_hidden_state for x in outputs_list], dim=-2),\n        past_key_values=past_key_values,\n        hidden_states=hidden_states,\n        attentions=outputs_list[-1].attentions,\n    )\n    if not return_dict:\n        outputs = tuple(v for v in [outputs.last_hidden_state, outputs.past_key_values, outputs.hidden_states, outputs.attentions] if v is not None)\n    return outputs\n    \n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    def __init__(self, config, toolkit_config:ToolkitConfig):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.toolkit = ToolKit(model_config=config,toolkit_config=toolkit_config,device=self.model.device)\n        self.max_mem_size = config.memory_size \n        # self.mem_group_size = config.mem_group_size\n        self.ret_group_size = config.ret_group_size\n        self.context_window_length = min(config.max_position_embeddings, self.toolkit.get_max_seq_length(),config.mem_group_size)\n        self.clear_memories_on_bos_token_id = config.clear_memories_on_bos_token_id\n        self.clear_memories_on_eos_token_id = config.clear_memories_on_eos_token_id\n        self.position_type = config.position_type \n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        past_input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        last_context_length: Optional[int] = None,\n        use_toolkit: Optional[bool] = True,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n\n        if not use_toolkit and self.toolkit:\n            self.toolkit = None\n            \n        if self.toolkit is not None and self.model.device != self.toolkit.device:\n            self.toolkit.to(self.model.device)\n            \n        last_context_length = (last_context_length if last_context_length is not None else self.config.last_context_length)\n        \n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        \n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = _handle_long_input(\n            model=self.model,\n            toolkit=self.toolkit,\n            # mem_group_size=self.mem_group_size,\n            ret_group_size=self.ret_group_size,\n            context_window_length=self.context_window_length,\n            last_context_length=last_context_length,\n            max_mem_size=self.max_mem_size,\n            clear_memories_on_bos_token_id=self.clear_memories_on_bos_token_id,\n            clear_memories_on_eos_token_id=self.clear_memories_on_eos_token_id,\n            input_ids=input_ids,\n            past_input_ids=past_input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def set_toolkit_tokenizer(self, tokenizer):\n        self.toolkit.set_tokenizer(tokenizer)\n    \n    def reset_memory(self):\n        self.toolkit.reset()\n    \n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        cache_position=None,\n        use_cache=True,\n        last_context_length=None,\n        **kwargs,\n    ):        \n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\n                max_cache_length = (\n                    torch.tensor(past_key_values.get_max_length(), device=input_ids.device)\n                    if past_key_values.get_max_length() is not None\n                    else None\n                )\n                cache_length = past_length if max_cache_length is None else torch.min(max_cache_length, past_length)\n            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                past_input_ids = input_ids[:,:-1]\n                input_ids = input_ids[:,-1:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if (\n                max_cache_length is not None\n                and attention_mask is not None\n                and cache_length + input_ids.shape[1] > max_cache_length\n            ):\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values is not None:\n                position_ids = position_ids[:, -1:]\n        \n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        if cache_position is None:\n            cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        elif use_cache:\n            cache_position = cache_position[-input_length:]\n        model_inputs.update(\n            {\n                \"past_input_ids\": past_input_ids,\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": use_cache,\n                \"attention_mask\": attention_mask,\n                \"last_context_length\": last_context_length,\n            }\n        )\n\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n"}
{"type": "source_file", "path": "eval/language_modeling/CEPE/modeling_llama_flash.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union, Dict, Any\nfrom dataclasses import dataclass\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast, MaskedLMOutput\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    logging,\n    replace_return_docstrings,\n    ModelOutput,\n)\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n    from flash_attn.layers.rotary import apply_rotary_emb_func\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(padding_mask):\n    seqlens_in_batch = padding_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(padding_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\n# Copied from transformers.models.bart.modeling_bart._expand_mask\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        # Build here to make `torch.jit.trace` work.\n        self._set_cos_sin_cache(\n            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n        )\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        if seq_len > self.max_seq_len_cached:\n            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\nclass FlashRotaryEmbedding(torch.nn.Module):\n    \"\"\"\n    The rotary position embeddings from RoFormer_ (Su et. al).\n    A crucial insight from the method is that the query and keys are\n    transformed by rotation matrices which depend on the relative positions.\n\n    Other implementations are available in the Rotary Transformer repo_ and in\n    GPT-NeoX_, GPT-NeoX was an inspiration\n\n    .. _RoFormer: https://arxiv.org/abs/2104.09864\n    .. _repo: https://github.com/ZhuiyiTechnology/roformer\n    .. _GPT-NeoX: https://github.com/EleutherAI/gpt-neox\n\n    If scale_base is not None, this implements XPos (Sun et al., https://arxiv.org/abs/2212.10554).\n    A recommended value for scale_base is 512: https://github.com/HazyResearch/flash-attention/issues/96\n    Reference: https://github.com/sunyt32/torchscale/blob/main/torchscale/component/xpos_relative_position.py\n    \"\"\"\n\n    def __init__(self, dim: int, base=10000.0, interleaved=False, scale_base=None,\n                 scaling_factor=1.0, pos_idx_in_fp32=True, device=None):\n        \"\"\"\n            interleaved: if True, rotate pairs of even and odd dimensions (GPT-J style) instead\n                of 1st half and 2nd half (GPT-NeoX style).\n            pos_idx_in_fp32: if True, the position indices [0.0, ..., seqlen - 1] are in fp32,\n                otherwise they might be in lower precision.\n                This option was added because previously (before 2023-07-02), when we construct\n                the position indices, we use the dtype of self.inv_freq. In most cases this would\n                be fp32, but if the model is trained in pure bf16 (not mixed precision), then\n                self.inv_freq would be bf16, and the position indices are also in bf16.\n                Because of the limited precision of bf16 (e.g. 1995.0 is rounded to 2000.0), the\n                embeddings for some positions will coincide.\n                To maintain compatibility with models previously trained in pure bf16,\n                we add this option.\n            scaling_factor: RotaryEmbedding extended with linear scaling.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.base = float(base)\n        self.pos_idx_in_fp32 = pos_idx_in_fp32\n        # Generate and save the inverse frequency buffer (non trainable)\n        inv_freq = self._compute_inv_freq(device)\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.interleaved = interleaved\n        self.scale_base = scale_base\n        self.scaling_factor = scaling_factor\n        scale = ((torch.arange(0, dim, 2, device=device, dtype=torch.float32) + 0.4 * dim)\n                 / (1.4 * dim) if scale_base is not None else None)\n        self.register_buffer(\"scale\", scale)\n\n        self._seq_len_cached = 0\n        self._cos_cached = None\n        self._sin_cached = None\n        self._cos_k_cached = None\n        self._sin_k_cached = None\n\n    def _compute_inv_freq(self, device=None):\n        return 1 / (self.base ** (torch.arange(0, self.dim, 2, device=device,\n                                                 dtype=torch.float32) / self.dim))\n\n\n    def _update_cos_sin_cache(self, seqlen, device=None, dtype=None):\n        # Reset the tables if the sequence length has changed,\n        # if we're on a new device (possibly due to tracing for instance),\n        # or if we're switching from inference mode to training\n        if (seqlen > self._seq_len_cached or self._cos_cached.device != device\n            or self._cos_cached.dtype != dtype\n            or (self.training and self._cos_cached.is_inference())):\n            self._seq_len_cached = seqlen\n            # We want fp32 here, not self.inv_freq.dtype, since the model could be loaded in bf16\n            # And the output of arange can be quite large, so bf16 would lose a lot of precision.\n            # However, for compatibility reason, we add an option to use the dtype of self.inv_freq.\n            if self.pos_idx_in_fp32:\n                t = torch.arange(seqlen, device=device, dtype=torch.float32)\n                t /= self.scaling_factor\n                # We want fp32 here as well since inv_freq will be multiplied with t, and the output\n                # will be large. Having it in bf16 will lose a lot of precision and cause the\n                # cos & sin output to change significantly.\n                # We want to recompute self.inv_freq if it was not loaded in fp32\n                if self.inv_freq.dtype != torch.float32:\n                    inv_freq = self.inv_freq.to(torch.float32)\n                else:\n                    inv_freq = self.inv_freq\n            else:\n                t = torch.arange(seqlen, device=device, dtype=self.inv_freq.dtype)\n                t /= self.scaling_factor\n                inv_freq = self.inv_freq\n            # Don't do einsum, it converts fp32 to fp16 under AMP\n            # freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            freqs = torch.outer(t, inv_freq)\n            if self.scale is None:\n                self._cos_cached = torch.cos(freqs).to(dtype)\n                self._sin_cached = torch.sin(freqs).to(dtype)\n            else:\n                power = ((torch.arange(seqlen, dtype=self.scale.dtype, device=self.scale.device)\n                          - seqlen // 2) / self.scale_base)\n                scale = self.scale.to(device=power.device) ** power.unsqueeze(-1)\n                # We want the multiplication by scale to happen in fp32\n                self._cos_cached = (torch.cos(freqs) * scale).to(dtype)\n                self._sin_cached = (torch.sin(freqs) * scale).to(dtype)\n                self._cos_k_cached = (torch.cos(freqs) / scale).to(dtype)\n                self._sin_k_cached = (torch.sin(freqs) / scale).to(dtype)\n\n    def forward(self, q: torch.Tensor, k: torch.Tensor, seqlen_offset: int = 0) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        q: (batch, seqlen, nheads, headdim)\n        k: (batch, seqlen, nheads, headdim)\n        seqlen_offset: can be used in generation where the qkv being passed in is only the last\n        token in the batch.\n        \"\"\"\n        self._update_cos_sin_cache(q.shape[1] + seqlen_offset, device=q.device, dtype=q.dtype)\n        if self.scale is None:\n            return apply_rotary_emb_func(\n                q, self._cos_cached[seqlen_offset:], self._sin_cached[seqlen_offset:],\n                self.interleaved, True # inplace=True\n            ), apply_rotary_emb_func(\n                k, self._cos_cached[seqlen_offset:], self._sin_cached[seqlen_offset:],\n                self.interleaved, True # inplace=True\n            )\n        else:\n            assert False\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n        t = t / self.scaling_factor\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        self.scaling_factor = scaling_factor\n        super().__init__(dim, max_position_embeddings, base, device)\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat(\n                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n            )\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [\n                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n            ]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n        is_causal: bool = True,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaCrossAttention(nn.Module):\n    \"\"\"Multi-headed cross attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        input_hidden_size = getattr(config, \"encoder_hidden_size\", self.hidden_size)\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(input_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(input_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n\n        # layernorm could go into the decoder layer instead of here, but this is better for FSDP wrapping\n        self.layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        # the encoder hidden states shouldn't need the positional embedding here.\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n        is_causal: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n        bsz_enc, k_len, _ = encoder_hidden_states.size()\n        assert bsz == bsz_enc\n\n        # apply layernorm first\n        hidden_states = self.layernorm(hidden_states)\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(encoder_hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(encoder_hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(encoder_hidden_states)\n            value_states = self.v_proj(encoder_hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, k_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, k_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n\n        # we don't cache the past key values for cross attention as they use the encoder hidden states\n        # we still can but it's more memory consumption for faster speed\n\n        # if past_key_value is not None:\n            # kv_seq_len += past_key_value[0].shape[-2]\n        # cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        # if past_key_value is not None:\n        #     # reuse k, v, self_attention\n        #     key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        #     value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    # def _init_rope(self):\n    #     if self.config.rope_scaling is None:\n    #         scaling_factor = 1\n    #     else:\n    #         # we default to linear scaling for now\n    #         # TODO: add DynamicNTKScaling?\n    #         scaling_factor = self.config.rope_scaling[\"factor\"]\n\n    #     self.rotary_emb = FlashRotaryEmbedding(\n    #         self.head_dim,\n    #         interleaved=False,\n    #         base=self.rope_theta,\n    #         scaling_factor=scaling_factor,\n    #     )\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n        is_causal: bool = True,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # LlamaFlashAttention2 attention does not support output_attentions\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        past_len = 0\n        if past_key_value is not None:\n            past_len = past_key_value[0].shape[-2]\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dime x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        past_len = 0\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n            past_len = past_key_value[0].shape[-2]\n\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        # this is using the flashrotary\n        # query_states, key_states = self.rotary_emb(query_states, key_states, past_len)\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = 0.0 #if not self.training else self.attn_dropout\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in float16 just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            logger.warning_once(\n                \"The input hidden states seems to be silently casted in float32, this might be related to\"\n                \" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                \" float16.\"\n            )\n\n            query_states = query_states.to(torch.float16)\n            key_states = key_states.to(torch.float16)\n            value_states = value_states.to(torch.float16)\n\n        attn_output = self._flash_attention_forward(\n            query_states, key_states, value_states, padding_mask, q_len, dropout=dropout_rate, is_causal=is_causal,\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = attn_output.to(self.o_proj.weight.dtype) # temp fix\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, padding_mask, query_length, dropout=0.0, softmax_scale=None, is_causal=True,\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            padding_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        # Contains at least one padding token in the sequence\n        if padding_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, padding_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            # https://github.com/Dao-AILab/flash-attention/blob/601b4dc48dbe9d87c468daa2b4c0c8388b83753c/flash_attn/flash_attn_interface.py#L843\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=is_causal,\n            )\n\n            # https://github.com/Dao-AILab/flash-attention/blob/601b4dc48dbe9d87c468daa2b4c0c8388b83753c/flash_attn/bert_padding.py#L197\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=is_causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, padding_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(padding_mask)\n        batch_size, kv_seq_len, num_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_heads, head_dim), indices_k)\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            padding_mask = padding_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, padding_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaCrossFlashAttention2(LlamaCrossAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        encoder_hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n        is_causal: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # LlamaFlashAttention2 attention does not support output_attentions\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n        bsz_enc, k_len, _ = encoder_hidden_states.size()\n        assert bsz == bsz_enc\n\n        # apply layernorm first\n        hidden_states = self.layernorm(hidden_states)\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(encoder_hidden_states)\n        value_states = self.v_proj(encoder_hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dime x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, k_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, k_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        # kv_seq_len = key_states.shape[-2]\n        # if past_key_value is not None:\n            # kv_seq_len += past_key_value[0].shape[-2]\n\n        # cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\n        # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n        # if past_key_value is not None:\n        #     # reuse k, v, self_attention\n        #     key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        #     value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = 0.0 #if not self.training else self.attn_dropout\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in float16 just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            logger.warning_once(\n                \"The input hidden states seems to be silently casted in float32, this might be related to\"\n                \" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                \" float16.\"\n            )\n\n            query_states = query_states.to(torch.float16)\n            key_states = key_states.to(torch.float16)\n            value_states = value_states.to(torch.float16)\n\n        attn_output = self._flash_attention_forward(\n            query_states, key_states, value_states, padding_mask, q_len, dropout=dropout_rate, is_causal=is_causal,\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = attn_output.to(self.o_proj.weight.dtype) # temp fix\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, padding_mask, query_length, dropout=0.0, softmax_scale=None, is_causal=True,\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            padding_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`int`, *optional*):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        # Contains at least one padding token in the sequence\n        if padding_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, padding_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            # https://github.com/Dao-AILab/flash-attention/blob/601b4dc48dbe9d87c468daa2b4c0c8388b83753c/flash_attn/flash_attn_interface.py#L843\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=is_causal,\n            )\n\n            # https://github.com/Dao-AILab/flash-attention/blob/601b4dc48dbe9d87c468daa2b4c0c8388b83753c/flash_attn/bert_padding.py#L197\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=is_causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, padding_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(padding_mask)\n        batch_size, kv_seq_len, num_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_heads, head_dim), indices_k)\n        value_layer = index_first_axis(value_layer.reshape(batch_size * kv_seq_len, num_heads, head_dim), indices_k)\n\n        # assume that we are keeping all queries, because in cross attention we only have access to the mask for keys & values, not queries\n\n        # https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py#L98\n        #\n        # TODO: test the speed difference between making a pad mask + unpad_input vs. using arange\n        # max_seqlen_in_batch_q = query_length\n        # cu_seqlens_q = torch.arange(\n        #    (batch_size + 1)*query_length, step=query_length, dtype=torch.int32, device=query_layer.device\n        # )\n        # indices_q = torch.arange((batch_size+1)*query_length, dtype=torch.int32, device=query_layer.device) ## not sure if this is correct\n        # query_layer = query_layer.view(-1, num_heads, head_dim)\n\n        padding_mask = torch.ones((batch_size, query_length), device=query_layer.device)\n        query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, padding_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = (\n            LlamaAttention(config=config)\n            if not getattr(config, \"_flash_attn_2_enabled\", False)\n            else LlamaFlashAttention2(config=config)\n        )\n\n        self.do_cross_attention = getattr(config, \"do_cross_attention\", False)\n        if self.do_cross_attention:\n            self.cross_attn = (\n                LlamaCrossAttention(config=config)\n                if not getattr(config, \"_flash_attn_2_enabled\", False)\n                else LlamaCrossFlashAttention2(config=config)\n            )\n\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        encoder_padding_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        padding_mask: Optional[torch.LongTensor] = None,\n        is_causal: bool = True,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            padding_mask=padding_mask,\n            is_causal=is_causal,\n        )\n        hidden_states = residual + hidden_states\n\n        # Cross Attention\n        if self.do_cross_attention and encoder_hidden_states is not None:\n            residual = hidden_states\n            hidden_states, cross_attn_weights, encoder_key_values = self.cross_attn(\n                hidden_states=hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                position_ids=position_ids,\n                past_key_value=None,\n                output_attentions=output_attentions,\n                use_cache=False,\n                padding_mask=encoder_padding_mask,\n                is_causal=False,\n            )\n            hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = \"past_key_values\"\n    _supports_flash_attn_2 = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlamaModel):\n            module.gradient_checkpointing = value\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._pjrepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n\n        self.do_cross_attention = getattr(config, \"do_cross_attention\", False)\n        self.num_cross_attn_layers = getattr(config, \"num_cross_attn_layers\", 0)\n        self.num_cross_attn_hidden_states = getattr(config, \"num_cross_attn_hidden_states\", 1)\n        self.is_decoder = getattr(config, \"is_decoder\", True)\n\n        layer_list = []\n        for i in range(config.num_hidden_layers):\n            config.do_cross_attention = (i >= config.num_hidden_layers - self.num_cross_attn_layers) and self.do_cross_attention\n            layer_list.append(LlamaDecoderLayer(config))\n        config.do_cross_attention = self.do_cross_attention\n\n        # self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.layers = nn.ModuleList(layer_list)\n\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1 and self.is_decoder:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n            )\n            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n            padding_mask = None\n        else:\n            # attention_mask = attention_mask.expand(-1, -1, seq_length_with_past, -1)\n            if 0 in attention_mask:\n                padding_mask = attention_mask\n            else:\n                padding_mask = None\n\n        encoder_padding_mask = None\n        if encoder_hidden_states is not None:\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(\n                    (batch_size, encoder_hidden_states[0].shape[1]), dtype=torch.bool, device=inputs_embeds.device\n                )\n                encoder_padding_mask = None\n            else:\n                if 0 in encoder_attention_mask:\n                    encoder_padding_mask = encoder_attention_mask\n                else:\n                    encoder_padding_mask = None\n            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=seq_length).to(inputs_embeds.device)\n\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            hidden_states.requires_grad_(True)\n            if encoder_hidden_states is not None:\n                encoder_hidden_states = [x.requires_grad_(True) for x in encoder_hidden_states]\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            encoder_hidden_state = None\n            if encoder_hidden_states is not None and decoder_layer.do_cross_attention:\n                # if we are not using the last one, then we count from the back\n                encoder_hidden_state = encoder_hidden_states[-1] if self.num_cross_attn_hidden_states == 1 else encoder_hidden_states[max(idx-len(self.layers), -len(self.layers))]\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, past_key_value, output_attentions, padding_mask=padding_mask, is_causal=self.is_decoder)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer), hidden_states, attention_mask, encoder_hidden_state, encoder_attention_mask, encoder_padding_mask, position_ids\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_state,\n                    encoder_attention_mask=encoder_attention_mask,\n                    encoder_padding_mask=encoder_padding_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    padding_mask=padding_mask,\n                    is_causal=self.is_decoder,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n\nclass LlamaEncoder(LlamaModel):\n    pass\n\nclass LlamaForMLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        assert not config.is_decoder, \"MLM model should not be decoder\"\n\n        self.model = LlamaModel(config)\n        self.vocab_size = config.vocab_size\n\n        # from RoBERTa https://github.com/huggingface/transformers/blob/e42587f596181396e1c4b63660abf0c736b10dae/src/transformers/models/roberta/modeling_roberta.py#L1117\n        self.lm_dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.lm_act_fn = ACT2FN[config.hidden_act]\n        self.lm_layer_norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n        self.lm_bias = nn.Parameter(torch.zeros(config.vocab_size))\n        self.lm_head.bias = self.lm_bias\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n\n        logits = self.lm_dense(hidden_states)\n        logits = self.lm_act_fn(logits)\n        logits = self.lm_layer_norm(logits)\n        logits = self.lm_head(logits)\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device).view(-1)\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.vocab_size), labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return MaskedLMOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        if not config.is_decoder:\n            logger.warning_once(\"The LlamaForCausalLM model has config set to False, but setting it to True to get the expected behavior. If this is not intended, check the code.\")\n            config.is_decoder = True\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).long().argmax(-1) - 1).to(\n                    logits.device\n                )\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n\n# based on LlamaForCausalLM\nclass LlamaForReplugCausalLM(LlamaForCausalLM):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        assert config.is_decoder, \"RePlug model should be decoder\"\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        self.replug_passage_temperature = getattr(config, 'replug_passage_temperature', 1.0)\n        self.separate_forward = getattr(config, 'separate_forward', False)\n        self.lm_eval_mode = getattr(config, 'lm_eval_mode', False)\n\n    def calculate_weighted_logits(self, logits, context_scores, bsz, n_ctx):\n        vocab_size = logits.size(-1)\n        logits = logits.view(bsz, n_ctx, -1, vocab_size)\n        # we do a weighted average of the probabilities\n        softmax_fn = nn.Softmax(dim=-1)\n        context_scores = context_scores.to(logits.device)\n        context_scores /= self.replug_passage_temperature\n        score_prob = softmax_fn(context_scores).view(bsz, n_ctx, 1, 1)\n        seq_prob = softmax_fn(logits) * score_prob\n        weighted_prob = seq_prob.sum(dim=1, keepdim=True) # shape is bsz, 1, seq_length, vocab_size\n        log_prob = torch.log(weighted_prob)\n\n        # log_prob is the weighted logits\n        final_logits = log_prob.expand(-1, n_ctx, -1, -1).contiguous().view(bsz*n_ctx, -1, vocab_size)\n        return final_logits\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        context_scores: Optional[torch.LongTensor] = None,\n        num_context: Optional[int] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # for generate, we expect the the input ids to only have two dimensions\n        if len(input_ids.shape) == 2:\n            assert num_context is not None\n            n_ctx = num_context\n            bsz, _ = input_ids.size()\n            assert bsz % n_ctx == 0\n            bsz = bsz // n_ctx\n        elif len(input_ids.shape) == 3:\n            bsz, n_ctx, _ = input_ids.size()\n            input_ids = input_ids.view(bsz*n_ctx, -1)\n\n        assert context_scores.shape == (bsz, n_ctx), \"RePlug expects context_scores of shape (bsz, n_ctx)\"\n\n        if attention_mask is not None:\n            attention_mask = attention_mask.view(bsz*n_ctx, -1)\n\n        if self.separate_forward or self.lm_eval_mode:\n            # due to memory restraint, we run forward on the gpu one seq at a time\n            all_last_hidden_states = []\n            all_past_kvs = []\n            all_hidden_states = []\n            all_attentions = []\n            all_logits = []\n            for i in range(len(input_ids)):\n                pkvs = tuple((k[i:i+1].to(input_ids.device), v[i:i+1].to(input_ids.device)) for k, v in past_key_values) if past_key_values is not None else None\n                \n                outputs = self.model(\n                    input_ids=input_ids[i:i+1] if input_ids is not None else None,\n                    attention_mask=attention_mask[i:i+1] if attention_mask is not None else None,\n                    position_ids=position_ids[i:i+1] if position_ids is not None else None,\n                    past_key_values=pkvs,\n                    inputs_embeds=inputs_embeds[i:i+1] if inputs_embeds is not None else None,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                    output_hidden_states=output_hidden_states,\n                    return_dict=return_dict,\n                )\n                all_last_hidden_states.append(outputs[0])\n                # for perplexity eval, we don't need to save the past kvs, and moving them to cpu is expensive\n                if not self.lm_eval_mode:\n                    if outputs[1] is not None and len(outputs[1]) > 0:\n                        all_past_kvs.append(tuple((kv[0].cpu(), kv[1].cpu()) for kv in outputs[1]))\n                    if output_hidden_states:\n                        all_hidden_states.append(outputs[2].cpu())\n                    if output_attentions:\n                        all_attentions.append(outputs[3].cpu())\n\n                hidden_states = outputs[0]\n                if self.config.pretraining_tp > 1:\n                    lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n                    logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n                    logits = torch.cat(logits, dim=-1)\n                else:\n                    logits = self.lm_head(hidden_states)\n\n                # moving logits to cpu yields additional overhead but it is necessary given the memory constraints \n                # calculating the weighted logits can be memory expensive due to the softmax\n                logits = logits.float().cpu()\n                all_logits.append(logits)\n\n            weighted_logits = []\n            for i in range(bsz):\n                logits = torch.cat(all_logits[i*n_ctx:(i+1)*n_ctx], dim=0)\n                # bsz = 1 here\n                logits = self.calculate_weighted_logits(logits, context_scores[i:i+1], 1, n_ctx)\n                weighted_logits.append(logits)\n            logits = torch.cat(weighted_logits, dim=0)\n\n            if not self.lm_eval_mode:\n                logits = logits.to(input_ids.device)\n                last_hidden_state = torch.cat(all_last_hidden_states, dim=0)\n                past_key_values = tuple(\n                    (\n                        torch.cat([x[layer][0] for x in all_past_kvs], dim=0),\n                        torch.cat([x[layer][1] for x in all_past_kvs], dim=0)\n                    ) for layer in range(len(all_past_kvs[0]))\n                ) if len(all_past_kvs) >= 1 else None\n\n                hidden_states = torch.cat(all_hidden_states, dim=0).to(input_ids.device) if output_hidden_states else None\n                attentions = torch.cat(all_attentions, dim=0).to(input_ids.device) if output_attentions else None\n\n            else:\n                last_hidden_state = None\n                past_key_values = None\n                hidden_states = None\n                attentions = None\n\n            outputs = BaseModelOutputWithPast(\n                last_hidden_state=last_hidden_state, \n                past_key_values=past_key_values, \n                hidden_states=hidden_states, \n                attentions=attentions\n            )\n\n        else:\n            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n            hidden_states = outputs[0]\n            if self.config.pretraining_tp > 1:\n                lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n                logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n                logits = torch.cat(logits, dim=-1)\n            else:\n                logits = self.lm_head(hidden_states)\n            logits = logits.float()\n\n            logits = self.calculate_weighted_logits(logits, context_scores, bsz, n_ctx)\n\n        loss = None\n        if labels is not None:\n            log_prob = logits.view(bsz, n_ctx, -1, logits.size(-1))\n            log_prob = log_prob[:, 0, :, :]\n            # Shift so that tokens < n predict n\n            shift_prob = log_prob[..., :-1, :].contiguous()\n            if len(labels.shape) == 3:\n                # the label for each passage should be the same\n                # only using one set of label per example makes our code faster\n                labels = labels[:, 0]\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            nll_loss_fn = nn.NLLLoss()\n            shift_prob = shift_prob.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_prob.device)\n            loss = nll_loss_fn(shift_prob, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, context_scores=None, num_context=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"context_scores\": context_scores,\n                \"num_context\": num_context,\n            }\n        )\n        return model_inputs\n\n@dataclass\nclass CausalLMOutputWithPastContext(CausalLMOutputWithPast):\n    encoder_hidden_states: Optional[List[torch.FloatTensor]] = None\n\n# based on LlamaForCausalLM\nclass LlamaForCausalContextLM(LlamaForCausalLM):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config, encoder=None):\n        super().__init__(config)\n\n        self.encoder_is_model = getattr(config, \"encoder_is_model\", False)\n        if not config.is_decoder:\n            logger.warning_once(\"The LlamaForContextCausalLM model has config set to False, but setting it to True to get the expected behavior. If this is not intended, check the code.\")\n            config.is_decoder = True\n        if self.encoder_is_model:\n            assert encoder is None\n            self.encoder = None\n        elif encoder is not None:\n            self.encoder = encoder\n        else:\n            if type(config.encoder_config) == dict:\n                encoder_config = LlamaConfig.from_dict(config.encoder_config)\n            else:\n                encoder_config = config.encoder_config\n            encoder_config._flash_attn_2_enabled = True\n            self.encoder = LlamaEncoder(encoder_config)\n\n        self.train_encoder = getattr(config, \"train_encoder\", False)\n        self.lm_loss_cof = getattr(config, \"lm_loss_cof\", 1.0)\n        self.kl_loss_cof = getattr(config, \"kl_loss_cof\", 1.0)\n        self.kl_loss_mode = getattr(config, \"kl_loss_mode\", \"smooth\")\n        self.encode_mode = getattr(config, \"encode_mode\", \"context_only\")\n        self.train_batch_mode = getattr(config, \"train_batch_mode\", \"none\")\n        # if we offload the unused hidden states to cpu\n        self.offload_hidden_states = getattr(config, \"offload_hidden_states\", False)\n        self.num_cross_attn_hidden_states = getattr(config, \"num_cross_attn_hidden_states\", 1)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def set_encoder(self, encoder):\n        if self.encoder_is_model:\n            print(f\"warning: currently the encoder is set to self.model, ignoring the set\")\n        else:\n            self.encoder = encoder\n\n    def get_encoder(self):\n        return self.encoder\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        encoder_input_ids: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[bool] = None,\n        distill_prob: Optional[torch.Tensor] = None,\n        distill_index: Optional[torch.Tensor] = None,\n        distill_tokens: Optional[torch.Tensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_hidden_states is None and encoder_input_ids is not None:\n            bsz, n_ctx, ctx_length = encoder_input_ids.size()\n\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones_like(encoder_input_ids, device=encoder_input_ids.device)\n\n            # TODO: we should add a separator token between the context and the query??\n            if \"with_query\" in self.encode_mode:\n                # when encoding with query, we need to add the query to the input_ids\n                query_length = int(self.encode_mode.split(\"-\")[-1])\n                assert query_length <= input_ids.size(-1), f\"query length {query_length} is longer than input_ids {input_ids.size(-1)}\"\n                encoder_input_ids = torch.concatenate([encoder_input_ids, input_ids[..., :query_length].unsqueeze(1).expand(-1, n_ctx, -1)], dim=2)\n                original_attention_mask = encoder_attention_mask\n                if attention_mask is not None:\n                    encoder_attention_mask = torch.concatenate([encoder_attention_mask, attention_mask[..., :query_length].unsqueeze(1).expand(-1, n_ctx, -1)], dim=2)\n                else:\n                    encoder_attention_mask = torch.concatenate([encoder_attention_mask, torch.ones([bsz, n_ctx, query_length], device = encoder_attention_mask.device)], dim=2)\n                ctx_length += query_length\n\n            encoder_input_ids = encoder_input_ids.view(-1, ctx_length)\n            encoder_attention_mask = encoder_attention_mask.view(-1, ctx_length)\n\n            train_encoder = self.training and self.encoder is not None and self.train_encoder\n            with torch.set_grad_enabled(train_encoder and self.training):\n                encoder = self.model if self.encoder_is_model else self.encoder\n                outputs = encoder(\n                    input_ids=encoder_input_ids,\n                    attention_mask=encoder_attention_mask,\n                    use_cache=use_cache,\n                    output_attentions=False,\n                    output_hidden_states=self.num_cross_attn_hidden_states > 1,\n                    return_dict=True,\n                )\n                if self.num_cross_attn_hidden_states > 1:\n                    encoder_hidden_states = outputs.hidden_states\n                else:\n                    encoder_hidden_states = [outputs.last_hidden_state]\n\n            if not train_encoder:\n                encoder_hidden_states = [x.detach() for x in encoder_hidden_states]\n\n            if \"with_query_no_hid\" in self.encode_mode:\n                ctx_length -= query_length\n                # we remove the query from the encoder_hidden_states after encoding with it\n                encoder_hidden_states = [x[:, :ctx_length] for x in encoder_hidden_states]\n                encoder_attention_mask = original_attention_mask\n                # encoder_attention_mask = encoder_attention_mask[:, :ctx_length]\n\n            # we can optimize for memory by not saving the masked tokens\n            # but then we need to reconstruct the hidden_states as well as the masks\n            encoder_hidden_states = [x.reshape(bsz, n_ctx*ctx_length, -1) for x in encoder_hidden_states]\n            if self.offload_hidden_states:\n                encoder_hidden_states = [x.cpu() if i < len(encoder_hidden_states)-1 else x for i, x in enumerate(encoder_hidden_states)]\n            encoder_attention_mask = encoder_attention_mask.view(bsz, n_ctx*ctx_length)\n\n        elif encoder_hidden_states is not None:\n            if encoder_attention_mask is not None:\n                encoder_attention_mask = encoder_attention_mask.view(\n                   encoder_hidden_states[0].size(0) , encoder_hidden_states[0].size(1)\n                )\n\n        if self.train_batch_mode == \"in_batch_negative\" and self.training:\n            bsz = input_ids.size(0)\n            encoder_hidden_states = [x.view(1, -1, x.size(-1)).expand(bsz, -1, -1) for x in encoder_hidden_states]\n            encoder_attention_mask = encoder_attention_mask.view(1, -1).expand(bsz, -1)\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n            if self.training:\n                # only multiply by coefficient during trainign\n                loss *= self.lm_loss_cof\n\n            if not loss.requires_grad:\n                loss.requires_grad = True\n\n        kl_loss = None\n        if distill_prob is not None and distill_index is not None:\n            kl_fct = nn.KLDivLoss(reduction=\"batchmean\")\n            # prob is shape (bsz, seq_length, vocab_size)\n            prob = F.softmax(logits[..., -distill_prob.size(1):, :].contiguous(), dim=-1)\n            # shape (bsz, seq_length, topk)\n            top_prob = torch.gather(prob, 2, distill_index)\n            # this is mathematically equivalent to summing the others\n            other_prob = 1 - top_prob.sum(-1)\n            distill_prob = distill_prob.view(-1, distill_prob.size(2))\n            together_prob = torch.cat([top_prob, other_prob.unsqueeze(2)], dim=2).view(-1, distill_prob.size(1))\n\n            if \"smooth\" in self.kl_loss_mode:\n                delta = float(self.kl_loss_mode.split(\"_\")[-1])\n                # smoothing to avoid log(0)\n                distill_prob = (distill_prob + delta) / (1 + delta * distill_prob.size(1))\n                together_prob = (together_prob + delta) / (1 + delta * together_prob.size(1))\n            elif self.kl_loss_mode == \"drop\":\n                # drop the last prob (sum of the rest of the prob) and hopefully that avoids the log(0)?\n                distill_prob = distill_prob[..., :-1]\n                together_prob = together_prob[..., :-1]\n\n            log_prob = torch.log(together_prob)\n\n            kl_loss = self.kl_loss_cof * kl_fct(log_prob, distill_prob)\n            loss = loss + kl_loss\n            kl_loss = kl_loss.item()\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPastContext(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            encoder_hidden_states=encoder_hidden_states,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        encoder_input_ids=None,\n        encoder_attention_mask=None,\n        encoder_hidden_states=None,\n        **kwargs\n    ):\n\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"encoder_input_ids\": encoder_input_ids,\n                \"encoder_attention_mask\": encoder_attention_mask,\n                \"encoder_hidden_states\": encoder_hidden_states,\n            }\n        )\n        return model_inputs\n\n    # https://github.com/huggingface/transformers/blob/66c240f3c950612fa05b2e14c85d4b86c88e473e/src/transformers/generation/utils.py#L751\n    def _update_model_kwargs_for_generation(\n        self,\n        outputs: ModelOutput,\n        model_kwargs: Dict[str, Any],\n        is_encoder_decoder: bool = False,\n        standardize_cache_format: bool = False,\n    ) -> Dict[str, Any]:\n        # update past_key_values\n        model_kwargs[\"past_key_values\"] = self._extract_past_from_model_output(\n            outputs, standardize_cache_format=standardize_cache_format\n        )\n        if getattr(outputs, \"state\", None) is not None:\n            model_kwargs[\"state\"] = outputs.state\n\n        # update token_type_ids with last value\n        if \"token_type_ids\" in model_kwargs:\n            token_type_ids = model_kwargs[\"token_type_ids\"]\n            model_kwargs[\"token_type_ids\"] = torch.cat([token_type_ids, token_type_ids[:, -1].unsqueeze(-1)], dim=-1)\n\n        if not is_encoder_decoder:\n            # update attention mask\n            if \"attention_mask\" in model_kwargs:\n                attention_mask = model_kwargs[\"attention_mask\"]\n                model_kwargs[\"attention_mask\"] = torch.cat(\n                    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n                )\n        else:\n            # update decoder attention mask\n            if \"decoder_attention_mask\" in model_kwargs:\n                decoder_attention_mask = model_kwargs[\"decoder_attention_mask\"]\n                model_kwargs[\"decoder_attention_mask\"] = torch.cat(\n                    [decoder_attention_mask, decoder_attention_mask.new_ones((decoder_attention_mask.shape[0], 1))],\n                    dim=-1,\n                )\n\n        model_kwargs[\"encoder_hidden_states\"] = outputs.encoder_hidden_states\n\n        return model_kwargs"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/configuration_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" LLaMA model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\nimport torch\n\nlogger = logging.get_logger(__name__)\n\n\nclass LlamaConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the LLaMA-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`LlamaModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer decoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer decoder.\n        num_key_value_heads (`int`, *optional*):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n            `num_attention_heads`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n            Llama 2 up to 4096, CodeLlama up to 16384.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        pad_token_id (`int`, *optional*):\n            Padding token id.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            Beginning of stream token id.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            End of stream token id.\n        pretraining_tp (`int`, *optional*, defaults to 1):\n            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n            issue](https://github.com/pytorch/pytorch/issues/76232).\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        rope_scaling (`Dict`, *optional*):\n            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n            these scaling strategies behave:\n            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n            experimental feature, subject to breaking API changes in future versions.\n        attention_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        mlp_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n\n    ```python\n    >>> from transformers import LlamaModel, LlamaConfig\n\n    >>> # Initializing a LLaMA llama-7b style configuration\n    >>> configuration = LlamaConfig()\n\n    >>> # Initializing a model from the llama-7b style configuration\n    >>> model = LlamaModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"llama\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=11008,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=None,\n        hidden_act=\"silu\",\n        max_position_embeddings=2048,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=0,\n        bos_token_id=1,\n        eos_token_id=2,\n        pretraining_tp=1,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling=None,\n        attention_bias=False,\n        attention_dropout=0.0,\n        mlp_bias=False,\n        mem_layer=None,\n        ret_attn_layers = [],\n        memory_size = 32768,\n        last_context_length=1024,\n        ret_group_size=16,\n        pooling_tokens=None,\n        clear_memories_on_eos_token_id = False,\n        clear_memories_on_bos_token_id = False,\n        update_boundary = None,\n        use_gate = False,\n        position_type = \"Continual\",\n        mem_positionals=True,\n        mem_dtype= \"bfloat16\",\n        mem_group_size=32,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.pretraining_tp = pretraining_tp\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self._rope_scaling_validation()\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.mlp_bias = mlp_bias\n        \n        self.position_type = position_type\n        # RET\n        self.mem_layer = mem_layer\n        self.ret_attn_layers = ret_attn_layers\n        self.memory_size = memory_size\n        self.last_context_length = last_context_length\n        self.pooling_tokens = pooling_tokens\n        self.clear_memories_on_eos_token_id = clear_memories_on_eos_token_id\n        self.clear_memories_on_bos_token_id = clear_memories_on_bos_token_id\n        self.use_gate = use_gate\n        self.update_boundary = update_boundary\n        self.mem_group_size = mem_group_size\n        self.mem_dtype = mem_dtype\n        self.mem_positionals = mem_positionals\n        self.ret_group_size = ret_group_size\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def _rope_scaling_validation(self):\n        \"\"\"\n        Validate the `rope_scaling` configuration.\n        \"\"\"\n        if self.rope_scaling is None:\n            return\n\n        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n            raise ValueError(\n                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n            )\n        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n            raise ValueError(\n                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n            )\n        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"}
{"type": "source_file", "path": "eval/language_modeling/longllama/modeling_longllama.py", "content": "# coding=utf-8\n# Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LongLLaMA model.\"\"\"\nfrom dataclasses import dataclass\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\nfrom .configuration_longllama import LongLlamaConfig\nfrom .longllama_utils import mem_apply_update, LongLlamaMemCache, LongLlamaMemConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LongLlamaConfig\"\n\n\n@dataclass\nclass LongLlamaModelOutputWithPast(BaseModelOutputWithPast):\n    \"\"\"\n    Based on BaseModelOutputWithPast\n\n    Args:\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n        mem_caches  (`tuple(LongLlamaMemCache))`, *optional*, returned for layers with memory cache enabled):\n            For the layers without memory None is returned\n    \"\"\"\n\n    mem_caches: Optional[LongLlamaMemCache] = None\n\n\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min, device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\n# Copied from transformers.models.bart.modeling_bart._expand_mask\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->LongLlama\nclass LongLlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LongLlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\n# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->LongLlama\nclass LongLlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n\n        # Build here to make `torch.jit.trace` work.\n        self._set_cos_sin_cache(\n            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n        )\n\n    def _set_cos_sin_cache(self, seq_len, device, dtype):\n        self.max_seq_len_cached = seq_len\n        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        if seq_len > self.max_seq_len_cached:\n            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\n# Based on transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef rotate_one(x, cos, sin, position_ids):\n    if len(position_ids.shape) != 2 or x.shape[0] != position_ids.shape[0] or x.shape[-2] != position_ids.shape[1]:\n        raise ValueError(f\"Position ids shoud have shape [bsz, seq_len] got {position_ids.shape}\")\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    x_embed = (x * cos) + (rotate_half(x) * sin)\n    return x_embed\n\n\ndef rotate_as_if_first(x, rotary_emb):\n    # x: [bs, num_attention_heads, seq_len, head_size]\n    # apply rotary as if all elements were first in the sequence\n    cos, sin = rotary_emb(x, x.shape[-2])\n    return rotate_one(x, cos, sin, torch.zeros(x.shape[0], x.shape[-2], dtype=torch.long, device=cos.device))\n\n\n# Based on an 4.30 transformers.models.llama.modeling_llama.LlamaMLP with Llama->LongLlama\nclass LongLlamaMLP(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\n# Modified transformers.models.llama.modeling_llama.LlamaAttention\nclass LongLlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper with FoT modifications\"\"\"\n\n    def __init__(self, config: LongLlamaConfig, mem_config: Optional[LongLlamaMemConfig] = None):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.max_cache = 32\n        self.rope_theta = config.rope_theta\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self._init_rope()\n        self.mem_config = mem_config\n\n    def _init_rope(self):\n        assert self.config.rope_scaling is None\n        self.rotary_emb = LongLlamaRotaryEmbedding(\n            self.head_dim,\n            max_position_embeddings=self.max_position_embeddings,\n            base=self.rope_theta,\n        )\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        mem_cache: Optional[LongLlamaMemCache] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if attention_mask is None:\n            tgt_seq_len = hidden_states.shape[-2]\n            if past_key_value is not None:\n                src_seq_len = past_key_value[0].shape[-2] + tgt_seq_len\n            else:\n                src_seq_len = tgt_seq_len\n\n            attention_mask = torch.zeros(\n                hidden_states.shape[0],\n                1,\n                tgt_seq_len,\n                src_seq_len,\n                device=hidden_states.device,\n                dtype=hidden_states.dtype,\n            )\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        position_ids = position_ids[:, None, :, None]\n\n        if position_ids.shape != (key_states.shape[0], 1, key_states.shape[-2], 1):\n            raise ValueError(\"position_ids should match batch and seq_len of the input\")\n\n        mem_no_local_cache = self.mem_config is not None and past_key_value is None and (not use_cache)\n        mem_and_local_cache = self.mem_config is not None and use_cache\n        # positonal embeddings can be disabled for memory layers\n        use_positionals = self.mem_config is None or self.mem_config.positionals\n\n        if mem_no_local_cache or mem_and_local_cache:\n            pass\n        \n        if mem_no_local_cache:\n            # the whole context window will be moved to memory cache after the attention\n            if use_positionals:\n                # positionally embedd memory content as first token in the sequence\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n            # attention_mask [bsz, 1, tgt_seq_len, src_seq_len]\n            # we base the mask on the last token in the context window\n            mem_update = LongLlamaMemCache(\n                keys=rfst_key_states.to(self.mem_config.cache_dtype),\n                values=value_states.to(self.mem_config.cache_dtype),\n                masks=attention_mask[..., -1, :, None],\n            )\n\n        if past_key_value is not None:\n            past_local_cache_size = past_key_value[0].shape[-2]\n            key_states = torch.cat([past_key_value[0], key_states], dim=-2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=-2)\n            # FoT additionally stores position_ids to support long inputs\n            position_ids = torch.cat([past_key_value[2], position_ids], dim=-2)\n\n            if attention_mask.shape[-1] != key_states.shape[-2] and attention_mask.shape[-2] != query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            # local cache is maintained so that it is <= self.max_cache\n            # remaining elements are either dropped or go to memory cache\n            if key_states.shape[-2] > self.max_cache:\n                num_elems_to_drop = past_local_cache_size\n\n                if mem_and_local_cache:\n                    drop_keys = key_states[:, :, :num_elems_to_drop, :]\n                    drop_values = value_states[:, :, :num_elems_to_drop, :]\n                    # as memory mask use the masking of the last key in context\n                    # attention_mask [bsz, 1, tgt_seq_len, src_seq_len]\n                    drop_masks = attention_mask[..., -1, :, None]\n                    drop_masks = drop_masks[:, :, :num_elems_to_drop, :]\n\n                    if use_positionals:\n                        rfst_drop_keys = rotate_as_if_first(drop_keys, self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    mem_update = LongLlamaMemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype),\n                        values=drop_values.to(self.mem_config.cache_dtype),\n                        masks=drop_masks,\n                    )\n                    if mem_cache is None:\n                        mem_cache = mem_update\n                    else:\n                        mem_cache = mem_apply_update(\n                            prev_mem_cache=mem_cache, new_mem_content=mem_update, mem_config=self.mem_config\n                        )\n\n                key_states = key_states[:, :, num_elems_to_drop:, :]\n                value_states = value_states[:, :, num_elems_to_drop:, :]\n                position_ids = position_ids[:, :, num_elems_to_drop:, :]\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n\n        # FoT additionally stores position_ids to support long inputs\n        past_key_value = (key_states, value_states, position_ids) if use_cache else None\n\n        kv_seq_len = key_states.shape[-2]\n\n        if use_positionals:\n            cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n            rel_pos_ids = position_ids - torch.min(position_ids, dim=-2, keepdim=True)[0]\n            rel_pos_ids = rel_pos_ids.squeeze(3).squeeze(1)\n            query_states = rotate_one(query_states, cos, sin, rel_pos_ids[:, -query_states.shape[-2] :])\n            key_states = rotate_one(key_states, cos, sin, rel_pos_ids)\n\n        if self.mem_config is not None and self.mem_config.attention_grouping is not None:\n            attn_grouping_h, attn_grouping_q = self.mem_config.attention_grouping\n            if attn_grouping_h <= 0 or attn_grouping_q <= 0:\n                raise ValueError(\"Attention grouping should be positive\")\n        else:\n            attn_grouping_h, attn_grouping_q = self.num_heads, q_len\n\n        attn_output_h = []\n        for beg_h in range(0, self.num_heads, attn_grouping_h):\n            end_h = min(beg_h + attn_grouping_h, self.num_heads)\n\n            attn_output_q = []\n            for beg_q in range(0, q_len, attn_grouping_q):\n                end_q = min(beg_q + attn_grouping_q, q_len)\n\n                if self.config.torch_attention:\n                    if mem_cache is not None:\n                        attn_keys = torch.concat(\n                            [key_states[:, beg_h:end_h], mem_cache.keys[:, beg_h:end_h].to(key_states.dtype)], dim=-2\n                        )\n                        attn_values = torch.concat(\n                            [value_states[:, beg_h:end_h], mem_cache.values[:, beg_h:end_h].to(value_states.dtype)],\n                            dim=-2,\n                        )\n                        mem_mask = mem_cache.masks.squeeze(-1).unsqueeze(-2)\n                        assert len(mem_mask.shape) == 4\n                        assert mem_mask.shape[2] == 1\n                        assert mem_mask.shape[3] == mem_cache.keys.shape[-2]\n                        mem_mask = torch.broadcast_to(\n                            mem_mask, (mem_mask.shape[0], mem_mask.shape[1], end_q - beg_q, mem_mask.shape[3])\n                        )\n                        attn_mask = torch.concat([attention_mask[:, :, beg_q:end_q], mem_mask], dim=-1)\n                        assert attn_mask.shape[-1] == attn_keys.shape[-2]\n                    else:\n                        attn_keys = key_states[:, beg_h:end_h]\n                        attn_values = value_states[:, beg_h:end_h]\n                        attn_mask = attention_mask[:, :, beg_q:end_q]\n\n                    attn_queries = query_states[:, beg_h:end_h, beg_q:end_q]\n\n                    attn_output = torch.nn.functional.scaled_dot_product_attention(\n                        query=attn_queries, key=attn_keys, value=attn_values, attn_mask=attn_mask\n                    )\n                    attn_output_q.append(attn_output)\n                else:\n                    attn_weights = torch.matmul(\n                        query_states[:, beg_h:end_h, beg_q:end_q], key_states[:, beg_h:end_h].transpose(2, 3)\n                    ) / math.sqrt(self.head_dim)\n\n                    if attn_weights.size() != (bsz, end_h - beg_h, end_q - beg_q, kv_seq_len):\n                        raise ValueError(\n                            f\"Attention weights should be of size {(bsz, end_h - beg_h, end_q - beg_q, kv_seq_len)}, but is\"\n                            f\" {attn_weights.size()}\"\n                        )\n\n                    if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                        raise ValueError(\n                            f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                        )\n                    attn_weights = attn_weights + attention_mask[:, :, beg_q:end_q]\n                    min_value = (\n                        torch.finfo(attn_weights.dtype).min\n                        if -1000000.0 < torch.finfo(attn_weights.dtype).min\n                        else -1000000.0\n                    )\n                    attn_weights = torch.max(\n                        attn_weights, torch.tensor(min_value, device=attn_weights.device, dtype=attn_weights.dtype)\n                    )\n\n                    if mem_cache is not None:\n                        mem_mask = mem_cache.masks.squeeze(-1).unsqueeze(-2)\n                        mem_attn_weights = torch.matmul(\n                            query_states[:, beg_h:end_h, beg_q:end_q],\n                            mem_cache.keys[:, beg_h:end_h].transpose(2, 3).to(key_states.dtype),\n                        ) / math.sqrt(self.head_dim)\n\n                        assert mem_mask.shape[2] == 1\n                        mem_attn_weights = mem_attn_weights + mem_mask\n                        min_value = (\n                            torch.finfo(mem_attn_weights.dtype).min\n                            if -1000000.0 < torch.finfo(mem_attn_weights.dtype).min\n                            else -1000000.0\n                        )\n                        mem_attn_weights = torch.max(\n                            mem_attn_weights,\n                            torch.tensor(min_value, device=mem_attn_weights.device, dtype=mem_attn_weights.dtype),\n                        )\n\n                        attn_weights = torch.concat([attn_weights, mem_attn_weights], dim=-1)\n                        combined_value_states = torch.concat(\n                            [value_states[:, beg_h:end_h], mem_cache.values[:, beg_h:end_h].to(value_states.dtype)],\n                            dim=-2,\n                        )\n                    else:\n                        combined_value_states = value_states[:, beg_h:end_h]\n                    # upcast attention to fp32\n                    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n                        query_states.dtype\n                    )\n                    attn_output = torch.matmul(attn_weights, combined_value_states)\n                    assert attn_output.shape[-2] == end_q - beg_q\n                    attn_output_q.append(attn_output)\n            attn_output_h.append(torch.concat(attn_output_q, dim=-2))\n\n        attn_output = torch.concat(attn_output_h, dim=-3)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        if mem_no_local_cache:\n            if mem_cache is not None:\n                mem_cache = mem_apply_update(\n                    prev_mem_cache=mem_cache, new_mem_content=mem_update, mem_config=self.mem_config\n                )\n            else:\n                mem_cache = mem_update\n\n        return attn_output, attn_weights, past_key_value, mem_cache\n\n\n# Modified transformers.models.llama.modeling_llama.LlamaDecoderLayer\nclass LongLlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LongLlamaConfig, mem_config: Optional[LongLlamaMemConfig] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LongLlamaAttention(config=config, mem_config=mem_config)\n        self.mlp = LongLlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LongLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LongLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        mem_cache: Optional[LongLlamaMemCache] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n                along with information about positions\n            mem_cache (`LongLlamaMemCache`, *optional*): memory cache for specific layers\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value, mem_cache = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            mem_cache=mem_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs + (mem_cache,)\n\n\nLONGLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LongLlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\nLONGLLAMA_MEML_DOCSTRING = r\"\"\"\n        mem_layers ([`int`], *optional*):\n            Indices of layers to be augmented with memory, if None then parameters from config will be used\n        mem_dtype (`str`, *optional*):\n            Keys and values will be casted to this type for storage.\n\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LongLLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LONGLLAMA_START_DOCSTRING,\n)\n# Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel with Llama->LongLlama\nclass LongLlamaPreTrainedModel(PreTrainedModel):\n    config_class = LongLlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LongLlamaDecoderLayer\"]\n    _skip_keys_device_placement = \"past_key_values\"\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LongLlamaModel):\n            module.gradient_checkpointing = value\n\n\nLONGLLAMA_COMMON_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`\n            or memory cache is enabled):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 1 additional tensor of shape\n            `(batch_size, 1, sequence_length, 1)`. For memory enriched layers it also contains content of memory cache.\n            It is padded with empty tensors so when returned it alwyas has 6 elements.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks) \n            that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail. This is NOT supported in LongLlamaForCausalLM and LongLlamaForSequenceClassification\n            due to the specific input processing.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\nLONGLLAMA_MODEL_INPUTS_DOCSTRING = r\"\"\"\n        mem_caches (`tuple(LongLlamaMemCache)`, *optional*) \n            Memory caches for specified layers, None for others\n\"\"\"\n\nLONGLLAMA_ADD_INPUTS_DOCSTRING = r\"\"\"\n        last_context_length (`int`, *optional*) \n            Useful for generation, specifies number of tokens that won't be loaded to memory and \n            will be left for generation cache\n\"\"\"\n\n\ndef _prepare_pos_ids(past_key_values, batch_size, input_length, device):\n    if past_key_values is not None:\n        # take previous max pos_id + 1\n        if past_key_values[0][2].shape[0] != batch_size:\n            raise ValueError(\n                f\"first dimension of past_key_values should match batch size: {batch_size}\"\n                f\"but got {past_key_values[0][2].shape[0]}\"\n            )\n        next_pos = torch.max(past_key_values[0][2].view(batch_size, -1), dim=-1)[0] + 1\n        next_pos = next_pos.view(batch_size, 1)\n    else:\n        next_pos = torch.zeros(batch_size, 1, device=device, dtype=torch.long)\n\n    position_ids = torch.arange(0, input_length, dtype=torch.long, device=device).view(1, input_length)\n    position_ids = position_ids + next_pos\n    return position_ids\n\n\n@add_start_docstrings(\n    \"The bare LongLLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LONGLLAMA_START_DOCSTRING,\n    LONGLLAMA_MEML_DOCSTRING,\n)\n# Modified transformers.models.llama.modeling_llama.LlamaModel\nclass LongLlamaModel(LongLlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LongLlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LongLlamaConfig):\n        super().__init__(config)\n        self.mem_layers = config.mem_layers\n        self.mem_config = LongLlamaMemConfig(\n            positionals=config.mem_positionals,\n            cache_dtype=getattr(torch, config.mem_dtype),\n            attention_grouping=config.mem_attention_grouping,\n        )\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n\n        for mem_layer_id in self.mem_layers:\n            if mem_layer_id < 0 or mem_layer_id >= config.num_hidden_layers:\n                raise ValueError(\n                    f\"Memory layer ids should be between 0 and {config.num_hidden_layers}, got {mem_layer_id}\"\n                )\n\n        layers = []\n        for layer_id in range(config.num_hidden_layers):\n            if layer_id in self.mem_layers:\n                layer = LongLlamaDecoderLayer(config, mem_config=self.mem_config)\n            else:\n                layer = LongLlamaDecoderLayer(config, mem_config=None)\n            layers.append(layer)\n\n        self.layers = nn.ModuleList(layers)\n        self.norm = LongLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    @add_start_docstrings_to_model_forward(LONGLLAMA_COMMON_INPUTS_DOCSTRING, LONGLLAMA_MODEL_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        mem_caches: Optional[Tuple[Optional[LongLlamaMemCache]]] = None,\n    ) -> Union[Tuple, LongLlamaModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[-2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n    \n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = _prepare_pos_ids(past_key_values, batch_size, seq_length, device)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = ()\n        next_mem_caches = ()\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n            mem_cache = mem_caches[idx] if mem_caches else None\n\n            if mem_cache is not None and idx not in self.mem_layers:\n                raise ValueError(\"Memory cache provided for a non-memory leayer\")\n\n            if (\n                self.gradient_checkpointing\n                and self.training\n                and mem_cache is None\n                and idx % self.config.gradient_checkpoint_every_ith == 0\n            ):\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, None, mem_cache=None)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    mem_cache=mem_cache,\n                )\n\n            new_mem_cache = layer_outputs[-1]\n            layer_outputs = layer_outputs[:-1]\n            next_mem_caches += (new_mem_cache,)\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n            else:\n                next_decoder_cache += (None,)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n\n        mem_cache_returned = False\n        for mem_cache in next_mem_caches:\n            if mem_cache is not None:\n                mem_cache_returned = True\n        next_mem_caches = next_mem_caches if mem_cache_returned else None\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, next_mem_caches]\n                if v is not None\n            )\n        return LongLlamaModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            mem_caches=next_mem_caches,\n        )\n\n\ndef _handle_output_of_past_key_values(outputs):\n    # merges local caches and memory caches into one single tuple of past_key_values\n    # in order to support generation\n    batch_size = outputs.last_hidden_state.shape[0]\n    if outputs.past_key_values is None and outputs.mem_caches is None:\n        return None\n\n    if outputs.past_key_values is None:\n        out_past_key_values = (None,) * len(outputs.mem_caches)\n    else:\n        out_past_key_values = outputs.past_key_values\n\n    if outputs.mem_caches is None:\n        out_mem_caches = (None,) * len(outputs.past_key_values)\n    else:\n        out_mem_caches = outputs.mem_caches\n\n    device = outputs.last_hidden_state.device\n    past_key_values = ()\n    for local_cache, mem_cache in zip(out_past_key_values, out_mem_caches):\n        layer = ()\n        if local_cache is not None:\n            assert len(local_cache) == 3\n            layer += local_cache\n        else:\n            layer += (torch.empty(batch_size, 0, 0, 0, device=device),) * 3\n\n        if mem_cache is not None:\n            layer += (mem_cache.keys, mem_cache.values, mem_cache.masks)\n        else:\n            layer += (torch.empty(batch_size, 0, 0, 0, device=device),) * 3\n\n        assert len(layer) == 6\n\n        past_key_values += (layer,)\n\n    return past_key_values\n\n\ndef _split_past_key_values(past_key_values):\n    # splits past_key_values to local cache and memory cache\n    local_cache_preset = False\n    mem_caches_present = False\n    if past_key_values is not None:\n        local_caches = ()\n        mem_caches = ()\n        for layer in past_key_values:\n            if len(layer) != 6:\n                raise ValueError(\n                    \"Expected elements of past_key_values to contain 6 elements.\"\n                    \"First 3 describing local cache and last 3 describing memory cache.\"\n                    f\"Instead got {len(layer)} elements\"\n                )\n            else:\n                lk, lv, li, memk, memv, memm = layer\n                if lk.shape[-2] != 0:\n                    local_cache_preset = True\n                    local_caches += ((lk, lv, li),)\n                else:\n                    local_caches += (None,)\n\n                if memk.shape[-2] != 0:\n                    mem_caches_present = True\n                    mem_caches += (LongLlamaMemCache(keys=memk, values=memv, masks=memm),)\n                else:\n                    mem_caches += (None,)\n\n    local_caches = local_caches if local_cache_preset else None\n    mem_caches = mem_caches if mem_caches_present else None\n\n    return local_caches, mem_caches\n\n\ndef _handle_long_input(\n    model,\n    input_ids,\n    attention_mask,\n    position_ids,\n    past_key_values,\n    inputs_embeds,\n    use_cache,\n    output_attentions,\n    output_hidden_states,\n    return_dict,\n    context_window_length,\n    last_context_length,\n):\n    if output_attentions:\n        logger.warning(\n            f\"Outputing attentions is not supported in LongLlamaForCausalLM and LongLlamaForSequenceClassification. \"\n            f\"Attention of the last window will be returned\"\n        )\n\n    past_key_values, mem_caches = _split_past_key_values(past_key_values)\n\n    if past_key_values is not None and use_cache is False:\n        raise ValueError(\"past_key_values it not None should imply use_cache == True\")\n\n    if past_key_values is not None:\n        initial_past_key_values_length = past_key_values[0][0].shape[-2]\n    else:\n        initial_past_key_values_length = 0\n\n    if input_ids is not None:\n        batch_size, input_length = input_ids.shape\n    else:\n        batch_size, input_length, _ = inputs_embeds.shape\n\n    if position_ids is None:\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        position_ids = _prepare_pos_ids(past_key_values, batch_size, input_length, device)\n\n    if position_ids.shape != (batch_size, input_length):\n        raise ValueError(f\"Shape of position_ids [{position_ids}] should match [{batch_size, input_length}]\")\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[..., -(initial_past_key_values_length + input_length) :]\n        if attention_mask is not None and (\n            attention_mask.shape != (batch_size, initial_past_key_values_length + input_length)\n        ):\n            raise ValueError(\n                \"Attention mask should be provided for both the local cache and the input\",\n                f\"Expected shape {(batch_size, initial_past_key_values_length + input_length)},\"\n                f\"got {attention_mask.shape}.\",\n            )\n\n    # First we load prefix to memory cache\n    mem_input_length = max(input_length - last_context_length, 0)\n    outputs_list = []\n    attn_offset = initial_past_key_values_length\n    if mem_input_length > 0:\n        for i in range(0, mem_input_length, context_window_length):\n            beg, end = i, min(mem_input_length, i + context_window_length)\n\n            if attention_mask is not None:\n                if past_key_values is not None:\n                    local_cache_size = past_key_values[0][0].shape[-2]\n                else:\n                    local_cache_size = 0\n                attn_length = attention_mask.shape[-1]\n                attn_beg = beg + attn_offset - local_cache_size\n                attn_end = end + attn_offset\n                assert attn_end <= attn_length\n                assert attn_beg >= 0 and attn_end > attn_beg\n\n            # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn, mem_caches)\n            outputs = model(\n                input_ids=input_ids[..., beg:end] if input_ids is not None else None,\n                attention_mask=attention_mask[..., attn_beg:attn_end] if attention_mask is not None else None,\n                position_ids=position_ids[..., beg:end],\n                past_key_values=past_key_values,\n                inputs_embeds=inputs_embeds[..., beg:end, :] if inputs_embeds is not None else None,\n                use_cache=False if past_key_values is None else use_cache,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=True,\n                mem_caches=mem_caches,\n            )\n            if i > 0:\n                if mem_caches is not None and past_key_values is None:\n                    for mc_layer in mem_caches:\n                        if mc_layer is not None:\n                            del mc_layer.keys\n                            del mc_layer.values\n                            del mc_layer.masks\n\n            mem_caches = outputs.mem_caches\n            outputs.mem_caches = None\n            past_key_values = outputs.past_key_values\n            outputs.past_key_values = None\n            outputs_list.append(outputs)\n\n    remaining_input_length = input_length - mem_input_length\n    beg = mem_input_length\n    attn_length = remaining_input_length\n    if past_key_values is not None:\n        attn_length += past_key_values[0][0].shape[-2]\n    attention_mask = attention_mask[..., -attn_length:] if attention_mask is not None else None\n\n    outputs = model(\n        input_ids=input_ids[..., beg:] if input_ids is not None else None,\n        attention_mask=attention_mask,\n        position_ids=position_ids[..., beg:],\n        past_key_values=past_key_values,\n        inputs_embeds=inputs_embeds[..., beg:, :] if inputs_embeds is not None else None,\n        use_cache=use_cache,\n        output_attentions=output_attentions,\n        output_hidden_states=output_hidden_states,\n        return_dict=True,\n        mem_caches=mem_caches,\n    )\n\n    outputs_list.append(outputs)\n\n    past_key_values = _handle_output_of_past_key_values(outputs_list[-1])\n\n    if output_hidden_states:\n        hidden_states = ()\n        for hd in zip(*[x.hidden_states for x in outputs_list]):\n            hidden_states += (torch.cat(hd, dim=-2),)\n    else:\n        hidden_states = None\n\n    outputs = BaseModelOutputWithPast(\n        last_hidden_state=torch.concat([x.last_hidden_state for x in outputs_list], dim=-2),\n        past_key_values=past_key_values,\n        hidden_states=hidden_states,\n        attentions=outputs_list[-1].attentions,\n    )\n\n    if not return_dict:\n        outputs = tuple(\n            v\n            for v in [outputs.last_hidden_state, outputs.past_key_values, outputs.hidden_states, outputs.attentions]\n            if v is not None\n        )\n    return outputs\n\n\n# Modified transformers.models.llama.modeling_llama.LlamaForCausalLM\nclass LongLlamaForCausalLM(LongLlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.context_window_length = config.max_position_embeddings\n\n        self.model = LongLlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    def _has_generation_cache(self, past_key_values):\n        if past_key_values is not None:\n            assert len(past_key_values[0]) == 6\n            return past_key_values[0][0].shape[-2] != 0\n\n        return False\n\n    @add_start_docstrings_to_model_forward(LONGLLAMA_COMMON_INPUTS_DOCSTRING, LONGLLAMA_ADD_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        last_context_length: Optional[int] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        last_context_length = (last_context_length if last_context_length is not None else self.config.last_context_length)\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = _handle_long_input(\n            model=self.model,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            context_window_length=self.context_window_length,\n            last_context_length=last_context_length,\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        last_context_length=None,\n        **kwargs,\n    ):\n        if self._has_generation_cache(past_key_values):\n            input_ids = input_ids[:, -1:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill(position_ids < 0, 0)\n            if self._has_generation_cache(past_key_values):\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n                \"last_context_length\": last_context_length,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LongLLaMA Model transformer with a sequence classification head on top (linear layer).\n\n    [`LongLlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LONGLLAMA_START_DOCSTRING,\n    LONGLLAMA_MEML_DOCSTRING,\n)\n# Modified from transformers.models.llama.modeling_llama.LlamaForSequenceClassification\nclass LongLlamaForSequenceClassification(LongLlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.context_window_length = config.max_position_embeddings\n        self.model = LongLlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LONGLLAMA_COMMON_INPUTS_DOCSTRING, LONGLLAMA_ADD_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        last_context_length: Optional[int] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        last_context_length = (\n            last_context_length if last_context_length is not None else self.config.last_context_length\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        transformer_outputs = _handle_long_input(\n            model=self.model,\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            context_window_length=self.context_window_length,\n            last_context_length=last_context_length,\n        )\n\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/toolkit.py", "content": "from .ret_embedder import llm_embedder, st_embedder,bge_embedder\nfrom .align_memory import ChunkMemory\nfrom typing import List\nimport torch\n\n\nclass ToolKit:\n    def __init__(self, model_config, toolkit_config,device=None):\n        self.device = device\n        embedder_name = toolkit_config.embedder_name\n        if embedder_name == \"llm_embedder\":\n            self.embedder = llm_embedder(toolkit_config=toolkit_config,device=device)\n        elif embedder_name == \"st_embedder\":\n            self.embedder = st_embedder(toolkit_config=toolkit_config,device=device)\n        elif embedder_name ==\"bge_embedder\":\n            self.embedder = bge_embedder(toolkit_config=toolkit_config,device=device)\n        else:\n            raise NotImplementedError\n        self.max_ret_length = self.embedder.get_max_seq_length()\n        self.tokenizer = None\n        self.chunk_memory = ChunkMemory(model_config=model_config, toolkit_config=toolkit_config,device=device)\n\n    def set_tokenizer(self, tokenizer):\n        self.tokenizer = tokenizer\n        \n    def to(self, device):\n        self.device = device\n        self.embedder.to(device)\n        self.chunk_memory.to(device)\n    \n    def update(self, mem_update):\n        bsz_texts = mem_update.texts\n        embeddings = []\n        for i, texts in enumerate(bsz_texts):\n            if texts is None:\n                embeddings.append(None)\n            else:\n                embeddings.append(self.embedder.get_embeddings(examples=texts, mode=\"key\"))\n        mem_update.embeddings = embeddings\n        self.chunk_memory.save(mem_update)\n    \n    def get_texts(self , input_ids):\n        if self.tokenizer == None:\n            raise ValueError(\"Before using MemLong, you should set tokenizer by calling set_toolkit_tokenizer(tokenizer)\")\n        \n        texts_list = [[] for _ in range(input_ids.shape[0])]\n        for i in range(input_ids.shape[0]):\n            texts_list[i].append(self.tokenizer.decode(input_ids[i], skip_special_tokens=True))\n        return texts_list\n        \n    # 1. 如果不满足，则返回mem中所有的cache\n    # 2. 如果满足，进行检索\n\n    def reset(self):\n        self.chunk_memory.reset()\n    \n    def retrieve(self, queries: List[str], k: int = 5):\n        k = min(k, self.chunk_memory.get_min_number())\n        if k == 0:\n            return None\n        queries_list = [self.embedder.get_embeddings(examples=query, mode=\"query\").to(torch.float32) for query in queries]\n        result = self.chunk_memory.retrieve_index(query_embeddings_list=queries_list, k=k)\n        scores_list = list(map(lambda x: x[0], result))\n        indices_list = list(map(lambda x: x[1], result))\n        mem_caches = self.chunk_memory.get(indices_list=indices_list)\n        return mem_caches\n    \n    def get_max_seq_length(self):\n        return self.embedder.get_max_seq_length()\n\n    def reset_by_batch(self, batch_indices_to_clear):\n        self.chunk_memory.reset_by_batch(batch_indices_to_clear)\n\n    def reset(self):\n        self.chunk_memory.reset()\n"}
{"type": "source_file", "path": "eval/language_modeling/eval.py", "content": "# Written by LIU WEIJIE\n# Some code based on https://github.com/epfml/landmark-attention\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#  \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nimport math\nimport torch\nimport argparse\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nimport transformers\nimport json\nfrom functools import partial\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n# import InfiniteTransformer\n# from LongLora import replace_llama_attn\nfrom CEPE import LlamaForCausalContextLM\nfrom datasets import Dataset\nfrom time import time\nfrom datetime import timedelta\nfrom utils import ContextDataCollator,BatchSequentialSampler\nfrom transformers.testing_utils import CaptureLogger\nfrom peft import PeftModel,AutoPeftModelForCausalLM,PeftConfig\nfrom MemLong import modeling_llama_position\nfrom MemLong import configuration_llama\nfrom MemLong.utils import ToolkitConfig,smart_tokenizer_and_embedding_resize\nfrom transformers import DataCollatorForLanguageModeling\nfrom longllama.configuration_longllama import LongLlamaConfig\nfrom longllama.modeling_longllama import LongLlamaForCausalLM\n\ndef parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--batch_size', type=int, default=32, help='batch size during inference')\n    # parser.add_argument('--base_model', type=str, default=\"/data1/pretrained-models/llama-7b-hf\")\n    parser.add_argument('--cache_dir', type=str, default=\"./cache\")\n    parser.add_argument('--seq_len', type=int, default=2048, help='context length during evaluation')\n    parser.add_argument('--context_size', type=int, default=-1, help='context size during fine-tuning')\n    parser.add_argument('--context_length', type=int, default=0, help='number of transformer layers')\n    parser.add_argument(\"--num_context\",type=int,default=0,help=\"FOR CEPE ARUGMENT,SET 0 FOR CAUSAL MODEL\")     # For CEPD\n    parser.add_argument('--peft_model', type=str, default=None, help='')\n    parser.add_argument('--flash_attn', type=bool, default=False, help='')\n    # parser.add_argument('--data_path', type=str, default=\"./test.bin\", help='')\n    parser.add_argument(\"--data_format\", type=str, required=True, choices=[\"llama-bin\", \"json\"])\n    parser.add_argument(\"--dataset_path_or_name\",type=str,nargs='+',required=True)\n    parser.add_argument(\"--model_provider\",type=str,choices=['LLaMA','Mistral','Gemma','CEPE',\"InfiniteTransformer\",\"LongLora\",\"OpenLLaMA\",\"MemLong\",\"LongLLaMA\",\"OpenLLaMA-peft\",\"Phi-3-mini-128k-instruct\",\"yarn-llama-2-7b-128k\"])\n    parser.add_argument(\"--model_path_or_name\",type=str)\n    parser.add_argument(\"--torch_dtype\",type=str,default=\"bfloat16\")\n    parser.add_argument(\"--output_dir\",type=str,default=\"./results\")\n    parser.add_argument(\"--num_proc\",type=int,default=1)\n    parser.add_argument(\"--data_batch_size\",type=int,default=1000)\n    parser.add_argument(\"--use_cache\",default=False,action=\"store_true\")\n    parser.add_argument(\"--sliding_window\",type=int,default=256)\n    parser.add_argument(\"--sequential\",action=\"store_true\",help=\"DATA SEQUENCIAL EVALUATION\")   \n    parser.add_argument(\"--filter_length\",type=int,default=32767,help=\"FOR CEPE ARUGMENT\")     # For CEPD\n    parser.add_argument(\"--segment_length\",type=int,default=512,help=\"FOR INFINITRANSFORMER ARUGMENT\")     # For InfiniteTransformer\n    parser.add_argument(\"--last_context_length\",type=int,default=-1,help=\"FOR MEMLONG MODEL\")     # For MemLong\n    parser.add_argument(\"--embedder_name\",choices=['llm_embedder','st_embedder','bge_embedder'],default='llm_embedder',help=\"FOR LLaMA MODEL\")     # For LLaMA\n    parser.add_argument(\"--override\",action=\"store_true\",help=\"override result\")     # For LLaMA\n    parser.add_argument(\"--validation_load_strategy\",choices=['put_in_decoder'],default=None) # None for CEPE, Normal is put_in_decoder\n    parser.add_argument(\"--memory_size\",default=None,type=int)\n    parser.add_argument(\"--no_toolkit\",action=\"store_true\",default=False)\n    parser.add_argument(\"--debug\",action=\"store_true\")\n    parser.add_argument(\"--model_name\",type=str,default=None)\n    args = parser.parse_args()\n    return args\n\ntok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n\ndef tokenize_fn(tokenizer,text_column_name,examples,):\n    with CaptureLogger(tok_logger) as cl:\n        outputs = tokenizer(examples[text_column_name])\n    # clm input could be much much longer than block_size\n    if \"Token indices sequence length is longer than the\" in cl.out:\n        tok_logger.warning(\n            \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n            \" before being passed to the model.\"\n        )\n    return outputs\n\n\"\"\"\ndef Sig_tokenize_fn(tokenizer, col_name, examples):\n    outputs = tokenizer(\n        (\" \"+ tokenizer.eos_token).join(examples[col_name]),\n        truncation=False,\n        padding=False,\n        add_special_tokens=False,\n    )\n    return {\"input_ids\": outputs[\"input_ids\"]}\n\"\"\"\ndef preprocess(args,examples):\n    # first calculate out the block size from n_ctx, ctx_size, and chunk_size and filter by length\n    # second take chunks out with a stride of the eval sliding window\n    # finally put things into the input_ids and encoder_input_ids if needed\n    results = []\n    for idx in range(len(examples[\"input_ids\"])):\n        input_ids = examples[\"input_ids\"][idx]\n        if len(input_ids) < args.filter_length:\n            continue\n        stride = args.sliding_window\n        total_length = args.num_context * args.context_length + args.seq_len\n\n        for i in range(0, len(input_ids) - total_length, stride):\n            # we don't need the mask because we tokenized sequences without padding (the collator/forward func will handle the mask)\n            ids = np.array(input_ids[i:i+total_length], dtype=np.int32)\n            if \"put_in_decoder\" in args.validation_load_strategy:\n                results.append({\"input_ids\": ids,})\n            else:\n                encoder_input_ids = ids[:args.num_context * args.context_length].reshape(args.num_context, args.context_length)\n                ids = ids[args.num_context * args.context_length:]\n                results.append({\n                    \"input_ids\": ids, \n                    \"encoder_input_ids\": encoder_input_ids, \n                })\n                \n            labels = np.copy(ids).astype(np.int32)\n            if stride < total_length:\n                labels[:-stride] = -100\n            results[-1][\"labels\"] = labels\n\n            if args.filter_length > 32768:\n                # only keep one sequence per document if the length is too long\n                # otherwise we might store a lot of tokens with sliding window --> oom\n                break\n\n    results = {k: np.stack([d[k] for d in results]) for k in results[0]}\n    return results\n\ndef get_as_batch(data, seq_length, batch_size, device='cpu', sliding_window=256):\n    all_ix = list(range(0, len(data) - seq_length, sliding_window))\n    all_ix.pop()\n\n    for idx in range(0, len(all_ix), batch_size):\n        ix = all_ix[idx:idx+batch_size]\n        assert all([idx + seq_length + 1 <= len(data) for idx in ix])\n        x = torch.stack([torch.from_numpy((data[i:i+seq_length]).astype(np.int64)) for i in ix])\n        y = torch.stack([torch.from_numpy((data[i+1:i+1+seq_length]).astype(np.int64)) for i in ix])\n        if device != 'cpu':\n            x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n        yield x, y\n\ndef iceildiv(x, y):\n    return (x + y - 1) // y\n\n\ndef seqential_evaluate(model, eval_dataloader, args, device, tokenizer=None):\n    stats = {}\n    model.eval()\n    loss_fct = torch.nn.CrossEntropyLoss()\n    with torch.no_grad():\n        all_losses = []\n        shard_start = 0\n        pbar = tqdm(eval_dataloader)\n        for idx, batch in enumerate(pbar):\n            if idx < shard_start:\n                continue\n        \n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            if args.model_provider == \"MemLong\":\n                if args.no_toolkit:\n                    batch['use_toolkit'] = False\n                else:\n                    batch[\"labels\"][:,:-args.last_context_length] = -100\n            \n            if args.model_provider in [\"LongLLaMA\",\"MemLong\"] and args.last_context_length:\n                batch['last_context_length'] = args.last_context_length\n                outputs = model(**batch)\n            else:\n                outputs = model(batch['input_ids'])\n            logits = outputs.logits\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = batch[\"labels\"][..., 1:].contiguous()\n            shift_logits = shift_logits.view(-1, model.config.vocab_size)\n            shift_labels = shift_labels.view(-1).to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n            pbar.set_description(f\"Loss: {loss.item()}\")\n            all_losses.append(loss.item())\n        # mem_usage = sum([torch.cuda.max_memory_allocated(i) for i in range(torch.cuda.device_count())])\n        eval_loss = torch.tensor(all_losses).mean().item()\n        print(f\"Eval loss: {eval_loss}\")\n    try:\n        perplexity = math.exp(eval_loss)\n    except OverflowError:\n        perplexity = float(\"inf\")\n\n    print(f\"val_perplexity: {perplexity}\")\n    stats['val_loss'] = eval_loss\n    stats['val_perplexity'] = perplexity\n    # stats['mem_usage'] = mem_usage\n    return stats\n    \ndef evaluate(model, data, batch_size, device, seq_length, sliding_window=256, use_cache=False,model_provider=None,segment_length=None,tokenizer=None):\n    stats = {}\n    model.eval()\n    loss_list_val, acc_list = [], []\n    loss_step_list_val = []\n    with torch.no_grad():\n        print(f\"Using seq length {seq_length}\")\n        torch.set_printoptions(sci_mode=False)\n        for idx, (x, y) in tqdm(\n            enumerate(get_as_batch(data['val'], seq_length, batch_size, device=device,sliding_window=sliding_window)),\n            total=iceildiv(iceildiv(len(data['val']), sliding_window),batch_size)\n        ):\n            if model_provider in ['InfiniteTransformer']:\n                input_ids = torch.tensor_split(x,list(range(segment_length,x.shape[1],segment_length)),dim=1)\n                labels = torch.tensor_split(\n                    x,\n                    list(range(segment_length,x.shape[1],segment_length)),\n                    dim=1\n                )\n                memory, norm_term = None,None\n                for i in range(len(input_ids)):\n                    with torch.no_grad():\n                        outputs = model(\n                            input_ids=input_ids[i],\n                            labels=labels[i],\n                            memory=memory,\n                            norm_term=norm_term,\n                        )\n                    memory = outputs.memory\n                    norm_term = outputs.norm_term\n                loss = outputs.loss\n                loss_list_val.append(loss.item())\n            else:\n                val_loss = 0.\n                acc = 0.\n                cnt = 0\n                # breakpoint()\n                # print(tokenizer.batch_decode(x,skip_special_tokens=True)[0])\n                for part_idx, i in enumerate(range(0, x.shape[1], seq_length)):\n                    part_len = x[:, i:i + seq_length].shape[1]\n                    outputs = model(input_ids=x[:, i:i + seq_length],labels=x[:, i:i+seq_length].contiguous(),use_cache=use_cache)\n                    val_loss = outputs.loss * part_len + val_loss\n                    acc = ((outputs.logits.argmax(-1) == y[:, i:i+seq_length]).float().sum()) + acc\n                    cnt += part_len\n                    while len(loss_step_list_val) <= part_idx:\n                        loss_step_list_val.append([])\n                    loss_step_list_val[part_idx].append(outputs.loss.item())\n                val_loss /= cnt\n                acc /= cnt\n                loss_list_val.append(val_loss.item())\n                acc_list.append(acc.item())\n\n    #stats['val_acc'] = torch.as_tensor(acc_list).mean().item()\n    stats['val_loss'] = torch.as_tensor(loss_list_val).mean().item()\n    stats['val_perplexity'] = 2.71828 ** stats['val_loss']\n    \n    #stats['val_perplexity_per_chunk'] = torch.exp(torch.as_tensor(loss_step_list_val).mean(dim=1))\n\n    return stats\n\ndef main(args):\n    seed = 2\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n    #FIXME\n    if args.data_format == \"llama-bin\":\n        data = {'val': np.memmap(args.data_path, dtype=np.uint16, mode='r')}\n        print(f\"Num validation tokens: {len(data['val'])}\")\n        print(\"data path\", args.data_path)\n        print(\"base model\", args.base_model)\n        print(\"peft model\", args.peft_model)\n        if args.flash_attn and args.model_provider == \"longlora\":\n            replace_llama_attn(use_flash_attn=True, use_full=True)\n        # Set RoPE scaling factor\n        config = transformers.AutoConfig.from_pretrained(args.base_model,cache_dir=args.cache_dir,)\n        context_size = args.context_size if args.context_size > 0 else args.seq_len\n        orig_ctx_len = getattr(config, \"max_position_embeddings\", None) # this value should be 4096 for LLaMA2 models\n        if orig_ctx_len and context_size > orig_ctx_len:\n            scaling_factor = float(math.ceil(context_size / orig_ctx_len))\n            config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n\n        # Load model and tokenizer\n        model = transformers.AutoModelForCausalLM.from_pretrained(\n            args.base_model,\n            config=config,\n            cache_dir=args.cache_dir,\n            torch_dtype=torch.float16,\n            device_map=\"auto\",\n        )\n        model.resize_token_embeddings(32001)\n\n        if args.peft_model:\n            trainable_params = os.path.join(args.peft_model, \"trainable_params.bin\")\n            if os.path.isfile(trainable_params):\n                model.load_state_dict(torch.load(trainable_params, map_location=model.device), strict=False)\n            else:\n                raise ValueError(\"Trainable input embedding and normalization are required.\")\n            model = PeftModel.from_pretrained(\n                model,\n                args.peft_model,\n                device_map=\"auto\",\n                torch_dtype=torch.float16,\n            )\n        stats = evaluate(model, data, args.batch_size, device, args.seq_len, sliding_window=256)\n        print(stats)\n\n    elif args.data_format == \"json\":\n        for dataset_path_or_name in args.dataset_path_or_name:\n            dataset_name = dataset_path_or_name.split('/')[-1].split('.')[0]\n            json_data = []\n            with open(dataset_path_or_name) as file:\n                for line in file:\n                    json_data.append(json.loads(line))\n            # load model and tokenizer\n            if \"/\" in args.model_path_or_name:\n                model_version = args.model_path_or_name.split(\"/\")[-1]\n            else:\n                model_version = args.model_path_or_name\n                \n            if args.model_name is not None:\n                location = f\"{args.output_dir}/{args.model_name}/{args.seq_len}\" if not args.debug else f\"{args.output_dir}/{args.model_name}/{args.seq_len}_debug\"\n            else:\n                location = f\"{args.output_dir}/{args.model_provider}/{args.seq_len}\" if not args.debug else f\"{args.output_dir}/{args.model_provider}/{args.seq_len}_debug\"\n\n            \n            if not os.path.exists(args.output_dir):\n                print(\"Creating output directory\", args.output_dir)\n                os.makedirs(args.output_dir)\n                \n            if not os.path.exists(location):\n                print(\"Creating directory\", location)\n                os.makedirs(location)\n\n            if not args.override:\n                if os.path.isfile(f\"{location}/{dataset_name}_eval.json\"):\n                    print(\"File exists, skip evaluation\")\n                    continue\n\n            # context scaling\n            if args.model_provider in ['InfiniteTransformer']:\n                config = InfiniteTransformer.GemmaConfig.from_pretrained(\n                    args.model_path_or_name,\n                    cache_dir=args.cache_dir,\n                )\n                config.segment_size = args.segment_length\n            \n            elif args.model_provider in ['MemLong']:\n                if args.peft_model:\n                    config = configuration_llama.LlamaConfig.from_pretrained(args.peft_model + \"/config.json\")\n                    \n                else:\n                    config = configuration_llama.LlamaConfig.from_pretrained(args.model_path_or_name + \"/config.json\")\n                config.use_cache = False\n                if args.memory_size is not None:\n                    config.memory_size = int(args.memory_size)\n                if args.last_context_length > 0:\n                    config.last_context_length = args.last_context_length\n                else:\n                    args.last_context_length = config.last_context_length\n\n            elif args.model_provider in ['LongLLaMA']:\n                config = LongLlamaConfig.from_pretrained(args.model_path_or_name)\n                config.use_cache = args.use_cache\n                config.last_context_length = args.last_context_length\n            \n            elif args.model_provider == \"yarn-llama-2-7b-128k\":\n                from scaled_rope.configuration_llama import LlamaConfig\n                from scaled_rope.modeling_llama_yarn import LlamaForCausalLM\n                config = LlamaConfig.from_pretrained(args.model_path_or_name)\n                config.rope_scaling = {\n                    \"type\": \"yarn\",\n                    \"factor\": 16,\n                    \"original_max_position_embeddings\": 4096,\n                }\n            \n            elif \"peft\" in args.model_provider :\n                config = PeftConfig.from_pretrained(args.peft_model)\n            else:\n                config = transformers.AutoConfig.from_pretrained(args.model_path_or_name,trust_remote_code=True)\n                \n            context_size = args.context_size if args.context_size > 0 else args.seq_len\n            orig_ctx_len = getattr(config, \"max_position_embeddings\", None) # this value should be 4096 for LLaMA2 models\n            if orig_ctx_len and context_size > orig_ctx_len and self.model_provider != \"yarn-llama-2-7b-128k\":\n                scaling_factor = float(math.ceil(context_size / orig_ctx_len))\n                config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n            \n            if args.model_provider in [\"LLaMA\",\"OpenLLaMA\",\"OpenLLaMA-peft\",\"Phi-3-mini-128k-instruct\"]:        \n                if args.peft_model:\n                    tokenizer = AutoTokenizer.from_pretrained(args.peft_model)\n                    model = AutoPeftModelForCausalLM.from_pretrained(\n                        args.peft_model,\n                        config=config,\n                        use_flash_attention_2=\"flash_attention_2\" if args.flash_attn else None,\n                        torch_dtype=torch.bfloat16,\n                        trust_remote_code=True,\n                        device_map=\"auto\",\n                    ).eval()\n                else:\n                    tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n                    model = AutoModelForCausalLM.from_pretrained(\n                        args.model_path_or_name,\n                        config=config,\n                        use_flash_attention_2=\"flash_attention_2\" if args.flash_attn else None,\n                        torch_dtype=torch.bfloat16,\n                        trust_remote_code=True,  \n                        device_map=\"auto\",\n                    ).eval()\n                tokenzier_vocab_size = len(tokenizer)\n                model_vocab_size = model.get_input_embeddings().weight.size(0)\n                num_new_tokens = tokenzier_vocab_size - model_vocab_size\n                if model_vocab_size < tokenzier_vocab_size:\n                    print(\"Resize model embeddings to fit tokenizer\")\n                    model.resize_token_embeddings(tokenzier_vocab_size)\n                    input_embeddings = model.get_input_embeddings().weight.data\n                    output_embeddings = model.get_output_embeddings().weight.data\n\n                    input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                        dim=0, keepdim=True\n                    )\n                    output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                        dim=0, keepdim=True\n                    )\n\n                    input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                    output_embeddings[-num_new_tokens:] = output_embeddings_avg\n                collator = ContextDataCollator()\n            elif args.model_provider == \"yarn-llama-2-7b-128k\":\n                tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n                model = LlamaForCausalLM.from_pretrained(args.model_path_or_name,\n                                    config=config,\n                                    use_flash_attention_2=\"flash_attention_2\" if args.flash_attn else None,\n                                    torch_dtype=torch.bfloat16,\n                                    trust_remote_code=True,  \n                                    device_map=\"auto\",\n                                ).eval()\n                collator = ContextDataCollator()\n                \n            elif args.model_provider == \"Gemma\":\n                tokenizer = GemmaTokenizer.from_pretrained(args.model_path_or_name)\n                model = GemmaForCausalLM.from_pretrained(\n                    args.model_path_or_name,\n                    config=config,\n                    use_flash_attention_2=\"flash_attention_2\" if args.flash_attn else None,\n                    torch_dtype=torch.bfloat16,\n                    device_map=\"auto\",\n                ).eval()\n                \n                collator = ContextDataCollator()\n                \n            elif args.model_provider == \"Mistral\":\n                tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n                model = MistralForCausalLM.from_pretrained(args.model_path_or_name,\n                    config=config,\n                    use_flash_attention_2=\"flash_attention_2\" if args.flash_attn else None,\n                    torch_dtype=torch.bfloat16,\n                    device_map=\"auto\",\n                ).eval()\n                \n                collator = ContextDataCollator()\n            \n            elif args.model_provider == \"LongLLaMA\":\n                tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n                model = LongLlamaForCausalLM.from_pretrained(args.model_path_or_name,torch_dtype=torch.float32,device_map=\"auto\").eval()\n                collator = ContextDataCollator()\n                \n            elif args.model_provider == \"LongLora\":\n                replace_llama_attn(use_flash_attn=args.flash_attn, use_full=True,inference=True)\n                # Load model and tokenizer\n                model = transformers.AutoModelForCausalLM.from_pretrained(\n                    args.model_path_or_name,\n                    config=config,\n                    cache_dir=args.cache_dir,\n                    torch_dtype=torch.float16,\n                    device_map=\"auto\",\n                )\n                model.resize_token_embeddings(32001)\n                tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path_or_name)\n                if args.peft_model:\n                    trainable_params = os.path.join(args.peft_model, \"trainable_params.bin\")\n                    if os.path.isfile(trainable_params):\n                        model.load_state_dict(torch.load(trainable_params, map_location=model.device), strict=False)\n                    else:\n                        raise ValueError(\"Trainable input embedding and normalization are required.\")\n                    model = PeftModel.from_pretrained(\n                        model,\n                        args.peft_model,\n                        device_map=\"auto\",\n                        torch_dtype=torch.float16,\n                    )\n                \n                collator = ContextDataCollator()\n                \n            elif args.model_provider == \"InfiniteTransformer\":\n                # TODO\n                tokenizer = GemmaTokenizer.from_pretrained(args.model_path_or_name)\n                model = InfiniteTransformer.GemmaForCausalLM(\n                    # args.model_path_or_name,\n                    config=config,\n                ).eval()\n                if args.peft_model:\n                    model = PeftModel.from_pretrained(\n                        model,\n                        args.peft_model,\n                        torch_dtype=torch.bfloat16,\n                    )\n                model.to(\"cuda:0\")    \n                collator = ContextDataCollator()\n                \n            elif args.model_provider == \"CEPE\":\n                # TODO\n                tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n                model = LlamaForCausalContextLM.from_pretrained(\n                    args.model_path_or_name,\n                    config=config,\n                    use_flash_attention_2=\"flash_attention_2\" if args.flash_attn else None,\n                    torch_dtype=torch.bfloat16,\n                    device_map=\"auto\",\n                ).eval()\n                collator = ContextDataCollator()\n                \n            elif args.model_provider == \"MemLong\":\n                tokenizer = AutoTokenizer.from_pretrained(args.model_path_or_name)\n                \n                if args.embedder_name == \"llm_embedder\":\n                    toolkit_config = ToolkitConfig(\n                            task=\"lrlm\",\n                            embedder_name=\"llm_embedder\",\n                            embedder_path=\"/opt/data/private/lwj/acl2023/FineRange/llm-embedder\",\n                            ret_embeddings_dim=768,\n                        )\n                elif args.embedder_name == \"bge_embedder\":\n                    toolkit_config = ToolkitConfig(\n                        task=\"lrlm\",\n                        embedder_name=\"bge_embedder\",\n                        embedder_path=\"/opt/data/private/lwj/memlong/bge-m3\",\n                        ret_embeddings_dim=1024,\n                    )\n                elif args.embedder_name == \"st_embedder\":\n                    toolkit_config = ToolkitConfig(\n                        task=\"lrlm\",\n                        embedder_name=\"st_embedder\",\n                        embedder_path=\"/opt/data/private/lwj/emnlp2024/st-m3\",\n                        ret_embeddings_dim=512,\n                    )\n\n                else:\n                    raise NotImplemented\n                \n                model = modeling_llama_position.LlamaForCausalLM.from_pretrained(\n                    args.model_path_or_name,\n                    config=config,\n                    toolkit_config=toolkit_config,\n                    attn_implementation=\"flash_attention_2\" if args.flash_attn else \"eager\",\n                    cache_dir=args.cache_dir,\n                    torch_dtype=torch.bfloat16,\n                    # device_map=\"auto\",\n                )\n                tokenzier_vocab_size = len(tokenizer)\n                model_vocab_size = model.get_input_embeddings().weight.size(0)\n                num_new_tokens = tokenzier_vocab_size - model_vocab_size\n                if num_new_tokens > 0:\n                    print(\"Resize model embeddings to fit tokenizer\")\n                    model = model.resize_token_embeddings(tokenzier_vocab_size)\n                    input_embeddings = model.get_input_embeddings().weight.data\n                    output_embeddings = model.get_output_embeddings().weight.data\n\n                    input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n                        dim=0, keepdim=True\n                    )\n                    output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n                        dim=0, keepdim=True\n                    )\n\n                    input_embeddings[-num_new_tokens:] = input_embeddings_avg\n                    output_embeddings[-num_new_tokens:] = output_embeddings_avg\n                else:\n                    tokenizer.pad_token_id = config.pad_token_id\n\n                if args.peft_model:\n                    print(\"Loading PEFT model\")\n                    model = PeftModel.from_pretrained(\n                        model,\n                        args.peft_model,\n                        #device_map=\"auto\",\n                        torch_dtype=torch.bfloat16,\n                    )\n                    model = model.merge_and_unload()\n                model.set_toolkit_tokenizer(tokenizer)\n                model.to(\"cuda:0\")\n                model.eval()\n                collator = ContextDataCollator()\n\n            tokenizer.model_max_length = 1000000000000000019884624838656\n            device = model.device    \n            # tokenize\n            if dataset_name == \"wikitext103\":\n                dict_data = {'text': [''.join(item['text'] for item in json_data)]}\n            else:\n                dict_data = {\"text\": [item['text'] for item in json_data]}\n            \n            dataset = Dataset.from_dict(dict_data)\n            tokenize_function = tokenize_fn\n            tokenized_datasets = dataset.map(\n                partial(tokenize_function, tokenizer, \"text\"), \n                batched=True,\n                batch_size=args.data_batch_size,\n                remove_columns=dataset.column_names,\n                num_proc=args.num_proc\n            )\n            # FIXME\n            # data = {\"val\":np.array(dataset[\"input_ids\"],dtype=np.uint16)}\n            \n            # if tokenize_function == Sig_tokenize_fn:\n            #     num_validation_tokens = len(np.array(dataset[\"input_ids\"],dtype=np.uint16))\n            #     data = {\"val\":np.array(dataset[\"input_ids\"],dtype=np.uint16)}\n            # else:\n            tokenized_datasets = tokenized_datasets.filter(lambda example: len(example[\"input_ids\"]) >= args.filter_length)\n            eval_dataset = tokenized_datasets.map(\n                partial(preprocess,args),\n                batched=True,\n                num_proc=args.num_proc,\n                remove_columns=tokenized_datasets.column_names,\n                batch_size=args.data_batch_size,\n            )\n            num_validation_tokens = sum([len(np.array(ele[\"input_ids\"],dtype=np.uint16)) for ele in eval_dataset])\n            # log\n            print(f\"Num validation tokens : {num_validation_tokens}\")\n            print(\"     data path  : \", dataset_path_or_name)\n            print(\"     data name  : \", dataset_name)\n            print(\"     base model : \" , model_version)\n            print(\"     peft model : \", args.peft_model)\n            print(f\"eval dataset size after filtering: {len(eval_dataset)}\")   \n            # print(\"Number of GPUs used : \", num_gpus_used) FIXME:get num_gpus_used\n            info = {}\n            info['Num validation tokens'] = num_validation_tokens\n            info['data_path'] = dataset_path_or_name\n            info['data_name'] = dataset_name\n            info['base_model'] = model_version\n            info['peft_model'] = args.peft_model\n            # info['Number_of_GPUs_used'] = num_gpus_used\n            \n            eval_dataloader = torch.utils.data.DataLoader(\n                    eval_dataset,\n                    batch_size=args.batch_size,\n                    collate_fn=collator,\n                    pin_memory=True,\n                )\n\n            start_time = time()   \n            \"\"\"\n            # CEPE Only support sequential evaluation     \n            if args.model_provider in [\"CEPE\"]:\n                eval_dataset = dataset.map(\n                    partial(preprocess,args),\n                    batched=True,\n                    num_proc=args.num_proc,\n                    remove_columns=dataset.column_names,\n                    batch_size=args.data_batch_size,\n                )\n                \n                eval_dataloader = torch.utils.data.DataLoader(\n                    eval_dataset,\n                    batch_size=args.batch_size,\n                    collate_fn=collator,\n                    pin_memory=True,\n                    num_workers=args.num_proc,\n                )\n                stats = seqential_evaluate(model, eval_dataloader, args, device)\n            elif args.model_provider in [\"MemLong\"]:\n                # FIXME\n                # eval_dataset = dataset.map(partial(preprocess,args),batched=True,num_proc=args.num_proc,remove_columns=dataset.column_names,batch_size=args.data_batch_size,)\n                # eval_dataloader = torch.utils.data.DataLoader(eval_dataset,shuffle=False,collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False,),sampler=BatchSequentialSampler(eval_dataset,args.per_device_eval_batch_size),batch_size=args.batch_size,pin_memory=True,)\n                # stats = seqential_evaluate(model, eval_dataloader, args, device)\n                stats = evaluate(model, data, args.batch_size, device, args.seq_len, sliding_window=args.sliding_window,model_provider=args.model_provider,segment_length=args.segment_length,use_cache=False,tokenizer=tokenizer)\n            else:\n                stats = evaluate(model, data, args.batch_size, device, args.seq_len, sliding_window=args.sliding_window,model_provider=args.model_provider,segment_length=args.segment_length)\n            \"\"\"\n            stats = seqential_evaluate(model, eval_dataloader, args, device,tokenizer=tokenizer)\n            end_time = time()\n            elapsed_seconds = end_time - start_time\n            info['test_timestamp_duration'] = str(timedelta(seconds=elapsed_seconds))\n    \n            info = {**info,**stats}\n            \n            with open(f\"{location}/{dataset_name}_eval.json\", \"w\") as f:\n                json.dump(info, f, indent=4)\n                print(\"Evaluation results saved to\", f\"{location}/{dataset_name}_eval.json\")\n                \nif __name__ == \"__main__\":\n    args = parse_config()\n    main(args)"}
{"type": "source_file", "path": "src/__init__.py", "content": ""}
{"type": "source_file", "path": "eval/language_modeling/longllama/longllama_utils.py", "content": "from collections import namedtuple\nfrom dataclasses import dataclass\nimport torch\nfrom typing import Tuple, Optional\n\n\n@dataclass\nclass LongLlamaMemConfig:\n    \"\"\"\n    Class for configuring memory caches for LongLlama model.\n\n    Args:\n        positionals (`boolean`)\n            Whether to use positional embeddings in memory layer\n        cache_dtype (`torch.dtype`)\n            Specifies storing type for keys and values\n        attention_grouping (`Tuple[int, int]`, *optional*)\n            One can trade speed for memory by performing attention\n            in memory layers sequentially.\n            When equal to `(4, 128)` the memory layers will process at most 4 heads and 128 queries\n            from each head at once. That is at most 512 queries at once.\n    \"\"\"\n\n    positionals: bool = True\n    cache_dtype: torch.dtype = torch.bfloat16\n    attention_grouping: Optional[Tuple[int, int]] = None\n\n\n@dataclass\nclass LongLlamaMemCache:\n    \"\"\"\n    Class with LongLlama's memory cache\n\n    Args:\n        keys (`torch.FloatTensor` of shape `(batch_size, num_heads, mem_length, embed_size_per_head)`)\n        values (`torch.FloatTensor` of shape `(batch_size, num_heads, mem_length, embed_size_per_head)`)\n        masks (`torch.FloatTensor` of shape `(batch_size, 1, mem_length, 1)`)\n            For masking out parts of memory\n    \"\"\"\n\n    keys: torch.FloatTensor\n    values: torch.FloatTensor\n    masks: torch.FloatTensor\n\n\ndef mem_apply_update(\n    prev_mem_cache: LongLlamaMemCache, new_mem_content: LongLlamaMemCache, mem_config: LongLlamaMemConfig\n):\n    def update_one(prev, new):\n        if len(prev.shape) != 4 or len(new.shape) != 4:\n            raise ValueError(f\"Memory cache content should be consistent in shape got {prev.shape} {new.shape}\")\n\n        return torch.concat([prev, new], dim=-2)\n\n    insert_size = new_mem_content.keys.shape[-2]\n\n    if new_mem_content.values.shape[-2] != insert_size or new_mem_content.masks.shape[-2] != insert_size:\n        raise ValueError(f\"Inconsistent mem_length in new_mem_content\")\n\n    return LongLlamaMemCache(\n        keys=update_one(prev_mem_cache.keys, new_mem_content.keys),\n        values=update_one(prev_mem_cache.values, new_mem_content.values),\n        masks=update_one(prev_mem_cache.masks, new_mem_content.masks),\n    )"}
{"type": "source_file", "path": "run_clm_no_trainer.py", "content": "#!/usr/bin/env python\n# coding=utf-8\n# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"\nFine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...)\non a text file or a dataset without using HuggingFace Trainer.\n\nHere is the full list of checkpoints on the hub that can be fine-tuned by this script:\nhttps://huggingface.co/models?filter=text-generation\n\"\"\"\n# You can also adapt this script on your own causal language modeling task. Pointers for this are left as comments.\nimport faiss\nimport argparse\nimport json\nimport logging\nimport math\nimport os\nimport random\nfrom itertools import chain\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport torch\nfrom torch.utils.data import DataLoader, Sampler\nfrom torch.cuda import device_count\nfrom accelerate import Accelerator, DistributedType\nfrom accelerate.logging import get_logger\nfrom accelerate.utils import set_seed\nfrom accelerate.utils import (\n    DummyOptim,\n    DummyScheduler,\n)\nimport transformers\nfrom transformers import (\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    SchedulerType,\n    DataCollatorForLanguageModeling,\n    get_scheduler,\n)\nfrom transformers.utils.versions import require_version\nfrom transformers import AutoTokenizer\nfrom huggingface_hub import Repository, create_repo\nimport datasets\nfrom datasets import load_from_disk, load_dataset\nfrom wandb.util import generate_id\nfrom functools import partial\nfrom src.utils import (\n    BatchSequentialSampler,\n    group_texts,\n    save_config,\n    ToolkitConfig,\n    tokenize_fn,\n    set_freeze_by_idxs,\n    convert_to_lora,\n)\n\nfrom src.configuration_llama import LlamaConfig\nfrom src.modeling_llama_position import LlamaForCausalLM\nfrom peft import PeftModel\n\nfrom deepspeed.runtime.zero.stage_1_and_2 import (estimate_zero2_model_states_mem_needs_all_live,)\nfrom functools import partial\n\n# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n\nIGNORE_INDEX = -100\nDEFAULT_PAD_TOKEN = \"[PAD]\"\nDEFAULT_EOS_TOKEN = \"</s>\"\nDEFAULT_BOS_TOKEN = \"<s>\"\nDEFAULT_UNK_TOKEN = \"<unk>\"\n\nlogger = get_logger(__name__)\nrequire_version(\n    \"datasets>=1.8.0\",\n    \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\",\n)\nMODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Finetune a transformers model on a causal language modeling task\")\n\n    parser.add_argument(\"--load_from_disk\",type=str,help=\"load dataset from disk\")\n    parser.add_argument(\"--embedder_path\",type=str)\n    parser.add_argument(\"--mem_layer\",type=int,help=\"set the memory layer \")\n    parser.add_argument(\"--ret_attn_layers\",nargs=\"+\",type=int,help=\"set the ret_attn_layers\")\n    parser.add_argument(\"--last_context_length\",type=int,help=\"argument for inference\")\n    parser.add_argument(\"--batch_sequential\",default=False,action=\"store_true\")\n    parser.add_argument(\"--clear_memories_on_eos_token_id\", default=None, action=\"store_true\")\n    parser.add_argument(\"--clear_memories_on_bos_token_id\", default=None, action=\"store_true\")\n    parser.add_argument(\"--circulate_steps\",default=512,type=int)\n    parser.add_argument(\"--ret_group_size\", type=int, help=\"each group for retrieval\")\n    parser.add_argument(\"--pooling_tokens\", default=None, required=False,type=int, help=\"compress the tokens size\")\n    parser.add_argument(\"--mem_group_size\", type=int)\n    parser.add_argument(\"--use_gate\",action=\"store_true\")\n    parser.add_argument(\"--use_gpu_to_search\", action=\"store_true\")\n    parser.add_argument(\"--trainable_params\",default=None,type=str,)\n    parser.add_argument(\"--targets\",default=None, type=str)\n    parser.add_argument(\"--use_cache\", action=\"store_true\",default=False)\n    parser.add_argument(\"--position_type\",choices=[\"Zero\",\"Continual\"],default=None)\n\n    parser.add_argument(\"--task\",type=str,default=\"lrlm\",choices=[\"qa\", \"icl\", \"chat\", \"lrlm\", \"tool\", \"convsearch\"],)\n    parser.add_argument(\"--load_llama_config\",type=str,default=None,)\n    parser.add_argument(\"--dataset_name\",type=str,default=None,help=\"The name of the dataset to use (via the datasets library).\",)\n    parser.add_argument(\"--dataset_config_name\",type=str,default=None,help=\"The configuration name of the dataset to use (via the datasets library).\",)\n    parser.add_argument(\"--train_file\",type=str,default=None,help=\"A csv, txt or a json file containing the training data.\",)\n    parser.add_argument(\"--validation_file\",type=str,default=None,help=\"A csv, txt or a json file containing the validation data.\",)\n    parser.add_argument(\"--validation_split_percentage\",default=5,help=\"The percentage of the train set used as validation set in case there's no validation split\",)\n    parser.add_argument(\"--model_name_or_path\",type=str,help=\"Path to pretrained model or model identifier from huggingface.co/models.\",)\n    parser.add_argument(\"--config_name\",type=str,default=None,help=\"Pretrained config name or path if not the same as model_name\",)\n    parser.add_argument(\"--tokenizer_name\",type=str,default=None,help=\"Pretrained tokenizer name or path if not the same as model_name\",)\n    parser.add_argument(\"--use_slow_tokenizer\",action=\"store_true\",help=\"If passed, will use a slow tokenizer (not backed by the 🤗 Tokenizers library).\",)\n    parser.add_argument(\"--per_device_train_batch_size\",type=int,default=8,help=\"Batch size (per device) for the training dataloader.\",)\n    parser.add_argument(\"--per_device_eval_batch_size\",type=int,default=8,help=\"Batch size (per device) for the evaluation dataloader.\",)\n    parser.add_argument(\"--learning_rate\",type=float,default=5e-5,help=\"Initial learning rate (after the potential warmup period) to use.\",)\n    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\")\n    parser.add_argument(\"--num_train_epochs\",type=int,default=1,help=\"Total number of training epochs to perform.\",)\n    parser.add_argument(\"--max_train_steps\",type=int,default=None,help=\"Total number of training steps to perform. If provided, overrides num_train_epochs.\",)\n    parser.add_argument(\"--embedder_name\",type=str,default=None,)\n    parser.add_argument(\"--gradient_accumulation_steps\",type=int,default=8,help=\"Number of updates steps to accumulate before performing a backward/update pass.\",)\n    parser.add_argument(\"--lr_scheduler_type\",type=SchedulerType,default=\"cosine_with_restarts\",help=\"The scheduler type to use.\",choices=[\"linear\",\"cosine\",\"cosine_with_restarts\",\"polynomial\",\"constant\",\"constant_with_warmup\",],)\n    parser.add_argument(\"--seq_len\", type=int, required=True)\n    parser.add_argument(\"--num_warmup_steps\",type=int,default=1000,help=\"Number of steps for the warmup in the lr scheduler.\",)\n    parser.add_argument(\"--output_dir\",type=str,required=True,help=\"Where to store the final model.\",)\n    parser.add_argument(\"--seed\", type=int, default=42, help=\"A seed for reproducible training.\")\n    parser.add_argument(\"--model_type\",type=str,default=None,help=\"Model type to use if training from scratch.\",choices=MODEL_TYPES)\n    parser.add_argument(\"--block_size\",type=int,default=None,help=(\"Optional input sequence length after tokenization. The training dataset will be truncated in block of\"\" this size for training. Default to the model max input length for single sentence inputs (take into\"\" account special tokens).\"),)\n    parser.add_argument(\"--train_mode\",type=str,default=None,required=True,choices=[\"lora-all\", \"lora-freeze\", \"partial-lora\",\"partial-freeze\"],)\n    parser.add_argument(\"--continual_finetuning\",action=\"store_true\",)\n\n    parser.add_argument(\"--freeze_layers\", type=str,default=None)\n    parser.add_argument(\"--update_boundary\",type=str,default=None,)\n    parser.add_argument(\"--memory_size\",type=int,)\n    parser.add_argument(\"--ret_embeddings_dim\",type=int,default=768,)\n    parser.add_argument(\"--preprocessing_num_workers\",type=int,default=32,help=\"The number of processes to use for the preprocessing.\",)\n    parser.add_argument(\"--overwrite_cache\",action=\"store_true\",help=\"Overwrite the cached training and evaluation sets\",)\n    parser.add_argument(\"--no_keep_linebreaks\",action=\"store_true\",help=\"Do not keep line breaks when using TXT files.\",)\n    parser.add_argument(\"--push_to_hub\",action=\"store_true\",help=\"Whether or not to push the model to the Hub.\",)\n    parser.add_argument(\"--hub_model_id\",type=str,help=\"The name of the repository to keep in sync with the local `output_dir`.\",)\n    parser.add_argument(\"--hub_token\", type=str, help=\"The token to use to push to the Model Hub.\")\n    parser.add_argument(\"--trust_remote_code\",type=bool,default=False,help=(\"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\"\"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\"execute code present on the Hub on your local machine.\"),)\n    parser.add_argument(\"--checkpointing_steps\",type=str,default=None,help=\"Whether the various states should be saved at the end of every n steps, or 'epoch' for each epoch.\",)\n    parser.add_argument(\"--resume_from_checkpoint\",type=str,default=None,help=\"If the training should continue from a checkpoint folder.\",)\n    parser.add_argument(\"--with_tracking\",action=\"store_true\",help=\"Whether to enable experiment trackers for logging.\",)\n    parser.add_argument(\"--report_to\",type=str,default=\"wandb\",\n        help=(\n            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`,'\n            ' `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations. '\n            \"Only applicable when `--with_tracking` is passed.\"\n        ),\n    )\n    parser.add_argument(\"--project\", type=str, default=None)\n    parser.add_argument(\"--project_name\", type=str, default=None)\n    parser.add_argument(\"--wandb_id\", type=str, default=None)\n    parser.add_argument(\"--wandb_resume\",type=str,default=\"auto\",choices=[\"auto\", \"must\", \"never\", \"allow\", None],)\n    parser.add_argument(\"--log_step\", type=int, default=10)\n    parser.add_argument(\"--low_cpu_mem_usage\",action=\"store_true\",\n        help=(\n            \"It is an option to create the model as an empty shell, then only materialize its parameters when the pretrained weights are loaded. \"\n            \"If passed, LLM loading time and RAM consumption will be benefited.\"\n        ),\n    )\n    \n    parser.add_argument(\"--load_best_model\", action=\"store_true\")\n    parser.add_argument(\"--dataset_tokenizer\", action=\"store_true\")\n    parser.add_argument(\"--debug\", action=\"store_true\")\n    parser.add_argument(\"--peft_model\",type=str,default=None)\n    args = parser.parse_args()\n    if args.load_from_disk is not None:\n        pass\n    # Sanity checks\n    elif (\n        args.dataset_name is None\n        and args.train_file is None\n        and args.validation_file is None\n    ):\n        raise ValueError(\"Need either a dataset name or a training/validation file.\")\n    else:\n        if args.train_file is not None:\n            extension = args.train_file.split(\".\")[-1]\n            if extension not in [\"csv\", \"json\", \"txt\"]:\n                raise ValueError(\"`train_file` should be a csv, json or txt file.\")\n        if args.validation_file is not None:\n            extension = args.validation_file.split(\".\")[-1]\n            if extension not in [\"csv\", \"json\", \"txt\"]:\n                raise ValueError(\"`validation_file` should be a csv, json or txt file.\")\n\n    if args.push_to_hub:\n        if args.output_dir is None:\n            raise ValueError(\n                \"Need an `output_dir` to create a repo when `--push_to_hub` is passed.\"\n            )\n\n    return args\n\ndef evaluate(args, model, eval_dataloader, accelerator):\n    model.eval()\n    losses = []\n    for step, batch in enumerate(eval_dataloader):\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        losses.append(\n            accelerator.gather_for_metrics(loss.repeat(args.per_device_eval_batch_size))\n        )\n\n    losses = torch.cat(losses)\n    try:\n        eval_loss = torch.mean(losses)\n        perplexity = math.exp(eval_loss)\n    except OverflowError:\n        perplexity = float(\"inf\")\n    return perplexity, eval_loss\n\ndef main():\n    args = parse_args()\n\n    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n    # send_example_telemetry(\"run_clm_no_trainer\", args)\n\n    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n    # in the environment\n    \n    accelerator_log_kwargs = {}\n\n    if args.with_tracking:\n        accelerator_log_kwargs[\"log_with\"] = args.report_to\n        accelerator_log_kwargs[\"project_dir\"] = args.output_dir\n\n    if args.with_tracking and args.log_step is None:\n        args.log_step = 8\n\n    accelerator = Accelerator(\n        gradient_accumulation_steps=args.gradient_accumulation_steps,\n        **accelerator_log_kwargs,\n    )\n    # Make one log on every process with the configuration for debugging.\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n    )\n    logger.info(accelerator.state, main_process_only=False)\n    if accelerator.is_local_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    # If passed along, set the training seed now.\n    if args.seed is not None:\n        set_seed(args.seed)\n\n    # Handle the repository creation\n    if accelerator.is_main_process:\n        if args.push_to_hub:\n            # Retrieve of infer repo_name\n            repo_name = args.hub_model_id\n            if repo_name is None:\n                repo_name = Path(args.output_dir).absolute().name\n            # Create repo and retrieve repo_id\n            repo_id = create_repo(\n                repo_name, exist_ok=True, token=args.hub_token\n            ).repo_id\n            # Clone repo locally\n            repo = Repository(args.output_dir, clone_from=repo_id, token=args.hub_token)\n\n            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n                if \"step_*\" not in gitignore:\n                    gitignore.write(\"step_*\\n\")\n                if \"epoch_*\" not in gitignore:\n                    gitignore.write(\"epoch_*\\n\")\n        elif args.output_dir is not None:\n            if args.debug:\n                args.output_dir = args.output_dir + \"_debug\"\n                os.makedirs(args.output_dir, exist_ok=True)\n            else:\n                os.makedirs(args.output_dir, exist_ok=True)\n    accelerator.wait_for_everyone()\n\n    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n    # (the dataset will be downloaded automatically from the datasets Hub).\n    #\n    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n    # 'text' is found. You can easily tweak this behavior (see below).\n    #\n    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n    # download the dataset.\n    if args.load_from_disk:\n        lm_datasets = load_from_disk(args.load_from_disk)\n        lm_datasets.set_format(\"torch\")\n    elif args.dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                args.dataset_name,\n                args.dataset_config_name,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n            )\n    else:\n        data_files = {}\n        dataset_args = {}\n        if args.train_file is not None:\n            data_files[\"train\"] = args.train_file\n        if args.validation_file is not None:\n            data_files[\"validation\"] = args.validation_file\n        extension = args.train_file.split(\".\")[-1]\n        if extension == \"txt\":\n            extension = \"text\"\n            dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n        raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n        if \"validation\" not in raw_datasets.keys():\n            raw_datasets[\"validation\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[:{args.validation_split_percentage}%]\",\n                **dataset_args,\n            )\n            raw_datasets[\"train\"] = load_dataset(\n                extension,\n                data_files=data_files,\n                split=f\"train[{args.validation_split_percentage}%:]\",\n                **dataset_args,\n            )\n\n    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n    # https://huggingface.co/docs/datasets/loading_datasets.\n\n    # Load pretrained model and tokenizer\n    #\n    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n    # download model & vocab.\n\n    if not args.continual_finetuning:\n        llama_config = LlamaConfig.from_pretrained(\n            pretrained_model_name_or_path=args.model_name_or_path,\n        )\n        \n        llama_config.mem_layer = args.mem_layer\n        llama_config.ret_attn_layers = args.ret_attn_layers\n        llama_config.memory_size = args.memory_size\n        llama_config.last_context_length = args.last_context_length\n        llama_config.ret_group_size = args.ret_group_size\n        llama_config.use_gate=args.use_gate\n        llama_config.update_boundary = args.update_boundary\n        # llama_config.mem_positionals=args.mem_positionals\n        llama_config.mem_group_size=args.mem_group_size\n        llama_config.use_cache=args.use_cache\n        save_config(llama_config, args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n        toolkit_config = ToolkitConfig(\n            # use_gpu_to_search=args.use_gpu_to_search,\n            embedder_name=args.embedder_name,\n            task=args.task,\n            embedder_path=args.embedder_path,\n            ret_embeddings_dim=args.ret_embeddings_dim,\n        )\n        model = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path=args.model_name_or_path,config=llama_config, toolkit_config=toolkit_config,attn_implementation=\"eager\")\n        # model = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path=args.model_name_or_path,config=llama_config,attn_implementation=\"eager\")\n    else:\n        # peft priority\n        if args.peft_model:\n            llama_config = LlamaConfig.from_pretrained(args.peft_model+\"/config.json\")\n        else:\n            llama_config = LlamaConfig.from_pretrained(pretrained_model_name_or_path=args.model_name_or_path)\n            \n        llama_config.use_gate = llama_config.use_gate if getattr(args,\"use_gate\",None) == None else args.use_gate\n        llama_config.use_cache =llama_config.use_cache if  getattr(args,\"use_cache\",None) == None else args.use_cache\n        llama_config.ret_attn_layers = llama_config.ret_attn_layers if getattr(args,\"ret_attn_layers\",None) == None else args.ret_attn_layers\n        llama_config.position_type = \"Zero\" if getattr(args,\"position_type\",None) == None else args.position_type\n        llama_config.last_context_length= llama_config.last_context_length if getattr(args,\"last_context_length\",None) == None else args.last_context_length\n        llama_config.mem_group_size = llama_config.mem_group_size if getattr(args,\"mem_group_size\",None) == None else args.mem_group_size\n        llama_config.ret_group_size = llama_config.ret_group_size if getattr(args,\"ret_group_size\",None) == None else args.ret_group_size\n\n        save_config(llama_config, args.output_dir)\n        if not args.peft_model:\n            tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n        else:\n            tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n        tokenizer.pad_token_id = llama_config.pad_token_id\n        toolkit_config = ToolkitConfig(\n            # use_gpu_to_search=args.use_gpu_to_search,\n            embedder_name=args.embedder_name,\n            task=args.task,\n            embedder_path=args.embedder_path,\n            ret_embeddings_dim=args.ret_embeddings_dim,\n        )\n        model = LlamaForCausalLM.from_pretrained(pretrained_model_name_or_path=args.model_name_or_path,config=llama_config, toolkit_config=toolkit_config,attn_implementation=\"eager\")\n        if args.peft_model:\n            model = PeftModel.from_pretrained(\n                model,\n                args.peft_model,\n                torch_dtype=\"bfloat16\",\n            )\n            model = model.merge_and_unload()\n            \n    accelerator.print(llama_config)\n    accelerator.print(toolkit_config)\n    \n    # total_params = sum(p.numel() for p in retree.parameters())\n    # estimate_zero2_model_states_mem_needs_all_live(etree, num_gpus_per_node=8, num_nodes=1)\n    model.set_toolkit_tokenizer(tokenizer)\n    trainable_params = (args.trainable_params.split(\",\") if args.trainable_params else None)\n    targets = args.targets.split(\",\") if args.targets else None\n    lower, upper = list(map(int, args.freeze_layers.split(\":\")))\n    # training mode\n    if args.train_mode == \"lora-all\":\n        model = convert_to_lora(model=model, targets=targets, trainable_params=trainable_params,task_type=\"CAUSAL_LM\")\n        model.print_trainable_parameters()\n    elif args.train_mode == \"lora-freeze\":\n        model = convert_to_lora(model=model, targets=targets, trainable_params=trainable_params)\n        set_freeze_by_idxs(model, range(lower, upper),freeze=True)\n        model.print_trainable_parameters()\n    elif args.train_mode == \"partial-lora\":\n        set_freeze_by_idxs(model,range(lower, upper),freeze=True)\n        model = convert_to_lora(model=model, trainable_params=trainable_params)\n        model.print_trainable_parameters()\n    elif args.train_mode == \"partial-freeze\":\n        set_freeze_by_idxs(model, range(lower, upper),freeze=True)\n        if args.peft_model:\n            model.print_trainable_parameters()\n\n    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n    # on a small vocab and want a smaller embedding size, remove this test.\n    # embedding_size = model.get_input_embeddings().weight.shape[0]\n    # if len(tokenizer) > embedding_size:\n    #    model.resize_token_embeddings(len(tokenizer))\n\n    # Preprocessing the datasets.\n    # First we tokenize all the texts.\n    tokenized_datasets = None\n    if not args.load_from_disk:\n        column_names = raw_datasets[\"train\"].column_names\n        text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n        def tokenize_function(examples):\n            return tokenizer(examples[text_column_name])\n\n        with accelerator.main_process_first():\n            tokenized_datasets = raw_datasets.map(\n                tokenize_function,\n                batched=True,\n                num_proc=args.preprocessing_num_workers,\n                remove_columns=column_names,\n                load_from_cache_file=not args.overwrite_cache,\n                desc=\"Running tokenizer on dataset\",\n            )\n\n        if args.block_size is None:\n            block_size = tokenizer.model_max_length\n            if block_size > llama_config.max_position_embeddings:\n                logger.warning(\n                    f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n                    f\"Using block_size={min(1024, llama_config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n                )\n                block_size = min(1024, llama_config.max_position_embeddings)\n        else:\n            if args.block_size > tokenizer.model_max_length:\n                logger.warning(\n                    f\"The block_size passed ({args.block_size}) is larger than the maximum length for the model \"\n                    f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n                )\n            block_size = min(args.block_size, tokenizer.model_max_length)\n\n        with accelerator.main_process_first():\n            lm_datasets = tokenized_datasets.map(\n                group_texts,\n                batched=True,\n                num_proc=args.preprocessing_num_workers,\n                load_from_cache_file=not args.overwrite_cache,\n                desc=f\"Grouping texts in chunks of {block_size}\",\n            )\n\n    if args.dataset_tokenizer:\n        with accelerator.main_process_first():\n            tokenized_datasets = lm_datasets.map(\n                partial(tokenize_fn, tokenizer, args.seq_len),\n                batched=True,\n                batch_size=64,\n                num_proc=56,\n                remove_columns=lm_datasets[\"train\"].column_names,\n            )\n\n    if tokenized_datasets is None:\n        # breakpoint()\n        train_dataset = lm_datasets[\"train\"]\n        eval_dataset = lm_datasets[\"validation\"]\n    else:\n        train_dataset = tokenized_datasets[\"train\"]\n        eval_dataset = tokenized_datasets[\"validation\"]\n\n    # Log a few random samples from the training set:\n    for index in random.sample(range(len(train_dataset)), 3):\n        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n    # DataLoaders creation:\n    train_dataloader = (\n        DataLoader(\n            train_dataset,\n            shuffle=True,\n            collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n            batch_size=args.per_device_train_batch_size,\n            pin_memory=True,\n        )\n        if not args.batch_sequential\n        else DataLoader(\n            train_dataset[\"input_ids\"],\n            shuffle=False,\n            collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n            sampler=BatchSequentialSampler(\n                train_dataset,\n                args.per_device_train_batch_size,\n                num_process=accelerator.num_processes,\n            ),\n            batch_size=args.per_device_train_batch_size,\n            pin_memory=True,\n        )\n    )\n    eval_dataloader = (\n        DataLoader(\n            eval_dataset,\n            shuffle=True,\n            collate_fn=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n            batch_size=args.per_device_eval_batch_size,\n            pin_memory=True,\n        )\n        if not args.batch_sequential\n        else DataLoader(\n            eval_dataset[\"input_ids\"],\n            shuffle=False,\n            collate_fn=DataCollatorForLanguageModeling(\n                tokenizer=tokenizer,\n                mlm=False,\n            ),\n            sampler=BatchSequentialSampler(\n                eval_dataset,\n                args.per_device_eval_batch_size,\n                num_process=accelerator.num_processes,\n            ),\n            batch_size=args.per_device_eval_batch_size,\n            pin_memory=True,\n        )\n    )\n    # Optimizer\n    # Split weights in two groups, one with weight decay and the other not.\n    no_decay = [\"bias\", \"layer_norm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if not any(nd in n for nd in no_decay) and p.requires_grad\n            ],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\n            \"params\": [\n                p\n                for n, p in model.named_parameters()\n                if any(nd in n for nd in no_decay) and p.requires_grad\n            ],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    \n    optimizer_cls = (\n        torch.optim.AdamW\n        if accelerator.state.deepspeed_plugin is None\n        or \"optimizer\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n        else DummyOptim\n    )\n    optimizer = optimizer_cls(\n        # filter(lambda x: x.requires_grad is not False, retree.parameters()),\n        optimizer_grouped_parameters,\n        lr=args.learning_rate,\n    )\n\n    # Scheduler and math around the number of training steps.\n    overrode_max_train_steps = False\n    num_update_steps_per_epoch = math.ceil(\n        len(train_dataloader) / args.gradient_accumulation_steps\n    )\n    if args.max_train_steps is None:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n        overrode_max_train_steps = True\n    accelerator.print(\"max_train_steps\", args.max_train_steps)\n    if (\n        accelerator.state.deepspeed_plugin is None\n        or \"scheduler\" not in accelerator.state.deepspeed_plugin.deepspeed_config\n    ):\n        lr_scheduler = get_scheduler(\n            name=args.lr_scheduler_type,\n            optimizer=optimizer,\n            num_warmup_steps=args.num_warmup_steps,\n            num_training_steps=args.max_train_steps,\n        )\n    else:\n        lr_scheduler = DummyScheduler(\n            optimizer,\n            total_num_steps=args.max_train_steps,\n            warmup_num_steps=args.num_warmup_steps,\n        )\n    # Prepare everything with our `accelerator`.\n    (model,optimizer,train_dataloader,eval_dataloader,lr_scheduler,) = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader, lr_scheduler)\n\n    # On TPU, the tie weights in our model have been disconnected, so we need to restore the ties.\n    if accelerator.distributed_type == DistributedType.TPU:\n        model.tie_weights()\n\n    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n    num_update_steps_per_epoch = math.ceil(\n        len(train_dataloader) / args.gradient_accumulation_steps\n    )\n    if overrode_max_train_steps:\n        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n    # Afterwards we recalculate our number of training epochs\n    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n\n    # Figure out how many steps we should save the Accelerator states\n    checkpointing_steps = args.checkpointing_steps\n    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n        checkpointing_steps = int(checkpointing_steps)\n\n    # We need to initialize the trackers we use, and also store our configuration.\n    # The trackers initializes automatically on the main process.\n    if args.with_tracking:\n        experiment_config = vars(args)\n        wandb_id = generate_id() if args.wandb_id is None else args.wandb_id\n        # TensorBoard cannot log Enums, need the raw value\n        experiment_config[\"lr_scheduler_type\"] = experiment_config[\n            \"lr_scheduler_type\"\n        ].value\n        experiment_config[\"wandb_id\"] = wandb_id\n        accelerator.init_trackers(\n            args.project,\n            experiment_config,\n            init_kwargs={\n                \"wandb\": {\n                    \"name\": args.project_name,\n                    \"resume\": args.wandb_resume,\n                    \"id\": wandb_id,\n                }\n            },\n        )\n\n    # Train!\n    total_batch_size = (\n        args.per_device_train_batch_size\n        * accelerator.num_processes\n        * args.gradient_accumulation_steps\n    )\n\n    logger.info(\"***** Running training *****\")\n    logger.info(f\"  Num examples = {len(train_dataset)}\")\n    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n    logger.info(\n        f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\"\n    )\n    logger.info(\n        f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n    )\n    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n    # Only show the progress bar once on each machine.\n    progress_bar = tqdm(\n        range(args.max_train_steps), disable=not accelerator.is_local_main_process\n    )\n    completed_steps = 0\n    starting_epoch = 0\n    best_metric = None\n    best_metric_checkpoint = None\n\n    # Potentially load in the weights and states from a previous save\n    if args.resume_from_checkpoint:\n        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n            checkpoint_path = args.resume_from_checkpoint\n            path = os.path.basename(args.resume_from_checkpoint)\n        else:\n            # Get the most recent checkpoint\n            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n            dirs.sort(key=os.path.getctime)\n            path = dirs[\n                -1\n            ]  # Sorts folders by date modified, most recent checkpoint is the last\n            checkpoint_path = path\n            path = os.path.basename(checkpoint_path)\n        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n        accelerator.load_state(checkpoint_path)\n        # Extract `epoch_{i}` or `step_{i}`\n        training_difference = os.path.splitext(path)[0]\n\n        if \"epoch\" in training_difference:\n            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n            resume_step = None\n            completed_steps = starting_epoch * num_update_steps_per_epoch\n        else:\n            # need to multiply `gradient_accumulation_steps` to reflect real steps\n            resume_step = (\n                int(training_difference.replace(\"step_\", \"\"))\n                * args.gradient_accumulation_steps\n            )\n            starting_epoch = resume_step // len(train_dataloader)\n            completed_steps = resume_step // args.gradient_accumulation_steps\n            resume_step -= starting_epoch * len(train_dataloader)\n\n    # update the progress_bar if load from checkpoint\n    progress_bar.update(completed_steps)\n\n    # print trainable params\n    if accelerator.is_local_main_process:\n        logger.info(\n            f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\"\n        )\n    for epoch in range(starting_epoch, args.num_train_epochs):\n        if (\n            args.resume_from_checkpoint\n            and epoch == starting_epoch\n            and resume_step is not None\n        ):\n            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n            active_dataloader = accelerator.skip_first_batches(\n                train_dataloader, resume_step\n            )\n        else:\n            active_dataloader = train_dataloader\n        # logger.info(\"sample batch:\", next(iter(active_dataloader)))\n        if args.with_tracking:\n            total_loss = 0\n            losses = []\n        for step, batch in enumerate(active_dataloader):\n            with accelerator.accumulate(model):\n                outputs =  model(**batch)\n                loss = outputs.loss\n                # We keep track of the loss at each epoch\n                if args.with_tracking:\n                    total_loss += loss.detach().float()\n                    losses.append(\n                        accelerator.gather_for_metrics(\n                            loss.detach().float().repeat(args.per_device_eval_batch_size)\n                        )\n                    )\n                accelerator.backward(loss)\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n            # Checks if the accelerator has performed an optimization step behind the scenes\n            if accelerator.sync_gradients:\n                progress_bar.update(1)\n                completed_steps += 1\n                \n            if args.with_tracking and completed_steps % args.log_step == 0:\n                step_loss = torch.mean(torch.cat(losses))\n                step_perplexity = math.exp(step_loss)\n                accelerator.log(\n                    {\n                        \"step_perplexity\": step_perplexity,\n                        \"step_loss\": step_loss,\n                        # \"epoch\": epoch,\n                        # \"step\": completed_steps,\n                    },\n                    step=completed_steps,\n                )\n                # losses = []\n\n            if isinstance(checkpointing_steps, int):\n                if completed_steps % checkpointing_steps == 0:\n                    output_dir = f\"step_{completed_steps}\"\n                    if args.output_dir is not None:\n                        output_dir = os.path.join(args.output_dir, output_dir)\n                    accelerator.save_state(output_dir)\n            if completed_steps >= args.max_train_steps:\n                break\n\n        \n\n        perplexity, eval_loss = evaluate(args, model, eval_dataloader, accelerator)\n        logger.info(f\"epoch {epoch}: perplexity: {perplexity} eval_loss: {eval_loss}\")\n\n        if args.with_tracking:\n            accelerator.log(\n                {\n                    \"eval_perplexity\": perplexity,\n                    \"eval_loss\": eval_loss,\n                    \"train_loss\": total_loss / len(train_dataloader),\n                    \"epoch\": epoch,\n                    # \"step\": completed_steps,\n                },\n                step=completed_steps,\n            )\n\n        if args.push_to_hub and epoch < args.num_train_epochs - 1:\n            accelerator.wait_for_everyone()\n            unwrapped_model = accelerator.unwrap_model(model)\n            unwrapped_model.save_pretrained(\n                args.output_dir,\n                is_main_process=accelerator.is_main_process,\n                save_function=accelerator.save,\n            )\n            if accelerator.is_main_process:\n                tokenizer.save_pretrained(args.output_dir)\n                repo.push_to_hub(\n                    commit_message=f\"Training in progress epoch {epoch}\",\n                    blocking=False,\n                    auto_lfs_prune=True,\n                )\n\n        if isinstance(checkpointing_steps, str) and checkpointing_steps == \"epoch\":\n            accelerator.save_state(os.path.join(args.output_dir, f\"epoch_{epoch}\"))\n\n        if best_metric is None or best_metric > perplexity:\n            best_metric = perplexity\n            best_metric_checkpoint = os.path.join(args.output_dir, \"best_checkpoint\")\n            accelerator.save_state(best_metric_checkpoint)\n            accelerator.print(f\"New best metric: {best_metric} at epoch {epoch}\")\n            accelerator.print(f\"best_metric_checkpoint: {best_metric_checkpoint}\")\n\n    \n    if args.load_best_model:\n        accelerator.load_state(best_metric_checkpoint)\n        perplexity, eval_loss = evaluate(\n            args, model, eval_dataloader, accelerator, eval_dataset\n        )\n        logger.info(\n            f\"Best model metrics: perplexity: {perplexity} eval_loss: {eval_loss}\"\n        )\n        if perplexity != best_metric:\n            raise AssertionError(\n                f\"Best metric {best_metric} does not match the metric {perplexity} of the loaded best model.\"\n            )\n\n    if args.with_tracking:\n        accelerator.end_training()\n\n    if args.output_dir is not None:\n        accelerator.wait_for_everyone()\n        unwrapped_model = accelerator.unwrap_model(model)\n        unwrapped_model.save_pretrained(\n            args.output_dir,\n            is_main_process=accelerator.is_main_process,\n            save_function=accelerator.save,\n            state_dict=accelerator.get_state_dict(model),\n        )\n        if accelerator.is_main_process:\n            tokenizer.save_pretrained(args.output_dir)\n            if args.push_to_hub:\n                repo.push_to_hub(commit_message=\"End of training\", auto_lfs_prune=True)\n\n        if accelerator.is_main_process:\n            with open(os.path.join(args.output_dir, \"all_results.json\"), \"w\") as f:\n                json.dump({\"perplexity\": perplexity, \"eval_loss\": eval_loss.item()}, f)\n\n\nif __name__ == \"__main__\":\n    main()\n"}
{"type": "source_file", "path": "eval/language_modeling/longllama/configuration_longllama.py", "content": "# coding=utf-8\n# Copyright 2023 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" LongLLaMA model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nLONGLLAMA_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"syzymon/long_llama_3b\": \"https://huggingface.co/syzymon/long_llama_3b/resolve/main/config.json\",\n}\n\n\nclass LongLlamaConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`LongLlamaModel`]. It is used to instantiate an LongLLaMA\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the LongLLaMA-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the LongLLaMA model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`LongLlamaModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer encoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-12):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        tie_word_embeddings(`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        mem_layers (`List[int]`, defaults to `[]`):\n            Layers with memory\n        mem_positionals (`bool`, *optional*, defaults to `True`):\n            Whether to use positional embeddings in memory layers\n        mem_dtype (`str`, *optional*, defaults to `\"bfloat16\"`):\n            Type for keys and values stored in memory\n        mem_attention_grouping (`Tuple[int, int]`, *optional*, defaults to `None`):\n            One can trade speed for memory by performing attention\n            in memory layers sequentially.\n            When equal to `(4, 2048)` the memory layers will process at most 4 heads and 2048 queries from each head at once.\n            That is at most 4*2048 queries at once.\n        torch_attention (`bool`, *optional*, defaults to `False`):\n            Whether to use torch scaled_dot_product_attention\n        gradient_checkpoint_every_ith (`int`, *optional*, defaults to `1`):\n            When gradient checkpointing is enabled checkpoint every ith layer\n\n        Example:\n\n    ```python\n    >>> from transformers import LongLlamaModel, LongLlamaConfig\n\n    >>> # Initializing a LongLLaMA longllama-7b style configuration\n    >>> configuration = LongLlamaConfig()\n\n    >>> # Initializing a model from the longllama-7b style configuration\n    >>> model = LongLlamaModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n    model_type = \"longllama\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=11008,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        hidden_act=\"silu\",\n        max_position_embeddings=2048,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=0,\n        bos_token_id=1,\n        eos_token_id=2,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling=None,\n        last_context_length=1024,\n        mem_layers=[],\n        mem_positionals=True,\n        mem_dtype=\"bfloat16\",\n        mem_attention_grouping=None,\n        torch_attention=False,\n        gradient_checkpoint_every_ith=1,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self.last_context_length = last_context_length\n        self.mem_layers = mem_layers\n        self.mem_positionals = mem_positionals\n        self.mem_dtype = mem_dtype\n        self.mem_attention_grouping = mem_attention_grouping\n        self.torch_attention = torch_attention\n        self.gradient_checkpoint_every_ith = gradient_checkpoint_every_ith\n\n        self._rope_scaling_validation()\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def _rope_scaling_validation(self):\n        \"\"\"\n        Validate the `rope_scaling` configuration.\n        \"\"\"\n        if self.rope_scaling is not None:\n            raise ValueError(\"LongLLaMA does not use rope_scaling\")"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/ret_embedder.py", "content": "from transformers import AutoTokenizer\nfrom transformers import AutoTokenizer, AutoModel\nfrom sentence_transformers import SentenceTransformer\nfrom FlagEmbedding import BGEM3FlagModel\nimport torch\nfrom typing import List, Literal\nfrom faiss import normalize_L2\n\nINSTRUCTIONS = {\n    \"qa\": {\n        \"query\": \"Represent this query for retrieving relevant documents: \",\n        \"key\": \"Represent this document for retrieval: \",\n    },\n    \"icl\": {\n        \"query\": \"Convert this example into vector to look for useful examples: \",\n        \"key\": \"Convert this example into vector for retrieval: \",\n    },\n    \"chat\": {\n        \"query\": \"Embed this dialogue to find useful historical dialogues: \",\n        \"key\": \"Embed this historical dialogue for retrieval: \",\n    },\n    \"lrlm\": {\n        \"query\": \"Embed this text chunk for finding useful historical chunks: \",\n        \"key\": \"Embed this historical text chunk for retrieval: \",\n    },\n    \"tool\": {\n        \"query\": \"Transform this user request for fetching helpful tool descriptions: \",\n        \"key\": \"Transform this tool description for retrieval: \",\n    },\n    \"convsearch\": {\n        \"query\": \"Encode this query and context for searching relevant passages: \",\n        \"key\": \"Encode this passage for retrieval: \",\n    },\n}\n\n\nclass llm_embedder:\n    def __init__(self, toolkit_config,device):\n        self.embedder_model = AutoModel.from_pretrained(toolkit_config.embedder_path).to(device)\n        self.embedder_tokenizer = AutoTokenizer.from_pretrained(toolkit_config.embedder_path)\n        self.instruction = INSTRUCTIONS[toolkit_config.task]\n    \n    def to(self,device):\n        print(f\"Move Retriever from {self.embedder_model.device} to {device}\")\n        self.embedder_model.to(device)\n\n    def get_embeddings(self, examples: List[str], mode: Literal[\"query\", \"key\"]):\n        examples = [self.instruction[mode] + example for example in examples]\n        inputs = self.embedder_tokenizer(examples, padding=True, truncation=True, return_tensors=\"pt\")\n        inputs = {name: tensor.to(self.embedder_model.device) for name, tensor in inputs.items()}\n        with torch.no_grad():\n            outputs = self.embedder_model(**inputs)\n            embeddings = outputs.last_hidden_state[:, 0]\n            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=-1)\n            # normalize_L2(embeddings)\n        return embeddings\n\n    def get_max_seq_length(self):\n        return 512\n\nclass st_embedder:\n    def __init__(self, toolkit_config):\n        self.embedder_model = SentenceTransformer(toolkit_config.embedder_path, device=toolkit_config.device)\n        self.max_seq_length = self.embedder_model.max_seq_length\n\n    def to(self,device):\n        print(f\"Move Retriever from {self.embedder_model.device} to {device}\")\n        self.embedder_model.to(device)\n    \n    def get_embeddings(self, examples: List[str], mode: Literal[\"query\", \"key\"]):\n        embeddings = self.embedder_model.encode(examples,convert_to_tensor=True,normalize_embeddings=True,show_progress_bar=False,)\n        # normalize_L2(embeddings)\n        return embeddings\n\n    def get_max_seq_length(self):\n        return self.max_seq_length\n\nclass bge_embedder:\n    def __init__(self,toolkit_config,device) -> None:\n        self.embedder_model = BGEM3FlagModel(toolkit_config.embedder_path, use_fp16=True,device=device)   \n        self.max_seq_length = 8192\n    \n    def to(self,device):\n        print(f\"Move Retriever from {self.embedder_model.device} to {device}\")\n        self.embedder_model.device = device\n        self.embedder_model.model.to(device)\n    \n    def get_embeddings(self, examples: List[str], mode=None):\n        embeddings = self.embedder_model.encode(examples,max_length=self.max_seq_length)['dense_vecs']\n        return torch.from_numpy(embeddings).to(self.embedder_model.device)\n    \n    def get_max_seq_length(self):\n        return self.max_seq_length\n        \n"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/modeling_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch LLaMA model.\"\"\"\n\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\nfrom .configuration_llama import LlamaConfig\n\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (\n                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n            )\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat(\n                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n            )\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [\n                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n            ]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        \n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        output_attentions = False\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n        if self.layer_idx == 0:\n            breakpoint()\n        attn_output = self._flash_attention_forward(\n            query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n        is_causal = True if causal_mask is None and q_len > 1 else False\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": LlamaAttention,\n    \"flash_attention_2\": LlamaFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n    _supports_static_cache = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList(\n            [LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n        )\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        \n        \n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        return_legacy_cache = False\n        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n            return_legacy_cache = True\n            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n        causal_mask = self._update_causal_mask(\n            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n        )\n\n        # embed positions\n        hidden_states = inputs_embeds\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                \n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                )\n            \n            if input_ids[0][-1] == 32000:\n                pass\n                #breakpoint()\n                \n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n        \n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if return_legacy_cache:\n            next_cache = next_cache.to_legacy_cache()\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n        # to infer the attention mask.\n        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n        using_static_cache = isinstance(past_key_values, StaticCache)\n\n        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                attention_mask,\n                inputs_embeds=input_tensor,\n                past_key_values_length=past_seen_tokens,\n                is_training=self.training,\n            ):\n                return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        min_dtype = torch.finfo(dtype).min\n        sequence_length = input_tensor.shape[1]\n        if using_static_cache:\n            target_length = past_key_values.get_max_length()\n        else:\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else past_seen_tokens + sequence_length + 1\n            )\n\n        if attention_mask is not None and attention_mask.dim() == 4:\n            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n            if attention_mask.max() != 0:\n                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n            causal_mask = attention_mask\n        else:\n            causal_mask = torch.full(\n                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n            )\n            if sequence_length != 1:\n                causal_mask = torch.triu(causal_mask, diagonal=1)\n            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n            if attention_mask is not None:\n                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                padding_mask = padding_mask == 0\n                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                    padding_mask, min_dtype\n                )\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n            and not output_attentions\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        \n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n        \n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        cache_position=None,\n        use_cache=True,\n        **kwargs,\n    ):\n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\n                max_cache_length = (\n                    torch.tensor(past_key_values.get_max_length(), device=input_ids.device)\n                    if past_key_values.get_max_length() is not None\n                    else None\n                )\n                cache_length = past_length if max_cache_length is None else torch.min(max_cache_length, past_length)\n            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                input_ids = input_ids[:, past_length:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if (\n                max_cache_length is not None\n                and attention_mask is not None\n                and cache_length + input_ids.shape[1] > max_cache_length\n            ):\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -input_ids.shape[1] :]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        if cache_position is None:\n            cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        elif use_cache:\n            cache_position = cache_position[-input_length:]\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": use_cache,\n                \"attention_mask\": attention_mask,\n            }\n        )\n        \n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n                sequence_lengths = sequence_lengths.to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\nThe Llama Model transformer with a span classification head on top for extractive question-answering tasks like\nSQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForQuestionAnswering(LlamaPreTrainedModel):\n    base_model_prefix = \"transformer\"\n\n    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n    def __init__(self, config):\n        super().__init__(config)\n        self.transformer = LlamaModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.transformer.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.transformer.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        start_positions: Optional[torch.LongTensor] = None,\n        end_positions: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n        r\"\"\"\n        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n            are not taken into account for computing the loss.\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1).contiguous()\n        end_logits = end_logits.squeeze(-1).contiguous()\n\n        total_loss = None\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1).to(start_logits.device)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1).to(end_logits.device)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions = start_positions.clamp(0, ignored_index)\n            end_positions = end_positions.clamp(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n        if not return_dict:\n            output = (start_logits, end_logits) + outputs[2:]\n            return ((total_loss,) + output) if total_loss is not None else output\n\n        return QuestionAnsweringModelOutput(\n            loss=total_loss,\n            start_logits=start_logits,\n            end_logits=end_logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"\n    The Llama Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n    output) e.g. for Named-Entity-Recognition (NER) tasks.\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForTokenClassification(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        if getattr(config, \"classifier_dropout\", None) is not None:\n            classifier_dropout = config.classifier_dropout\n        elif getattr(config, \"hidden_dropout\", None) is not None:\n            classifier_dropout = config.hidden_dropout\n        else:\n            classifier_dropout = 0.1\n        self.dropout = nn.Dropout(classifier_dropout)\n        self.score = nn.Linear(config.hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.score(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )"}
{"type": "source_file", "path": "instruction_finetuning/data_processing.py", "content": "import copy\nimport json\nimport logging\nimport re\nimport sys\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Union\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\nfrom transformers import PreTrainedTokenizer\nimport functools\nfrom .arguments import *\n\nLOGGER = logging.Logger(\"Data Processing\", level=logging.INFO)\nLOGGER_HANDLER = logging.StreamHandler(sys.stderr)\nLOGGER_HANDLER.setFormatter(logging.Formatter(\"[%(asctime)s] Fine-Tuning [%(levelname)s] : %(message)s\"))\nLOGGER.addHandler(LOGGER_HANDLER)\nIGNORE_INDEX = -100  # pytorch cross_entropy loss ignores tokens with this id\n\n\n# note how we choose the padding token\n# later attention mask will be applied to all padding tokens\ndef get_padding_token(tokenizer: PreTrainedTokenizer):\n    if hasattr(tokenizer, \"pad_token_id\") and tokenizer.pad_token_id is not None:\n        return tokenizer.pad_token_id\n    else:\n        return tokenizer.unk_token_id\n\n\ndef handle_data_padding(\n    token_arrays: List[np.array], padding_ids: List[int], tokenization_args: TokenizationArgs\n) -> List[np.array]:\n    \"\"\"\n    For padding the data equally.\n    Applies padding only if tokenization_args.always_pad is true.\n    Supports two modes of padding:\n      * default - add pad tokens on the right side\n      * random - sample how many will be added to the right and how many will be added to the left\n    Args:\n      token_arrays - should be a list of 1D numpy arrays of the same length (for example input_ids and labels)\n      padding_ids - values to use for padding\n      tokenization_args -\n        tokenization_args.always_pad - whether to pad\n        tokenization_args.max_total_length - length to pad to (assumes the input is not longer)\n        tokenization_args.random_pad - whether to use random or default padding mode\n    \"\"\"\n\n    if len(token_arrays) != len(padding_ids):\n        raise ValueError(\"Number of paddings_ids should match number of token_arrays\")\n\n    for ta in token_arrays:\n        if len(ta.shape) != 1:\n            raise ValueError(\"token_arrays should be 1D\")\n        if ta.shape != token_arrays[0].shape:\n            raise ValueError(\"token_arrays should have the same length\")\n\n    if tokenization_args.always_pad:\n        padding = tokenization_args.max_total_length - token_arrays[0].shape[-1]\n        if padding > 0:\n            if tokenization_args.random_pad:\n                padding_left = np.random.randint(0, padding + 1)\n                padding_right = padding - padding_left\n            else:\n                padding_left = 0\n                padding_right = padding\n\n            result = []\n            for ta, pi in zip(token_arrays, padding_ids):\n                result.append(\n                    np.pad(\n                        ta,\n                        ((padding_left, padding_right),),\n                        \"constant\",\n                        constant_values=pi,\n                    )\n                )\n            token_arrays = result\n\n    return token_arrays\n\n\ndef tokenize_text_no_special_tokens(text: str, tokenizer: PreTrainedTokenizer) -> np.array:\n    if not isinstance(text, str):\n        raise ValueError(f\"Expected string got {text}\")\n    return tokenizer.encode(text, add_special_tokens=False, return_tensors=\"np\")[0].astype(np.int64)\n\n\ndef inst_tuning_data_processor(\n    data_args: DataArgs, data, tokenizer: PreTrainedTokenizer, tokenization_args: TokenizationArgs\n) -> Dict[str, np.array]:\n    def prepare_input_text(\n        pre_prompt_text: str,\n        prompt_field: Optional[str],\n        post_prompt_text: str,\n        pre_question_text: str,\n        question_field: str,\n        post_question_text: str,\n        pre_response_text: str,\n        response_field: str,\n        post_response_text: str,\n        data_dict: Dict[str, str],\n    ):\n        input_data = []\n\n        if prompt_field is not None:\n            input_data.append(pre_prompt_text)\n            input_data.append(data_dict[prompt_field])\n            input_data.append(post_prompt_text)\n\n        if question_field is None:\n            raise ValueError(\"For insturction fine-tuning question_field is required\")\n\n        input_data.append(pre_question_text)\n        input_data.append(data_dict[question_field])\n        input_data.append(post_question_text)\n\n        input_text = \"\".join(input_data)\n\n        if response_field is None:\n            raise ValueError(\"For insturction fine-tuning response_field is required\")\n\n        response_text = \"\".join([pre_response_text, data_dict[response_field], post_response_text])\n\n        return input_text, response_text\n\n    def tokenize_data(\n        input_response: List[Tuple[str, str]],\n        tokenizer: PreTrainedTokenizer,\n        tokenization_args: TokenizationArgs,\n    ) -> List[Dict[str, np.array]]:\n        def tokenize_one(input_text: str, response_text: str):\n            input_tokens = tokenize_text_no_special_tokens(text=input_text, tokenizer=tokenizer)\n            input_tokens = np.pad(input_tokens, ((1, 0),), \"constant\", constant_values=tokenizer.bos_token_id)\n\n            input_tokens = input_tokens[: tokenization_args.max_input_length]\n\n            response_tokens = tokenize_text_no_special_tokens(text=response_text, tokenizer=tokenizer)\n            response_tokens = np.pad(response_tokens, ((0, 1),), \"constant\", constant_values=tokenizer.eos_token_id)\n            response_tokens = response_tokens[: tokenization_args.max_output_length]\n\n            assert len(input_tokens.shape) == 1 and len(response_tokens.shape) == 1\n            all_tokens = np.concatenate([input_tokens, response_tokens], axis=-1)\n\n            labels = np.concatenate([np.full_like(input_tokens, IGNORE_INDEX), response_tokens])\n\n            assert labels.shape == all_tokens.shape\n            assert len(labels.shape) == 1\n\n            all_tokens = all_tokens[: tokenization_args.max_total_length]\n            labels = labels[: tokenization_args.max_total_length]\n\n            all_tokens, labels = handle_data_padding(\n                token_arrays=[all_tokens, labels],\n                padding_ids=[get_padding_token(tokenizer), IGNORE_INDEX],\n                tokenization_args=tokenization_args,\n            )\n\n            attention_mask = all_tokens != get_padding_token(tokenizer)\n\n            return all_tokens, labels, attention_mask\n\n        def tokenize_portion(input_response_portion: List[Tuple[str, str]]) -> List[Dict[str, np.array]]:\n            result = []\n            for input_text, response_text in input_response_portion:\n                all_tokens, labels, attention_mask = tokenize_one(input_text, response_text)\n                result.append(dict(input_ids=all_tokens, labels=labels, attention_mask=attention_mask))\n\n            return result\n\n        return tokenize_portion(input_response)\n\n    input_response = prepare_input_text(\n        pre_prompt_text=data_args.pre_prompt_text,\n        prompt_field=data_args.prompt_field,\n        post_prompt_text=data_args.post_prompt_text,\n        pre_question_text=data_args.pre_question_text,\n        question_field=data_args.question_field,\n        post_question_text=data_args.post_question_text,\n        pre_response_text=data_args.pre_response_text,\n        response_field=data_args.response_field,\n        post_response_text=data_args.post_response_text,\n        data_dict=data,\n    )\n\n    elem = tokenize_data([input_response], tokenizer=tokenizer, tokenization_args=tokenization_args)[0]\n\n    return elem\n\n\ndef chat_tuning_data_processor(\n    data_args: DataArgs, data, tokenizer: PreTrainedTokenizer, tokenization_args: TokenizationArgs\n) -> Dict[str, np.array]:\n    input_ids = []\n    labels = []\n    data = data[data_args.chat_conversations_field]\n\n    model_prefix = tokenize_text_no_special_tokens(text=data_args.chat_model_response_prefix, tokenizer=tokenizer)\n    human_prefix = tokenize_text_no_special_tokens(text=data_args.chat_human_response_prefix, tokenizer=tokenizer)\n\n    input_ids.append(tokenize_text_no_special_tokens(data_args.chat_initial_prompt, tokenizer=tokenizer))\n    labels.append(np.full_like(input_ids[0], IGNORE_INDEX))\n\n    replace_rules_raw_list = (\n        data_args.chat_replace_rules.split(\"<;>\") if data_args.chat_replace_rules is not None else []\n    )\n    replace_rules_list = []\n    for replace_rule in replace_rules_raw_list:\n        regex, target = replace_rule.split(\"<R>\")\n        LOGGER.debug(f\"Chat: Compiling regex {regex} for replacing with {target}\")\n        regex = re.compile(regex, flags=re.DOTALL)\n        replace_rules_list.append((regex, target))\n\n    LOGGER.debug(f\"Chat: Compiled {len(replace_rules_list)} replace rules\")\n\n    def handle_replacements(text: str) -> str:\n        for regex, target in replace_rules_list:\n            text = regex.sub(target, text)\n        return text\n\n    for part in data:\n        text = part[data_args.chat_data_field]\n\n        text = handle_replacements(text)\n\n        tokenized_text = tokenize_text_no_special_tokens(text=text, tokenizer=tokenizer)\n\n        is_model = part[data_args.chat_source_name_field] == data_args.chat_model_source_name\n        if is_model:\n            token_prefix = model_prefix\n            token_sufix = np.array([tokenizer.eos_token_id])\n        else:\n            token_prefix = human_prefix\n            token_sufix = np.empty(0, dtype=np.int64)\n\n        input_ids.append(token_prefix)\n        input_ids.append(tokenized_text)\n        input_ids.append(token_sufix)\n\n        labels.append(np.full_like(token_prefix, IGNORE_INDEX))\n        if is_model:\n            labels.append(tokenized_text)\n            labels.append(token_sufix)\n        else:\n            labels.append(np.full_like(tokenized_text, IGNORE_INDEX))\n            labels.append(np.full_like(token_sufix, IGNORE_INDEX))\n\n    input_ids = [np.array([tokenizer.bos_token_id])] + input_ids\n    input_ids = np.concatenate(input_ids, axis=-1)\n    labels = [np.array([IGNORE_INDEX])] + labels\n    labels = np.concatenate(labels, axis=-1)\n\n    assert input_ids.shape == labels.shape\n    assert len(input_ids.shape) == 1\n\n    input_ids = input_ids[: tokenization_args.max_total_length]\n    labels = labels[: tokenization_args.max_total_length]\n\n    input_ids, labels = handle_data_padding(\n        token_arrays=[input_ids, labels],\n        padding_ids=[get_padding_token(tokenizer), IGNORE_INDEX],\n        tokenization_args=tokenization_args,\n    )\n\n    attention_mask = input_ids != get_padding_token(tokenizer)\n    attention_mask = np.logical_and(attention_mask, input_ids != tokenizer.eos_token_id)\n\n    result = dict(input_ids=input_ids.astype(np.int64), labels=labels.astype(np.int64), attention_mask=attention_mask)\n    return result\n\n\ndef get_data_processor(data_args: DataArgs):\n    if data_args.data_type == \"instructions\":\n        return inst_tuning_data_processor\n    elif data_args.data_type == \"chat\":\n        return chat_tuning_data_processor\n\n\ndef separate_data_args(data_args: DataArgs) -> List[DataArgs]:\n    \"\"\"\n    Given the data_args creates a separate instance for each dataset.\n    \"\"\"\n\n    data_types = data_args.data_type.split(\",\")\n    num_datasets = len(data_types)\n    data_paths = data_args.data_path.split(\",\")\n    revisions = data_args.data_revision.split(\",\")\n    data_splits = data_args.dataset_split.split(\",\")\n\n    def split_field(field: Optional[str], dataset_type: str, separator: str, process_field_name_fn):\n        if field is None:\n            return [None] * num_datasets\n        else:\n            field_list = field.split(separator)\n            if len(field_list) == 1:\n                LOGGER.info(f\"Broadcastin used for {dataset_type} fields {field_list}.\")\n                field_list_getter = lambda _: field_list[0]\n            else:\n                field_list_getter = lambda x: field_list[x]\n            result = []\n            appended = 0\n            for dt in data_types:\n                if dt == dataset_type:\n                    converted_field_data = process_field_name_fn(field_list_getter(appended))\n                    result.append(converted_field_data)\n                    appended += 1\n                else:\n                    result.append(None)\n            return result\n\n    def none_str_to_none(x):\n        if x == \"None\":\n            return None\n        else:\n            return x\n\n    split_basic_instruct_field = functools.partial(\n        split_field, dataset_type=\"instructions\", separator=\",\", process_field_name_fn=none_str_to_none\n    )\n\n    prompt_fields = split_basic_instruct_field(data_args.prompt_field)\n    question_fields = split_basic_instruct_field(data_args.question_field)\n    response_fields = split_basic_instruct_field(data_args.response_field)\n\n    split_adv_instruct_field = functools.partial(\n        split_field, dataset_type=\"instructions\", separator=\"<,>\", process_field_name_fn=lambda x: x\n    )\n\n    pre_prompt_texts = split_adv_instruct_field(data_args.pre_prompt_text)\n    post_prompt_texts = split_adv_instruct_field(data_args.post_prompt_text)\n\n    pre_question_texts = split_adv_instruct_field(data_args.pre_question_text)\n    post_question_texts = split_adv_instruct_field(data_args.post_question_text)\n\n    pre_response_texts = split_adv_instruct_field(data_args.pre_response_text)\n    post_response_texts = split_adv_instruct_field(data_args.post_response_text)\n\n    split_basic_chat_field = functools.partial(\n        split_field, dataset_type=\"chat\", separator=\",\", process_field_name_fn=none_str_to_none\n    )\n\n    chat_conversations_fields = split_basic_chat_field(data_args.chat_conversations_field)\n    chat_data_fields = split_basic_chat_field(data_args.chat_data_field)\n    chat_source_name_fields = split_basic_chat_field(data_args.chat_source_name_field)\n    chat_model_source_names = split_basic_chat_field(data_args.chat_model_source_name)\n\n    if data_args.data_filter is None:\n        data_filters = [None for _ in range(num_datasets)]\n    else:\n        data_filters = data_args.data_filter.split(\"<,>\")\n\n    data_proportions = data_args.data_proportions\n\n    LOGGER.info(\n        \"Praparing configs:\\n\"\n        f\"data_types : {data_types}\\n\"\n        f\"data_paths : {data_paths}\\n\"\n        f\"revisions : {revisions}\\n\"\n        f\"data_splits : {data_splits}\\n\"\n        f\"data_proportions : {data_proportions}\\n\"\n        f\"data_filters : {data_filters}\\n\"\n    )\n\n    if (\n        num_datasets != len(data_paths)\n        or num_datasets != len(revisions)\n        or num_datasets != len(data_splits)\n        or num_datasets != len(data_proportions)\n        or num_datasets != len(data_filters)\n    ):\n        raise ValueError(\n            \"When preparing the mixture provide the same number of elements in: \"\n            \"data_path, data_revision, data_type, data_proportions (separated by ','), data_filters (separated by <,>)\"\n        )\n\n    splitted_data_args = []\n    for d_id in range(num_datasets):\n        d_args = copy.deepcopy(data_args)\n        d_args.data_type = data_types[d_id]\n        d_args.data_path = data_paths[d_id]\n        d_args.data_revision = revisions[d_id]\n        d_args.dataset_split = data_splits[d_id]\n        d_args.data_proportions = data_proportions[d_id]\n        d_args.data_filter = data_filters[d_id]\n\n        d_args.prompt_field = prompt_fields[d_id]\n        d_args.question_field = question_fields[d_id]\n        d_args.response_field = response_fields[d_id]\n\n        d_args.pre_prompt_text = pre_prompt_texts[d_id]\n        d_args.post_prompt_text = post_prompt_texts[d_id]\n\n        d_args.pre_question_text = pre_question_texts[d_id]\n        d_args.post_question_text = post_question_texts[d_id]\n\n        d_args.pre_response_text = pre_response_texts[d_id]\n        d_args.post_response_text = post_response_texts[d_id]\n\n        d_args.chat_conversations_field = chat_conversations_fields[d_id]\n        d_args.chat_data_field = chat_data_fields[d_id]\n        d_args.chat_source_name_field = chat_source_name_fields[d_id]\n        d_args.chat_model_source_name = chat_model_source_names[d_id]\n        splitted_data_args.append(d_args)\n\n    return splitted_data_args\n\n\ndef filter_dataset(dataset, data_args: DataArgs, tokenizer: PreTrainedTokenizer):\n    \"\"\"\n    For filtering the dataset according to the rules described in data_args.\n    data_args should be separated using separate_data_args.\n    \"\"\"\n\n    if data_args.data_filter is not None and data_args.data_filter != \"\":\n        match_mode = \"<M>\"\n        lenlt_mode = \"<LENLT>\"\n        lengt_mode = \"<LENGT>\"\n        toklt_mode = \"<TOKLT>\"\n        tokgt_mode = \"<TOKGT>\"\n\n        all_modes = [match_mode, lenlt_mode, lengt_mode, toklt_mode, tokgt_mode]\n\n        raw_rules = data_args.data_filter.split(\"<;>\")\n        rules = []\n        for rr in raw_rules:\n            mode_matched = False\n            for mode in all_modes:\n                if mode in rr:\n                    if mode_matched:\n                        raise ValueError(\"Only one mode can be matched\")\n                    the_trigger = mode\n                    mode_matched = True\n\n            if not mode_matched:\n                raise ValueError(f\"In {rr} at lest one mode must be matched. Modes: {all_modes}\")\n\n            field, regex = rr.split(the_trigger)\n\n            LOGGER.info(f\"Dataset: Compiling {regex} for mode {the_trigger} with field {field}\")\n            regex = re.compile(regex, flags=re.DOTALL) if the_trigger == match_mode else int(regex)\n            rules.append((field, regex, the_trigger))\n\n        def filtering(x: Dict[str, Any]):\n            int_trigger = \"<int>\"\n\n            def recursive_check_match(regex, v: Union[Dict[str, Any], str], field_nesting: List[str]):\n                if len(field_nesting) == 0:\n                    assert isinstance(v, str)\n                    return regex.match(v) is not None\n\n                fn = field_nesting[0]\n                if fn.startswith(int_trigger):\n                    fn = fn[len(int_trigger) :]\n                    if fn == \"*v\" or fn == \"*^\":\n                        and_mode = fn == \"*^\"\n                        or_mode = fn == \"*v\"\n                        for elem in v:\n                            result = recursive_check_match(regex, elem, field_nesting[1:])\n                            if result and or_mode:\n                                return True\n                            elif (not result) and and_mode:\n                                return False\n                        return and_mode\n                    else:\n                        fn = int(fn)\n                        return recursive_check_match(regex, v[fn], field_nesting[1:])\n                else:\n                    return recursive_check_match(regex, v[fn], field_nesting[1:])\n\n            def recursive_count(\n                processor: Callable[[str], int], v: Union[Dict[str, Any], str], field_nesting: List[str]\n            ):\n                if len(field_nesting) == 0:\n                    assert isinstance(v, str)\n                    return processor(v)\n                fn = field_nesting[0]\n\n                if fn.startswith(int_trigger):\n                    fn = fn[len(int_trigger) :]\n                    if fn == \"*\":\n                        result = 0\n                        for elem in v:\n                            result += recursive_count(processor, elem, field_nesting[1:])\n                        return result\n                    else:\n                        fn = int(fn)\n                        return recursive_count(processor, v[fn], field_nesting[1:])\n                else:\n                    return recursive_count(processor, v[fn], field_nesting[1:])\n\n            for field, regex, mode in rules:\n                field_nesting = field.split(\".\")\n                if mode == match_mode:\n                    if not recursive_check_match(regex, x, field_nesting):\n                        return False\n                elif mode == lenlt_mode:\n                    if recursive_count(lambda v: len(v), x, field_nesting) >= regex:\n                        return False\n                elif mode == lengt_mode:\n                    if recursive_count(lambda v: len(v), x, field_nesting) <= regex:\n                        return False\n                elif mode == toklt_mode:\n                    if (\n                        recursive_count(\n                            lambda v: tokenizer.encode(v, add_special_tokens=False, return_tensors=\"np\").shape[-1],\n                            x,\n                            field_nesting,\n                        )\n                        >= regex\n                    ):\n                        return False\n                elif mode == tokgt_mode:\n                    if (\n                        recursive_count(\n                            lambda v: tokenizer.encode(v, add_special_tokens=False, return_tensors=\"np\").shape[-1],\n                            x,\n                            field_nesting,\n                        )\n                        <= regex\n                    ):\n                        return False\n\n            return True\n\n        LOGGER.info(f\"Dataset: Compiling {len(rules)} filtering rules\")\n\n        return dataset.filter(filtering)\n    else:\n        return dataset\n\n\nclass SingleTuneDataset(Dataset):\n    \"\"\"\n    For handling a single dataset.\n    data_args should be separated using separate_data_args.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_args: DataArgs,\n        tokenizer: PreTrainedTokenizer,\n        tokenization_args: TokenizationArgs,\n        return_pt: bool = True,\n        data_processor=None,\n    ):\n        super().__init__()\n\n        self.data_args = data_args\n        self.tokenizer = tokenizer\n        self.tokenization_args = tokenization_args\n        self.return_pt = return_pt\n        self.data_processor = data_processor if data_processor is not None else get_data_processor(data_args=data_args)\n\n        LOGGER.info(\n            f\"Loading data from {self.data_args.data_path}, revision {self.data_args.data_revision}, split {self.data_args.dataset_split}\"\n        )\n        if self.data_args.data_path is None:\n            raise ValueError(\"No dataset (data_path) specified\")\n        raw_dataset = load_dataset(\n            self.data_args.data_path, revision=self.data_args.data_revision, split=self.data_args.dataset_split\n        )\n\n        raw_dataset = filter_dataset(dataset=raw_dataset, data_args=data_args, tokenizer=tokenizer)\n\n        self.raw_data = raw_dataset\n        self.length = len(self.raw_data)\n        LOGGER.info(f\"Single dataset size is {self.length}\")\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        raw_data = self.raw_data[i]\n        elem = self.data_processor(\n            data_args=self.data_args, data=raw_data, tokenizer=self.tokenizer, tokenization_args=self.tokenization_args\n        )\n\n        if self.return_pt:\n            converter = lambda x: torch.from_numpy(x).to(torch.long)\n            converter_mask = lambda x: torch.from_numpy(x).to(torch.bool)\n        else:\n            converter = lambda x: x\n            converter_mask = lambda x: x\n        return dict(\n            input_ids=converter(elem[\"input_ids\"]),\n            labels=converter(elem[\"labels\"]),\n            attention_mask=converter_mask(elem[\"attention_mask\"]),\n        )\n\n\n@dataclass\nclass DatasetProcessingStats:\n    ds_name: str\n    padding_token_id: int = 0\n    processed_tokens: int = 0\n    processed_loss_tokens: int = 0\n    number_of_get_calls: int = 0\n\n    def update(self, data: Dict[str, Any]):\n        def convert_to_numpy(x):\n            if isinstance(x, np.ndarray):\n                return x\n            elif isinstance(x, torch.tensor):\n                return x.numpy()\n            else:\n                raise ValueError(\"DatasetProcessingStats: Type not supported\")\n\n        input_ids = convert_to_numpy(data[\"input_ids\"])\n        labels = convert_to_numpy(data[\"labels\"])\n\n        self.processed_tokens += (\n            np.logical_and(input_ids != self.padding_token_id, input_ids != IGNORE_INDEX).astype(np.int64).sum().item()\n        )\n        self.processed_loss_tokens += (\n            np.logical_and(labels != self.padding_token_id, labels != IGNORE_INDEX).astype(np.int64).sum().item()\n        )\n        self.number_of_get_calls += 1\n\n\ndef show_data_stats(data_stats: List[DatasetProcessingStats]):\n    total_proc_tokens = 0\n    total_proc_loss_tokens = 0\n    total_get_calls = 0\n    for ds in data_stats:\n        total_proc_tokens += ds.processed_tokens\n        total_proc_loss_tokens += ds.processed_loss_tokens\n        total_get_calls += ds.number_of_get_calls\n\n    result = {}\n    for ds in data_stats:\n        result[ds.ds_name] = {\n            \"processed_tokens\": ds.processed_tokens,\n            \"processed_tokens%\": 100.0 * ds.processed_tokens / max(total_proc_tokens, 1),\n            \"processed_loss_tokens\": ds.processed_loss_tokens,\n            \"processed_loss_tokens%\": 100.0 * ds.processed_loss_tokens / max(total_proc_loss_tokens, 1),\n            \"get_calls\": ds.number_of_get_calls,\n            \"get_calls%\": ds.number_of_get_calls / max(total_get_calls, 1),\n        }\n\n    return json.dumps(result, indent=2)\n\n\nclass MixedTuneDataset(Dataset):\n    def __init__(\n        self,\n        data_args: DataArgs,\n        tokenizer: PreTrainedTokenizer,\n        tokenization_args: TokenizationArgs,\n        return_pt: bool = True,\n        mix_seed=42,\n        log_stats=False,\n    ):\n        self.org_data_args = data_args\n        self.tokenizer = tokenizer\n        self.tokenization_args = tokenization_args\n        self.return_pt = return_pt\n\n        data_args = separate_data_args(data_args)\n        LOGGER.info(f\"Will mix {len(data_args)} datasets\")\n\n        self.datasets = []\n        total_length = 0\n        for da in data_args:\n            LOGGER.info(f\"Creating dataset with config: {da}\")\n            ds = SingleTuneDataset(\n                data_args=da, tokenizer=tokenizer, tokenization_args=tokenization_args, return_pt=return_pt\n            )\n            self.datasets.append(ds)\n            total_length += len(ds)\n\n        self.total_length = total_length\n        LOGGER.info(f\"Cumulative dataset size is {self.total_length}\")\n\n        if log_stats:\n            self.per_dataset_stats = [\n                DatasetProcessingStats(\n                    ds_name=f\"ds{ds_id}={ds.data_args.data_path}\",\n                    padding_token_id=get_padding_token(tokenizer=tokenizer),\n                )\n                for ds_id, ds in enumerate(self.datasets)\n            ]\n        else:\n            self.per_dataset_stats = None\n\n        ds_indices = np.arange(len(self.datasets), dtype=np.int32)\n        if len(ds_indices) > 1:\n            rnd_state = np.random.get_state()\n            np.random.seed(mix_seed)\n            self.mapping = np.random.choice(\n                ds_indices, self.total_length, replace=True, p=self.org_data_args.data_proportions\n            )\n            np.random.set_state(rnd_state)\n        else:\n            self.mapping = np.zeros(self.total_length, dtype=np.int32)\n\n    def __len__(self):\n        return self.total_length\n\n    def __getitem__(self, i):\n        # Not perfect but simple solution\n        ds_id = self.mapping[i]\n        ds = self.datasets[ds_id]\n        i = i % len(ds)\n        result = ds[i]\n\n        if self.per_dataset_stats is not None:\n            self.per_dataset_stats[ds_id].update(result)\n            LOGGER.info(show_data_stats(self.per_dataset_stats))\n\n        return result\n\n\nclass DataCollator:\n    tokenizer: PreTrainedTokenizer\n\n    def __init__(self, tokenizer: PreTrainedTokenizer,use_toolkit:bool):\n        self.tokenizer = tokenizer\n        self.use_toolkit = use_toolkit\n\n    def __call__(self, inputs):\n        input_ids, labels, attention_masks = [], [], []\n        for elem in inputs:\n            input_ids.append(elem[\"input_ids\"])\n            labels.append(elem[\"labels\"])\n            attention_masks.append(elem[\"attention_mask\"])\n\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids, batch_first=True, padding_value=get_padding_token(self.tokenizer)\n        )\n\n        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n        attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=False)\n\n        return dict(input_ids=input_ids, labels=labels, attention_mask=attention_masks,use_toolkit=self.use_toolkit)"}
{"type": "source_file", "path": "src/align_memory.py", "content": "import numpy as np\nimport faiss\nimport faiss.contrib.torch_utils\nimport torch\nfrom typing import Literal, Tuple, List, Optional, Union\nfrom .utils import MemCache\nfrom dataclasses import dataclass\n\n\n\"\"\"\n记忆与存储机制\n** Memory **\n- 维护一个固定大小的记忆单元，用于存储历史信息\n- 利用 save 模块来更新记忆\n- 每次传入的记忆单元是一个 mem_update , 其包含了长度为不固定的 SigMemCache 对象\n- save 模块需要完成的操作包括：将SigMemCache对象拆解为文本，检索嵌入，Key-Value-Mask三元组；利用拆解对象，维护一个MemCache用于之后的检索\n** Retrieval **\n- 利用 retrieve 模块来检索记忆\n- 记忆长度分析：如果 bsz 间长度不一致，则用 PAD 填充\n\"\"\"\n\n@dataclass\nclass MemRecoder:\n    counters:List[List[int]]\n    dstore_idx:List[int]\n    length:List[List[int]]\n\nclass ChunkMemory:\n    def __init__(self, model_config, toolkit_config, device=None):\n        # llama config\n        self.hidden_size = model_config.hidden_size\n        self.num_heads = model_config.num_attention_heads\n        self.head_dim = int(self.hidden_size / self.num_heads)\n        self.memory_size = model_config.memory_size\n        self.pooling_tokens = model_config.pooling_tokens\n        self.dtype = model_config.mem_dtype\n        boundary = model_config.update_boundary\n        # toolkit config\n        self.task = toolkit_config.task\n        self.ret_embeddings_dim = toolkit_config.ret_embeddings_dim\n        # other config\n        if boundary is not None:\n            self.lower, self.upper = list(map(int, boundary.split(\",\")))\n        else:\n            self.lower , self.upper = 0 , 100\n        assert self.lower < self.upper and self.lower >= 0 and self.upper <= 100\n        # init\n        self.device = device\n        self.mem_caches = MemCache()\n        self.mem_caches.texts = []\n        self.mem_caches.embeddings = []\n        self.res = None\n        if device != torch.device(\"cpu\"):\n            self.res = faiss.StandardGpuResources()\n            \n        self.mem_recoder = MemRecoder(\n            counters = [],\n            dstore_idx = [],\n            length = []\n        )\n        \n        self.log_time = 1\n\n    def to(self,device):\n        old_device = self.device\n        self.device = device\n        if self.device != torch.device(\"cpu\"):\n            if self.res == None :\n                self.res = faiss.StandardGpuResources() \n            \n            for i,index in enumerate(self.mem_caches.embeddings):\n                print(f\"put index from {old_device} to {self.device}\")\n                if index == torch.device(\"cpu\"):\n                    index = faiss.index_cpu_to_gpu(self.res, self.device.index, index)\n                else:\n                    temp = faiss.index_gpu_to_cpu(index)\n                    index = faiss.index_cpu_to_gpu(self.res, self.device.index, temp)\n                    \n                self.mem_caches.embeddings[i] = index\n        \n        if self.mem_caches.keys is not None:\n            self.mem_caches.keys = self.mem_caches.keys.to(device)\n            self.mem_caches.values = self.mem_caches.values.to(device)\n            self.mem_caches.masks = self.mem_caches.masks.to(device)\n        \n    \n    def reset(self):\n        self.mem_caches = MemCache()\n        self.mem_caches.texts = []\n        self.mem_caches.embeddings = []\n        self.res = None\n        if self.device != torch.device(\"cpu\"):\n            self.res = faiss.StandardGpuResources()      \n        self.mem_recoder = MemRecoder(\n            counters = [],\n            dstore_idx = [],\n            length = []\n        )\n        \n    #DELETE\n    def _fill_and_update(self, kv_list) -> List:\n        device = self.device\n        pooling_list = []\n        for i, kv in enumerate(kv_list):\n            if kv is None:\n                pooling_list.append(None)\n                continue\n            k = kv[\"k\"]\n            v = kv[\"v\"]\n            pad_len = 0\n            if k.shape[0] % self.mem_granularity != 0:\n                pad_len = self.mem_granularity - k.shape[0] % self.mem_granularity\n                # fill\n                k = torch.cat((k,torch.zeros((pad_len, self.num_heads, self.head_dim),device=device,),),dim=0,)\n                v = torch.cat((v,torch.zeros((pad_len, self.num_heads, self.head_dim),device=device,),),dim=0,)\n\n            update_nums = k.shape[0] // self.mem_granularity\n            k = k.view(update_nums, self.mem_granularity, self.num_heads, self.head_dim)\n            v = v.view(update_nums, self.mem_granularity, self.num_heads, self.head_dim)\n            pooling_keys = [torch.zeros((update_nums, self.capacity, self.head_dim),device=device,)for _ in range(self.num_heads)]\n            pooling_values = [torch.zeros(\n(update_nums, self.capacity, self.head_dim),\n                    device=device,\n                )\n                for _ in range(self.num_heads)\n            ]\n            for i in range(self.num_heads):\n                # TODO: optimize\n                k_norm = [\n                    torch.mean(\n                        k[\n                            : update_nums - 1,\n                            num * self.pooling_tokens : (num + 1) * self.pooling_tokens,\n                            i,\n                            :,\n                        ],\n                        dim=1,\n                    )\n                    for num in range(self.capacity)\n                ]\n                k_chg = [\n                    (\n                        torch.mean(\n                            k[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                        if num != self.capacity\n                        else torch.mean(\n                            k[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens\n                                - pad_len,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                    )\n                    for num in range(self.capacity)\n                ]\n                v_norm = [\n                    torch.mean(\n                        v[\n                            : update_nums - 1,\n                            num * self.pooling_tokens : (num + 1) * self.pooling_tokens,\n                            i,\n                            :,\n                        ],\n                        dim=1,\n                    )\n                    for num in range(self.capacity)\n                ]\n                v_chg = [\n                    (\n                        torch.mean(\n                            v[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                        if num != self.capacity\n                        else torch.mean(\n                            v[\n                                update_nums - 1,\n                                num\n                                * self.pooling_tokens : (num + 1)\n                                * self.pooling_tokens\n                                - pad_len,\n                                i,\n                                :,\n                            ],\n                            dim=0,\n                        )\n                    )\n                    for num in range(self.capacity)\n                ]\n\n                merged_k = [\n                    torch.cat((item1, item2.unsqueeze(0)), dim=0)\n                    for item1, item2 in zip(k_norm, k_chg)\n                ]\n                merged_v = [\n                    torch.cat((item1, item2.unsqueeze(0)), dim=0)\n                    for item1, item2 in zip(v_norm, v_chg)\n                ]\n                pooling_keys[i] = torch.stack(\n                    merged_k,\n                    dim=1,\n                )\n                pooling_values[i] = torch.stack(\n                    merged_v,\n                    dim=1,\n                )\n                # add\n            pooling_list.append(\n                {\n                    \"pooling_keys\": torch.stack(pooling_keys, dim=-2),\n                    \"pooling_values\": torch.stack(pooling_values, dim=-2),\n                }\n            )\n        return pooling_list\n\n    def get_key_embeddings(self):\n        return self.ret_embeddings\n\n    def achieve_condition(self, condition=5):\n        for idx in self.dstore_idx:\n            if idx < condition:\n                return False\n        return True\n    \n    def get_min_number(self):\n        return min(self.mem_recoder.dstore_idx) if self.mem_recoder.dstore_idx != [] else 0\n    \n    def save(self,mem_update):\n        if self.log_time>0:\n            print('use mem!!!')\n            self.log_time -= 1\n        # 1. Text\n        texts = mem_update.texts\n        # 2. embeddings\n        embeddings = mem_update.embeddings\n        # 3. key-value-mask\n        keys , values, masks = mem_update.keys , mem_update.values , mem_update.masks\n        \n        if len(keys.shape) != 4 or len(values.shape) != 4 or len(masks.shape) != 4:\n            raise ValueError(f\"Memory cache content should be consistent in shape got {keys.shape} {values.shape} {masks.shape}\")\n        \n        # No pooling\n        if self.pooling_tokens is None:\n            # check overflow\n            if self.mem_caches.keys is not None and self.mem_caches.keys.shape[-2] + keys.shape[-2] > self.memory_size:\n                self.mem_caches.keys = self.mem_caches.keys[...,-self.memory_size//2:,:]\n                self.mem_caches.values = self.mem_caches.values[...,-self.memory_size//2:,:]\n                self.mem_caches.masks = self.mem_caches.masks[...,-self.memory_size//2:,:]\n\n                for i in range(len(self.mem_caches.keys)):\n                    if self.device != torch.device(\"cpu\"):\n                        if self.log_time == 0:\n                            print(\"remove index to cpu\")\n                            self.log_time -= 1\n                        temp = faiss.index_gpu_to_cpu(self.mem_caches.embeddings[i])\n                    else:\n                        if self.log_time == 0:\n                            print(\"remove in cpu\")\n                            self.log_time -= 1\n                        temp = self.mem_caches.embeddings[i]\n                    assert temp.ntotal == len(self.mem_caches.texts[i]) == len(self.mem_recoder.length[i]) == self.mem_recoder.dstore_idx[i]\n                    rm_indices = torch.arange(0,temp.ntotal//2)\n                    rm_total = temp.ntotal // 2\n                    temp.remove_ids(rm_indices.cpu().numpy())\n                    if self.device != torch.device(\"cpu\"):\n                        self.mem_caches.embeddings[i] = faiss.index_cpu_to_gpu(self.res, self.device.index, temp)\n                    else:\n                        self.mem_caches.embeddings[i] = temp\n                    self.mem_caches.texts[i] = self.mem_caches.texts[i][-rm_total:]\n                    self.mem_recoder.length[i] = self.mem_recoder.length[i][-rm_total:]\n                    self.mem_recoder.dstore_idx[i] = self.mem_recoder.dstore_idx[i]//2\n\n            self.mem_caches.keys = torch.cat((self.mem_caches.keys, keys), dim=-2) if self.mem_caches.keys is not None else keys\n            self.mem_caches.values = torch.cat((self.mem_caches.values, values), dim=-2) if self.mem_caches.values is not None else values\n            self.mem_caches.masks = torch.cat((self.mem_caches.masks, masks), dim=-2) if self.mem_caches.masks is not None else masks\n            \n            bsz = len(texts)\n            for i in range(bsz):\n                if len(self.mem_caches.texts) < i + 1:\n                    # init\n                    index = faiss.IndexFlatIP(self.ret_embeddings_dim)\n                    if self.device != torch.device(\"cpu\"):\n                        print(f\"put index {i} from cpu to gpu {self.device}\")\n                        index = faiss.index_cpu_to_gpu(self.res, self.device.index, index)\n                    self.mem_caches.embeddings.append(index)\n                    self.mem_caches.embeddings[i].add(embeddings[i].view(1,-1).to(torch.float32))\n                    self.mem_caches.texts.append(texts[i])\n                    self.mem_recoder.length.append([keys.shape[-2]])\n                    self.mem_recoder.dstore_idx.append(1)\n                else:    \n                    self.mem_caches.texts[i].append(texts[i])\n                    self.mem_caches.embeddings[i].add(embeddings[i].view(1,-1).to(torch.float32))\n                    self.mem_recoder.length[i].append(keys.shape[-2])\n                    self.mem_recoder.dstore_idx[i] += 1\n                \n        else:\n            # TODO: Pooling\n            pass\n    \n    def _expand_index_tensor(self, x):\n        return torch.cat(list(map(lambda x: torch.arange(x * self.div, (x + 1) * self.div), x)))\n\n    def retrieve_index(self, query_embeddings_list, k):\n        return [self.mem_caches.embeddings[i].search(query_embeddings_list[i].to(torch.float32), k) for i in range(len(query_embeddings_list))]    \n        \n    def expand_elements_2d(self,length_list,indices_list):\n        expand_indices = []\n        for length , indices in zip(length_list,indices_list):\n            indices = indices.tolist()[0]\n            tmp = []\n            for i in indices:\n                if i == 0:\n                    tmp.extend(list(range(length[i])))\n                else:\n                    total = sum(length[:i])\n                    tmp.extend(list(range(total,length[i] + total)))\n            expand_indices.append((tmp))\n        return expand_indices\n        \n    def get(self,indices_list:Optional[List[torch.Tensor]]) -> MemCache:\n        indices_list = [torch.sort(indices,dim=1)[0] for indices in indices_list]\n        mem_caches = MemCache()\n        expand_indices = self.expand_elements_2d(self.mem_recoder.length,indices_list)\n        expand_indices = torch.LongTensor(expand_indices).to(self.device)\n        kv_expand_indices = expand_indices.unsqueeze(1).unsqueeze(-1).expand(-1,self.num_heads,-1,self.head_dim)\n        mask_expand_indices = expand_indices.unsqueeze(1).unsqueeze(-1)\n        mem_caches.keys =  torch.gather(self.mem_caches.keys,2,kv_expand_indices)\n        mem_caches.values = torch.gather(self.mem_caches.values,2,kv_expand_indices)\n        mem_caches.masks = torch.gather(self.mem_caches.masks,2,mask_expand_indices)\n        \n        # TODO: ret_counters\n        \n        return mem_caches\n        \n\n"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/utils.py", "content": "from dataclasses import dataclass\nimport faiss\nimport torch\nfrom torch.utils.data import Sampler\nfrom typing import Dict, Iterator, Literal, Iterable, Union, List , Optional\nfrom transformers import AutoTokenizer\nfrom peft import PeftModel, get_peft_model, LoraConfig\nfrom itertools import chain\nimport os\nimport transformers\nfrom .configuration_llama import LlamaConfig\n\nTASK_CHOICE = Literal[\"qa\", \"icl\", \"chat\", \"lrlm\", \"tool\", \"convsearch\"]\n\n\nclass BatchSequentialSampler(Sampler):\n    def __init__(self, data, batch_size, num_process=None) -> None:\n        self.total_len = len(data)\n        # self.data = data[: self.total_len - self.total_len % batch_size]\n        self.batch_size = batch_size\n        self.num_process = num_process\n\n    def __iter__(self) -> Iterator[int]:\n        if self.num_process is None:\n            per_batch_nums = self.total_len // self.batch_size\n            for i in range(per_batch_nums):\n                for j in range(self.batch_size):\n                    yield i + j * per_batch_nums\n        else:\n            per_batch_nums = self.total_len // self.batch_size // self.num_process\n            for i in range(per_batch_nums):\n                for j in range(self.batch_size):\n                    for k in range(self.num_process):\n                        yield i + j * per_batch_nums + k * per_batch_nums * self.batch_size\n\n    def __len__(self,) -> int:\n        return self.total_len\n    \ndef set_freeze_by_idxs(model, idxs: Union[int, List[int]], freeze: bool):\n    if not isinstance(idxs, Iterable):\n        idxs = [idxs]\n\n    if \"llama\" in type(model).__name__.lower():\n        iter_model = list(list(model.children())[0].children())[1]\n        num_child = len(iter_model)\n    elif \"peft\" in type(model).__name__.lower():\n        iter_model = list(list(list(list(model.children())[0].children())[0].children())[0].children())[-1]\n        num_child = len(iter_model)\n    else:\n        iter_model = list(model.children())\n        num_child = len(iter_model)\n    idxs = tuple(map(lambda idx: num_child + idx if idx < 0 else idx, idxs))\n    for idx, child in enumerate(iter_model):\n        if idx not in idxs:\n            continue\n        for name, param in child.named_parameters():\n            if \"peft\" in type(model).__name__.lower():\n                if \"lora\" in name or \"modules_to_save\" in name:\n                    param.requires_grad = not freeze\n                    \ndef group_texts(block_size, examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n    # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\ndef tokenize_fn(tokenizer, seq_len, examples):\n    tokenizer.padding_side = \"right\"\n    text = list(map(lambda x: x + \" \" + tokenizer.eos_token,examples[\"raw_content\"],))\n    outputs = tokenizer(\n        text,\n        truncation=True,\n        max_length=seq_len,\n        return_overflowing_tokens=True,\n        padding=\"max_length\",\n        return_length=True,\n    )\n    input_batch = []\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n        if length == seq_len:\n            input_batch.append(input_ids)\n\n    return {\"input_ids\": input_batch}\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True\n        )\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n            dim=0, keepdim=True\n        )\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n        \ndef convert_to_lora(\n    model,\n    trainable_params: List[str] = None,\n    targets: List[str] = None,\n    r=64,\n    lora_alpha=8,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n):\n    # Check the model type\n    lora_config = LoraConfig(\n        r=r,\n        lora_alpha=lora_alpha,\n        target_modules=targets,\n        lora_dropout=lora_dropout,\n        bias=bias,\n        modules_to_save=trainable_params,\n        task_type=task_type,\n    )\n    model = get_peft_model(model, lora_config)\n    return model\n\ndef save_config(config, output_dir):\n    config.save_pretrained(output_dir)\n\ndef load_config(config_path):\n    if os.path.isdir(config_path):\n        config_path = os.path.join(config_path, \"config.json\")\n    return LlamaConfig.from_pretrained(config_path)\n\n@dataclass\nclass ToolkitConfig:\n    task: TASK_CHOICE\n    embedder_name: str\n    embedder_path: str\n    ret_embeddings_dim: int\n    dtype: torch.dtype = torch.bfloat16\n    \n@dataclass\nclass MemConfig:\n    positionals: bool\n    use_gate: bool\n    cache_dtype: torch.dtype\n    \n@dataclass\nclass MemCache:\n    texts: List[Optional[List[str]]] = None\n    embeddings:List[Union[faiss.IndexFlatIP,List[torch.Tensor]]] = None\n    keys: torch.Tensor = None\n    values: torch.Tensor = None\n    masks: torch.Tensor = None\n\n@dataclass\nclass SigMemCache:\n    texts: List[str]\n    embeddings: List[torch.Tensor]\n    keys: torch.Tensor\n    values: torch.Tensor \n    masks: torch.Tensor\n    length: int\n"}
{"type": "source_file", "path": "eval/language_modeling/get_data.py", "content": "from datasets import load_dataset\nimport argparse\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--dataset_path_or_name\", type=str, nargs='+',required=True)\n    parser.add_argument(\"--split\", type=str, default=\"test\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"./data\")\n    parser.add_argument(\"--num_proc\", type=int, default=1)\n    parser.add_argument(\"--save_format\", type=str, default=\"json\", choices=[\"json\", \"llama-bin\"])\n    return parser.parse_args()\n\ndef main():\n    \n    args = parse_args()\n    dataset_path_or_name_list = args.dataset_path_or_name\n    split = args.split\n    for dataset_path_or_name in dataset_path_or_name_list:\n        dataset_name = dataset_path_or_name.split('/')[-1]\n        if dataset_name == \"wikitext\":\n            dataset = load_dataset(dataset_path_or_name,\"wikitext-103-raw-v1\",split=split,num_proc=args.num_proc,cache_dir=\".\")\n        else:\n            dataset = load_dataset(dataset_path_or_name, split)\n        #save_to_disk(dataset, f\"{args.output_dir}/{data}_{split}\")\n\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "instruction_finetuning/arguments.py", "content": "from dataclasses import dataclass, field\nfrom typing import List, Optional\nfrom transformers import TrainingArguments\n\n@dataclass\nclass ModelArgs:\n    model_path: str = field(default=None)\n    peft_path: str = field(default=None)\n    last_context_length: int = field(default=1024)\n    torch_dtype: str = field(default=\"bfloat16\")\n    attn_implementation: str = field(default=\"eager\")\n    embedder_name: str = field(default=\"bge_embedder\")\n    embedder_path: str = field(default=\"/opt/data/private/lwj/emnlp2024/bge-m3\")\n    embedder_task: str = field(default=\"lrlm\")\n    embedder_dim: int = field(default=1024)\n    use_toolkit: bool = field(default=True)\n    # gradient_checkpoint_every_ith: int = field(default=1)\n\n@dataclass\nclass CustomedTrainingArgs(TrainingArguments):\n    trainable_params: str = field(default=None)\n    targets: str = field(default=None)\n    train_mode: str = field(default=\"lora-all\")\n    freeze_layers: str = field(default=\"None\")\n\n@dataclass\nclass DataArgs:\n    data_type: str = field(\n        default=\"instructions\",\n        metadata={\n            \"help\": \"\"\"\n                `,` separated list indicating the type of each dataset.\n                Available types: 'instructions' and 'chat'.\n                Examples:\n                  * 'instructions'\n                  * 'instructions,chat' - the first dataset is for instruction tuning\n                    whereas the second one is for chat tuning.\n                \"\"\"\n        },\n    )  # instructions or chat\n\n    data_filter: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"\"\"\n                '<,>' separated list of '<;>' separated rules of format field_name<M>regex for filtering out the data.\n                For example 'lang<M>en<;>conversations.<int>*^.source<M>gpt<,>lang<M>pl'\n                will take from the first dataset the records such that their field lang matches the regex en\n                and the following property holds: when we take the conversation field and look at all its elements\n                then each of them has a field source that matches the regex gpt.\n                From the second dataset, it will take the records with field lang matching regex pl.\n                Consider another example: '<,>lang<M>^en$<;>conversations.<int>*^.value<M>(?i)^((?!openai).)*$<;>conversations.<int>*^.value<M>^((?!DAN).)*$<;>conversations.<int>0.value<LENLT>8000'.\n                Here we do not filter the data coming from the first dataset.\n                From the second dataset we take the records such that:\n                  * field lang is equal to 'en'\n                  * the conversations mention neither openai nor DAN.\n                  * the first part of the conversation has at most 8000 chars\n                \"\"\"\n            )\n        },\n    )\n\n    data_path: str = field(\n        default=None,\n        metadata={\n            \"help\": \"\"\"\n                    Hugging Face dataset(s) name/path; separator ','\n                    Examples: \n                        * Open-Orca/OpenOrca\n                        * 'Open-Orca/OpenOrca,zetavg/ShareGPT-Processed'\n                    \"\"\"\n        },\n    )\n    data_revision: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"\"\"\n                    Revision for each Hugging Face dataset; separator ','\n                    Examples:\n                        * 'f0823c7ffc48c9d33a42f16cf0b885fed4a7d0a1'\n                        * 'f0823c7ffc48c9d33a42f16cf0b885fed4a7d0a1,15968d6dfa02529988da12e382af3ab7c857ebcd'\n                    \"\"\"\n        },\n    )\n    dataset_split: str = field(\n        default=\"train\",\n        metadata={\n            \"help\": \"\"\"\n                    Split for each Hugging Face dataset; separator ','\n                    Examples:\n                        * 'train'\n                        * 'train,train'\n                    \"\"\"\n        },\n    )\n\n    # instructions\n    pre_prompt_text: str = field(\n        default=\"\",\n        metadata={\n            \"help\": \"\"\"\n                    Field with pre-prompt text. One for each instruction dataset.\n                    Separator '<,>'. In case no '<,>' is present value will be replicated\n                    for all instructions datasets.\n                    Examples:\n                        * PROMPT:\n                        * PROMPT<,>PROMPT:\n                    \"\"\"\n        },\n    )\n    prompt_field: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": \"\"\"\n                    Field with the prompt. One for each instruction dataset.\n                    Separator ','. 'None' is interpreted as None. In case no ',' is present value will be replicated\n                    for all instructions datasets.\n                    Examples:\n                        * system_prompt\n                        * system_prompt,prompt\n                        * system_prompt,None\n                    \"\"\"\n        },\n    )\n    post_prompt_text: str = field(\n        default=\"\\n\",\n        metadata={\n            \"help\": \"\"\"\n                    Field with post-prompt text. One for each instruction dataset.\n                    Separator '<,>'. In case no '<,>' is present value will be replicated\n                    for all instructions datasets.\n                    \"\"\"\n        },\n    )\n\n    pre_question_text: str = field(\n        default=\"\",\n        metadata={\n            \"help\": \"\"\"\n                    Field with pre-question text. One for each instruction dataset.\n                    Separator '<,>'. In case no '<,>' is present value will be replicated\n                    for all instructions datasets.\n                    \"\"\"\n        },\n    )\n    question_field: str = field(\n        default=None,\n        metadata={\n            \"help\": \"\"\"\n                    Field with question. One for each instruction dataset.\n                    Separator ','. 'None' is interpreted as None. In case no ',' is present value will be replicated\n                    for all instructions datasets.\n                    Examples:\n                        * question\n                        * question,instruction\n                        * question,None\n                    \"\"\"\n        },\n    )\n    post_question_text: str = field(\n        default=\"\\n\",\n        metadata={\n            \"help\": \"\"\"\n                    Field with post-question text. One for each instruction dataset.\n                    Separator '<,>'. In case no '<,>' is present value will be replicated\n                    for all instructions datasets.\n                    \"\"\"\n        },\n    )\n\n    pre_response_text: str = field(\n        default=\"\",\n        metadata={\n            \"help\": \"\"\"\n                    Field with pre-response text. One for each instruction dataset.\n                    Separator '<,>'. In case no '<,>' is present value will be replicated\n                    for all instructions datasets.\n                    \"\"\"\n        },\n    )\n    response_field: str = field(\n        default=None,\n        metadata={\n            \"help\": \"\"\"\n                    Field with the expected response. One for each instruction dataset.\n                    Separator ','. 'None' is interpreted as None. In case no ',' is present value will be replicated\n                    for all instructions datasets.\n                    Examples:\n                        * response\n                        * response,output\n                        * response,None\n                    \"\"\"\n        },\n    )\n    post_response_text: str = field(\n        default=\"\",\n        metadata={\n            \"help\": \"\"\"\n                    Field with post response text. One for each instruction dataset.\n                    Separator '<,>'. In case no '<,>' is present value will be replicated\n                    for all instructins datasets.\n                    \"\"\"\n        },\n    )\n\n    # chat\n    chat_conversations_field: str = field(\n        default=\"conversations\",\n        metadata={\n            \"help\": \"\"\"\n                    Name of the field with conversations list. One for each chat dataset.\n                    Separator ','. 'None' is interpreted as None.\n                    In case no ',' is present value will be replicated\n                    for all chat datasets.\n                    \"\"\"\n        },\n    )\n    chat_data_field: str = field(\n        default=\"value\",\n        metadata={\n            \"help\": \"\"\"\n                    Name of field with text.\n                    One for each chat dataset.\n                    Separator ','. 'None' is interpreted as None.\n                    In case no ',' is present value will be replicated\n                    for all chat datasets.\n                    \"\"\"\n        },\n    )\n    chat_source_name_field: str = field(\n        default=\"from\",\n        metadata={\n            \"help\": \"\"\"Name of field describing the source (human/ai) of the text.\n                    One for each chat dataset.\n                    Separator ','. 'None' is interpreted as None.\n                    In case no ',' is present value will be replicated\n                    for all chat datasets.\n                    \"\"\"\n        },\n    )\n    chat_model_source_name: str = field(\n        default=\"gpt\",\n        metadata={\n            \"help\": \"\"\"Name of the text source that should be used to tune the model. \n                    One for each chat dataset.\n                    Separator ','. 'None' is interpreted as None.\n                    In case no ',' is present value will be replicated\n                    for all chat datasets.\n                    \"\"\"\n        },\n    )\n    chat_initial_prompt: str = field(default=\"You are a helpful ASSISTANT.\\n\\n\")\n    chat_replace_rules: Optional[str] = field(\n        default=None,\n        metadata={\n            \"help\": (\n                \"'<;>' separated list o describing pairs of replace regular expressions\"\n                \"for example, 'a<R>b<;>c<R>d' means first replace text that matches regex 'a' with string 'b'\"\n                \"then do the same for 'c' and 'd'.\"\n            )\n        },\n    )\n\n    chat_model_response_prefix: str = field(default=\"\\nASSISTANT: \")\n    chat_human_response_prefix: str = field(default=\"\\nUSER: \")\n\n    # proportions (for mixed dataset)\n    data_proportions: List[float] = field(\n        default_factory=lambda: [1.0], metadata={\"help\": \"Space separated probability of sampling (for each dataset)\"}\n    )\n\n\n@dataclass\nclass TokenizationArgs:\n    # Note that max_input_length and max_output_length are only used for instructions data (not for chat)\n    # max_total_length is used for both\n    max_input_length: int = field(default=2048)\n    max_output_length: int = field(default=2048)\n    max_total_length: int = field(default=2048)\n    always_pad: bool = field(default=True, metadata={\"help\": \"Whether to always pad data to max_total_length tokens\"})\n    random_pad: bool = field(\n        default=True,\n        metadata={\n            \"help\": \"Whether add padding tokens to the right only or sample the amount of left and right padding\"\n        },\n    )"}
{"type": "source_file", "path": "instruction_finetuning/supervised_finetuning.py", "content": "import json\nfrom dataclasses import asdict\nfrom datetime import datetime\nfrom peft import PeftModel\nimport torch\nfrom src.modeling_llama_position import LlamaForCausalLM\nfrom src.configuration_llama import LlamaConfig\nfrom src.utils import ToolkitConfig,convert_to_lora,set_freeze_by_idxs\nfrom transformers import HfArgumentParser, LlamaTokenizer, Trainer\n\nfrom .arguments import DataArgs, ModelArgs, TokenizationArgs,CustomedTrainingArgs\nfrom .data_processing import LOGGER, DataCollator, MixedTuneDataset\nfrom .utils import get_packages, metrics_assign_group, non_numeric_to_str,smart_tokenizer_and_embedding_resize\nimport os\n\ndef main():\n    hf_parser = HfArgumentParser((ModelArgs, DataArgs, TokenizationArgs, CustomedTrainingArgs))\n    (\n        model_args,\n        data_args,\n        tokenization_args,\n        trainer_args,\n    ) = hf_parser.parse_args_into_dataclasses()\n    LOGGER.info(f\"Preparing tokenizer {model_args.model_path}\")\n    tokenizer = LlamaTokenizer.from_pretrained(model_args.model_path, padding_side=\"right\", use_fast=False)\n    \n    if model_args.peft_path:\n        LOGGER.info(f\"Preparing Config {model_args.peft_path}\")\n        config = LlamaConfig.from_pretrained(model_args.peft_path + \"/config.json\")\n    else:\n        LOGGER.info(f\"Preparing Config {model_args.model_path}\")\n        config = LlamaConfig.from_pretrained(model_args.model_path + \"/config.json\")\n    config.use_cache = False\n    toolkit_config = ToolkitConfig(\n        task=model_args.embedder_task,\n        embedder_name=model_args.embedder_name,\n        embedder_path=model_args.embedder_path,\n        ret_embeddings_dim=model_args.embedder_dim,\n    )\n        \n    LOGGER.info(f\"Preparing model {model_args.model_path}\")\n    model = LlamaForCausalLM.from_pretrained(model_args.model_path , config=config , toolkit_config = toolkit_config,attn_implementation=model_args.attn_implementation,torch_dtype=torch.bfloat16)\n    \n    tokenzier_vocab_size = len(tokenizer)\n    model_vocab_size = model.get_input_embeddings().weight.size(0)\n    num_new_tokens = tokenzier_vocab_size - model_vocab_size\n    if model_vocab_size != tokenzier_vocab_size:\n        model.resize_token_embeddings(tokenzier_vocab_size)\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n    else:\n        tokenizer.pad_token_id = config.pad_token_id\n    model.set_toolkit_tokenizer(tokenizer)\n    if model_args.peft_path:\n        print(\"Loading PEFT model\")\n        model = PeftModel.from_pretrained(\n            model,\n            model_args.peft_path,\n            # config=peft_config,\n            # device_map=\"auto\",\n            torch_dtype=\"bfloat16\",\n        )\n        model = model.merge_and_unload()\n    \n    trainable_params = (trainer_args.trainable_params.split(\",\") if trainer_args.trainable_params else None)\n    targets = trainer_args.targets.split(\",\") if trainer_args.targets else None\n    lower, upper = list(map(int, trainer_args.freeze_layers.split(\":\")))\n    # training mode\n    if trainer_args.train_mode == \"lora-all\":\n        model = convert_to_lora(model=model, targets=targets, trainable_params=trainable_params,task_type=\"CAUSAL_LM\")\n        model.print_trainable_parameters()\n    elif trainer_args.train_mode == \"lora-freeze\":\n        model = convert_to_lora(model=model, targets=targets, trainable_params=trainable_params)\n        set_freeze_by_idxs(model, range(lower, upper),freeze=True)\n        model.print_trainable_parameters()\n    elif trainer_args.train_mode == \"partial-lora\":\n        set_freeze_by_idxs(model,range(lower, upper),freeze=True)\n        model = convert_to_lora(model=model, trainable_params=trainable_params)\n        model.print_trainable_parameters()\n    elif trainer_args.train_mode == \"partial-freeze\":\n        set_freeze_by_idxs(model, range(lower, upper),freeze=True)\n        if model_args.peft_model:\n            model.print_trainable_parameters()\n        \n    LOGGER.info(\"Preparing dataset\")\n    dataset = MixedTuneDataset(data_args=data_args, tokenizer=tokenizer, tokenization_args=tokenization_args)\n    LOGGER.info(\"Preparing trainer\")\n\n    trainer = Trainer(\n        model=model,\n        tokenizer=tokenizer,\n        args=trainer_args,\n        train_dataset=dataset,\n        eval_dataset=dataset,\n        data_collator=DataCollator(tokenizer=tokenizer,use_toolkit=model_args.use_toolkit),\n    )\n\n    model_args_dict = metrics_assign_group(asdict(model_args), \"model_args\")\n    data_args_dict = metrics_assign_group(asdict(data_args), \"data_args\")\n    tokenization_args_dict = metrics_assign_group(asdict(tokenization_args), \"tokenization_args\")\n    trainer_args_dict = metrics_assign_group(asdict(trainer_args), \"trainer_args\")\n    packages_dict = metrics_assign_group(get_packages(), \"packages\")\n    all_params = {**model_args_dict, **data_args_dict, **tokenization_args_dict, **trainer_args_dict, **packages_dict}\n\n    trainer.save_metrics(\"train\", all_params, combined=True)\n\n    str_params = json.dumps(all_params, indent=2)\n    LOGGER.info(str_params)\n\n    cur_time = datetime.now().strftime(\"%d.%m.%Y_%H:%M:%S\")\n    with open(f\"{trainer_args.output_dir}/params_{cur_time}.json\", \"w\") as f:\n        f.write(str_params)\n\n    LOGGER.info(\"Running trainer\")\n\n    trainer.train()\n    trainer.save_state()\n    trainer.save_model(output_dir=os.path.join(trainer_args.output_dir, \"final_model\"))\n\n\nif __name__ == \"__main__\":\n    main()"}
{"type": "source_file", "path": "eval/language_modeling/MemLong/cache_utils.py", "content": "from dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\n\n@dataclass\nclass Cache:\n    \"\"\"\n    Base, abstract class for all caches. The actual data structure is specific to each subclass.\n    \"\"\"\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. These are specific to each subclass and allow new types of\n                cache to be created.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        raise NotImplementedError(\"Make sure to implement `update` in a subclass.\")\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states, if there is any.\"\"\"\n        raise NotImplementedError(\"Make sure to implement `get_max_length` in a subclass.\")\n\n    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Given the sequence length of the new inputs, returns the usable length of the cache.\"\"\"\n        # Cache without size limit -> all cache is usable\n        # Cache with size limit -> if the length cache plus the length of the new inputs is larger the maximum cache\n        #   length, we will need to evict part of the cache (and thus not all cache is usable)\n        max_length = self.get_max_length()\n        previous_seq_length = self.get_seq_length(layer_idx)\n        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n            return max_length - new_seq_length\n        return previous_seq_length\n\n    def reorder_cache(self, beam_idx: torch.LongTensor):\n        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n        for layer_idx in range(len(self.key_cache)):\n            device = self.key_cache[layer_idx].device\n            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n            device = self.value_cache[layer_idx].device\n            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n\n    @property\n    def seen_tokens(self):\n        logger.warning_once(\n            \"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` \"\n            \"model input instead.\"\n        )\n        if hasattr(self, \"_seen_tokens\"):\n            return self._seen_tokens\n        else:\n            return None\n\n\nclass DynamicCache(Cache):\n    \"\"\"\n    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        self.position_ids_cache: List[torch.Tensor] = []\n        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n\n    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n        \"\"\"\n        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n        sequence length.\n        \"\"\"\n        if layer_idx < len(self):\n            return (self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx])\n        else:\n            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n\n    def __iter__(self):\n        \"\"\"\n        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n        keys and values\n        \"\"\"\n        for layer_idx in range(len(self)):\n            yield (self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx])\n\n    def __len__(self):\n        \"\"\"\n        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n        to the number of layers in the model.\n        \"\"\"\n        return len(self.key_cache)\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        position_ids: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the cache\n        if len(self.key_cache) <= layer_idx:\n            self.key_cache.append(key_states)\n            self.value_cache.append(value_states)\n            self.position_ids_cache.append(position_ids)\n        else:\n            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n            self.position_ids_cache[layer_idx] = torch.cat([self.position_ids_cache[layer_idx], position_ids], dim=-1)\n            \n        return self.key_cache[layer_idx], self.value_cache[layer_idx] , self.position_ids_cache[layer_idx]\n\n    def drop_and_update(self,\n        keys_states : torch.Tensor,\n        value_states : torch.Tensor,\n        position_ids : torch.Tensor,\n        num_elems_to_drop : int,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        \n        assert keys_states.shape[-2] == value_states.shape[-2], \"The number of keys and values to drop must be the same\"\n        assert self.key_cache[layer_idx].shape[-2] >= num_elems_to_drop, \"The number of keys to drop must be less than the number of keys in the cache\"\n        \n        self.key_cache[layer_idx] = torch.concat((self.key_cache[layer_idx][:, :, num_elems_to_drop:],keys_states), dim=-2)\n        self.value_cache[layer_idx] = torch.concat((self.value_cache[layer_idx][:, :, num_elems_to_drop:], value_states), dim=-2)\n        self.position_ids_cache[layer_idx] = torch.concat((self.position_ids_cache[layer_idx][:, num_elems_to_drop:], position_ids), dim=-1)\n                        \n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens -= num_elems_to_drop\n            self._seen_tokens += keys_states.shape[-2]\n        \n        return self.key_cache[layer_idx], self.value_cache[layer_idx] , self.value_cache[layer_idx]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        if len(self.key_cache) <= layer_idx:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n        return None\n\n    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n        \"\"\"Converts the `DynamicCache` instance into the its equivalent in the legacy cache format.\"\"\"\n        legacy_cache = ()\n        for layer_idx in range(len(self)):\n            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx]),)\n        return legacy_cache\n\n    @classmethod\n    def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n        \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`.\"\"\"\n        cache = cls()\n        if past_key_values is not None:\n            for layer_idx in range(len(past_key_values)):\n                key_states, value_states,position_ids= past_key_values[layer_idx]\n                cache.update(key_states, value_states, position_ids,layer_idx)\n        return cache\n\n\nclass SinkCache(Cache):\n    \"\"\"\n    A cache that as described in the [Attention Sinks paper](https://arxiv.org/abs/2309.17453). It allows the model to\n    generate beyond the length of its context window, without losing fluency in the conversation. As it discards past\n    tokens, the model will lose the ability to generate tokens that depend on the context that was discarded.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n\n    Parameters:\n        window_length (`int`):\n            The length of the context window.\n        num_sink_tokens (`int`):\n            The number of sink tokens. See the original paper for more information.\n    \"\"\"\n\n    def __init__(self, window_length: int, num_sink_tokens: int) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        self.window_length = window_length\n        self.num_sink_tokens = num_sink_tokens\n        self.cos_sin_rerotation_cache = {}\n        self._cos_cache = None\n        self._sin_cache = None\n        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n\n    @staticmethod\n    def _rotate_half(x):\n        x1 = x[..., : x.shape[-1] // 2]\n        x2 = x[..., x.shape[-1] // 2 :]\n        return torch.cat((-x2, x1), dim=-1)\n\n    def _apply_key_rotary_pos_emb(\n        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n    ) -> torch.Tensor:\n        rotated_key_states = (key_states * cos) + (self._rotate_half(key_states) * sin)\n        return rotated_key_states\n\n    def _get_rerotation_cos_sin(\n        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if key_states.shape[-2] not in self.cos_sin_rerotation_cache:\n            # Upcast to float32 temporarily for better accuracy\n            cos = cos.to(torch.float32)\n            sin = sin.to(torch.float32)\n\n            # Compute the cos and sin required for back- and forward-rotating to one position earlier in the sequence\n            original_cos = cos[self.num_sink_tokens + key_states.shape[-2] :]\n            shifted_cos = cos[self.num_sink_tokens : -key_states.shape[-2]]\n            original_sin = sin[self.num_sink_tokens + key_states.shape[-2] :]\n            shifted_sin = sin[self.num_sink_tokens : -key_states.shape[-2]]\n            rerotation_cos = original_cos * shifted_cos + original_sin * shifted_sin\n            rerotation_sin = -original_sin * shifted_cos + original_cos * shifted_sin\n\n            self.cos_sin_rerotation_cache[key_states.shape[-2]] = (\n                rerotation_cos.to(key_states.dtype).unsqueeze(0),\n                rerotation_sin.to(key_states.dtype).unsqueeze(0),\n            )\n        return self.cos_sin_rerotation_cache[key_states.shape[-2]]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        # Workaround to make 'key_states.shape[-2] + past_key_value.get_seq_length(self.layer_idx)' <= window_length\n        if len(self.key_cache) <= layer_idx:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n        return self.window_length\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. The following arguments can be used in `SinkCache`: `sin`,\n                `cos` and `partial_rotation_size`. These arguments are used with models using RoPE, to recompute the\n                rotation as the tokens are shifted.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # Optional kwargs for `SinkCache` -- needed on models using RoPE. `partial_rotation_size` is used on models\n        # with partially rotated position embeddings, like Phi or Persimmon.\n        sin = cache_kwargs.get(\"sin\")\n        cos = cache_kwargs.get(\"cos\")\n        partial_rotation_size = cache_kwargs.get(\"partial_rotation_size\")\n        using_rope = cos is not None and sin is not None\n\n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the sin/cos cache, which holds sin/cos values for all possible positions\n        if using_rope and layer_idx == 0:\n            # BC: some models still pass `sin`/`cos` with 2 dims. In those models, they are the full sin/cos. Remove\n            # after all RoPE models have a llama-like cache utilization.\n            if cos.dim() == 2:\n                self._cos_cache = cos\n                self._sin_cache = sin\n            else:\n                if self._cos_cache is None:\n                    self._cos_cache = cos[0, ...]\n                    self._sin_cache = sin[0, ...]\n                elif self._cos_cache.shape[0] < self.window_length:\n                    self._cos_cache = torch.cat([self._cos_cache, cos[0, ...]], dim=0)\n                    self._sin_cache = torch.cat([self._sin_cache, sin[0, ...]], dim=0)\n\n        # [bsz, num_heads, seq_len, head_dim]\n        if len(self.key_cache) <= layer_idx:\n            # Empty cache\n            self.key_cache.append(key_states)\n            self.value_cache.append(value_states)\n\n        elif key_states.shape[-2] + self.get_seq_length(layer_idx) < self.window_length:\n            # Growing cache\n            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n\n        else:\n            # Shifting cache\n            keys_to_keep = self.key_cache[layer_idx][\n                :, :, -self.window_length + self.num_sink_tokens + key_states.shape[-2] :\n            ]\n\n            # On RoPE models, we need to recompute the Key rotation as the tokens are shifted\n            if using_rope:\n                rerotation_cos, rerotation_sin = self._get_rerotation_cos_sin(\n                    key_states, self._cos_cache[: self.window_length], self._sin_cache[: self.window_length]\n                )\n                if partial_rotation_size is not None:\n                    keys_to_keep, keys_pass = (\n                        keys_to_keep[..., :partial_rotation_size],\n                        keys_to_keep[..., partial_rotation_size:],\n                    )\n                keys_to_keep = self._apply_key_rotary_pos_emb(keys_to_keep, rerotation_cos, rerotation_sin)\n                if partial_rotation_size is not None:\n                    keys_to_keep = torch.cat((keys_to_keep, keys_pass), dim=-1)\n\n            # Concatenate sink tokens, shifted & rotated tokens (if needed), and new tokens\n            sink_keys = self.key_cache[layer_idx][:, :, : self.num_sink_tokens]\n            self.key_cache[layer_idx] = torch.cat([sink_keys, keys_to_keep, key_states], dim=-2)\n\n            sink_values = self.value_cache[layer_idx][:, :, : self.num_sink_tokens]\n            values_to_keep = self.value_cache[layer_idx][\n                :, :, -self.window_length + self.num_sink_tokens + value_states.shape[-2] :\n            ]\n            self.value_cache[layer_idx] = torch.cat([sink_values, values_to_keep, value_states], dim=-2)\n\n        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n\n\nclass StaticCache(Cache):\n    \"\"\"\n    Static Cache class to be used with `torch.compile(model)`.\n\n    Parameters:\n        config (`PretrainedConfig):\n            The configuration file defining the shape-related attributes required to initialize the static cache.\n        max_batch_size (`int`):\n            The maximum batch size with which the model will be used.\n        max_cache_len (`int`):\n            The maximum sequence length with which the model will be used.\n        device (`torch.device`):\n            The device on which the cache should be initialized. Should be the same as the layer.\n        dtype (*optional*, defaults to `torch.float32`):\n            The default `dtype` to use when initializing the layer.\n    \"\"\"\n\n    def __init__(self, config: PretrainedConfig, max_batch_size: int, max_cache_len: int, device, dtype=None) -> None:\n        super().__init__()\n        self.max_batch_size = max_batch_size\n        self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n        # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n        self.head_dim = (\n            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n        )\n\n        self.dtype = dtype if dtype is not None else torch.float32\n        self.num_key_value_heads = (\n            config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n        )\n\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        cache_shape = (max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n        for _ in range(config.num_hidden_layers):\n            # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n            # breaks when updating the cache.\n            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n            torch._dynamo.mark_static_address(new_layer_key_cache)\n            torch._dynamo.mark_static_address(new_layer_value_cache)\n            self.key_cache.append(new_layer_key_cache)\n            self.value_cache.append(new_layer_value_cache)\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n        It is VERY important to index using a tensor, otherwise you introduce a copy to the device.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. The `StaticCache` needs the `cache_position` input\n                to know how where to write in the cache.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        cache_position = cache_kwargs.get(\"cache_position\")\n        k_out = self.key_cache[layer_idx]\n        v_out = self.value_cache[layer_idx]\n\n        k_out[:, :, cache_position] = key_states\n        v_out[:, :, cache_position] = value_states\n\n        return k_out, v_out\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states that were seen by the model.\"\"\"\n        # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n        # limit the check to the first batch member and head dimension.\n        # TODO: deprecate this function in favor of `cache_position`\n        return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n        return self.max_cache_len\n\n    def reset(self):\n        \"\"\"Resets the cache values while preserving the objects\"\"\"\n        for layer_idx in range(len(self.key_cache)):\n            # In-place ops prevent breaking the static address\n            self.key_cache[layer_idx].zero_()\n            self.value_cache[layer_idx].zero_()\n"}
{"type": "source_file", "path": "instruction_finetuning/utils.py", "content": "from typing import Dict, Any\nimport transformers\n\ndef metrics_assign_group(metrics_dict: Dict[str, Any], group: str, index: int = 0):\n    result = {}\n    for k, v in metrics_dict.items():\n        groups = k.split(\"/\")\n        abs_index = index % len(groups)\n        groups = groups[:abs_index] + [group] + groups[abs_index:]\n        new_k = \"/\".join(groups)\n        result[new_k] = v\n    return result\n\n\ndef non_numeric_to_str(metrics_dict: Dict[str, Any]):\n    result = {}\n    for k, v in metrics_dict.items():\n        if not isinstance(v, int) and not isinstance(v, float):\n            result[k] = str(v)\n        else:\n            result[k] = v\n    return result\n\n\ndef get_packages():\n    \"\"\"For getting the list of installed packages\"\"\"\n    import pkg_resources\n\n    result = {}\n    for pkg in pkg_resources.working_set:\n        pkg_name, pkg_ver = str(pkg.project_name), str(pkg.version)\n        result[pkg_name] = pkg_ver\n    return result\n\ndef smart_tokenizer_and_embedding_resize(\n    special_tokens_dict: Dict,\n    tokenizer: transformers.PreTrainedTokenizer,\n    model: transformers.PreTrainedModel,\n):\n    \"\"\"Resize tokenizer and embedding.\n\n    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n    \"\"\"\n    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n    model.resize_token_embeddings(len(tokenizer))\n\n    if num_new_tokens > 0:\n        input_embeddings = model.get_input_embeddings().weight.data\n        output_embeddings = model.get_output_embeddings().weight.data\n\n        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(dim=0, keepdim=True)\n\n        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n        output_embeddings[-num_new_tokens:] = output_embeddings_avg"}
{"type": "source_file", "path": "instruction_finetuning/__init__.py", "content": ""}
{"type": "source_file", "path": "eval/language_modeling/utils.py", "content": "from collections import defaultdict\nfrom dataclasses import dataclass\nimport torch\nfrom typing import Iterator\nfrom torch.utils.data import Sampler\n\n@dataclass\nclass ContextDataCollator:\n    def __call__(self, batch):\n        new_batch = defaultdict(list)\n\n        for item in batch:\n            new_batch[\"input_ids\"].append(torch.tensor(item[\"input_ids\"], dtype=torch.long))\n            labels = torch.tensor(item[\"labels\"], dtype=torch.long)\n            new_batch[\"labels\"].append(labels)\n            if \"encoder_input_ids\" in item:\n                new_batch[\"encoder_input_ids\"].append(torch.tensor(item[\"encoder_input_ids\"], dtype=torch.long))\n\n            if \"encoder_attention_mask\" in item:\n                new_batch[\"encoder_attention_mask\"].append(torch.tensor(item[\"encoder_attention_mask\"], dtype=torch.long))\n\n            if \"distill_prob\" in item:\n                new_batch[\"distill_prob\"].append(torch.tensor(item[\"distill_prob\"], dtype=torch.float32))\n                new_batch[\"distill_index\"].append(torch.tensor(item[\"distill_index\"], dtype=torch.long))\n\n        for key in new_batch:\n            new_batch[key] = torch.stack(new_batch[key])\n            if key == \"encoder_input_ids\" and len(new_batch[key].shape) == 4:\n                # each item maybe have two encoder input, and we want to merge them in the second dimension\n                # shape is (bsz, 2, num_context, context_size)\n                new_batch[key] = new_batch[key].view(new_batch[key].size(0), -1, new_batch[key].size(-1))\n\n        return dict(new_batch)\n\nclass BatchSequentialSampler(Sampler):\n    def __init__(self, data, batch_size, num_process=None) -> None:\n        self.total_len = len(data)\n        # self.data = data[: self.total_len - self.total_len % batch_size]\n        self.batch_size = batch_size\n        self.num_process = num_process\n\n    def __iter__(self) -> Iterator[int]:\n        if self.num_process is None:\n            per_batch_nums = self.total_len // self.batch_size\n            for i in range(per_batch_nums):\n                for j in range(self.batch_size):\n                    yield i + j * per_batch_nums\n        else:\n            per_batch_nums = self.total_len // self.batch_size // self.num_process\n            for i in range(per_batch_nums):\n                for j in range(self.batch_size):\n                    for k in range(self.num_process):\n                        yield i + j * per_batch_nums + k * per_batch_nums * self.batch_size"}
{"type": "source_file", "path": "src/modeling_llama_position.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch LLaMA model.\"\"\"\n\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom .cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\n\nfrom .configuration_llama import LlamaConfig\nfrom .utils import ToolkitConfig,MemConfig,MemCache\nfrom typing import Literal\nfrom dataclasses import dataclass\nfrom .toolkit import ToolKit\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\n\n@dataclass\nclass BaseModelOutputWithMem(BaseModelOutputWithPast):\n    mem_update: Optional[MemCache] = None\n\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (\n                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n            )\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\ndef apply_rotary_pos_emb_for_relative_query(q, cos, sin,unsqueeze_dim=1):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)  # q: [bs, nh, seq_len, dim]\n    return q_embed\n\ndef apply_rotary_pos_emb_for_relative_keys(k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return k_embed\n\n# Based on transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef rotate_one(x, cos, sin, position_ids):\n    if len(position_ids.shape) != 2 or x.shape[0] != position_ids.shape[0] or x.shape[-2] != position_ids.shape[1]:\n        raise ValueError(f\"Position ids shoud have shape [bsz, seq_len] got {position_ids.shape}\")\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    x_embed = (x * cos) + (rotate_half(x) * sin)\n    return x_embed\n\ndef rotate_as_if_first(x, rotary_emb):\n    # x: [bs, num_attention_heads, seq_len, head_size]\n    # apply rotary as if all elements were first in the sequence\n    cos, sin = rotary_emb(x, torch.arange(x.shape[-2]).view(1,-1).type_as(x))\n    return rotate_one(x, cos, sin, torch.zeros(x.shape[0], x.shape[-2], dtype=torch.long, device=cos.device))\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat(\n                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n            )\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [\n                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n            ]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\nclass RetGate(nn.Module):\n    def __init__(self, num_heads):\n        super().__init__()\n        self.output_gate = torch.nn.Parameter(torch.zeros(1, num_heads, 1, 1))\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, ret_score, attn_score):\n        # print(self.output_gate[0][0][0])\n        # print(self.sigmoid(self.output_gate[0][0][0]))\n        return torch.cat((self.sigmoid(self.output_gate) * ret_score,(1 - self.sigmoid(self.output_gate)) * attn_score,),dim=-1,)\n\n\nclass RetrievalCausalAttention(nn.Module):\n    \n    def __init__(self, config:LlamaConfig, mem_config: MemConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.max_local_cache = config.max_position_embeddings\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        \n        # control mem behavior\n        self.mem_config = mem_config\n        # begin of sentence\n        self.clear_memory_on_bos_token_id = getattr(mem_config,\"clear_memory_on_bos_token_id\",False)\n        # end of sentence\n        self.clear_memory_on_eos_token_id = getattr(mem_config,\"clear_memory_on_eos_token_id\",False)\n        # None;Zero;Continual\n        self.position_type = config.position_type\n        self.ret_gate = None\n        # Only for Retrieval\n        if getattr(mem_config,\"use_gate\",False):\n            self.ret_gate = RetGate(self.num_heads)\n        \n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n    \n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return (\n            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n            .contiguous()\n        )\n    \n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[MemCache] = None,\n        output_mem: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\n        bsz, q_len, _ = hidden_states.size()\n        if self.config.pretraining_tp > 1:\n            raise NotImplementedError(\"pretraining_tp > 1 not supported for RetrievalCausalAttention\")\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n        \n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        \n        # Whether to use positional encoding when saving \n        use_positionals = self.mem_config is None or getattr(self.mem_config,\"positionals\",True)\n        # two conditions \n        # 1. use for saving or training \n        mem_no_local_cache = output_mem is True and past_key_value is None and (not use_cache)\n        # 2. use for generation\n        mem_and_local_cache = output_mem is True and use_cache\n        mem_update = None\n        if mem_no_local_cache:\n            if use_positionals and self.position_type == \"Zero\":\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n            \n            mem_update = MemCache(\n                keys=rfst_key_states.detach().to(self.mem_config.cache_dtype),\n                values=value_states.detach().to(self.mem_config.cache_dtype),\n                masks=attention_mask[...,-1,:,None].detach()\n            )   \n            \n        # Get local_seq_length\n        if use_cache and past_key_value is not None:\n            past_local_cache_size = past_key_value.get_seq_length(layer_idx=self.layer_idx)\n            local_seq_length = past_local_cache_size + hidden_states.shape[-2]\n        else:\n            local_seq_length = hidden_states.shape[-2]\n                \n        if past_key_value is not None:\n            if len(past_key_value) <= self.layer_idx:\n                merged_key_states = key_states\n                merged_value_states = value_states\n            else:\n                merged_key_states = torch.cat([past_key_value.key_cache[self.layer_idx], key_states], dim=-2)\n                merged_value_states = torch.cat([past_key_value.value_cache[self.layer_idx], value_states], dim=-2)\n            # merged_position_ids = torch.cat([past_key_value.position_ids_cache[self.layer_idx], loc_position_ids], dim=-1)\n            \n            if attention_mask.shape[-1] != merged_key_states.shape[-2] and attention_mask.shape[-2] == query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            assert local_seq_length == merged_key_states.shape[-2]\n            \n            if merged_key_states.shape[-2] > self.max_local_cache:\n                # We drop half of max_local_cache for Memory    \n                num_elems_to_drop = past_local_cache_size // 2\n                # key_states,value_states = past_key_value.drop_and_update(drop_keys,drop_values,key_states,value_states,self.layer_idx)\n                if mem_and_local_cache:\n                    drop_keys = merged_key_states[..., :num_elems_to_drop, :]\n                    drop_values = merged_value_states[...,:num_elems_to_drop,:]\n                    drop_masks = attention_mask[...,-1,:,None]\n                    drop_masks = drop_masks[:,:, :num_elems_to_drop, :]\n                    \n                    if use_positionals and self.position_type == \"Zero\":\n                        rfst_drop_keys = rotate_as_if_first(drop_keys,self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    \n                    mem_update = MemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype).detach(),\n                        values=drop_values.to(self.mem_config.cache_dtype).detach(),\n                        masks=drop_masks.to(self.mem_config.cache_dtype).detach(),\n                    )            \n                key_states, value_states , position_ids = past_key_value.drop_and_update(key_states,value_states,position_ids,num_elems_to_drop,self.layer_idx)\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n            else:\n                key_states, value_states , position_ids = past_key_value.update(key_states, value_states, position_ids , self.layer_idx)\n\n        kv_seq_len = key_states.shape[-2]\n        \n        # Get mem_caches_length\n        if mem_caches is not None:\n            mem_caches_length = mem_caches.keys.shape[-2]\n        else:\n            mem_caches_length = 0\n        \n        if use_positionals and self.position_type == \"Zero\":\n            loc_position_ids = torch.arange(1,kv_seq_len+1).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.zeros((1,mem_caches_length)).type_as(position_ids)\n        elif use_positionals and self.position_type == \"Continual\":\n            loc_position_ids = torch.arange(mem_caches_length,mem_caches_length+kv_seq_len).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.arange(mem_caches_length).view(1,-1).type_as(position_ids)\n        else:\n            # FIXME\n            raise NotImplementedError(\"For normal generation\")\n        \n        retrieval_upper =  mem_caches is not None and self.layer_idx in self.config.ret_attn_layers\n\n        # We rotate the mem firstly\n        if retrieval_upper and self.position_type==\"Continual\":\n            mem_cos , mem_sin = self.rotary_emb(value_states, mem_position_ids)\n            mem_caches.keys = apply_rotary_pos_emb_for_relative_keys(mem_caches.keys, mem_cos,mem_sin)\n            \n        loc_cos, loc_sin = self.rotary_emb(value_states, loc_position_ids)\n        query_states = apply_rotary_pos_emb_for_relative_query(query_states, loc_cos[:,-query_states.shape[-2]:], loc_sin[:,-query_states.shape[-2]:])\n        key_states = apply_rotary_pos_emb_for_relative_keys(key_states, loc_cos, loc_sin)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        loc_attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n            )\n        loc_attn_weights = loc_attn_weights + attention_mask\n\n        if retrieval_upper:\n            mem_mask = mem_caches.masks.squeeze(-1).unsqueeze(-2)\n            # TODO: ret granularity\n            mem_attn_weights = torch.matmul(query_states,mem_caches.keys.transpose(2, 3).to(key_states.dtype),) / math.sqrt(self.head_dim)\n            \n            assert mem_mask.shape[2] == 1\n            mem_attn_weights = mem_attn_weights + mem_mask\n            if self.ret_gate:\n                attn_weights = self.ret_gate(mem_attn_weights, loc_attn_weights)\n            else:\n                attn_weights = torch.concat((mem_attn_weights,loc_attn_weights),dim=-1)\n            combined_value_states = torch.concat([mem_caches.values.to(value_states.dtype),value_states],dim=-2,)\n        else:\n            attn_weights = loc_attn_weights\n            combined_value_states = value_states\n        \n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, combined_value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n        \n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value, mem_update\n\nclass RetrievalFlashAttention2(RetrievalCausalAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self.ret_gate = None\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[MemCache] = None,\n        output_mem: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n        \n        if attention_mask == None:\n            tgt_seq_len = hidden_states.shape[-2]\n            if past_key_value is not None:\n                src_seq_len = past_key_value.get_seq_length(self.layer_idx) + tgt_seq_len\n            else:\n                src_seq_len = tgt_seq_len\n            attention_mask = torch.ones(hidden_states.shape[0],tgt_seq_len).type_as(hidden_states)\n            rfst_attention_mask = self._gen_causal_mask(attention_mask,cache_position,src_seq_len,tgt_seq_len)\n        else:\n            rfst_attention_mask = attention_mask\n\n        output_attentions = False\n        bsz, q_len, _ = hidden_states.size()\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        # Whether to use positional encoding when saving \n        use_positionals = self.mem_config is None or getattr(self.mem_config,\"positionals\",True)\n        # two conditions \n        # 1. use for saving or training \n        mem_no_local_cache = output_mem is True and past_key_value is None and (not use_cache)\n        # 2. use for generation\n        mem_and_local_cache = output_mem is True and use_cache\n        mem_update = None\n        if mem_no_local_cache:\n            if use_positionals and self.position_type == \"Zero\":\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n                \n            mem_update = MemCache(\n                keys=rfst_key_states.detach().to(self.mem_config.cache_dtype),\n                values=value_states.detach().to(self.mem_config.cache_dtype),\n                masks=rfst_attention_mask[...,-1,:,None].detach()\n            )\n        # Get local_seq_length\n        if use_cache and past_key_value is not None:\n            past_local_cache_size = past_key_value.get_seq_length(layer_idx=self.layer_idx)\n            local_seq_length = past_local_cache_size + hidden_states.shape[-2]\n        else:\n            local_seq_length = hidden_states.shape[-2]\n        \n        if past_key_value is not None:\n            if len(past_key_value) <= self.layer_idx:\n                merged_key_states = key_states\n                merged_value_states = value_states\n            else:\n                merged_key_states = torch.cat([past_key_value.key_cache[self.layer_idx], key_states], dim=-2)\n                merged_value_states = torch.cat([past_key_value.value_cache[self.layer_idx], value_states], dim=-2)\n            if rfst_attention_mask.shape[-1] != merged_key_states.shape[-2] and rfst_attention_mask.shape[-2] == query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            assert local_seq_length == merged_key_states.shape[-2]\n            \n            if merged_key_states.shape[-2] > self.max_local_cache:\n                # We drop half of max_local_cache for Memory    \n                num_elems_to_drop = past_local_cache_size // 2\n                # key_states,value_states = past_key_value.drop_and_update(drop_keys,drop_values,key_states,value_states,self.layer_idx)\n                if mem_and_local_cache:\n                    drop_keys = merged_key_states[..., :num_elems_to_drop, :]\n                    drop_values = merged_value_states[...,:num_elems_to_drop,:]\n                    drop_masks = rfst_attention_mask[...,-1,:,None]\n                    drop_masks = drop_masks[:,:, :num_elems_to_drop, :]\n                    \n                    if use_positionals and self.position_type == \"Zero\":\n                        rfst_drop_keys = rotate_as_if_first(drop_keys,self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    \n                    mem_update = MemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype).detach(),\n                        values=drop_values.to(self.mem_config.cache_dtype).detach(),\n                        masks=drop_masks.to(self.mem_config.cache_dtype).detach(),\n                    )            \n                key_states, value_states , position_ids = past_key_value.drop_and_update(key_states,value_states,position_ids,num_elems_to_drop,self.layer_idx)\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n            else:\n                key_states, value_states , position_ids = past_key_value.update(key_states, value_states, position_ids , self.layer_idx)\n\n        kv_seq_len = key_states.shape[-2]\n        \n        # Get mem_caches_length\n        if mem_caches is not None:\n            mem_caches_length = mem_caches.keys.shape[-2]\n        else:\n            mem_caches_length = 0\n        \n        if use_positionals and self.position_type == \"Zero\":\n            loc_position_ids = torch.arange(1,kv_seq_len+1).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.zeros((1,mem_caches_length)).type_as(position_ids)\n        elif use_positionals and self.position_type == \"Continual\":\n            loc_position_ids = torch.arange(mem_caches_length,mem_caches_length+kv_seq_len).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.arange(mem_caches_length).view(1,-1).type_as(position_ids)\n        else:\n            # FIXME\n            raise NotImplementedError(\"For normal generation\")\n        \n        retrieval_upper =  mem_caches is not None and self.layer_idx in self.config.ret_attn_layers\n\n        # We rotate the mem firstly\n        if retrieval_upper and self.position_type==\"Continual\":\n            mem_cos , mem_sin = self.rotary_emb(value_states, mem_position_ids)\n            mem_caches.keys = apply_rotary_pos_emb_for_relative_keys(mem_caches.keys, mem_cos,mem_sin)\n            \n        loc_cos, loc_sin = self.rotary_emb(value_states, loc_position_ids)\n        query_states = apply_rotary_pos_emb_for_relative_query(query_states, loc_cos[:,-query_states.shape[-2]:], loc_sin[:,-query_states.shape[-2]:])\n        key_states = apply_rotary_pos_emb_for_relative_keys(key_states, loc_cos, loc_sin)\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n        \n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n            \n        loc_attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n        \n        if retrieval_upper:\n            input_dtype = query_states.dtype\n            if input_dtype == torch.float32:\n                if torch.is_autocast_enabled():\n                    target_dtype = torch.get_autocast_gpu_dtype()\n                # Handle the case where the model is quantized\n                elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                    target_dtype = self.config._pre_quantization_dtype\n                else:\n                    target_dtype = self.q_proj.weight.dtype\n\n                logger.warning_once(\n                    f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                    f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                    f\" {target_dtype}.\"\n                )\n                ret_key_states = mem_caches.keys.to(target_dtype)\n                ret_value_states = mem_caches.values.to(target_dtype)\n                mem_mask = mem_caches.masks.squeeze(-1).squeeze(-2)\n                mem_mask = (mem_mask == 0).to(target_dtype)\n            else:\n                ret_key_states = mem_caches.keys\n                ret_value_states = mem_caches.values\n                mem_mask = mem_caches.masks.squeeze(-1).squeeze(-2)\n                mem_mask = (mem_mask == 0).to(input_dtype)\n            ret_key_states = ret_key_states.transpose(1, 2)\n            ret_value_states = ret_value_states.transpose(1, 2)\n            ret_attn_output = self._flash_attention_forward(query_states, ret_key_states, ret_value_states, mem_mask, q_len, dropout=dropout_rate)\n            attn_output = loc_attn_output + ret_attn_output\n        else:\n            attn_output = loc_attn_output\n        \n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value, mem_update\n    \n    def _gen_causal_mask(self,attention_mask,cache_position,sequence_length,target_length):\n        dtype, device = attention_mask.dtype, attention_mask.device\n        min_dtype = torch.finfo(dtype).min\n        causal_mask = torch.full(\n                    (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n        )\n        if sequence_length != 1:\n            causal_mask = torch.triu(causal_mask, diagonal=1)\n        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n        causal_mask = causal_mask[None, None, :, :].expand(attention_mask.shape[0], 1, -1, -1)\n        if attention_mask is not None:\n            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n            mask_length = attention_mask.shape[-1]\n            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n            padding_mask = padding_mask == 0\n            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                padding_mask, min_dtype\n        )\n        return causal_mask\n    \n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        output_attentions = False\n        bsz, q_len, _ = hidden_states.size()\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(\n            query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        # We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\n        # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n        is_causal = True if causal_mask is None and q_len > 1 else False\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": RetrievalCausalAttention,\n    \"flash_attention_2\": RetrievalFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int,mem_config: Optional[MemConfig] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.mem_config = mem_config\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, mem_config=mem_config,layer_idx=layer_idx)\n        self.output_mem = layer_idx == config.mem_layer\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches=None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n        \n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value , mem_update= self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            mem_caches=mem_caches,\n            output_mem=self.output_mem,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        outputs += (mem_update,)\n        \n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n    _supports_static_cache = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n        if config.mem_layer < 0 or config.mem_layer >= config.num_hidden_layers:\n            raise ValueError(f\"mem_layer should be between 0 and {config.num_hidden_layers - 1}\")\n        assert isinstance(config.ret_attn_layers, list), \"ret_attn_layers should be a list\"\n        self.mem_config = MemConfig(\n            positionals=config.mem_positionals,\n            use_gate=config.use_gate,\n            cache_dtype=getattr(torch, config.mem_dtype),\n        )\n        self.ret_attn_layers = config.ret_attn_layers\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])        \n        layers = []\n        for layer_id in range(config.num_hidden_layers):\n            if layer_id in self.ret_attn_layers or layer_id == config.mem_layer:\n                layer = LlamaDecoderLayer(config, layer_id, mem_config=self.mem_config)\n            else:\n                layer = LlamaDecoderLayer(config, layer_id)\n            layers.append(layer)\n\n        self.layers = nn.ModuleList(layers)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[Tuple[Optional[MemCache]]] = None,\n    ) -> Union[Tuple, BaseModelOutputWithMem]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        return_legacy_cache = False\n        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n            return_legacy_cache = True\n            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n            \n        causal_mask = self._update_causal_mask(\n            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n        )\n        \n        # embed positions\n        hidden_states = inputs_embeds\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n        mem_update = None\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                    mem_caches=mem_caches,\n                )\n            if input_ids[0][-1] == 32000:\n                pass\n                #breakpoint()\n\n            hidden_states = layer_outputs[0]\n            \n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n            \n            if layer_outputs[-1] is not None:\n                mem_update = layer_outputs[-1]\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if return_legacy_cache:\n            next_cache = next_cache.to_legacy_cache()\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        \n        return BaseModelOutputWithMem(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            mem_update=mem_update,\n        )\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n        # to infer the attention mask.\n        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n        using_static_cache = isinstance(past_key_values, StaticCache)\n\n        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                attention_mask,\n                inputs_embeds=input_tensor,\n                past_key_values_length=past_seen_tokens,\n                is_training=self.training,\n            ):\n                return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        min_dtype = torch.finfo(dtype).min\n        sequence_length = input_tensor.shape[1]\n        if using_static_cache:\n            target_length = past_key_values.get_max_length()\n        else:\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else past_seen_tokens + sequence_length\n            )\n\n        if attention_mask is not None and attention_mask.dim() == 4:\n            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n            if attention_mask.max() != 0:\n                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n            causal_mask = attention_mask\n        else:\n            causal_mask = torch.full(\n                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n            )\n            if sequence_length != 1:\n                causal_mask = torch.triu(causal_mask, diagonal=1)\n            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n            if attention_mask is not None:\n                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                padding_mask = padding_mask == 0\n                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                    padding_mask, min_dtype\n                )\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n            and not output_attentions\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\ndef _split_past_key_values(past_key_values):\n    # splits past_key_values to local cache and memory cache\n    local_cache_present = False\n    mem_caches_present = False\n    if past_key_values is not None:\n        local_caches = ()\n        mem_caches = ()\n        for layer in past_key_values:\n            if len(layer) != 6:\n                raise ValueError(\n                    \"Expected elements of past_key_values to contain 6 elements.\"\n                    \"First 3 describing local cache and last 3 describing memory cache.\"\n                    f\"Instead got {len(layer)} elements\"\n                )\n            else:\n                lk, lv, li, memk, memv, memm = layer\n                if lk.shape[-2] != 0:\n                    local_cache_present = True\n                    local_caches += ((lk, lv, li),)\n                else:\n                    local_caches += (None,)\n\n                if memk.shape[-2] != 0:\n                    mem_caches_present = True\n                    #FIXME\n                    mem_caches += (MemCache(keys=memk, values=memv, masks=memm),)\n                else:\n                    mem_caches += (None,)\n\n    local_caches = local_caches if local_cache_present else None\n    mem_caches = mem_caches if mem_caches_present else None\n\n    return local_caches, mem_caches\n\ndef _clear_memory(input_ids, toolkit, target:Literal[\"bos\",\"eos\"]=None):\n    if target == \"bos\":\n        clear_memory = (input_ids == toolkit.bos_token_id).any(dim=-1)\n    else:\n        clear_memory = (input_ids == toolkit.eos_token_id).any(dim=-1)\n    indices = [i for i, x in enumerate(clear_memory) if x]\n    if indices != []:\n        print(f\"Meet special token. Clear memory at {indices}\")\n        toolkit.reset_by_batch(indices)\n    return indices\n\ndef _retrieve(\n    toolkit,\n    input_ids,\n    ret_group_size,\n):\n    if input_ids.shape[-1] > toolkit.get_max_seq_length():\n        input_ids = input_ids[..., -toolkit.get_max_seq_length():]\n        \n    # Retrieve the memory cache\n    queries = toolkit.get_texts(input_ids)\n    # [bsz,k*?,num_heads,head_dim] | k (bsz,?,num_heads,head_dim)\n    mem_caches = toolkit.retrieve(queries=queries,k=ret_group_size)\n    return mem_caches\n\ndef _update(\n    input_ids,\n    mem_update:MemCache,\n    context_window_length,\n    toolkit,\n    clear_memory=None,\n    clear_mode: Literal[\"bos\",\"eos\"] = None\n) -> None :\n    if mem_update == None:\n        return\n\n    mem_length = mem_update.keys.shape[-2]\n    assert mem_length <= input_ids.shape[-1]\n    re_input_ids = input_ids[..., :mem_length]\n    spc_token = None\n    if clear_mode == \"bos\":\n        spc_token = toolkit.bos_token_id\n    elif clear_mode == \"eos\":\n        spc_token = toolkit.eos_token_id    \n    for length in range(0,mem_length,context_window_length):\n        now_mem_update = MemCache()\n        now_mem_update.keys = mem_update.keys[...,length:length+context_window_length,:]\n        now_mem_update.values = mem_update.values[...,length:length+context_window_length,:]\n        now_mem_update.masks = mem_update.masks[...,length:length+context_window_length,:]\n        now_mem_update.texts = toolkit.get_texts(re_input_ids[...,length:length+context_window_length])\n        # FIXME: \n        if clear_memory == True:\n            positions = (re_input_ids == spc_token).nonzero(as_tuple=True)\n            batch_indices,seq_indices = positions\n            for bsz_idx in range(re_input_ids.shape[0]):\n                if bsz_idx in batch_indices:\n                    idxs = (batch_indices == bsz_idx).nonzero(as_tuple=True)[0]\n                    last_spc_position = seq_indices[idxs].max().item()\n                    if last_spc_position == re_input_ids.shape[1] - 1:\n                        textList.append(None)\n                        kvList.append(None)\n                    else:\n                        tokens = re_input_ids[bsz_idx, last_spc_position + 1 :]\n                        examples = toolkit.get_texts(\n                            [\n                                tokens[i : i + toolkit.mem_granularity]\n                                for i in range(0, len(tokens), toolkit.mem_granularity)\n                            ],\n                            skip_special_tokens=True,\n                        )\n                        textList.append(examples)\n                        kvList.append(\n                            {\n                                \"k\": mem_update[\"k\"][bsz_idx, last_spc_position + 1 :],\n                                \"v\": mem_update[\"v\"][bsz_idx, last_spc_position + 1 :],\n                            }\n                        )\n                else:\n                    tokens = re_input_ids[bsz_idx]\n                    examples = toolkit.get_texts(\n                        [\n                            tokens[i : i + toolkit.mem_granularity]\n                            for i in range(0, len(tokens), toolkit.mem_granularity)\n                        ],\n                        skip_special_tokens=True,\n                    )\n                    textList.append(examples)\n                    kvList.append(\n                        {\n                            \"k\": mem_update[\"k\"][bsz_idx],\n                            \"v\": mem_update[\"v\"][bsz_idx],\n                        }\n                    )\n        else:\n            toolkit.update(mem_update=now_mem_update)\n\ndef _prepare_pos_ids(past_key_values, batch_size, input_length, device):\n    if past_key_values is not None:\n        # take previous max pos_id + 1\n        if past_key_values[0][2].shape[0] != batch_size:\n            raise ValueError(\n                f\"first dimension of past_key_values should match batch size: {batch_size}\"\n                f\"but got {past_key_values[0][2].shape[0]}\"\n            )\n        next_pos = torch.max(past_key_values[0][2].view(batch_size, -1), dim=-1)[0] + 1\n        next_pos = next_pos.view(batch_size, 1)\n    else:\n        next_pos = torch.zeros(batch_size, 1, device=device, dtype=torch.long)\n\n    position_ids = torch.arange(0, input_length, dtype=torch.long, device=device).view(1, input_length)\n    position_ids = position_ids + next_pos\n    return position_ids\n\n# 如果position_type是Zero，那么position_ids之前就已经定义了\n# 如果position_type是Continual，那么position_ids需要在这里定义\n\ndef _handle_mem_caches(\n    mem_caches_list:Optional[List[List[Optional[MemCache]]]],\n):\n    if mem_caches_list == None:\n        return None\n    \n    def _concat(mem_caches_list:List[List[Optional[MemCache]]]) -> List[Optional[MemCache]]:\n        sigMemCaches = []\n        max_length = 0    \n        for i , mem_caches in enumerate(mem_caches_list):\n            if mem_caches is None:\n                sigMemCaches.append(None)\n                continue\n            mem_caches = [mem_cache[i] for mem_cache in mem_caches if mem_cache is not None]\n            if mem_caches == []:\n                sigMemCaches.append(None)\n                continue\n            texts = [mem_cache.text[i] for mem_cache in mem_caches]\n            embeddings = [mem_cache.embeddings[i] for mem_cache in mem_caches]\n            keys = torch.cat([mem_cache.keys[i] for mem_cache in mem_caches],dim=1)\n            values = torch.cat([mem_cache.values[i] for mem_cache in mem_caches],dim=1)\n            masks = torch.cat([mem_cache.masks[i] for mem_cache in mem_caches],dim=1)\n            assert keys.shape[0] == values.shape[0] == masks.shape[0]\n            max_length = max(max_length,keys.shape[0])\n            sigMemCaches.append(MemCache(texts=texts,embeddings=embeddings,keys=keys,values=values,masks=masks,length=keys.shape[0]))\n        return sigMemCaches , max_length\n    \n    # mem_caches_list 包含bsz 个列表，每个列表中包含 k 个 mem_caches\n    sigMemCaches , max_length = _concat(mem_caches_list=mem_caches_list)\n    # to one cache\n    mem_caches = MemCache()\n    # fill into max_length\n    for sigMemCache in sigMemCaches:\n        length = sigMemCache.length\n        if length < max_length:\n            pad_length = max_length - length\n            sigMemCache.keys = F.pad(sigMemCache.keys,(0,0,0,pad_length))\n            sigMemCache.values = F.pad(sigMemCache.values,(0,0,0,pad_length))\n            sigMemCache.masks = F.pad(sigMemCache.masks,(0,0,0,pad_length))\n    texts,embeddings, keys,values,masks = [], [] ,[], [] , []\n    for sigMemCache in sigMemCaches:\n        texts.append(sigMemCache.texts)\n        embeddings.append(sigMemCache.embeddings)\n        keys.append(sigMemCache.keys)\n        values.append(sigMemCache.values)\n        masks.append(sigMemCache.masks)\n    mem_caches.texts = texts\n    mem_caches.embeddings = embeddings\n    mem_caches.keys = torch.stack(keys,dim=0)\n    mem_caches.values = torch.stack(values,dim=0)\n    mem_caches.masks = torch.stack(masks,dim=0)\n    \n    return mem_caches\n\ndef _handle_long_input(\n    model,\n    toolkit,\n    ret_group_size,\n    last_context_length,\n    max_mem_size,\n    context_window_length,\n    clear_memories_on_bos_token_id,\n    clear_memories_on_eos_token_id,\n    input_ids,\n    past_input_ids,\n    attention_mask,\n    position_ids,\n    past_key_values,\n    inputs_embeds,\n    use_cache,\n    output_attentions,\n    output_hidden_states,\n    return_dict,\n    cache_position,\n):\n    \"\"\"\n    Handle input that is too long for the model by splitting it in chunks and then concatenating the result.\n    \"\"\"\n    \n    \n    # Split the input into context window and last context\n    if output_attentions:\n        logger.warning(\n            f\"Outputing attentions is not supported in MemLong\"\n            f\"Attention of the last window will be returned\"\n        )\n\n    if past_key_values is not None and use_cache is False:\n        raise ValueError(\"past_key_values it not None should imply use_cache == True\")\n\n    if past_key_values is not None:\n        initial_past_key_values_length = past_key_values[0][0].shape[-2]\n    else:\n        initial_past_key_values_length = 0\n    \n    if input_ids is not None:\n        batch_size , input_length = input_ids.shape\n    else:\n        batch_size , input_length , _ = inputs_embeds.shape \n\n    if position_ids is None:\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        position_ids = _prepare_pos_ids(past_key_values, batch_size, input_length , device)\n        \n    if position_ids.shape != (batch_size, input_length):\n        raise ValueError(f\"Shape of position_ids [{position_ids}] should match [{batch_size, input_length}]\")\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[..., -(initial_past_key_values_length + input_length) :]\n        if attention_mask is not None and (\n            attention_mask.shape != (batch_size, initial_past_key_values_length + input_length)\n        ):\n            raise ValueError(\n                \"Attention mask should be provided for both the local cache and the input\",\n                f\"Expected shape {(batch_size, initial_past_key_values_length + input_length)},\"\n                f\"got {attention_mask.shape}.\",\n            )\n\n    if toolkit == None:\n        outputs = model(\n            input_ids=input_ids if input_ids is not None else None,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds if inputs_embeds is not None else None,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n            mem_caches=None,\n        )\n        outputs_list = [outputs]\n    else:\n        MemForwardNum = max(input_length - last_context_length,0)\n        MemBankSize = min(max(MemForwardNum, 0), max_mem_size)\n        outputs_list = []\n        attn_offset = initial_past_key_values_length\n        if MemBankSize > 0:\n            for i in range(MemForwardNum-MemBankSize , MemForwardNum , context_window_length):\n                beg,end = i, min(MemForwardNum, i + context_window_length)    \n                if attention_mask is not None:\n                    if past_key_values is not None:\n                        local_cache_size = past_key_values[0][0].shape[-2]\n                    else:\n                        local_cache_size = 0\n                    attn_length = attention_mask.shape[-1]\n                    attn_beg = beg + attn_offset - local_cache_size\n                    attn_end = end + attn_offset\n\n                mem_caches = _retrieve(toolkit, input_ids[..., beg:end] if input_ids is not None else None,ret_group_size=ret_group_size)\n                with torch.no_grad():\n                    outputs = model(\n                        input_ids=input_ids[..., beg:end] if input_ids is not None else None,\n                        attention_mask=attention_mask[..., attn_beg:attn_end] if attention_mask is not None else None,\n                        position_ids=position_ids[..., beg:end],\n                        past_key_values=past_key_values,\n                        inputs_embeds=inputs_embeds[..., beg:end, :] if inputs_embeds is not None else None,\n                        use_cache=False if past_key_values is None else use_cache,\n                        output_attentions=output_attentions,\n                        output_hidden_states=output_hidden_states,\n                        return_dict=True,\n                        cache_position=cache_position[beg:end] if cache_position is not None else None,\n                        mem_caches=mem_caches,\n                    )\n                # TODO\n                mem_update = outputs.mem_update  \n                outputs.mem_update = None\n                past_key_values = outputs.past_key_values  \n                outputs.past_key_values = None\n                outputs_list.append(outputs)\n                clear_memory , clear_mode = False , None\n                if clear_memories_on_bos_token_id:\n                    _clear_memory(input_ids,token_id=\"bos\",toolkit=toolkit)\n                    clear_memory , clear_mode = True , \"bos\"\n                elif clear_memories_on_eos_token_id:\n                    _clear_memory(input_ids,token_id=\"eos\",toolkit=toolkit)\n                    clear_memory , clear_mode = True , \"eos\"\n                _update(input_ids[...,beg:end],mem_update,context_window_length,toolkit,clear_memory,clear_mode)\n                \n        remaining_input_length = input_length - MemForwardNum\n        beg = MemForwardNum\n        attn_length = remaining_input_length\n        if past_key_values is not None:\n            attn_length += past_key_values[0][0].shape[-2]\n        attention_mask = attention_mask[..., -attn_length:] if attention_mask is not None else None\n        if past_input_ids is None:\n            mem_caches = _retrieve(toolkit, input_ids[..., beg:] if input_ids is not None else None,ret_group_size=ret_group_size)\n        else:\n            mem_caches = _retrieve(toolkit, input_ids=torch.concat((past_input_ids,input_ids),dim=-1) if input_ids is not None else None,ret_group_size=ret_group_size)\n        \n        outputs = model(\n            input_ids=input_ids[..., beg:] if input_ids is not None else None,\n            attention_mask=attention_mask,\n            position_ids=position_ids[..., beg:],\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds[..., beg:, :] if inputs_embeds is not None else None,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n            mem_caches=mem_caches,\n        )\n        outputs_list.append(outputs) \n        clear_memory , clear_mode = False , None\n        if clear_memories_on_bos_token_id:\n            _clear_memory(input_ids,token_id=\"bos\",toolkit=toolkit)\n            clear_memory , clear_mode = True , \"bos\"\n        elif clear_memories_on_eos_token_id:\n            _clear_memory(input_ids,token_id=\"eos\",toolkit=toolkit)\n            clear_memory , clear_mode = True , \"eos\"\n\n        _update(past_input_ids if past_input_ids is not None else input_ids[...,beg:],outputs.mem_update,context_window_length, toolkit,clear_memory,clear_mode)\n\n    if output_hidden_states:\n        hidden_states = ()\n        for hd in zip(*[x.hidden_states for x in outputs_list]):\n            hidden_states += (torch.cat(hd, dim=-2))\n    else:\n        hidden_states = None\n            \n    past_key_values = outputs_list[-1].past_key_values\n    outputs = BaseModelOutputWithPast(\n        last_hidden_state=torch.concat([x.last_hidden_state for x in outputs_list], dim=-2),\n        past_key_values=past_key_values,\n        hidden_states=hidden_states,\n        attentions=outputs_list[-1].attentions,\n    )\n    if not return_dict:\n        outputs = tuple(v for v in [outputs.last_hidden_state, outputs.past_key_values, outputs.hidden_states, outputs.attentions] if v is not None)\n    return outputs\n    \n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    def __init__(self, config, toolkit_config:ToolkitConfig):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.toolkit = ToolKit(model_config=config,toolkit_config=toolkit_config,device=self.model.device)\n        self.max_mem_size = config.memory_size \n        # self.mem_group_size = config.mem_group_size\n        self.ret_group_size = config.ret_group_size\n        self.context_window_length = min(config.max_position_embeddings, self.toolkit.get_max_seq_length(),config.mem_group_size)\n        self.clear_memories_on_bos_token_id = config.clear_memories_on_bos_token_id\n        self.clear_memories_on_eos_token_id = config.clear_memories_on_eos_token_id\n        self.position_type = config.position_type \n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.move_time = 1\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        past_input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        last_context_length: Optional[int] = None,\n        use_toolkit: Optional[bool] = True,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n\n        if not use_toolkit and self.toolkit:\n            self.toolkit = None\n        \n        if self.toolkit is not None and self.model.device != self.toolkit.device and self.move_time:\n            self.toolkit.to(self.model.device)\n            self.move_time -= 1\n            \n        last_context_length = (last_context_length if last_context_length is not None else self.config.last_context_length)\n        \n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        \n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = _handle_long_input(\n            model=self.model,\n            toolkit=self.toolkit,\n            # mem_group_size=self.mem_group_size,\n            ret_group_size=self.ret_group_size,\n            context_window_length=self.context_window_length,\n            last_context_length=last_context_length,\n            max_mem_size=self.max_mem_size,\n            clear_memories_on_bos_token_id=self.clear_memories_on_bos_token_id,\n            clear_memories_on_eos_token_id=self.clear_memories_on_eos_token_id,\n            input_ids=input_ids,\n            past_input_ids=past_input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def set_toolkit_tokenizer(self, tokenizer):\n        self.toolkit.set_tokenizer(tokenizer)\n    \n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        cache_position=None,\n        use_cache=True,\n        last_context_length=None,\n        **kwargs,\n    ):        \n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\n                max_cache_length = (\n                    torch.tensor(past_key_values.get_max_length(), device=input_ids.device)\n                    if past_key_values.get_max_length() is not None\n                    else None\n                )\n                cache_length = past_length if max_cache_length is None else torch.min(max_cache_length, past_length)\n            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                past_input_ids = input_ids[:,:-1]\n                input_ids = input_ids[:,-1:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if (\n                max_cache_length is not None\n                and attention_mask is not None\n                and cache_length + input_ids.shape[1] > max_cache_length\n            ):\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values is not None:\n                position_ids = position_ids[:, -1:]\n        \n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        if cache_position is None:\n            cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        elif use_cache:\n            cache_position = cache_position[-input_length:]\n        model_inputs.update(\n            {\n                \"past_input_ids\": past_input_ids,\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": use_cache,\n                \"attention_mask\": attention_mask,\n                \"last_context_length\": last_context_length,\n            }\n        )\n\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n"}
{"type": "source_file", "path": "src/configuration_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" LLaMA model configuration\"\"\"\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\nimport torch\n\nlogger = logging.get_logger(__name__)\n\n\nclass LlamaConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the LLaMA-7B.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 32000):\n            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`LlamaModel`]\n        hidden_size (`int`, *optional*, defaults to 4096):\n            Dimension of the hidden representations.\n        intermediate_size (`int`, *optional*, defaults to 11008):\n            Dimension of the MLP representations.\n        num_hidden_layers (`int`, *optional*, defaults to 32):\n            Number of hidden layers in the Transformer decoder.\n        num_attention_heads (`int`, *optional*, defaults to 32):\n            Number of attention heads for each attention layer in the Transformer decoder.\n        num_key_value_heads (`int`, *optional*):\n            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n            by meanpooling all the original heads within that group. For more details checkout [this\n            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n            `num_attention_heads`.\n        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n            The non-linear activation function (function or string) in the decoder.\n        max_position_embeddings (`int`, *optional*, defaults to 2048):\n            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n            Llama 2 up to 4096, CodeLlama up to 16384.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n            The epsilon used by the rms normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models). Only\n            relevant if `config.is_decoder=True`.\n        pad_token_id (`int`, *optional*):\n            Padding token id.\n        bos_token_id (`int`, *optional*, defaults to 1):\n            Beginning of stream token id.\n        eos_token_id (`int`, *optional*, defaults to 2):\n            End of stream token id.\n        pretraining_tp (`int`, *optional*, defaults to 1):\n            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to understand more about it. This value is\n            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n            issue](https://github.com/pytorch/pytorch/issues/76232).\n        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n            Whether to tie weight embeddings\n        rope_theta (`float`, *optional*, defaults to 10000.0):\n            The base period of the RoPE embeddings.\n        rope_scaling (`Dict`, *optional*):\n            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n            these scaling strategies behave:\n            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n            experimental feature, subject to breaking API changes in future versions.\n        attention_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        mlp_bias (`bool`, *optional*, defaults to `False`):\n            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n\n    ```python\n    >>> from transformers import LlamaModel, LlamaConfig\n\n    >>> # Initializing a LLaMA llama-7b style configuration\n    >>> configuration = LlamaConfig()\n\n    >>> # Initializing a model from the llama-7b style configuration\n    >>> model = LlamaModel(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"llama\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    def __init__(\n        self,\n        vocab_size=32000,\n        hidden_size=4096,\n        intermediate_size=11008,\n        num_hidden_layers=32,\n        num_attention_heads=32,\n        num_key_value_heads=None,\n        hidden_act=\"silu\",\n        max_position_embeddings=2048,\n        initializer_range=0.02,\n        rms_norm_eps=1e-6,\n        use_cache=True,\n        pad_token_id=0,\n        bos_token_id=1,\n        eos_token_id=2,\n        pretraining_tp=1,\n        tie_word_embeddings=False,\n        rope_theta=10000.0,\n        rope_scaling=None,\n        attention_bias=False,\n        attention_dropout=0.0,\n        mlp_bias=False,\n        mem_layer=None,\n        ret_attn_layers = [],\n        memory_size = 32768,\n        last_context_length=1024,\n        ret_group_size=16,\n        pooling_tokens=None,\n        clear_memories_on_eos_token_id = False,\n        clear_memories_on_bos_token_id = False,\n        update_boundary = None,\n        use_gate = False,\n        position_type = \"Continual\",\n        mem_positionals=True,\n        mem_dtype= \"bfloat16\",\n        mem_group_size=32,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.hidden_size = hidden_size\n        self.intermediate_size = intermediate_size\n        self.num_hidden_layers = num_hidden_layers\n        self.num_attention_heads = num_attention_heads\n\n        # for backward compatibility\n        if num_key_value_heads is None:\n            num_key_value_heads = num_attention_heads\n\n        self.num_key_value_heads = num_key_value_heads\n        self.hidden_act = hidden_act\n        self.initializer_range = initializer_range\n        self.rms_norm_eps = rms_norm_eps\n        self.pretraining_tp = pretraining_tp\n        self.use_cache = use_cache\n        self.rope_theta = rope_theta\n        self.rope_scaling = rope_scaling\n        self._rope_scaling_validation()\n        self.attention_bias = attention_bias\n        self.attention_dropout = attention_dropout\n        self.mlp_bias = mlp_bias\n        \n        self.position_type = position_type\n        # RET\n        self.mem_layer = mem_layer\n        self.ret_attn_layers = ret_attn_layers\n        self.memory_size = memory_size\n        self.last_context_length = last_context_length\n        self.pooling_tokens = pooling_tokens\n        self.clear_memories_on_eos_token_id = clear_memories_on_eos_token_id\n        self.clear_memories_on_bos_token_id = clear_memories_on_bos_token_id\n        self.use_gate = use_gate\n        self.update_boundary = update_boundary\n        self.mem_group_size = mem_group_size\n        self.mem_dtype = mem_dtype\n        self.mem_positionals = mem_positionals\n        self.ret_group_size = ret_group_size\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            tie_word_embeddings=tie_word_embeddings,\n            **kwargs,\n        )\n\n    def _rope_scaling_validation(self):\n        \"\"\"\n        Validate the `rope_scaling` configuration.\n        \"\"\"\n        if self.rope_scaling is None:\n            return\n\n        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n            raise ValueError(\n                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n            )\n        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n            raise ValueError(\n                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n            )\n        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")"}
{"type": "source_file", "path": "eval/language_modeling/longllama/__init__.py", "content": ""}
{"type": "source_file", "path": "eval/language_modeling/MemLong/modeling_llama_position.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"PyTorch LLaMA model.\"\"\"\n\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom .cache_utils import Cache, DynamicCache, StaticCache\nfrom transformers.modeling_attn_mask_utils import AttentionMaskConverter\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import ALL_LAYERNORM_LAYERS\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_flash_attn_2_available,\n    is_flash_attn_greater_or_equal_2_10,\n    logging,\n    replace_return_docstrings,\n)\n\nfrom .configuration_llama import LlamaConfig\nfrom .utils import ToolkitConfig,MemConfig,MemCache\nfrom typing import Literal\nfrom dataclasses import dataclass\nfrom .toolkit import ToolKit\n\nif is_flash_attn_2_available():\n    from flash_attn import flash_attn_func, flash_attn_varlen_func\n    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n\n\n\n@dataclass\nclass BaseModelOutputWithMem(BaseModelOutputWithPast):\n    mem_update: Optional[MemCache] = None\n\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _get_unpad_data(attention_mask):\n    seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n    indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n    max_seqlen_in_batch = seqlens_in_batch.max().item()\n    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n    return (\n        indices,\n        cu_seqlens,\n        max_seqlen_in_batch,\n    )\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        input_dtype = hidden_states.dtype\n        hidden_states = hidden_states.to(torch.float32)\n        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * hidden_states.to(input_dtype)\n\n\nALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n\n\nclass LlamaRotaryEmbedding(nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n        super().__init__()\n        self.scaling_factor = scaling_factor\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        # For BC we register cos and sin cached\n        self.max_seq_len_cached = max_position_embeddings\n\n    @torch.no_grad()\n    def forward(self, x, position_ids):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n        position_ids_expanded = position_ids[:, None, :].float()\n        # Force float32 since bfloat16 loses precision on long contexts\n        # See https://github.com/huggingface/transformers/pull/29285\n        device_type = x.device.type\n        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n        with torch.autocast(device_type=device_type, enabled=False):\n            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n            emb = torch.cat((freqs, freqs), dim=-1)\n            cos = emb.cos()\n            sin = emb.sin()\n        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n\n\nclass LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: a scaling factor is aplied to the position ids\n        position_ids = position_ids.float() / self.scaling_factor\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\nclass LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n\n    def forward(self, x, position_ids):\n        # difference to the original RoPE: inv_freq is recomputed when the sequence length > original length\n        seq_len = torch.max(position_ids) + 1\n        if seq_len > self.max_position_embeddings:\n            base = self.base * (\n                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n            ) ** (self.dim / (self.dim - 2))\n            inv_freq = 1.0 / (\n                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n            )\n            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n\n        cos, sin = super().forward(x, position_ids)\n        return cos, sin\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n\n    Args:\n        q (`torch.Tensor`): The query tensor.\n        k (`torch.Tensor`): The key tensor.\n        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n        sin (`torch.Tensor`): The sine part of the rotary embedding.\n        position_ids (`torch.Tensor`, *optional*):\n            Deprecated and unused.\n        unsqueeze_dim (`int`, *optional*, defaults to 1):\n            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n    Returns:\n        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n    \"\"\"\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\ndef apply_rotary_pos_emb_for_relative_query(q, cos, sin,unsqueeze_dim=1):\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    q_embed = (q * cos) + (rotate_half(q) * sin)  # q: [bs, nh, seq_len, dim]\n    return q_embed\n\ndef apply_rotary_pos_emb_for_relative_keys(k, cos, sin, unsqueeze_dim=1):\n    cos = cos.unsqueeze(unsqueeze_dim)\n    sin = sin.unsqueeze(unsqueeze_dim)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return k_embed\n\n# Based on transformers.models.llama.modeling_llama.apply_rotary_pos_emb\ndef rotate_one(x, cos, sin, position_ids):\n    if len(position_ids.shape) != 2 or x.shape[0] != position_ids.shape[0] or x.shape[-2] != position_ids.shape[1]:\n        raise ValueError(f\"Position ids shoud have shape [bsz, seq_len] got {position_ids.shape}\")\n    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n    x_embed = (x * cos) + (rotate_half(x) * sin)\n    return x_embed\n\ndef rotate_as_if_first(x, rotary_emb):\n    # x: [bs, num_attention_heads, seq_len, head_size]\n    # apply rotary as if all elements were first in the sequence\n    cos, sin = rotary_emb(x, torch.arange(x.shape[-2]).view(1,-1).type_as(x))\n    return rotate_one(x, cos, sin, torch.zeros(x.shape[0], x.shape[-2], dtype=torch.long, device=cos.device))\n\nclass LlamaMLP(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.intermediate_size = config.intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.mlp_bias)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.mlp_bias)\n        self.act_fn = ACT2FN[config.hidden_act]\n\n    def forward(self, x):\n        if self.config.pretraining_tp > 1:\n            slice = self.intermediate_size // self.config.pretraining_tp\n            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n\n            gate_proj = torch.cat(\n                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n            )\n            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n\n            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n            down_proj = [\n                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n            ]\n            down_proj = sum(down_proj)\n        else:\n            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n        return down_proj\n\n\ndef repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n    \"\"\"\n    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n    \"\"\"\n    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n    if n_rep == 1:\n        return hidden_states\n    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        if self.config.pretraining_tp > 1:\n            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n            query_slices = self.q_proj.weight.split(\n                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n            )\n            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n            query_states = torch.cat(query_states, dim=-1)\n\n            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n            key_states = torch.cat(key_states, dim=-1)\n\n            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n            value_states = torch.cat(value_states, dim=-1)\n\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attention_mask is not None:  # no matter the length, we just slice it\n            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n            attn_weights = attn_weights + causal_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\nclass RetGate(nn.Module):\n    def __init__(self, num_heads):\n        super().__init__()\n        self.output_gate = torch.nn.Parameter(torch.zeros(1, num_heads, 1, 1))\n        self.sigmoid = torch.nn.Sigmoid()\n\n    def forward(self, ret_score, attn_score):\n        # print(self.output_gate[0][0][0])\n        # print(self.sigmoid(self.output_gate[0][0][0]))\n        return torch.cat((self.sigmoid(self.output_gate) * ret_score,(1 - self.sigmoid(self.output_gate)) * attn_score,),dim=-1,)\n\n\nclass RetrievalCausalAttention(nn.Module):\n    \n    def __init__(self, config:LlamaConfig, mem_config: MemConfig, layer_idx: Optional[int] = None):\n        super().__init__()\n        self.config = config\n        self.layer_idx = layer_idx\n        if layer_idx is None:\n            logger.warning_once(\n                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                \"when creating this class.\"\n            )\n\n        self.attention_dropout = config.attention_dropout\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n        self.max_position_embeddings = config.max_position_embeddings\n        self.rope_theta = config.rope_theta\n        self.is_causal = True\n        self.max_local_cache = config.max_position_embeddings\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n        \n        # control mem behavior\n        self.mem_config = mem_config\n        # begin of sentence\n        self.clear_memory_on_bos_token_id = getattr(mem_config,\"clear_memory_on_bos_token_id\",False)\n        # end of sentence\n        self.clear_memory_on_eos_token_id = getattr(mem_config,\"clear_memory_on_eos_token_id\",False)\n        # None;Zero;Continual\n        self.position_type = config.position_type\n        self.ret_gate = None\n        # Only for Retrieval\n        if getattr(mem_config,\"use_gate\",False):\n            self.ret_gate = RetGate(self.num_heads)\n        \n        self._init_rope()\n\n    def _init_rope(self):\n        if self.config.rope_scaling is None:\n            self.rotary_emb = LlamaRotaryEmbedding(\n                self.head_dim,\n                max_position_embeddings=self.max_position_embeddings,\n                base=self.rope_theta,\n            )\n        else:\n            scaling_type = self.config.rope_scaling[\"type\"]\n            scaling_factor = self.config.rope_scaling[\"factor\"]\n            if scaling_type == \"linear\":\n                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            elif scaling_type == \"dynamic\":\n                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n                    self.head_dim,\n                    max_position_embeddings=self.max_position_embeddings,\n                    scaling_factor=scaling_factor,\n                    base=self.rope_theta,\n                )\n            else:\n                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n    \n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return (\n            tensor.view(bsz, seq_len, self.num_heads, self.head_dim)\n            .transpose(1, 2)\n            .contiguous()\n        )\n    \n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[MemCache] = None,\n        output_mem: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n\n        bsz, q_len, _ = hidden_states.size()\n        if self.config.pretraining_tp > 1:\n            raise NotImplementedError(\"pretraining_tp > 1 not supported for RetrievalCausalAttention\")\n        else:\n            query_states = self.q_proj(hidden_states)\n            key_states = self.k_proj(hidden_states)\n            value_states = self.v_proj(hidden_states)\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        \n        # Whether to use positional encoding when saving \n        use_positionals = self.mem_config is None or getattr(self.mem_config,\"positionals\",True)\n        # two conditions \n        # 1. use for saving or training \n        mem_no_local_cache = output_mem is True and past_key_value is None and (not use_cache)\n        # 2. use for generation\n        mem_and_local_cache = output_mem is True and use_cache\n        mem_update = None\n        if mem_no_local_cache:\n            if use_positionals and self.position_type == \"Zero\":\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n            \n            mem_update = MemCache(\n                keys=rfst_key_states.detach().to(self.mem_config.cache_dtype),\n                values=value_states.detach().to(self.mem_config.cache_dtype),\n                masks=attention_mask[...,-1,:,None].detach()\n            )   \n            \n        # Get local_seq_length\n        if use_cache and past_key_value is not None:\n            past_local_cache_size = past_key_value.get_seq_length(layer_idx=self.layer_idx)\n            local_seq_length = past_local_cache_size + hidden_states.shape[-2]\n        else:\n            local_seq_length = hidden_states.shape[-2]\n                \n        if past_key_value is not None:\n            if len(past_key_value) <= self.layer_idx:\n                merged_key_states = key_states\n                merged_value_states = value_states\n            else:\n                merged_key_states = torch.cat([past_key_value.key_cache[self.layer_idx], key_states], dim=-2)\n                merged_value_states = torch.cat([past_key_value.value_cache[self.layer_idx], value_states], dim=-2)\n            # merged_position_ids = torch.cat([past_key_value.position_ids_cache[self.layer_idx], loc_position_ids], dim=-1)\n            \n            if attention_mask.shape[-1] != merged_key_states.shape[-2] and attention_mask.shape[-2] == query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            assert local_seq_length == merged_key_states.shape[-2]\n            \n            if merged_key_states.shape[-2] > self.max_local_cache:\n                # We drop half of max_local_cache for Memory    \n                num_elems_to_drop = past_local_cache_size // 2\n                # key_states,value_states = past_key_value.drop_and_update(drop_keys,drop_values,key_states,value_states,self.layer_idx)\n                if mem_and_local_cache:\n                    drop_keys = merged_key_states[..., :num_elems_to_drop, :]\n                    drop_values = merged_value_states[...,:num_elems_to_drop,:]\n                    drop_masks = attention_mask[...,-1,:,None]\n                    drop_masks = drop_masks[:,:, :num_elems_to_drop, :]\n                    \n                    if use_positionals and self.position_type == \"Zero\":\n                        rfst_drop_keys = rotate_as_if_first(drop_keys,self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    \n                    mem_update = MemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype).detach(),\n                        values=drop_values.to(self.mem_config.cache_dtype).detach(),\n                        masks=drop_masks.to(self.mem_config.cache_dtype).detach(),\n                    )            \n                key_states, value_states , position_ids = past_key_value.drop_and_update(key_states,value_states,position_ids,num_elems_to_drop,self.layer_idx)\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n            else:\n                key_states, value_states , position_ids = past_key_value.update(key_states, value_states, position_ids , self.layer_idx)\n\n        kv_seq_len = key_states.shape[-2]\n        \n        # Get mem_caches_length\n        if mem_caches is not None:\n            mem_caches_length = mem_caches.keys.shape[-2]\n        else:\n            mem_caches_length = 0\n        \n        if use_positionals and self.position_type == \"Zero\":\n            loc_position_ids = torch.arange(1,kv_seq_len+1).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.zeros((1,mem_caches_length)).type_as(position_ids)\n        elif use_positionals and self.position_type == \"Continual\":\n            loc_position_ids = torch.arange(mem_caches_length,mem_caches_length+kv_seq_len).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.arange(mem_caches_length).view(1,-1).type_as(position_ids)\n        else:\n            # FIXME\n            raise NotImplementedError(\"For normal generation\")\n        \n        retrieval_upper =  mem_caches is not None and self.layer_idx in self.config.ret_attn_layers\n\n        # We rotate the mem firstly\n        if retrieval_upper and self.position_type==\"Continual\":\n            mem_cos , mem_sin = self.rotary_emb(value_states, mem_position_ids)\n            mem_caches.keys = apply_rotary_pos_emb_for_relative_keys(mem_caches.keys, mem_cos,mem_sin)\n            \n        loc_cos, loc_sin = self.rotary_emb(value_states, loc_position_ids)\n        query_states = apply_rotary_pos_emb_for_relative_query(query_states, loc_cos[:,-query_states.shape[-2]:], loc_sin[:,-query_states.shape[-2]:])\n        key_states = apply_rotary_pos_emb_for_relative_keys(key_states, loc_cos, loc_sin)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        loc_attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n        if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n            )\n        loc_attn_weights = loc_attn_weights + attention_mask\n\n        if retrieval_upper:\n            mem_mask = mem_caches.masks.squeeze(-1).unsqueeze(-2)\n            # TODO: ret granularity\n            mem_attn_weights = torch.matmul(query_states,mem_caches.keys.transpose(2, 3).to(key_states.dtype),) / math.sqrt(self.head_dim)\n            \n            assert mem_mask.shape[2] == 1\n            mem_attn_weights = mem_attn_weights + mem_mask\n            if self.ret_gate:\n                attn_weights = self.ret_gate(mem_attn_weights, loc_attn_weights)\n            else:\n                attn_weights = torch.concat((mem_attn_weights,loc_attn_weights),dim=-1)\n            combined_value_states = torch.concat([mem_caches.values.to(value_states.dtype),value_states],dim=-2,)\n        else:\n            attn_weights = loc_attn_weights\n            combined_value_states = value_states\n        \n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n        attn_output = torch.matmul(attn_weights, combined_value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n        \n        attn_output = attn_output.transpose(1, 2).contiguous()\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        if self.config.pretraining_tp > 1:\n            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n        else:\n            attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value, mem_update\n\nclass RetrievalFlashAttention2(RetrievalCausalAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self.ret_gate = None\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[MemCache] = None,\n        output_mem: Optional[bool] = False,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n        \n        if attention_mask == None:\n            tgt_seq_len = hidden_states.shape[-2]\n            if past_key_value is not None:\n                src_seq_len = past_key_value.get_seq_length(self.layer_idx) + tgt_seq_len\n            else:\n                src_seq_len = tgt_seq_len\n            attention_mask = torch.ones(hidden_states.shape[0],tgt_seq_len).type_as(hidden_states)\n            rfst_attention_mask = self._gen_causal_mask(attention_mask,cache_position,src_seq_len,tgt_seq_len)\n        else:\n            rfst_attention_mask = attention_mask\n\n        output_attentions = False\n        bsz, q_len, _ = hidden_states.size()\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        # Whether to use positional encoding when saving \n        use_positionals = self.mem_config is None or getattr(self.mem_config,\"positionals\",True)\n        # two conditions \n        # 1. use for saving or training \n        mem_no_local_cache = output_mem is True and past_key_value is None and (not use_cache)\n        # 2. use for generation\n        mem_and_local_cache = output_mem is True and use_cache\n        mem_update = None\n        if mem_no_local_cache:\n            if use_positionals and self.position_type == \"Zero\":\n                rfst_key_states = rotate_as_if_first(key_states, self.rotary_emb)\n            else:\n                rfst_key_states = key_states\n                \n            mem_update = MemCache(\n                keys=rfst_key_states.detach().to(self.mem_config.cache_dtype),\n                values=value_states.detach().to(self.mem_config.cache_dtype),\n                masks=rfst_attention_mask[...,-1,:,None].detach()\n            )\n        # Get local_seq_length\n        if use_cache and past_key_value is not None:\n            past_local_cache_size = past_key_value.get_seq_length(layer_idx=self.layer_idx)\n            local_seq_length = past_local_cache_size + hidden_states.shape[-2]\n        else:\n            local_seq_length = hidden_states.shape[-2]\n        \n        if past_key_value is not None:\n            if len(past_key_value) <= self.layer_idx:\n                merged_key_states = key_states\n                merged_value_states = value_states\n            else:\n                merged_key_states = torch.cat([past_key_value.key_cache[self.layer_idx], key_states], dim=-2)\n                merged_value_states = torch.cat([past_key_value.value_cache[self.layer_idx], value_states], dim=-2)\n            if rfst_attention_mask.shape[-1] != merged_key_states.shape[-2] and rfst_attention_mask.shape[-2] == query_states.shape[-2]:\n                raise ValueError(\"attention_mask should be provided for all key_states in local context\")\n\n            assert local_seq_length == merged_key_states.shape[-2]\n            \n            if merged_key_states.shape[-2] > self.max_local_cache:\n                # We drop half of max_local_cache for Memory    \n                num_elems_to_drop = past_local_cache_size // 2\n                # key_states,value_states = past_key_value.drop_and_update(drop_keys,drop_values,key_states,value_states,self.layer_idx)\n                if mem_and_local_cache:\n                    drop_keys = merged_key_states[..., :num_elems_to_drop, :]\n                    drop_values = merged_value_states[...,:num_elems_to_drop,:]\n                    drop_masks = rfst_attention_mask[...,-1,:,None]\n                    drop_masks = drop_masks[:,:, :num_elems_to_drop, :]\n                    \n                    if use_positionals and self.position_type == \"Zero\":\n                        rfst_drop_keys = rotate_as_if_first(drop_keys,self.rotary_emb)\n                    else:\n                        rfst_drop_keys = drop_keys\n                    \n                    mem_update = MemCache(\n                        keys=rfst_drop_keys.to(self.mem_config.cache_dtype).detach(),\n                        values=drop_values.to(self.mem_config.cache_dtype).detach(),\n                        masks=drop_masks.to(self.mem_config.cache_dtype).detach(),\n                    )            \n                key_states, value_states , position_ids = past_key_value.drop_and_update(key_states,value_states,position_ids,num_elems_to_drop,self.layer_idx)\n                attention_mask = attention_mask[..., num_elems_to_drop:]\n            else:\n                key_states, value_states , position_ids = past_key_value.update(key_states, value_states, position_ids , self.layer_idx)\n\n        kv_seq_len = key_states.shape[-2]\n        \n        # Get mem_caches_length\n        if mem_caches is not None:\n            mem_caches_length = mem_caches.keys.shape[-2]\n        else:\n            mem_caches_length = 0\n        \n        if use_positionals and self.position_type == \"Zero\":\n            loc_position_ids = torch.arange(1,kv_seq_len+1).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.zeros((1,mem_caches_length)).type_as(position_ids)\n        elif use_positionals and self.position_type == \"Continual\":\n            loc_position_ids = torch.arange(mem_caches_length,mem_caches_length+kv_seq_len).view(1,-1).type_as(position_ids)\n            mem_position_ids = torch.arange(mem_caches_length).view(1,-1).type_as(position_ids)\n        else:\n            # FIXME\n            raise NotImplementedError(\"For normal generation\")\n        \n        retrieval_upper =  mem_caches is not None and self.layer_idx in self.config.ret_attn_layers\n\n        # We rotate the mem firstly\n        if retrieval_upper and self.position_type==\"Continual\":\n            mem_cos , mem_sin = self.rotary_emb(value_states, mem_position_ids)\n            mem_caches.keys = apply_rotary_pos_emb_for_relative_keys(mem_caches.keys, mem_cos,mem_sin)\n            \n        loc_cos, loc_sin = self.rotary_emb(value_states, loc_position_ids)\n        query_states = apply_rotary_pos_emb_for_relative_query(query_states, loc_cos[:,-query_states.shape[-2]:], loc_sin[:,-query_states.shape[-2]:])\n        key_states = apply_rotary_pos_emb_for_relative_keys(key_states, loc_cos, loc_sin)\n\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n        \n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n            \n        loc_attn_output = self._flash_attention_forward(query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate)\n        \n        if retrieval_upper:\n            input_dtype = query_states.dtype\n            if input_dtype == torch.float32:\n                if torch.is_autocast_enabled():\n                    target_dtype = torch.get_autocast_gpu_dtype()\n                # Handle the case where the model is quantized\n                elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                    target_dtype = self.config._pre_quantization_dtype\n                else:\n                    target_dtype = self.q_proj.weight.dtype\n\n                logger.warning_once(\n                    f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                    f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                    f\" {target_dtype}.\"\n                )\n                ret_key_states = mem_caches.keys.to(target_dtype)\n                ret_value_states = mem_caches.values.to(target_dtype)\n                mem_mask = mem_caches.masks.squeeze(-1).squeeze(-2)\n                mem_mask = (mem_mask == 0).to(target_dtype)\n            else:\n                ret_key_states = mem_caches.keys\n                ret_value_states = mem_caches.values\n                mem_mask = mem_caches.masks.squeeze(-1).squeeze(-2)\n                mem_mask = (mem_mask == 0).to(input_dtype)\n            ret_key_states = ret_key_states.transpose(1, 2)\n            ret_value_states = ret_value_states.transpose(1, 2)\n            ret_attn_output = self._flash_attention_forward(query_states, ret_key_states, ret_value_states, mem_mask, q_len, dropout=dropout_rate)\n            attn_output = loc_attn_output + ret_attn_output\n        else:\n            attn_output = loc_attn_output\n        \n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value, mem_update\n    \n    def _gen_causal_mask(self,attention_mask,cache_position,sequence_length,target_length):\n        dtype, device = attention_mask.dtype, attention_mask.device\n        min_dtype = torch.finfo(dtype).min\n        causal_mask = torch.full(\n                    (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n        )\n        if sequence_length != 1:\n            causal_mask = torch.triu(causal_mask, diagonal=1)\n        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n        causal_mask = causal_mask[None, None, :, :].expand(attention_mask.shape[0], 1, -1, -1)\n        if attention_mask is not None:\n            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n            mask_length = attention_mask.shape[-1]\n            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n            padding_mask = padding_mask == 0\n            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                padding_mask, min_dtype\n        )\n        return causal_mask\n    \n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaFlashAttention2(LlamaAttention):\n    \"\"\"\n    Llama flash attention module. This module inherits from `LlamaAttention` as the weights of the module stays\n    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n    flash attention and deal with padding tokens in case the input contains any of them.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.LongTensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        **kwargs,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if isinstance(past_key_value, StaticCache):\n            raise ValueError(\n                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n            )\n\n        output_attentions = False\n        bsz, q_len, _ = hidden_states.size()\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        # Flash attention requires the input to have the shape\n        # batch_size x seq_length x head_dim x hidden_dim\n        # therefore we just need to keep the original shape\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n        # to be able to avoid many of these transpose/reshape/view.\n        query_states = query_states.transpose(1, 2)\n        key_states = key_states.transpose(1, 2)\n        value_states = value_states.transpose(1, 2)\n\n        dropout_rate = self.attention_dropout if self.training else 0.0\n\n        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n        # therefore the input hidden states gets silently casted in float32. Hence, we need\n        # cast them back in the correct dtype just to be sure everything works as expected.\n        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n        # in fp32. (LlamaRMSNorm handles it correctly)\n\n        input_dtype = query_states.dtype\n        if input_dtype == torch.float32:\n            if torch.is_autocast_enabled():\n                target_dtype = torch.get_autocast_gpu_dtype()\n            # Handle the case where the model is quantized\n            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                target_dtype = self.config._pre_quantization_dtype\n            else:\n                target_dtype = self.q_proj.weight.dtype\n\n            logger.warning_once(\n                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n                f\" {target_dtype}.\"\n            )\n\n            query_states = query_states.to(target_dtype)\n            key_states = key_states.to(target_dtype)\n            value_states = value_states.to(target_dtype)\n\n        attn_output = self._flash_attention_forward(\n            query_states, key_states, value_states, attention_mask, q_len, dropout=dropout_rate\n        )\n\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n    def _flash_attention_forward(\n        self, query_states, key_states, value_states, attention_mask, query_length, dropout=0.0, softmax_scale=None\n    ):\n        \"\"\"\n        Calls the forward method of Flash Attention - if the input hidden states contain at least one padding token\n        first unpad the input, then computes the attention scores and pad the final attention scores.\n\n        Args:\n            query_states (`torch.Tensor`):\n                Input query states to be passed to Flash Attention API\n            key_states (`torch.Tensor`):\n                Input key states to be passed to Flash Attention API\n            value_states (`torch.Tensor`):\n                Input value states to be passed to Flash Attention API\n            attention_mask (`torch.Tensor`):\n                The padding mask - corresponds to a tensor of size `(batch_size, seq_len)` where 0 stands for the\n                position of padding tokens and 1 for the position of non-padding tokens.\n            dropout (`float`):\n                Attention dropout\n            softmax_scale (`float`, *optional*):\n                The scaling of QK^T before applying softmax. Default to 1 / sqrt(head_dim)\n        \"\"\"\n        if not self._flash_attn_uses_top_left_mask:\n            causal = self.is_causal\n        else:\n            # TODO: Remove the `query_length != 1` check once Flash Attention for RoCm is bumped to 2.1. For details, please see the comment in LlamaFlashAttention2 __init__.\n            causal = self.is_causal and query_length != 1\n\n        # Contains at least one padding token in the sequence\n        if attention_mask is not None:\n            batch_size = query_states.shape[0]\n            query_states, key_states, value_states, indices_q, cu_seq_lens, max_seq_lens = self._upad_input(\n                query_states, key_states, value_states, attention_mask, query_length\n            )\n\n            cu_seqlens_q, cu_seqlens_k = cu_seq_lens\n            max_seqlen_in_batch_q, max_seqlen_in_batch_k = max_seq_lens\n\n            attn_output_unpad = flash_attn_varlen_func(\n                query_states,\n                key_states,\n                value_states,\n                cu_seqlens_q=cu_seqlens_q,\n                cu_seqlens_k=cu_seqlens_k,\n                max_seqlen_q=max_seqlen_in_batch_q,\n                max_seqlen_k=max_seqlen_in_batch_k,\n                dropout_p=dropout,\n                softmax_scale=softmax_scale,\n                causal=causal,\n            )\n\n            attn_output = pad_input(attn_output_unpad, indices_q, batch_size, query_length)\n        else:\n            attn_output = flash_attn_func(\n                query_states, key_states, value_states, dropout, softmax_scale=softmax_scale, causal=causal\n            )\n\n        return attn_output\n\n    def _upad_input(self, query_layer, key_layer, value_layer, attention_mask, query_length):\n        indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n        batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n\n        key_layer = index_first_axis(\n            key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        value_layer = index_first_axis(\n            value_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k\n        )\n        if query_length == kv_seq_len:\n            query_layer = index_first_axis(\n                query_layer.reshape(batch_size * kv_seq_len, self.num_heads, head_dim), indices_k\n            )\n            cu_seqlens_q = cu_seqlens_k\n            max_seqlen_in_batch_q = max_seqlen_in_batch_k\n            indices_q = indices_k\n        elif query_length == 1:\n            max_seqlen_in_batch_q = 1\n            cu_seqlens_q = torch.arange(\n                batch_size + 1, dtype=torch.int32, device=query_layer.device\n            )  # There is a memcpy here, that is very bad.\n            indices_q = cu_seqlens_q[:-1]\n            query_layer = query_layer.squeeze(1)\n        else:\n            # The -q_len: slice assumes left padding.\n            attention_mask = attention_mask[:, -query_length:]\n            query_layer, indices_q, cu_seqlens_q, max_seqlen_in_batch_q = unpad_input(query_layer, attention_mask)\n\n        return (\n            query_layer,\n            key_layer,\n            value_layer,\n            indices_q,\n            (cu_seqlens_q, cu_seqlens_k),\n            (max_seqlen_in_batch_q, max_seqlen_in_batch_k),\n        )\n\n\nclass LlamaSdpaAttention(LlamaAttention):\n    \"\"\"\n    Llama attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n    `LlamaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n    SDPA API.\n    \"\"\"\n\n    # Adapted from LlamaAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n        cache_position: Optional[torch.LongTensor] = None,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        if output_attentions:\n            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n            logger.warning_once(\n                \"LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n            )\n            return super().forward(\n                hidden_states=hidden_states,\n                attention_mask=attention_mask,\n                position_ids=position_ids,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                cache_position=cache_position,\n            )\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n        cos, sin = self.rotary_emb(value_states, position_ids)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\n        if past_key_value is not None:\n            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n\n        key_states = repeat_kv(key_states, self.num_key_value_groups)\n        value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n        causal_mask = attention_mask\n        if attention_mask is not None:\n            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n\n        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n        if query_states.device.type == \"cuda\" and causal_mask is not None:\n            query_states = query_states.contiguous()\n            key_states = key_states.contiguous()\n            value_states = value_states.contiguous()\n\n        # We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\n        # inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\n        is_causal = True if causal_mask is None and q_len > 1 else False\n\n        attn_output = torch.nn.functional.scaled_dot_product_attention(\n            query_states,\n            key_states,\n            value_states,\n            attn_mask=causal_mask,\n            dropout_p=self.attention_dropout if self.training else 0.0,\n            is_causal=is_causal,\n        )\n\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        return attn_output, None, past_key_value\n\n\nLLAMA_ATTENTION_CLASSES = {\n    \"eager\": RetrievalCausalAttention,\n    \"flash_attention_2\": RetrievalFlashAttention2,\n    \"sdpa\": LlamaSdpaAttention,\n}\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig, layer_idx: int,mem_config: Optional[MemConfig] = None):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.mem_config = mem_config\n        self.self_attn = LLAMA_ATTENTION_CLASSES[config._attn_implementation](config=config, mem_config=mem_config,layer_idx=layer_idx)\n        self.output_mem = layer_idx == config.mem_layer\n        self.mlp = LlamaMLP(config)\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Cache] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches=None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*):\n                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                query_sequence_length, key_sequence_length)` if default attention is used.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n        \n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value , mem_update= self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n            cache_position=cache_position,\n            mem_caches=mem_caches,\n            output_mem=self.output_mem,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        outputs += (mem_update,)\n        \n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _skip_keys_device_placement = [\"past_key_values\"]\n    _supports_flash_attn_2 = True\n    _supports_sdpa = True\n    _supports_cache_class = True\n    _supports_static_cache = True\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n\n            Two formats are allowed:\n            - a [`~cache_utils.Cache`] instance;\n            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n            cache format.\n\n            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n            legacy cache format will be returned.\n\n            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n            of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n            the complete sequence length.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.gradient_checkpointing = False\n        if config.mem_layer < 0 or config.mem_layer >= config.num_hidden_layers:\n            raise ValueError(f\"mem_layer should be between 0 and {config.num_hidden_layers - 1}\")\n        assert isinstance(config.ret_attn_layers, list), \"ret_attn_layers should be a list\"\n        self.mem_config = MemConfig(\n            positionals=config.mem_positionals,\n            use_gate=config.use_gate,\n            cache_dtype=getattr(torch, config.mem_dtype),\n        )\n        self.ret_attn_layers = config.ret_attn_layers\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])        \n        layers = []\n        for layer_id in range(config.num_hidden_layers):\n            if layer_id in self.ret_attn_layers or layer_id == config.mem_layer:\n                layer = LlamaDecoderLayer(config, layer_id, mem_config=self.mem_config)\n            else:\n                layer = LlamaDecoderLayer(config, layer_id)\n            layers.append(layer)\n\n        self.layers = nn.ModuleList(layers)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        mem_caches: Optional[Tuple[Optional[MemCache]]] = None,\n    ) -> Union[Tuple, BaseModelOutputWithMem]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        if (input_ids is None) ^ (inputs_embeds is not None):\n            raise ValueError(\n                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n            )\n\n        if self.gradient_checkpointing and self.training and use_cache:\n            logger.warning_once(\n                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n            )\n            use_cache = False\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        return_legacy_cache = False\n        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n            return_legacy_cache = True\n            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n\n        if cache_position is None:\n            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n            cache_position = torch.arange(\n                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n            )\n        if position_ids is None:\n            position_ids = cache_position.unsqueeze(0)\n            \n        causal_mask = self._update_causal_mask(\n            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n        )\n        \n        # embed positions\n        hidden_states = inputs_embeds\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = None\n        mem_update = None\n        for decoder_layer in self.layers:\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    causal_mask,\n                    position_ids,\n                    past_key_values,\n                    output_attentions,\n                    use_cache,\n                    cache_position,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=causal_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_values,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                    cache_position=cache_position,\n                    mem_caches=mem_caches,\n                )\n\n            hidden_states = layer_outputs[0]\n            \n            if use_cache:\n                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n            \n            if layer_outputs[-1] is not None:\n                mem_update = layer_outputs[-1]\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if return_legacy_cache:\n            next_cache = next_cache.to_legacy_cache()\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        \n        return BaseModelOutputWithMem(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            mem_update=mem_update,\n        )\n\n    def _update_causal_mask(\n        self,\n        attention_mask: torch.Tensor,\n        input_tensor: torch.Tensor,\n        cache_position: torch.Tensor,\n        past_key_values: Cache,\n        output_attentions: bool,\n    ):\n        # TODO: As of torch==2.2.0, the `attention_mask` passed to the model in `generate` is 2D and of dynamic length even when the static\n        # KV cache is used. This is an issue for torch.compile which then recaptures cudagraphs at each decode steps due to the dynamic shapes.\n        # (`recording cudagraph tree for symint key 13`, etc.), which is VERY slow. A workaround is `@torch.compiler.disable`, but this prevents using\n        # `fullgraph=True`. See more context in https://github.com/huggingface/transformers/pull/29114\n\n        if self.config._attn_implementation == \"flash_attention_2\":\n            if attention_mask is not None and 0.0 in attention_mask:\n                return attention_mask\n            return None\n\n        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n        # to infer the attention mask.\n        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n        using_static_cache = isinstance(past_key_values, StaticCache)\n\n        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                attention_mask,\n                inputs_embeds=input_tensor,\n                past_key_values_length=past_seen_tokens,\n                is_training=self.training,\n            ):\n                return None\n\n        dtype, device = input_tensor.dtype, input_tensor.device\n        min_dtype = torch.finfo(dtype).min\n        sequence_length = input_tensor.shape[1]\n        if using_static_cache:\n            target_length = past_key_values.get_max_length()\n        else:\n            target_length = (\n                attention_mask.shape[-1]\n                if isinstance(attention_mask, torch.Tensor)\n                else past_seen_tokens + sequence_length\n            )\n\n        if attention_mask is not None and attention_mask.dim() == 4:\n            # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n            if attention_mask.max() != 0:\n                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n            causal_mask = attention_mask\n        else:\n            causal_mask = torch.full(\n                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n            )\n            if sequence_length != 1:\n                causal_mask = torch.triu(causal_mask, diagonal=1)\n            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n            if attention_mask is not None:\n                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n                mask_length = attention_mask.shape[-1]\n                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                padding_mask = padding_mask == 0\n                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                    padding_mask, min_dtype\n                )\n        if (\n            self.config._attn_implementation == \"sdpa\"\n            and attention_mask is not None\n            and attention_mask.device.type == \"cuda\"\n            and not output_attentions\n        ):\n            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n            # Details: https://github.com/pytorch/pytorch/issues/110213\n            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n\n        return causal_mask\n\ndef _split_past_key_values(past_key_values):\n    # splits past_key_values to local cache and memory cache\n    local_cache_present = False\n    mem_caches_present = False\n    if past_key_values is not None:\n        local_caches = ()\n        mem_caches = ()\n        for layer in past_key_values:\n            if len(layer) != 6:\n                raise ValueError(\n                    \"Expected elements of past_key_values to contain 6 elements.\"\n                    \"First 3 describing local cache and last 3 describing memory cache.\"\n                    f\"Instead got {len(layer)} elements\"\n                )\n            else:\n                lk, lv, li, memk, memv, memm = layer\n                if lk.shape[-2] != 0:\n                    local_cache_present = True\n                    local_caches += ((lk, lv, li),)\n                else:\n                    local_caches += (None,)\n\n                if memk.shape[-2] != 0:\n                    mem_caches_present = True\n                    #FIXME\n                    mem_caches += (MemCache(keys=memk, values=memv, masks=memm),)\n                else:\n                    mem_caches += (None,)\n\n    local_caches = local_caches if local_cache_present else None\n    mem_caches = mem_caches if mem_caches_present else None\n\n    return local_caches, mem_caches\n\ndef _clear_memory(input_ids, toolkit, target:Literal[\"bos\",\"eos\"]=None):\n    if target == \"bos\":\n        clear_memory = (input_ids == toolkit.bos_token_id).any(dim=-1)\n    else:\n        clear_memory = (input_ids == toolkit.eos_token_id).any(dim=-1)\n    indices = [i for i, x in enumerate(clear_memory) if x]\n    if indices != []:\n        print(f\"Meet special token. Clear memory at {indices}\")\n        toolkit.reset_by_batch(indices)\n    return indices\n\ndef _retrieve(\n    toolkit,\n    input_ids,\n    ret_group_size,\n):\n    if input_ids.shape[-1] > toolkit.get_max_seq_length():\n        input_ids = input_ids[..., -toolkit.get_max_seq_length():]\n        \n    # Retrieve the memory cache\n    queries = toolkit.get_texts(input_ids)\n    # [bsz,k*?,num_heads,head_dim] | k (bsz,?,num_heads,head_dim)\n    mem_caches = toolkit.retrieve(queries=queries,k=ret_group_size)\n    return mem_caches\n\ndef _update(\n    input_ids,\n    mem_update:MemCache,\n    context_window_length,\n    toolkit,\n    clear_memory=None,\n    clear_mode: Literal[\"bos\",\"eos\"] = None\n) -> None :\n    if mem_update == None:\n        return\n\n    mem_length = mem_update.keys.shape[-2]\n    assert mem_length <= input_ids.shape[-1]\n    re_input_ids = input_ids[..., :mem_length]\n    spc_token = None\n    if clear_mode == \"bos\":\n        spc_token = toolkit.bos_token_id\n    elif clear_mode == \"eos\":\n        spc_token = toolkit.eos_token_id    \n    for length in range(0,mem_length,context_window_length):\n        now_mem_update = MemCache()\n        now_mem_update.keys = mem_update.keys[...,length:length+context_window_length,:]\n        now_mem_update.values = mem_update.values[...,length:length+context_window_length,:]\n        now_mem_update.masks = mem_update.masks[...,length:length+context_window_length,:]\n        now_mem_update.texts = toolkit.get_texts(re_input_ids[...,length:length+context_window_length])\n        # FIXME: \n        if clear_memory == True:\n            positions = (re_input_ids == spc_token).nonzero(as_tuple=True)\n            batch_indices,seq_indices = positions\n            for bsz_idx in range(re_input_ids.shape[0]):\n                if bsz_idx in batch_indices:\n                    idxs = (batch_indices == bsz_idx).nonzero(as_tuple=True)[0]\n                    last_spc_position = seq_indices[idxs].max().item()\n                    if last_spc_position == re_input_ids.shape[1] - 1:\n                        textList.append(None)\n                        kvList.append(None)\n                    else:\n                        tokens = re_input_ids[bsz_idx, last_spc_position + 1 :]\n                        examples = toolkit.get_texts(\n                            [\n                                tokens[i : i + toolkit.mem_granularity]\n                                for i in range(0, len(tokens), toolkit.mem_granularity)\n                            ],\n                            skip_special_tokens=True,\n                        )\n                        textList.append(examples)\n                        kvList.append(\n                            {\n                                \"k\": mem_update[\"k\"][bsz_idx, last_spc_position + 1 :],\n                                \"v\": mem_update[\"v\"][bsz_idx, last_spc_position + 1 :],\n                            }\n                        )\n                else:\n                    tokens = re_input_ids[bsz_idx]\n                    examples = toolkit.get_texts(\n                        [\n                            tokens[i : i + toolkit.mem_granularity]\n                            for i in range(0, len(tokens), toolkit.mem_granularity)\n                        ],\n                        skip_special_tokens=True,\n                    )\n                    textList.append(examples)\n                    kvList.append(\n                        {\n                            \"k\": mem_update[\"k\"][bsz_idx],\n                            \"v\": mem_update[\"v\"][bsz_idx],\n                        }\n                    )\n        else:\n            toolkit.update(mem_update=now_mem_update)\n\ndef _prepare_pos_ids(past_key_values, batch_size, input_length, device):\n    if past_key_values is not None:\n        # take previous max pos_id + 1\n        if past_key_values[0][2].shape[0] != batch_size:\n            raise ValueError(\n                f\"first dimension of past_key_values should match batch size: {batch_size}\"\n                f\"but got {past_key_values[0][2].shape[0]}\"\n            )\n        next_pos = torch.max(past_key_values[0][2].view(batch_size, -1), dim=-1)[0] + 1\n        next_pos = next_pos.view(batch_size, 1)\n    else:\n        next_pos = torch.zeros(batch_size, 1, device=device, dtype=torch.long)\n\n    position_ids = torch.arange(0, input_length, dtype=torch.long, device=device).view(1, input_length)\n    position_ids = position_ids + next_pos\n    return position_ids\n\n# 如果position_type是Zero，那么position_ids之前就已经定义了\n# 如果position_type是Continual，那么position_ids需要在这里定义\n\ndef _handle_mem_caches(\n    mem_caches_list:Optional[List[List[Optional[MemCache]]]],\n):\n    if mem_caches_list == None:\n        return None\n    \n    def _concat(mem_caches_list:List[List[Optional[MemCache]]]) -> List[Optional[MemCache]]:\n        sigMemCaches = []\n        max_length = 0    \n        for i , mem_caches in enumerate(mem_caches_list):\n            if mem_caches is None:\n                sigMemCaches.append(None)\n                continue\n            mem_caches = [mem_cache[i] for mem_cache in mem_caches if mem_cache is not None]\n            if mem_caches == []:\n                sigMemCaches.append(None)\n                continue\n            texts = [mem_cache.text[i] for mem_cache in mem_caches]\n            embeddings = [mem_cache.embeddings[i] for mem_cache in mem_caches]\n            keys = torch.cat([mem_cache.keys[i] for mem_cache in mem_caches],dim=1)\n            values = torch.cat([mem_cache.values[i] for mem_cache in mem_caches],dim=1)\n            masks = torch.cat([mem_cache.masks[i] for mem_cache in mem_caches],dim=1)\n            assert keys.shape[0] == values.shape[0] == masks.shape[0]\n            max_length = max(max_length,keys.shape[0])\n            sigMemCaches.append(MemCache(texts=texts,embeddings=embeddings,keys=keys,values=values,masks=masks,length=keys.shape[0]))\n        return sigMemCaches , max_length\n    \n    # mem_caches_list 包含bsz 个列表，每个列表中包含 k 个 mem_caches\n    sigMemCaches , max_length = _concat(mem_caches_list=mem_caches_list)\n    # to one cache\n    mem_caches = MemCache()\n    # fill into max_length\n    for sigMemCache in sigMemCaches:\n        length = sigMemCache.length\n        if length < max_length:\n            pad_length = max_length - length\n            sigMemCache.keys = F.pad(sigMemCache.keys,(0,0,0,pad_length))\n            sigMemCache.values = F.pad(sigMemCache.values,(0,0,0,pad_length))\n            sigMemCache.masks = F.pad(sigMemCache.masks,(0,0,0,pad_length))\n    texts,embeddings, keys,values,masks = [], [] ,[], [] , []\n    for sigMemCache in sigMemCaches:\n        texts.append(sigMemCache.texts)\n        embeddings.append(sigMemCache.embeddings)\n        keys.append(sigMemCache.keys)\n        values.append(sigMemCache.values)\n        masks.append(sigMemCache.masks)\n    mem_caches.texts = texts\n    mem_caches.embeddings = embeddings\n    mem_caches.keys = torch.stack(keys,dim=0)\n    mem_caches.values = torch.stack(values,dim=0)\n    mem_caches.masks = torch.stack(masks,dim=0)\n    \n    return mem_caches\n\ndef _handle_long_input(\n    model,\n    toolkit,\n    ret_group_size,\n    last_context_length,\n    max_mem_size,\n    context_window_length,\n    clear_memories_on_bos_token_id,\n    clear_memories_on_eos_token_id,\n    input_ids,\n    past_input_ids,\n    attention_mask,\n    position_ids,\n    past_key_values,\n    inputs_embeds,\n    use_cache,\n    output_attentions,\n    output_hidden_states,\n    return_dict,\n    cache_position,\n):\n    \"\"\"\n    Handle input that is too long for the model by splitting it in chunks and then concatenating the result.\n    \"\"\"\n    \n    \n    # Split the input into context window and last context\n    if output_attentions:\n        logger.warning(\n            f\"Outputing attentions is not supported in MemLong\"\n            f\"Attention of the last window will be returned\"\n        )\n\n    if past_key_values is not None and use_cache is False:\n        raise ValueError(\"past_key_values it not None should imply use_cache == True\")\n\n    if past_key_values is not None:\n        initial_past_key_values_length = past_key_values[0][0].shape[-2]\n    else:\n        initial_past_key_values_length = 0\n    \n    if input_ids is not None:\n        batch_size , input_length = input_ids.shape\n    else:\n        batch_size , input_length , _ = inputs_embeds.shape \n\n    if position_ids is None:\n        device = input_ids.device if input_ids is not None else inputs_embeds.device\n        position_ids = _prepare_pos_ids(past_key_values, batch_size, input_length , device)\n        \n    if position_ids.shape != (batch_size, input_length):\n        raise ValueError(f\"Shape of position_ids [{position_ids}] should match [{batch_size, input_length}]\")\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[..., -(initial_past_key_values_length + input_length) :]\n        if attention_mask is not None and (\n            attention_mask.shape != (batch_size, initial_past_key_values_length + input_length)\n        ):\n            raise ValueError(\n                \"Attention mask should be provided for both the local cache and the input\",\n                f\"Expected shape {(batch_size, initial_past_key_values_length + input_length)},\"\n                f\"got {attention_mask.shape}.\",\n            )\n\n    if toolkit == None:\n        outputs = model(\n            input_ids=input_ids if input_ids is not None else None,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds if inputs_embeds is not None else None,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n            mem_caches=None,\n        )\n        outputs_list = [outputs]\n    else:\n        MemForwardNum = max(input_length - last_context_length,0)\n        MemBankSize = max(MemForwardNum, 0)\n        outputs_list = []\n        attn_offset = initial_past_key_values_length\n        if MemBankSize > 0:\n            for i in range(MemForwardNum-MemBankSize , MemForwardNum , context_window_length):\n                beg,end = i, min(MemForwardNum, i + context_window_length)    \n                if attention_mask is not None:\n                    if past_key_values is not None:\n                        local_cache_size = past_key_values[0][0].shape[-2]\n                    else:\n                        local_cache_size = 0\n                    attn_length = attention_mask.shape[-1]\n                    attn_beg = beg + attn_offset - local_cache_size\n                    attn_end = end + attn_offset\n\n                mem_caches = _retrieve(toolkit, input_ids[..., beg:end] if input_ids is not None else None,ret_group_size=ret_group_size)\n                with torch.no_grad():\n                    outputs = model(\n                        input_ids=input_ids[..., beg:end] if input_ids is not None else None,\n                        attention_mask=attention_mask[..., attn_beg:attn_end] if attention_mask is not None else None,\n                        position_ids=position_ids[..., beg:end],\n                        past_key_values=past_key_values,\n                        inputs_embeds=inputs_embeds[..., beg:end, :] if inputs_embeds is not None else None,\n                        use_cache=False if past_key_values is None else use_cache,\n                        output_attentions=output_attentions,\n                        output_hidden_states=output_hidden_states,\n                        return_dict=True,\n                        cache_position=cache_position[beg:end] if cache_position is not None else None,\n                        mem_caches=mem_caches,\n                    )\n\n                mem_update = outputs.mem_update  \n                outputs.mem_update = None\n                past_key_values = outputs.past_key_values  \n                outputs.past_key_values = None\n                outputs_list.append(outputs)\n                clear_memory , clear_mode = False , None\n                if clear_memories_on_bos_token_id:\n                    _clear_memory(input_ids,token_id=\"bos\",toolkit=toolkit)\n                    clear_memory , clear_mode = True , \"bos\"\n                elif clear_memories_on_eos_token_id:\n                    _clear_memory(input_ids,token_id=\"eos\",toolkit=toolkit)\n                    clear_memory , clear_mode = True , \"eos\"\n                _update(input_ids[...,beg:end],mem_update,context_window_length,toolkit,clear_memory,clear_mode)\n        \n        remaining_input_length = input_length - MemForwardNum\n        beg = MemForwardNum\n        attn_length = remaining_input_length\n        if past_key_values is not None:\n            attn_length += past_key_values[0][0].shape[-2]\n        attention_mask = attention_mask[..., -attn_length:] if attention_mask is not None else None\n        if past_input_ids is None:\n            mem_caches = _retrieve(toolkit, input_ids[..., beg:] if input_ids is not None else None,ret_group_size=ret_group_size)\n        else:\n            mem_caches = _retrieve(toolkit, input_ids=torch.concat((past_input_ids,input_ids),dim=-1) if input_ids is not None else None,ret_group_size=ret_group_size)\n        outputs = model(\n            input_ids=input_ids[..., beg:] if input_ids is not None else None,\n            attention_mask=attention_mask,\n            position_ids=position_ids[..., beg:],\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds[..., beg:, :] if inputs_embeds is not None else None,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=True,\n            mem_caches=mem_caches,\n        )\n        outputs_list.append(outputs) \n        clear_memory , clear_mode = False , None\n        if clear_memories_on_bos_token_id:\n            _clear_memory(input_ids,token_id=\"bos\",toolkit=toolkit)\n            clear_memory , clear_mode = True , \"bos\"\n        elif clear_memories_on_eos_token_id:\n            _clear_memory(input_ids,token_id=\"eos\",toolkit=toolkit)\n            clear_memory , clear_mode = True , \"eos\"\n        _update(past_input_ids if past_input_ids is not None else input_ids[...,beg:],outputs.mem_update,context_window_length, toolkit,clear_memory,clear_mode)\n\n    if output_hidden_states:\n        hidden_states = ()\n        for hd in zip(*[x.hidden_states for x in outputs_list]):\n            hidden_states += (torch.cat(hd, dim=-2))\n    else:\n        hidden_states = None\n            \n    past_key_values = outputs_list[-1].past_key_values\n    outputs = BaseModelOutputWithPast(\n        last_hidden_state=torch.concat([x.last_hidden_state for x in outputs_list], dim=-2),\n        past_key_values=past_key_values,\n        hidden_states=hidden_states,\n        attentions=outputs_list[-1].attentions,\n    )\n    if not return_dict:\n        outputs = tuple(v for v in [outputs.last_hidden_state, outputs.past_key_values, outputs.hidden_states, outputs.attentions] if v is not None)\n    return outputs\n    \n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _tied_weights_keys = [\"lm_head.weight\"]\n    def __init__(self, config, toolkit_config:ToolkitConfig):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n        self.toolkit = ToolKit(model_config=config,toolkit_config=toolkit_config,device=self.model.device)\n        self.max_mem_size = config.memory_size \n        # self.mem_group_size = config.mem_group_size\n        self.ret_group_size = config.ret_group_size\n        self.context_window_length = min(config.max_position_embeddings, self.toolkit.get_max_seq_length(),config.mem_group_size)\n        self.clear_memories_on_bos_token_id = config.clear_memories_on_bos_token_id\n        self.clear_memories_on_eos_token_id = config.clear_memories_on_eos_token_id\n        self.position_type = config.position_type \n        self.vocab_size = config.vocab_size\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        past_input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        cache_position: Optional[torch.LongTensor] = None,\n        last_context_length: Optional[int] = None,\n        use_toolkit: Optional[bool] = True,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n        ```\"\"\"\n        if self.toolkit and not use_toolkit:\n            print(\"Warning: You are not using toolkit which may infect the performance of the model.\")\n            self.toolkit = None\n            \n        if self.toolkit is not None and self.model.device != self.toolkit.device:\n            self.toolkit.to(self.model.device)\n            \n        last_context_length = (last_context_length if last_context_length is not None else self.config.last_context_length)\n        \n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        \n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = _handle_long_input(\n            model=self.model,\n            toolkit=self.toolkit,\n            # mem_group_size=self.mem_group_size,\n            ret_group_size=self.ret_group_size,\n            context_window_length=self.context_window_length,\n            last_context_length=last_context_length,\n            max_mem_size=self.max_mem_size,\n            clear_memories_on_bos_token_id=self.clear_memories_on_bos_token_id,\n            clear_memories_on_eos_token_id=self.clear_memories_on_eos_token_id,\n            input_ids=input_ids,\n            past_input_ids=past_input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            cache_position=cache_position,\n        )\n        hidden_states = outputs[0]\n        if self.config.pretraining_tp > 1:\n            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n            logits = torch.cat(logits, dim=-1)\n        else:\n            logits = self.lm_head(hidden_states)\n        logits = logits.float()\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def set_toolkit_tokenizer(self, tokenizer):\n        self.toolkit.set_tokenizer(tokenizer)\n    \n    def reset_memory(self):\n        self.toolkit.reset()\n    \n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past_input_ids=None,\n        past_key_values=None,\n        attention_mask=None,\n        inputs_embeds=None,\n        cache_position=None,\n        use_cache=True,\n        last_context_length=None,\n        use_toolkit=None,\n        **kwargs,\n    ):        \n        past_length = 0\n        if past_key_values is not None:\n            if isinstance(past_key_values, Cache):\n                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\n                max_cache_length = (\n                    torch.tensor(past_key_values.get_max_length(), device=input_ids.device)\n                    if past_key_values.get_max_length() is not None\n                    else None\n                )\n                cache_length = past_length if max_cache_length is None else torch.min(max_cache_length, past_length)\n            # TODO joao: remove this `else` after `generate` prioritizes `Cache` objects\n            else:\n                cache_length = past_length = past_key_values[0][0].shape[2]\n                max_cache_length = None\n            # Keep only the unprocessed tokens:\n            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as input)\n            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n            # input_ids based on the past_length.\n            elif past_length < input_ids.shape[1]:\n                past_input_ids = input_ids[:,:-1]\n                input_ids = input_ids[:,-1:]\n            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n\n            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n            if (\n                max_cache_length is not None\n                and attention_mask is not None\n                and cache_length + input_ids.shape[1] > max_cache_length\n            ):\n                attention_mask = attention_mask[:, -max_cache_length:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values is not None:\n                position_ids = position_ids[:, -1:]\n        \n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n            # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n            # TODO: use `next_tokens` directly instead.\n            model_inputs = {\"input_ids\": input_ids.contiguous()}\n\n        input_length = position_ids.shape[-1] if position_ids is not None else input_ids.shape[-1]\n        if cache_position is None:\n            cache_position = torch.arange(past_length, past_length + input_length, device=input_ids.device)\n        elif use_cache:\n            cache_position = cache_position[-input_length:]\n        model_inputs.update(\n            {\n                \"past_input_ids\": past_input_ids,\n                \"position_ids\": position_ids,\n                \"cache_position\": cache_position,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": use_cache,\n                \"attention_mask\": attention_mask,\n                \"use_toolkit\":use_toolkit,\n                \"last_context_length\": last_context_length,\n            }\n        )\n\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n            )\n        return reordered_past\n"}
{"type": "source_file", "path": "src/cache_utils.py", "content": "from dataclasses import dataclass\nfrom typing import Any, Dict, List, Optional, Tuple\n\nimport torch\n\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\n\n@dataclass\nclass Cache:\n    \"\"\"\n    Base, abstract class for all caches. The actual data structure is specific to each subclass.\n    \"\"\"\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. These are specific to each subclass and allow new types of\n                cache to be created.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        raise NotImplementedError(\"Make sure to implement `update` in a subclass.\")\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        raise NotImplementedError(\"Make sure to implement `get_seq_length` in a subclass.\")\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states, if there is any.\"\"\"\n        raise NotImplementedError(\"Make sure to implement `get_max_length` in a subclass.\")\n\n    def get_usable_length(self, new_seq_length: int, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Given the sequence length of the new inputs, returns the usable length of the cache.\"\"\"\n        # Cache without size limit -> all cache is usable\n        # Cache with size limit -> if the length cache plus the length of the new inputs is larger the maximum cache\n        #   length, we will need to evict part of the cache (and thus not all cache is usable)\n        max_length = self.get_max_length()\n        previous_seq_length = self.get_seq_length(layer_idx)\n        if max_length is not None and previous_seq_length + new_seq_length > max_length:\n            return max_length - new_seq_length\n        return previous_seq_length\n\n    def reorder_cache(self, beam_idx: torch.LongTensor):\n        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n        for layer_idx in range(len(self.key_cache)):\n            device = self.key_cache[layer_idx].device\n            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n            device = self.value_cache[layer_idx].device\n            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n\n    @property\n    def seen_tokens(self):\n        logger.warning_once(\n            \"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` \"\n            \"model input instead.\"\n        )\n        if hasattr(self, \"_seen_tokens\"):\n            return self._seen_tokens\n        else:\n            return None\n\n\nclass DynamicCache(Cache):\n    \"\"\"\n    A cache that grows dynamically as more tokens are generated. This is the default for generative models.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        self.position_ids_cache: List[torch.Tensor] = []\n        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n\n    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n        \"\"\"\n        Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n        sequence length.\n        \"\"\"\n        if layer_idx < len(self):\n            return (self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx])\n        else:\n            raise KeyError(f\"Cache only has {len(self)} layers, attempted to access layer with index {layer_idx}\")\n\n    def __iter__(self):\n        \"\"\"\n        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n        keys and values\n        \"\"\"\n        for layer_idx in range(len(self)):\n            yield (self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx])\n\n    def __len__(self):\n        \"\"\"\n        Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\n        to the number of layers in the model.\n        \"\"\"\n        return len(self.key_cache)\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        position_ids: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the cache\n        if len(self.key_cache) <= layer_idx:\n            self.key_cache.append(key_states)\n            self.value_cache.append(value_states)\n            self.position_ids_cache.append(position_ids)\n        else:\n            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n            self.position_ids_cache[layer_idx] = torch.cat([self.position_ids_cache[layer_idx], position_ids], dim=-1)\n            \n        return self.key_cache[layer_idx], self.value_cache[layer_idx] , self.position_ids_cache[layer_idx]\n\n    def drop_and_update(self,\n        keys_states : torch.Tensor,\n        value_states : torch.Tensor,\n        position_ids : torch.Tensor,\n        num_elems_to_drop : int,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. No additional arguments are used in `DynamicCache`.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        \n        assert keys_states.shape[-2] == value_states.shape[-2], \"The number of keys and values to drop must be the same\"\n        assert self.key_cache[layer_idx].shape[-2] >= num_elems_to_drop, \"The number of keys to drop must be less than the number of keys in the cache\"\n        \n        self.key_cache[layer_idx] = torch.concat((self.key_cache[layer_idx][:, :, num_elems_to_drop:],keys_states), dim=-2)\n        self.value_cache[layer_idx] = torch.concat((self.value_cache[layer_idx][:, :, num_elems_to_drop:], value_states), dim=-2)\n        self.position_ids_cache[layer_idx] = torch.concat((self.position_ids_cache[layer_idx][:, num_elems_to_drop:], position_ids), dim=-1)\n                        \n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens -= num_elems_to_drop\n            self._seen_tokens += keys_states.shape[-2]\n        \n        return self.key_cache[layer_idx], self.value_cache[layer_idx] , self.value_cache[layer_idx]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        if len(self.key_cache) <= layer_idx:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n        return None\n\n    def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n        \"\"\"Converts the `DynamicCache` instance into the its equivalent in the legacy cache format.\"\"\"\n        legacy_cache = ()\n        for layer_idx in range(len(self)):\n            legacy_cache += ((self.key_cache[layer_idx], self.value_cache[layer_idx],self.position_ids_cache[layer_idx]),)\n        return legacy_cache\n\n    @classmethod\n    def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n        \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`.\"\"\"\n        cache = cls()\n        if past_key_values is not None:\n            for layer_idx in range(len(past_key_values)):\n                key_states, value_states,position_ids= past_key_values[layer_idx]\n                cache.update(key_states, value_states, position_ids,layer_idx)\n        return cache\n\n\nclass SinkCache(Cache):\n    \"\"\"\n    A cache that as described in the [Attention Sinks paper](https://arxiv.org/abs/2309.17453). It allows the model to\n    generate beyond the length of its context window, without losing fluency in the conversation. As it discards past\n    tokens, the model will lose the ability to generate tokens that depend on the context that was discarded.\n\n    It stores the Key and Value states as a list of tensors, one for each layer. The expected shape for each tensor is\n    `[batch_size, num_heads, seq_len, head_dim]`.\n\n    Parameters:\n        window_length (`int`):\n            The length of the context window.\n        num_sink_tokens (`int`):\n            The number of sink tokens. See the original paper for more information.\n    \"\"\"\n\n    def __init__(self, window_length: int, num_sink_tokens: int) -> None:\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        self.window_length = window_length\n        self.num_sink_tokens = num_sink_tokens\n        self.cos_sin_rerotation_cache = {}\n        self._cos_cache = None\n        self._sin_cache = None\n        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n\n    @staticmethod\n    def _rotate_half(x):\n        x1 = x[..., : x.shape[-1] // 2]\n        x2 = x[..., x.shape[-1] // 2 :]\n        return torch.cat((-x2, x1), dim=-1)\n\n    def _apply_key_rotary_pos_emb(\n        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n    ) -> torch.Tensor:\n        rotated_key_states = (key_states * cos) + (self._rotate_half(key_states) * sin)\n        return rotated_key_states\n\n    def _get_rerotation_cos_sin(\n        self, key_states: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        if key_states.shape[-2] not in self.cos_sin_rerotation_cache:\n            # Upcast to float32 temporarily for better accuracy\n            cos = cos.to(torch.float32)\n            sin = sin.to(torch.float32)\n\n            # Compute the cos and sin required for back- and forward-rotating to one position earlier in the sequence\n            original_cos = cos[self.num_sink_tokens + key_states.shape[-2] :]\n            shifted_cos = cos[self.num_sink_tokens : -key_states.shape[-2]]\n            original_sin = sin[self.num_sink_tokens + key_states.shape[-2] :]\n            shifted_sin = sin[self.num_sink_tokens : -key_states.shape[-2]]\n            rerotation_cos = original_cos * shifted_cos + original_sin * shifted_sin\n            rerotation_sin = -original_sin * shifted_cos + original_cos * shifted_sin\n\n            self.cos_sin_rerotation_cache[key_states.shape[-2]] = (\n                rerotation_cos.to(key_states.dtype).unsqueeze(0),\n                rerotation_sin.to(key_states.dtype).unsqueeze(0),\n            )\n        return self.cos_sin_rerotation_cache[key_states.shape[-2]]\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n        # TODO: deprecate this function in favor of `cache_position`\n        # Workaround to make 'key_states.shape[-2] + past_key_value.get_seq_length(self.layer_idx)' <= window_length\n        if len(self.key_cache) <= layer_idx:\n            return 0\n        return self.key_cache[layer_idx].shape[-2]\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n        return self.window_length\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. The following arguments can be used in `SinkCache`: `sin`,\n                `cos` and `partial_rotation_size`. These arguments are used with models using RoPE, to recompute the\n                rotation as the tokens are shifted.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        # Optional kwargs for `SinkCache` -- needed on models using RoPE. `partial_rotation_size` is used on models\n        # with partially rotated position embeddings, like Phi or Persimmon.\n        sin = cache_kwargs.get(\"sin\")\n        cos = cache_kwargs.get(\"cos\")\n        partial_rotation_size = cache_kwargs.get(\"partial_rotation_size\")\n        using_rope = cos is not None and sin is not None\n\n        # Update the number of seen tokens\n        if layer_idx == 0:\n            self._seen_tokens += key_states.shape[-2]\n\n        # Update the sin/cos cache, which holds sin/cos values for all possible positions\n        if using_rope and layer_idx == 0:\n            # BC: some models still pass `sin`/`cos` with 2 dims. In those models, they are the full sin/cos. Remove\n            # after all RoPE models have a llama-like cache utilization.\n            if cos.dim() == 2:\n                self._cos_cache = cos\n                self._sin_cache = sin\n            else:\n                if self._cos_cache is None:\n                    self._cos_cache = cos[0, ...]\n                    self._sin_cache = sin[0, ...]\n                elif self._cos_cache.shape[0] < self.window_length:\n                    self._cos_cache = torch.cat([self._cos_cache, cos[0, ...]], dim=0)\n                    self._sin_cache = torch.cat([self._sin_cache, sin[0, ...]], dim=0)\n\n        # [bsz, num_heads, seq_len, head_dim]\n        if len(self.key_cache) <= layer_idx:\n            # Empty cache\n            self.key_cache.append(key_states)\n            self.value_cache.append(value_states)\n\n        elif key_states.shape[-2] + self.get_seq_length(layer_idx) < self.window_length:\n            # Growing cache\n            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2)\n            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2)\n\n        else:\n            # Shifting cache\n            keys_to_keep = self.key_cache[layer_idx][\n                :, :, -self.window_length + self.num_sink_tokens + key_states.shape[-2] :\n            ]\n\n            # On RoPE models, we need to recompute the Key rotation as the tokens are shifted\n            if using_rope:\n                rerotation_cos, rerotation_sin = self._get_rerotation_cos_sin(\n                    key_states, self._cos_cache[: self.window_length], self._sin_cache[: self.window_length]\n                )\n                if partial_rotation_size is not None:\n                    keys_to_keep, keys_pass = (\n                        keys_to_keep[..., :partial_rotation_size],\n                        keys_to_keep[..., partial_rotation_size:],\n                    )\n                keys_to_keep = self._apply_key_rotary_pos_emb(keys_to_keep, rerotation_cos, rerotation_sin)\n                if partial_rotation_size is not None:\n                    keys_to_keep = torch.cat((keys_to_keep, keys_pass), dim=-1)\n\n            # Concatenate sink tokens, shifted & rotated tokens (if needed), and new tokens\n            sink_keys = self.key_cache[layer_idx][:, :, : self.num_sink_tokens]\n            self.key_cache[layer_idx] = torch.cat([sink_keys, keys_to_keep, key_states], dim=-2)\n\n            sink_values = self.value_cache[layer_idx][:, :, : self.num_sink_tokens]\n            values_to_keep = self.value_cache[layer_idx][\n                :, :, -self.window_length + self.num_sink_tokens + value_states.shape[-2] :\n            ]\n            self.value_cache[layer_idx] = torch.cat([sink_values, values_to_keep, value_states], dim=-2)\n\n        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n\n\nclass StaticCache(Cache):\n    \"\"\"\n    Static Cache class to be used with `torch.compile(model)`.\n\n    Parameters:\n        config (`PretrainedConfig):\n            The configuration file defining the shape-related attributes required to initialize the static cache.\n        max_batch_size (`int`):\n            The maximum batch size with which the model will be used.\n        max_cache_len (`int`):\n            The maximum sequence length with which the model will be used.\n        device (`torch.device`):\n            The device on which the cache should be initialized. Should be the same as the layer.\n        dtype (*optional*, defaults to `torch.float32`):\n            The default `dtype` to use when initializing the layer.\n    \"\"\"\n\n    def __init__(self, config: PretrainedConfig, max_batch_size: int, max_cache_len: int, device, dtype=None) -> None:\n        super().__init__()\n        self.max_batch_size = max_batch_size\n        self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n        # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n        self.head_dim = (\n            config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n        )\n\n        self.dtype = dtype if dtype is not None else torch.float32\n        self.num_key_value_heads = (\n            config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n        )\n\n        self.key_cache: List[torch.Tensor] = []\n        self.value_cache: List[torch.Tensor] = []\n        cache_shape = (max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n        for _ in range(config.num_hidden_layers):\n            # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n            # breaks when updating the cache.\n            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n            torch._dynamo.mark_static_address(new_layer_key_cache)\n            torch._dynamo.mark_static_address(new_layer_value_cache)\n            self.key_cache.append(new_layer_key_cache)\n            self.value_cache.append(new_layer_value_cache)\n\n    def update(\n        self,\n        key_states: torch.Tensor,\n        value_states: torch.Tensor,\n        layer_idx: int,\n        cache_kwargs: Optional[Dict[str, Any]] = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"\n        Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\n        It is VERY important to index using a tensor, otherwise you introduce a copy to the device.\n\n        Parameters:\n            key_states (`torch.Tensor`):\n                The new key states to cache.\n            value_states (`torch.Tensor`):\n                The new value states to cache.\n            layer_idx (`int`):\n                The index of the layer to cache the states for.\n            cache_kwargs (`Dict[str, Any]`, `optional`):\n                Additional arguments for the cache subclass. The `StaticCache` needs the `cache_position` input\n                to know how where to write in the cache.\n\n        Return:\n            A tuple containing the updated key and value states.\n        \"\"\"\n        cache_position = cache_kwargs.get(\"cache_position\")\n        k_out = self.key_cache[layer_idx]\n        v_out = self.value_cache[layer_idx]\n\n        k_out[:, :, cache_position] = key_states\n        v_out[:, :, cache_position] = value_states\n\n        return k_out, v_out\n\n    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n        \"\"\"Returns the sequence length of the cached states that were seen by the model.\"\"\"\n        # Occupied cache == any slot in the 3rd dim (sequence length) holds a non-zero value. To save on compute, let's\n        # limit the check to the first batch member and head dimension.\n        # TODO: deprecate this function in favor of `cache_position`\n        return (self.key_cache[layer_idx][0, 0].any(dim=-1)).sum()\n\n    def get_max_length(self) -> Optional[int]:\n        \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n        return self.max_cache_len\n\n    def reset(self):\n        \"\"\"Resets the cache values while preserving the objects\"\"\"\n        for layer_idx in range(len(self.key_cache)):\n            # In-place ops prevent breaking the static address\n            self.key_cache[layer_idx].zero_()\n            self.value_cache[layer_idx].zero_()\n"}
