{"repo_info": {"repo_name": "vl-interp", "repo_owner": "nickjiang2378", "repo_url": "https://github.com/nickjiang2378/vl-interp"}}
{"type": "source_file", "path": "demos/run_blip.py", "content": "import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\nos.chdir(os.environ[\"VL_ROOT_DIR\"])\n\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimport numpy as np\nimport torch\n\ntry:\n  from src.caption.instruct_blip_engine import InstructBLIPVicuna7B\nexcept: # first call for some reason fails\n  pass\n\nfrom src.caption.instruct_blip_engine import InstructBLIPVicuna7B\nfrom methods.blip_utils import get_image_embeddings, get_vocab_embeddings_blip, get_hidden_text_embedding, run_blip_model\nfrom methods.utils import coco_img_id_to_name\n\ndevice = \"cuda:0\"\ncaptioner = InstructBLIPVicuna7B(device=device, return_embeds=True)\n\nvocab_embeddings = get_vocab_embeddings_blip(captioner)\n\nimg_path = os.path.join(\"./images\", coco_img_id_to_name(562150) + \".jpg\")\nimg_embeddings = get_image_embeddings(img_path, captioner)\n\ncaption = run_blip_model(img_embeddings, captioner)\nprint(caption)"}
{"type": "source_file", "path": "demos/run_llava.py", "content": "import os\nimport pickle\nimport torch\nimport tqdm\nimport json\nfrom collections import defaultdict\n\nfrom llava.constants import (\n    IMAGE_TOKEN_INDEX,\n    DEFAULT_IMAGE_TOKEN,\n    DEFAULT_IM_START_TOKEN,\n    DEFAULT_IM_END_TOKEN,\n    IMAGE_PLACEHOLDER,\n)\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import (\n    process_images,\n    tokenizer_image_token,\n    get_model_name_from_path,\n    KeywordsStoppingCriteria,\n)\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom methods.utils import coco_img_id_to_name\n\n# Change to the root directory, make sure to set the VL_ROOT_DIR environment variable\nos.chdir(os.environ[\"VL_ROOT_DIR\"])\n\ndef load_image(image_file):\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    else:\n        image = Image.open(image_file).convert(\"RGB\")\n    return image\n\ndef load_images(image_files):\n    out = []\n    for image_file in image_files:\n        image = load_image(image_file)\n        out.append(image)\n    return out\n\ndisable_torch_init()\n\n# model_path = \"liuhaotian/llava-v1.6-vicuna-7b\"\nmodel_path = \"liuhaotian/llava-v1.5-7b\"\n\nmodel_name = get_model_name_from_path(model_path)\ntokenizer, model, image_processor, context_len = load_pretrained_model(\n    model_path, None, model_name\n)\n\nqs = \"Write a detailed description.\"\nimage_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\nif IMAGE_PLACEHOLDER in qs:\n    if model.config.mm_use_im_start_end:\n        qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n    else:\n        qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\nelse:\n    if model.config.mm_use_im_start_end:\n        qs = image_token_se + \"\\n\" + qs\n    else:\n        qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n\nif \"llama-2\" in model_name.lower():\n    conv_mode = \"llava_llama_2\"\nelif \"v1\" in model_name.lower():\n    conv_mode = \"llava_v1\"\nelif \"mpt\" in model_name.lower():\n    conv_mode = \"mpt\"\nelse:\n    conv_mode = \"llava_v0\"\n\nconv = conv_templates[conv_mode].copy()\nconv.append_message(conv.roles[0], qs)\nconv.append_message(conv.roles[1], None)\nprompt = conv.get_prompt()\n\n# Add in a path to an image here\nimg_path = os.path.join(\"./images\", coco_img_id_to_name(562150) + \".jpg\")\nimage_files = [img_path]\nimages = load_images(image_files)\nimage_sizes = [x.size for x in images]\n\nimages_tensor = process_images(\n    images,\n    image_processor,\n    model.config\n).to(model.device, dtype=torch.float16)\n\ninput_ids = (\n    tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n    .unsqueeze(0)\n    .cuda()\n)\n\nstop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\nkeywords = [stop_str]\nstopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\ntemperature = 1.0\ntop_p = None\nnum_beams = 5\nmax_new_tokens = 500\n\nwith torch.inference_mode():\n    output = model.generate(\n        input_ids,\n        images=images_tensor,\n        temperature=temperature,\n        num_beams=num_beams,\n        max_new_tokens=max_new_tokens,\n        # use_cache=True,\n        use_cache=False,\n        stopping_criteria=[stopping_criteria],\n        # output_attentions=False,\n        return_dict_in_generate=True,\n        image_sizes = image_sizes\n    )\n\noutputs = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)[0].strip()\nimage_features = model.encode_images(images_tensor)\nprint(outputs)"}
{"type": "source_file", "path": "methods/algorithms.py", "content": "import torch\nfrom .utils import get_device_from_module\n\ndef get_phrase_embedding(phrase, vocab_embeddings, tokenizer, remove_first=True):\n    # returns size (1, 5120)\n    text_embeddings = []\n    for token_id in tokenizer(phrase)[\"input_ids\"]:\n        text_embeddings.append(vocab_embeddings[:, token_id])\n    if remove_first:\n        text_embeddings = text_embeddings[1:]\n    phrase_embedding = torch.sum(\n        torch.concat(text_embeddings), dim=0, keepdim=True\n    ) / len(text_embeddings)\n    return phrase_embedding\n\n\ndef projection(image_embeddings, text_embedding):\n    return (image_embeddings @ text_embedding.T)[0, :, 0] / (\n        text_embedding @ text_embedding.T\n    ).squeeze()\n\n\ndef subtract_projection(image_embeddings, text_embedding, weight=1, device = None):\n       # if device is None, don't move the embeddings to any device - keep their pre-existing device configs in tact\n    if device != None:\n        image_embeddings = image_embeddings.to(device)\n        text_embedding = text_embedding.to(device)\n    image_embeddings = image_embeddings.clone()\n    proj = projection(image_embeddings, text_embedding)\n    for i in range(image_embeddings.shape[1]):\n        if proj[i] > 0:\n            image_embeddings[:, i] -= weight * proj[i] * text_embedding\n    return image_embeddings\n\n\ndef subtract_projections(image_embeddings, text_embeddings, weight=1, device = None):\n    # text_embeddings: (# embeds, 1, # dim size)\n    # if device is None, don't move the embeddings to any device - keep their pre-existing device configs in tact\n    img_embeddings = image_embeddings.clone()\n    for text_embedding in text_embeddings:\n        img_embeddings = subtract_projection(img_embeddings, text_embedding, weight, device=device)\n    return img_embeddings\n\n\ndef remove_all_hooks(model):\n    # Iterate over all modules in the model\n    for module in model.modules():\n        # Clear forward hooks\n        if hasattr(module, \"_forward_hooks\"):\n            module._forward_hooks.clear()\n        # Clear backward hooks (if any)\n        if hasattr(module, \"_backward_hooks\"):\n            module._backward_hooks.clear()\n        # Clear forward pre-hooks (if any)\n        if hasattr(module, \"_forward_pre_hooks\"):\n            module._forward_pre_hooks.clear()\n\n\ndef generate_mass_edit_hook(\n    text_embeddings, start_edit_index, end_edit_index, layer, weight=1, minimum_size=32\n):\n    if len(text_embeddings) == 0:\n        print(\"No text embeddings found. Note that no editing will occur.\")\n    def edit_embeddings(module, input, output):\n        device = get_device_from_module(module)\n        new_output = list(output)\n        if new_output[0].shape[1] > minimum_size:\n            print(f\"Editing layer {layer}\")\n            new_output[0][:, start_edit_index:end_edit_index] = subtract_projections(\n                new_output[0][:, start_edit_index:end_edit_index],\n                text_embeddings,\n                weight=weight,\n                device=device\n            )\n        return tuple(new_output)\n\n    return edit_embeddings\n\n\ndef generate_mass_edit_pre_hook(\n    text_embeddings, start_edit_index, end_edit_index, layer, weight=1, minimum_size=32\n):\n    if len(text_embeddings) == 0:\n        print(\"No text embeddings found. Note that no editing will occur.\")\n    def edit_embeddings(module, input):\n        device = get_device_from_module(module)\n        new_input = list(input)\n        if new_input[0].shape[1] > minimum_size:\n            print(f\"Editing layer {layer}\")\n            new_input[0][:, start_edit_index:end_edit_index] = subtract_projections(\n                new_input[0][:, start_edit_index:end_edit_index],\n                text_embeddings,\n                weight=weight,\n                device=device\n            )\n        return tuple(new_input)\n\n    return edit_embeddings\n\n\ndef internal_confidence(tokenizer, softmax_probs, class_):\n    class_token_indices = tokenizer.encode(class_)[1:]\n    return softmax_probs[class_token_indices].max()\n\n\ndef internal_confidence_heatmap(tokenizer, softmax_probs, class_):\n    class_token_indices = tokenizer.encode(class_)[1:]\n    return softmax_probs[class_token_indices].max(axis=0).T\n\n\ndef internal_confidence_segmentation(tokenizer, softmax_probs, class_, num_patches=24):\n    class_token_indices = tokenizer.encode(class_)[1:]\n    return (\n        softmax_probs[class_token_indices]\n        .max(axis=0)\n        .max(axis=0)\n        .reshape(num_patches, num_patches)\n        .astype(float)\n    )\n"}
{"type": "source_file", "path": "metric/chair.py", "content": "'''\nCopied from: https://github.com/LisaAnne/Hallucination/blob/master/utils/chair.py\n\nModified by: Maxlinn\n\n1. adapt calculation of CHAIR-i and CHAIR-s for Python3, supports for both json and jsonl file input.\n2. integrate synonyms.txt to make the script standalone.\n3. remove machine-translation based metrics BLEU-n, CIDEr, ROGUE\n4. add new metric Recall, which represents the node words(i.e. lemmas of objects) coverage overall.\n5. add pickle cache mechanism to make it fast for repetitive evaluations.\n'''\n\n\nimport os\nimport sys\nimport nltk\nimport json\nfrom pattern.en import singularize\nimport argparse\nimport tqdm\nimport pickle\nfrom collections import defaultdict\n\n\n# copied from: https://github.com/LisaAnne/Hallucination/blob/master/data/synonyms.txt\nsynonyms_txt = '''\nperson, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman, cop, officer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder, skater, skateboarder, person, woman, guy, foreigner, child, gentleman, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker, drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager\nbicycle, bike, bicycle, bike, unicycle, minibike, trike\ncar, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi\nmotorcycle, scooter,  motor bike, motor cycle, motorbike, scooter, moped\nairplane, jetliner, plane, air plane, monoplane, aircraft, jet, jetliner, airbus, biplane, seaplane\nbus, minibus, trolley\ntrain, locomotive, tramway, caboose\ntruck, pickup, lorry, hauler, firetruck\nboat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel, rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat, battleship, steamship\ntraffic light, street light, traffic signal, stop light, streetlight, stoplight\nfire hydrant, hydrant\nstop sign\nparking meter\nbench, pew\nbird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, finch, pigeon, sparrow, seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingfisher, buzzard, willet, gull, swan, bluejay, flamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie, peacock, cockatiel, lorikeet, puffin, vulture, condor, macaw, peafowl, cockatoo, songbird\ncat, kitten, feline, tabby\ndog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie, doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky\nhorse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco\nsheep, lamb, ram, lamb, goat, ewe\ncow, cattle, oxen, ox, calf, cattle, holstein, heifer, buffalo, bull, zebu, bison \nelephant\nbear, panda\nzebra\ngiraffe\nbackpack, knapsack\numbrella\nhandbag, wallet, purse, briefcase\ntie, bow, bow tie\nsuitcase, suit case, luggage\nfrisbee\nskis, ski\nsnowboard\nsports ball, ball\nkite\nbaseball bat\nbaseball glove\nskateboard\nsurfboard, longboard, skimboard, shortboard, wakeboard\ntennis racket, racket\nbottle\nwine glass\ncup\nfork\nknife, pocketknife, knive\nspoon\nbowl, container\nbanana\napple\nsandwich, burger, sub, cheeseburger, hamburger\norange\nbroccoli\ncarrot\nhot dog\npizza\ndonut, doughnut, bagel\ncake,  cheesecake, cupcake, shortcake, coffeecake, pancake\nchair, seat, stool\ncouch, sofa, recliner, futon, loveseat, settee, chesterfield \npotted plant, houseplant\nbed\ndining table, table, desk\ntoilet, urinal, commode, toilet, lavatory, potty\ntv, monitor, televison, television\nlaptop, computer, notebook, netbook, lenovo, macbook, laptop computer\nmouse\nremote\nkeyboard\ncell phone, mobile phone, phone, cellphone, telephone, phon, smartphone, iPhone\nmicrowave\noven, stovetop, stove, stove top oven\ntoaster\nsink\nrefrigerator, fridge, fridge, freezer\nbook\nclock\nvase\nscissors\nteddy bear, teddybear\nhair drier, hairdryer\ntoothbrush\n'''\n\n\ndef combine_coco_captions(annotation_path):\n\n    if not os.path.exists('%s/captions_%s2014.json' %(annotation_path, 'val')):\n        raise Exception(\"Please download MSCOCO caption annotations for val set\")\n    if not os.path.exists('%s/captions_%s2014.json' %(annotation_path, 'train')):\n        raise Exception(\"Please download MSCOCO caption annotations for train set\")\n\n    val_caps = json.load(open('%s/captions_%s2014.json' %(annotation_path, 'val')))\n    train_caps = json.load(open('%s/captions_%s2014.json' %(annotation_path, 'train')))\n    all_caps = {'info': train_caps['info'],\n                'licenses': train_caps['licenses'],\n                'images': val_caps['images'] + train_caps['images'],\n                'annotations': val_caps['annotations'] + train_caps['annotations']}\n\n    return all_caps \n\ndef combine_coco_instances(annotation_path):\n\n    if not os.path.exists('%s/instances_%s2014.json' %(annotation_path, 'val')):\n        raise Exception(\"Please download MSCOCO instance annotations for val set\")\n    if not os.path.exists('%s/instances_%s2014.json' %(annotation_path, 'train')):\n        raise Exception(\"Please download MSCOCO instance annotations for train set\")\n\n    val_instances = json.load(open('%s/instances_%s2014.json' %(annotation_path, 'val')))\n    train_instances = json.load(open('%s/instances_%s2014.json' %(annotation_path, 'train')))\n    all_instances = {'info': train_instances['info'],\n                     'licenses': train_instances['licenses'],\n                     'type': train_instances['licenses'],\n                     'categories': train_instances['categories'],\n                     'images': train_instances['images'] + val_instances['images'],\n                     'annotations': val_instances['annotations'] + train_instances['annotations']}\n\n    return all_instances \n\nclass CHAIR(object):\n\n    def __init__(self, coco_path):\n\n        self.imid_to_objects = defaultdict(list) # later become a dict of sets\n\n        self.coco_path = coco_path\n\n        #read in synonyms\n        synonyms = synonyms_txt.splitlines()\n        synonyms = [s.strip().split(', ') for s in synonyms]\n        self.mscoco_objects = [] #mscoco objects and *all* synonyms\n        self.inverse_synonym_dict = {}\n        for synonym in synonyms:\n            self.mscoco_objects.extend(synonym)\n            for s in synonym:\n                self.inverse_synonym_dict[s] = synonym[0]\n\n        #Some hard coded rules for implementing CHAIR metrics on MSCOCO\n        \n        #common 'double words' in MSCOCO that should be treated as a single word\n        coco_double_words = ['motor bike', 'motor cycle', 'air plane', 'traffic light', 'street light', 'traffic signal', 'stop light', 'fire hydrant', 'stop sign', 'parking meter', 'suit case', 'sports ball', 'baseball bat', 'baseball glove', 'tennis racket', 'wine glass', 'hot dog', 'cell phone', 'mobile phone', 'teddy bear', 'hair drier', 'potted plant', 'bow tie', 'laptop computer', 'stove top oven', 'hot dog', 'teddy bear', 'home plate', 'train track']\n        \n        #Hard code some rules for special cases in MSCOCO\n        #qualifiers like 'baby' or 'adult' animal will lead to a false fire for the MSCOCO object 'person'.  'baby bird' --> 'bird'.\n        animal_words = ['bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'animal', 'cub']\n        #qualifiers like 'passenger' vehicle will lead to a false fire for the MSCOCO object 'person'.  'passenger jet' --> 'jet'.\n        vehicle_words = ['jet', 'train']\n        \n        #double_word_dict will map double words to the word they should be treated as in our analysis\n        \n        self.double_word_dict = {}\n        for double_word in coco_double_words:\n            self.double_word_dict[double_word] = double_word\n        for animal_word in animal_words:\n            self.double_word_dict['baby %s' %animal_word] = animal_word\n            self.double_word_dict['adult %s' %animal_word] = animal_word\n        for vehicle_word in vehicle_words:\n            self.double_word_dict['passenger %s' %vehicle_word] = vehicle_word\n        self.double_word_dict['bow tie'] = 'tie'\n        self.double_word_dict['toilet seat'] = 'toilet'\n        self.double_word_dict['wine glas'] = 'wine glass'\n        \n        self.get_annotations()\n\n    def _load_generated_captions_into_evaluator(self, cap_file, image_id_key, caption_key):\n\n        '''\n        Meant to save time so imid_to_objects does not always need to be recomputed.\n        '''\n        #Read in captions        \n        self.caps, self.eval_imids = load_generated_captions(cap_file, image_id_key, caption_key)\n        assert len(self.caps) == len(self.eval_imids)\n        \n\n    def caption_to_words(self, caption):\n    \n        '''\n        Input: caption\n        Output: MSCOCO words in the caption\n        '''\n    \n        #standard preprocessing\n        words = nltk.word_tokenize(caption.lower())\n        words = [singularize(w) for w in words]\n    \n        #replace double words\n        i = 0\n        double_words = []\n        idxs = []\n        while i < len(words):\n           idxs.append(i) \n           double_word = ' '.join(words[i:i+2])\n           if double_word in self.double_word_dict: \n               double_words.append(self.double_word_dict[double_word])\n               i += 2\n           else:\n               double_words.append(words[i])\n               i += 1\n        words = double_words\n    \n        #toilet seat is not chair (sentences like \"the seat of the toilet\" will fire for \"chair\" if we do not include this line)\n        if ('toilet' in words) & ('seat' in words): words = [word for word in words if word != 'seat']\n    \n        #get synonyms for all words in the caption\n        idxs = [idxs[idx] for idx, word in enumerate(words) \\\n                if word in set(self.mscoco_objects)]\n        words = [word for word in words if word in set(self.mscoco_objects)]\n        node_words = []\n        for word in words:\n            node_words.append(self.inverse_synonym_dict[word])\n        #return all the MSCOCO objects in the caption\n        return words, node_words, idxs, double_words\n\n    def get_annotations_from_segments(self):\n        '''\n        Add objects taken from MSCOCO segmentation masks\n        '''\n\n        coco_segments = combine_coco_instances(self.coco_path )\n        segment_annotations = coco_segments['annotations']\n\n        #make dict linking object name to ids\n        id_to_name = {} #dict with id to synsets \n        for cat in coco_segments['categories']:\n            id_to_name[cat['id']] = cat['name']\n\n        for i, annotation in enumerate(segment_annotations):\n            sys.stdout.write(\"\\rGetting annotations for %d/%d segmentation masks\" \n                              %(i, len(segment_annotations)))\n            imid = annotation['image_id']\n            \n            node_word = self.inverse_synonym_dict[id_to_name[annotation['category_id']]]\n            self.imid_to_objects[imid].append(node_word)\n        print(\"\\n\")\n\n    def get_annotations_from_captions(self):\n        '''\n        Add objects taken from MSCOCO ground truth captions \n        '''\n\n        coco_caps = combine_coco_captions(self.coco_path)\n        caption_annotations = coco_caps['annotations']\n\n        for i, annotation in enumerate(caption_annotations):\n            sys.stdout.write('\\rGetting annotations for %d/%d ground truth captions' \n                              %(i, len(coco_caps['annotations'])))\n            imid = annotation['image_id']\n            \n            _, node_words, _, _ = self.caption_to_words(annotation['caption'])\n            # note here is update, so call get_annotations_from_segments first\n            self.imid_to_objects[imid].extend(node_words)\n        print(\"\\n\")\n\n\n    def get_annotations(self):\n\n        '''\n        Get annotations from both segmentation and captions.  Need both annotation types for CHAIR metric.\n        '''\n        \n        self.get_annotations_from_segments() \n        self.get_annotations_from_captions()\n        # deduplicate\n        for imid in self.imid_to_objects:\n            self.imid_to_objects[imid] = set(self.imid_to_objects[imid])\n\n    def compute_chair(self, cap_file, image_id_key, caption_key):\n        '''\n        Given ground truth objects and generated captions, determine which sentences have hallucinated words.\n        '''\n        self._load_generated_captions_into_evaluator(cap_file, image_id_key, caption_key)\n        \n        imid_to_objects = self.imid_to_objects\n        caps = self.caps\n        eval_imids = self.eval_imids\n \n        num_caps = 0.\n        num_hallucinated_caps = 0.\n        hallucinated_word_count = 0.\n        coco_word_count = 0.\n        \n        # :add:\n        num_recall_gt_objects = 0.\n        num_gt_objects = 0.\n\n        output = {'sentences': []} \n        \n        for i in tqdm.trange(len(caps)):\n            cap :str = caps[i]\n            imid :int = eval_imids[i]\n    \n            #get all words in the caption, as well as corresponding node word\n            words, node_words, idxs, raw_words = self.caption_to_words(cap) \n \n            gt_objects = imid_to_objects[imid]\n            cap_dict = {'image_id': imid, \n                        'caption': cap,\n                        'mscoco_hallucinated_words': [],\n                        'mscoco_gt_words': list(gt_objects),\n                        'mscoco_generated_words': list(node_words),\n                        'hallucination_idxs': [], \n                        'words': raw_words \n                        }\n\n            # :add:\n            cap_dict['metrics'] = {'CHAIRs': 0,\n                                   'CHAIRi': 0,\n                                   'Recall': 0}\n \n            #count hallucinated words\n            coco_word_count += len(node_words) \n            hallucinated = False\n            \n            # add\n            recall_gt_objects = set()\n            for word, node_word, idx in zip(words, node_words, idxs):\n                if node_word not in gt_objects:\n                    hallucinated_word_count += 1 \n                    cap_dict['mscoco_hallucinated_words'].append((word, node_word))\n                    cap_dict['hallucination_idxs'].append(idx)\n                    hallucinated = True\n                else:\n                    recall_gt_objects.add(node_word)\n    \n            #count hallucinated caps\n            num_caps += 1\n            if hallucinated:\n               num_hallucinated_caps += 1\n            \n            # add\n            num_gt_objects += len(gt_objects)\n            num_recall_gt_objects += len(recall_gt_objects)\n    \n            cap_dict['metrics']['CHAIRs'] = int(hallucinated)\n            cap_dict['metrics']['CHAIRi'] = 0.\n            cap_dict['metrics']['Recall'] = 0.\n            \n            if len(words) > 0:\n                cap_dict['metrics']['CHAIRi'] = len(cap_dict['mscoco_hallucinated_words'])/float(len(words))\n            \n            # add\n            if len(gt_objects) > 0:\n                cap_dict['metrics']['Recall'] = len(recall_gt_objects) / len(gt_objects)\n   \n            output['sentences'].append(cap_dict)\n \n        chair_s = (num_hallucinated_caps/num_caps)\n        chair_i = (hallucinated_word_count/coco_word_count)\n\n        recall = num_recall_gt_objects / num_gt_objects\n    \n        output['overall_metrics'] = {'CHAIRs': chair_s,\n                                     'CHAIRi': chair_i,\n                                     'Recall': recall}\n\n        return output \n\n    def compute_hallucinations(self, imid, cap):        \n        imid_to_objects = self.imid_to_objects\n     \n        #get all words in the caption, as well as corresponding node word\n        words, node_words, idxs, raw_words = self.caption_to_words(cap) \n\n        gt_objects = imid_to_objects[imid]\n        cap_dict = {\n            'mscoco_hallucinated_words': [],\n            'mscoco_gt_words': list(gt_objects),\n            'mscoco_generated_words': list(node_words),\n            'hallucination_idxs': [], \n            'hallucinated_words': 0,\n            'recall_words': [],\n            'recall_idxs': [],\n        }\n\n        for word, node_word, idx in zip(words, node_words, idxs):\n            if node_word not in gt_objects:\n                cap_dict['hallucinated_words'] += 1\n                cap_dict['mscoco_hallucinated_words'].append((word, node_word))\n                cap_dict['hallucination_idxs'].append(idx)\n            else:\n                cap_dict['recall_words'].append((word, node_word))\n                cap_dict['recall_idxs'].append(idx)\n        \n        return cap_dict\n\ndef load_generated_captions(cap_file, image_id_key:str, caption_key:str):\n    #Read in captions        \n    # it should be list of dict\n    ext = os.path.splitext(cap_file)[-1]\n    if ext == '.json':\n        caps = json.load(open(cap_file))\n    elif ext == '.jsonl':\n        caps = [json.loads(s) for s in open(cap_file)]\n    else:\n        raise ValueError(f'Unspported extension {ext} for cap_file: {cap_file}')\n\n    # list of int\n    imids = [obj[image_id_key] for obj in caps]\n    \n    # list of str\n    caps = [obj[caption_key] for obj in caps]\n       \n    return caps, imids\n\ndef save_hallucinated_words(cap_file, cap_dict): \n    with open(cap_file, 'w') as f:\n        json.dump(cap_dict, f, indent=2, ensure_ascii=False)\n\ndef print_metrics(hallucination_cap_dict, quiet=False):\n    sentence_metrics = hallucination_cap_dict['overall_metrics']\n    \n    for k, v in sentence_metrics.items():\n        k_str = str(k).ljust(10)\n        v_str = f'{v * 100:.01f}'\n        print(k_str, v_str, sep=': ')\n \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument(\"--cap_file\", type=str, default='',\n                        help=\"path towards json or jsonl saving image ids and their captions in list of dict.\")\n    parser.add_argument(\"--image_id_key\", type=str, default=\"image_id\",\n                        help=\"in each dict of cap_file, which key stores image id of coco.\")\n    parser.add_argument(\"--caption_key\", type=str, default=\"caption\",\n                        help=\"in each dict of cap_file, which key stores caption of the image.\")\n    \n    parser.add_argument(\"--cache\", type=str, default=\"chair.pkl\",\n                        help=\"pre inited CHAIR evaluator object, for fast loading.\")\n    parser.add_argument(\"--coco_path\", type=str, default='coco_annotations',\n                        help=\"only use for regenerating CHAIR evaluator object, will be ignored if uses cached evaluator.\")\n    \n    parser.add_argument(\"--save_path\", type=str, default=\"\",\n                        help=\"saving CHAIR evaluate and results to json, useful for debugging the caption model.\")\n    \n    args = parser.parse_args()\n\n    if args.cache and os.path.exists(args.cache):\n        evaluator = pickle.load(open(args.cache, 'rb'))\n        print(f\"loaded evaluator from cache: {args.cache}\")\n    else:\n        print(f\"cache not setted or not exist yet, building from scratch...\")\n        evaluator = CHAIR(args.coco_path)\n        pickle.dump(evaluator, open(args.cache, 'wb'))\n        print(f\"cached evaluator to: {args.cache}\")\n\n    cap_dict = evaluator.compute_chair(args.cap_file, args.image_id_key, args.caption_key) \n    \n    print_metrics(cap_dict)\n    \n    if args.save_path:\n        save_hallucinated_words(args.save_path, cap_dict)\n"}
{"type": "source_file", "path": "methods/cambrian_utils.py", "content": "import torch\nimport sys\n# ADD PATH TO CAMBRIAN REPOSITORY\nsys.path.append('/home/anish/vl-interp/cambrian')\n\nfrom cambrian.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN\nfrom cambrian.conversation import conv_templates\nfrom cambrian.model.builder import load_pretrained_model\nfrom cambrian.mm_utils import process_images, tokenizer_image_token\nfrom methods.utils import load_images, string_to_token_ids\nfrom PIL import Image\nimport requests\n\ndef projection(image_embeddings, text_embedding):\n    return (image_embeddings @ text_embedding.T)[0, :, 0] / (\n        text_embedding @ text_embedding.T\n    ).squeeze()\n\ndef subtract_projection(image_embeddings, text_embedding, weight=1):\n    image_embeddings = image_embeddings.clone()\n    proj = projection(image_embeddings, text_embedding)\n    for i in range(image_embeddings.shape[1]):\n        if proj[i] > 0:\n            image_embeddings[:, i] -= weight * proj[i] * text_embedding\n            # image_embeddings[:, i] += weight * proj[i] * text_embedding\n    return image_embeddings\n\ndef subtract_projections(image_embeddings, text_embeddings, weight=1):\n    # text_embeddings: (# embeds, 1, # dim size)\n    img_embeddings = image_embeddings.clone()\n    for text_embedding in text_embeddings:\n        img_embeddings = subtract_projection(img_embeddings, text_embedding, weight)\n    return img_embeddings\n\ndef generate_mass_edit_hook(\n    text_embeddings, start_edit_index, end_edit_index, layer, weight=1, minimum_size=32\n):\n    def edit_embeddings(module, input, output):\n        new_output = list(output)\n        if new_output[0].shape[1] > minimum_size:\n            print(f\"Editing layer {layer}\")\n            new_output[0][:, start_edit_index:end_edit_index] = subtract_projections(\n                new_output[0][:, start_edit_index:end_edit_index],\n                text_embeddings,\n                weight=weight,\n            )\n        return tuple(new_output)\n\n    return edit_embeddings\n\ndef get_vocab_embeddings_cambrian(llm_model, tokenizer, device=\"cuda\"):\n    vocab = tokenizer.get_vocab()\n    llm_tokens = torch.tensor(list(vocab.values()), dtype=torch.long).unsqueeze(0).to(device)\n    token_embeddings = llm_model.get_input_embeddings()(llm_tokens)\n    return token_embeddings\n\ndef generate_text_prompt(model, text_prompt):\n    qs = text_prompt\n    if model.config.mm_use_im_start_end:\n        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n    else:\n        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n\n    # Using llama_3 for Cambrian-8b\n    conv_mode = \"llama_3\"\n    conv = conv_templates[conv_mode].copy()\n    conv.append_message(conv.roles[0], qs)\n    conv.append_message(conv.roles[1], None)\n    return conv\n\ndef generate_images_tensor(model, img_path, image_processor):\n    image_files = [img_path]\n    images = load_images(image_files)\n    image_sizes = [x.size for x in images]\n    \n    images_tensor = process_images(images, image_processor, model.config).to(\n        model.device, dtype=torch.float16\n    )\n    \n    return images_tensor, images, image_sizes\n\ndef prompt_to_img_input_ids(prompt, tokenizer):\n    input_ids = tokenizer_image_token(\n        prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n    ).unsqueeze(0).cuda()\n    return input_ids\n\ndef run_cambrian_model(\n    model,\n    images_tensor,\n    image_sizes,\n    tokenizer,\n    text_prompt=None,\n    hidden_states=False,\n    temperature=1.0\n):\n    if text_prompt is None:\n        text_prompt = \"Please describe this image in detail.\"\n\n    conv = generate_text_prompt(model, text_prompt)\n    input_ids = prompt_to_img_input_ids(conv.get_prompt(), tokenizer)\n\n    with torch.inference_mode():\n        output = model.generate(\n            input_ids,\n            images=images_tensor,\n            image_sizes=image_sizes,\n            do_sample=True if temperature > 0 else False,\n            temperature=temperature,\n            num_beams=5,\n            max_new_tokens=512,\n            use_cache=True,\n            return_dict_in_generate=True,\n            output_hidden_states=hidden_states\n        )\n\n    if hidden_states:\n        return input_ids, output\n\n    outputs = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)[0].strip()\n    return outputs\n\ndef reshape_cambrian_prompt_hidden_layers(hidden_states):\n    prompt_hidden_states = hidden_states[0]\n    first_beam_layers = torch.stack(list(prompt_hidden_states), dim=0)[:, 0]\n    return first_beam_layers\n\ndef get_hidden_text_embedding(\n    target_word, model, vocab_embeddings, tokenizer, layer=22, device=\"cuda\"\n):\n    token_ids = string_to_token_ids(target_word)\n    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n\n    with torch.inference_mode():\n        output = model.generate(\n            input_ids,\n            temperature=1.0,\n            num_beams=5,\n            max_new_tokens=10,\n            return_dict_in_generate=True,\n            output_hidden_states=True,\n            use_cache=False\n        )\n\n    hidden_states = reshape_cambrian_prompt_hidden_layers(output[\"hidden_states\"])\n\n    # Validation check\n    dist = torch.norm(\n        hidden_states[0, len(token_ids) - 1] \n        - vocab_embeddings[0, token_ids[len(token_ids) - 1]]\n    )\n    if dist > 0.1:\n        print(f\"Validation check failed: caption word {target_word} didn't match: {dist}\")\n\n    return hidden_states[layer, len(token_ids) - 1].unsqueeze(0)\n\ndef get_caption_from_cambrian(\n    img_path, model, tokenizer, image_processor, text_prompt=None\n):\n    images_tensor, images, image_sizes = generate_images_tensor(\n        model, img_path, image_processor\n    )\n\n    new_caption = run_cambrian_model(\n        model,\n        images_tensor,\n        image_sizes,\n        tokenizer,\n        text_prompt=text_prompt,\n    )\n\n    return new_caption\n\ndef process(image, question, tokenizer, image_processor, model_config):\n    conv_mode = \"llama_3\" \n    qs = question\n\n    if model_config.mm_use_im_start_end:\n        qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\\n' + qs\n    else:\n        qs = DEFAULT_IMAGE_TOKEN + '\\n' + qs\n\n    conv = conv_templates[conv_mode].copy()\n    conv.append_message(conv.roles[0], qs)\n    conv.append_message(conv.roles[1], None)\n    prompt = conv.get_prompt()\n    \n    image_size = [image.size]\n    image_tensor = process_images([image], image_processor, model_config)\n\n    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n\n    return input_ids, image_tensor, image_size, prompt\n\ndef retrieve_logit_lens_cambrian(state, img_path):\n    \"\"\"Get logit lens analysis for an image using Cambrian.\"\"\"\n    device = state[\"model\"].device\n    \n    question = \"Please describe this image in detail.\"\n    \n    # Use process() function for image processing\n    if isinstance(img_path, str):\n        if img_path.startswith('http'):\n            image = Image.open(requests.get(img_path, stream=True).raw)\n        else:\n            image = Image.open(img_path)\n    else:\n        image = img_path\n\n    input_ids, image_tensor, image_sizes, prompt = process(\n        image, \n        question, \n        state[\"tokenizer\"], \n        state[\"image_processor\"], \n        state[\"model\"].config\n    )\n    \n    input_ids = input_ids.to(device=device, non_blocking=True)\n\n    with torch.inference_mode():\n        outputs = state[\"model\"].generate(\n            input_ids,\n            images=image_tensor,\n            image_sizes=image_sizes,\n            do_sample=True,\n            temperature=1.0,\n            num_beams=5,\n            max_new_tokens=512,\n            use_cache=True,\n            return_dict_in_generate=True,\n            output_hidden_states=True\n        )\n\n    caption = state[\"tokenizer\"].decode(outputs.sequences[0], skip_special_tokens=True)\n\n    x = torch.stack(outputs.hidden_states[0]).max(dim=1).values.to(device)\n\n    logits = []\n    for i in range(x.shape[0]):\n        with torch.no_grad():\n            logits.append(state[\"model\"].lm_head(x[i, :, :]).detach().cpu())\n    \n    logit_lens = torch.stack(logits)\n    \n    with torch.no_grad():\n        softmax_probs = torch.softmax(logit_lens, dim=-1)\n    \n    softmax_probs = softmax_probs.permute(2, 0, 1)\n    \n    try:\n        image_token_region = softmax_probs[:, :, 91:-12]\n    except Exception as e:\n        print(f\"Warning: Error slicing token region: {e}\")\n        print(f\"Softmax probs shape: {softmax_probs.shape}\")\n        image_token_region = softmax_probs\n    \n    image_token_region = image_token_region.detach().cpu().float().numpy()\n    return caption, image_token_region\n\ndef load_cambrian_state(device=\"cuda\"):\n    model_path = \"nyu-visionx/cambrian-8b\"\n    model_name = model_path\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\n        model_path, None, model_name\n    )\n\n    vocabulary = tokenizer.get_vocab()\n    vocab_embeddings = get_vocab_embeddings_cambrian(model, tokenizer, device=device)\n\n    execute_model = lambda img_path, text_prompt=None: get_caption_from_cambrian(\n        img_path, model, tokenizer, image_processor, text_prompt\n    )\n    register_hook = lambda hook, layer: model.model.layers[layer].register_forward_hook(hook)\n    register_pre_hook = lambda pre_hook, layer: model.model.layers[layer].register_forward_pre_hook(pre_hook)\n    hidden_layer_embedding = lambda text, layer: get_hidden_text_embedding(\n        text, model, vocab_embeddings, tokenizer, layer, device=device\n    )\n\n    return {\n        \"vocabulary\": vocabulary,\n        \"vocab_embeddings\": vocab_embeddings,\n        \"tokenizer\": tokenizer,\n        \"execute_model\": execute_model,\n        \"register_hook\": register_hook,\n        \"register_pre_hook\": register_pre_hook,\n        \"hidden_layer_embedding\": hidden_layer_embedding,\n        \"model\": model,\n        \"model_name\": model_name,\n        \"image_processor\": image_processor,\n    }"}
{"type": "source_file", "path": "methods/blip_utils.py", "content": "import requests\nfrom PIL import Image\nfrom io import BytesIO\nimport numpy as np\nimport torch\nfrom transformers import LlamaTokenizer\nfrom methods.utils import load_image, string_to_token_ids\n\ntry:\n    from src.caption.instruct_blip_engine import InstructBLIPVicuna7B\nexcept:  # first call for some reason fails\n    pass\n\nfrom src.caption.instruct_blip_engine import InstructBLIPVicuna7B\n\nTOKEN_UNDERSCORE = chr(\n    9601\n)  # a lot of tokens have _rain, etc. but the underscore in front is not normal\n\n\ndef get_image_embeddings(image_file, captioner, prompt = None):\n    if prompt == None:\n        prompt = captioner.prompt\n\n    image = load_image(image_file)\n    baseline_caption, inputs_embeds, inputs_query, outputs = captioner(\n        image, prompt=prompt, return_embeds=True\n    )\n    image_embeddings = inputs_query\n    return image_embeddings\n\n\ndef get_vocab_embeddings_blip(captioner, device=\"cuda\"):\n    vocab = captioner.tokenizer.get_vocab()\n    llm_tokens = (\n        torch.tensor(list(vocab.values()), dtype=torch.long).unsqueeze(0).to(device)\n    )\n    token_embeddings = captioner.model.llm_model.get_input_embeddings()(llm_tokens)\n    return token_embeddings.to(device)\n\n\n# reshape so that image embeddings and text embeddings put together\ndef reshape_hidden_states(hidden_states):\n    # converts hidden states into shape (layer #s, beam count, hidden state #s, dims)\n    all_embeddings = []\n    for i in range(len(hidden_states)):\n        if i == 0:  # only get the image embeddings\n            all_embeddings.append(torch.stack(hidden_states[i])[:, :, :32])\n        else:\n            all_embeddings.append(torch.stack(hidden_states[i]))\n    all_hidden_states = torch.concat(all_embeddings, dim=2)\n    return all_hidden_states\n\n\ndef run_blip_model(inputs_embeds, captioner, text_prompt=None, caption_only=True):\n    if text_prompt == None:\n        text_prompt = captioner.prompt\n\n    # Run image embeddings into the model\n    test_input = {\"image\": torch.ones(1).to(\"cuda\"), \"prompt\": text_prompt}\n    atts_llm = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long).to(\n        inputs_embeds.device\n    )\n\n    with torch.no_grad():\n        out = captioner.model.generate(  # type: ignore\n            test_input,\n            num_beams=5,\n            max_length=512,\n            return_embeds=True,\n            atts_vision=atts_llm,\n            inputs_vision=inputs_embeds,  # stick input embeddings from vision here\n            return_dict=True,\n            temperature=1,\n        )\n        if caption_only:\n            return out[0][0]\n        return out\n\n\ndef retrieve_logit_lens_blip(state, img_path, text_prompt = None):\n    input_embeds = get_image_embeddings(img_path, state[\"model\"], prompt = text_prompt)\n    out = run_blip_model(input_embeds, state[\"model\"], caption_only=False, text_prompt=text_prompt)\n    caption = out[0][0]\n    hidden_states = torch.stack(out[4][\"hidden_states\"][0])[:, :, :32]\n    with torch.no_grad():\n        softmax_probs = torch.nn.functional.softmax(\n            state[\"model\"].model.llm_model.lm_head(hidden_states.half()), dim=-1\n        )\n    # max over beams\n    softmax_probs = softmax_probs.max(axis=1).values\n    softmax_probs = softmax_probs.cpu().numpy()\n    # reshape into (vocab_size, num_layers, num_tokens)\n    softmax_probs = softmax_probs.transpose(2, 0, 1)\n    return caption, softmax_probs\n\n\ndef get_hidden_text_embedding(\n    input_text, captioner, img_embeddings, vocab_embeddings, tokenizer, layer=10, device=\"cuda\"\n):\n    test_input = {\"image\": torch.ones(1).to(\"cuda\"), \"prompt\": input_text}\n    inputs_embeds = img_embeddings.clone().to(device)\n\n    atts_llm = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long).to(device)\n    with torch.no_grad():\n        out = captioner.model.generate(  # type: ignore\n            test_input,\n            num_beams=5,\n            max_length=10,\n            return_embeds=True,\n            atts_vision=atts_llm,\n            inputs_vision=inputs_embeds,  # stick input embeddings from vision here\n            return_dict=True,\n            pure_llm=True,  # meaning only text model used\n        )\n\n        hidden_states = reshape_hidden_states(out[4].hidden_states)\n\n        # Extract the size of the tokens split up\n        token_ids = string_to_token_ids(input_text, tokenizer)\n        dist = torch.norm(\n            hidden_states[0, 0, len(token_ids) - 1]\n            - vocab_embeddings[0, token_ids[len(token_ids) - 1]]\n        )\n        if dist > 0.1:\n            print(f\"Coco class {input_text} didn't match: {dist}\")\n\n        return hidden_states[layer, 0, len(token_ids) - 1].unsqueeze(0)\n\n\ndef load_blip_state(device=\"cuda\"):\n    captioner = InstructBLIPVicuna7B(device=device, return_embeds=True)\n\n    vocabulary = captioner.tokenizer.get_vocab()\n    vocab_embeddings = get_vocab_embeddings_blip(captioner, device=device)\n\n    def execute_model(img_path, image_embeddings=None, text_prompt = None):\n        if image_embeddings == None:\n            image_embeddings = get_image_embeddings(img_path, captioner, prompt=text_prompt).to(device)\n        return run_blip_model(image_embeddings, captioner, text_prompt=text_prompt)\n\n    register_hook = lambda hook, layer: captioner.model.llm_model.model.layers[\n        layer\n    ].register_forward_hook(hook)\n    register_pre_hook = lambda pre_hook, layer: captioner.model.llm_model.model.layers[\n        layer\n    ].register_forward_pre_hook(pre_hook)\n    hidden_layer_embedding = lambda text, layer: get_hidden_text_embedding(\n        text,\n        captioner,\n        torch.zeros((1, 32, vocab_embeddings.shape[2])),\n        vocab_embeddings,\n        captioner.tokenizer,\n        layer,\n        device=device,\n    )\n\n    return {\n        \"vocabulary\": vocabulary,\n        \"vocab_embeddings\": vocab_embeddings,\n        \"tokenizer\": captioner.tokenizer,\n        \"execute_model\": execute_model,\n        \"register_hook\": register_hook,\n        \"register_pre_hook\": register_pre_hook,\n        \"hidden_layer_embedding\": hidden_layer_embedding,\n        \"model\": captioner,\n    }\n"}
{"type": "source_file", "path": "methods/utils.py", "content": "import torch\nfrom PIL import Image\nfrom io import BytesIO\nimport matplotlib.pyplot as plt\nimport requests\n\n\ndef load_image(image_file):\n    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n        response = requests.get(image_file)\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    else:\n        image = Image.open(image_file).convert(\"RGB\")\n    return image\n\n\ndef load_images(image_files):\n    out = []\n    for image_file in image_files:\n        image = load_image(image_file)\n        out.append(image)\n    return out\n\n\ndef display_image(image_path):\n    # Open an image file\n    with Image.open(image_path) as img:\n        # Display image\n        plt.imshow(img)\n        plt.axis(\"off\")  # Hide axes\n        plt.show()\n\n\ndef string_to_token_ids(string, tokenizer):\n    return tokenizer(string)[\"input_ids\"]\n\n\ndef coco_img_id_to_name(img_id, train=False):\n    if train:\n        return f\"COCO_train2014_{(12 - len(str(img_id))) * '0' + str(img_id)}\"\n    else:\n        return f\"COCO_val2014_{(12 - len(str(img_id))) * '0' + str(img_id)}\"\n\ndef display_chair_results(chair_evals):\n    print(f\"Hallucinations: {chair_evals['mscoco_hallucinated_words']}\\nCorrectly detected (recall) objects: {chair_evals['recall_words']}\\nGround truth: {chair_evals['mscoco_gt_words']}\")\n\ndef get_device_from_module(module):\n    return next(module.parameters()).device"}
{"type": "source_file", "path": "src/__init__.py", "content": "from dotenv import load_dotenv\n\n__version__ = \"0.0.1\"\nload_dotenv()\n"}
{"type": "source_file", "path": "src/caption/__init__.py", "content": "from typing import Dict, Type\n\nfrom .base import CaptionEngine  # noqa: F401\nfrom .ofa_engine import OFACaptionEngine\nfrom .blip_engine import (\n    BLIP2COCOBase,\n    BLIP2Base,\n    BLIP2COCOLarge,\n    BLIP2Large,\n    BLIP2COCOT5Large,\n    BLIP2T5Large,\n)\nfrom .entropy_threshold import (\n    EntropyThresholdBLIP2Base,\n    EntropyThresholdBLIP2COCOBase,\n    EntropyThresholdInstructBLIPEngine,\n)\nfrom .instruct_blip_engine import (\n    InstructBLIPVicuna7B,\n    InstructBLIPVicuna13B,\n    InstructBLIPFlanT5XL,\n    InstructBLIPFlanT5XXL,\n)\nfrom .llava_engine import LLaVA7B, LLaVA13B\n\nCAPTION_ENGINES: Dict[str, Type[CaptionEngine]] = {\n    \"OFA (Large + Caption)\": OFACaptionEngine,\n    \"BLIP2 (OPT, COCO, 6.7B)\": BLIP2COCOLarge,\n    \"BLIP2 (OPT, COCO, 2.7B)\": BLIP2COCOBase,\n    \"BLIP2 (T5, COCO, flanT5XL)\": BLIP2COCOT5Large,\n    \"BLIP2 (OPT, 6.7B)\": BLIP2Large,\n    \"BLIP2 (OPT, 2.7B)\": BLIP2Base,\n    \"BLIP2 (T5, flanT5XL)\": BLIP2T5Large,\n}\n\nCAPTION_ENGINES_CLI: Dict[str, Type[CaptionEngine]] = {\n    \"ofa\": OFACaptionEngine,\n    \"blip2-coco\": BLIP2COCOLarge,\n    \"blip2-base-coco\": BLIP2COCOBase,\n    \"blip2-t5-coco\": BLIP2COCOT5Large,\n    \"blip2\": BLIP2Large,\n    \"blip2-base\": BLIP2Base,\n    \"blip2-t5\": BLIP2T5Large,\n    \"ent-blip2-base-coco\": EntropyThresholdBLIP2COCOBase,\n    \"ent-blip2-base\": EntropyThresholdBLIP2Base,\n    \"instruct-blip-7b\": InstructBLIPVicuna7B,\n    \"instruct-blip-13b\": InstructBLIPVicuna13B,\n    \"instruct-blip-flanxl\": InstructBLIPFlanT5XL,\n    \"instruct-blip-flanxxl\": InstructBLIPFlanT5XXL,\n    \"ent-instruct-blip\": EntropyThresholdInstructBLIPEngine,\n    \"llava-7b\": LLaVA7B,\n    \"llava-13b\": LLaVA13B,\n}\n"}
{"type": "source_file", "path": "src/caption/entropy_threshold.py", "content": "from typing import Dict, List, Literal, Optional, Union, Any, Tuple\nimport logging\nfrom PIL import Image\nimport string\nimport warnings\nimport torch\nfrom torch import nn\nimport torch.distributed as dist\n\nimport transformers\nfrom transformers import PreTrainedModel, LogitsProcessor\nfrom transformers.modeling_outputs import ModelOutput\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nfrom transformers.generation.utils import (\n    GenerationMixin,\n    BeamSearchOutput,\n    BeamSampleEncoderDecoderOutput,\n    BeamSearchDecoderOnlyOutput\n)\nfrom transformers.generation.logits_process import (\n    LogitsProcessorList,\n)\nfrom transformers.generation.stopping_criteria import (\n    StoppingCriteriaList,\n    validate_stopping_criteria,\n)\nfrom transformers.generation.beam_search import BeamScorer\n\nfrom src.caption.base import CaptionEngine\nfrom src.caption.blip_engine import BLIP2CaptionEngine, _BLIP_DEFAULT_PROMPT\nfrom src.caption.instruct_blip_engine import InstructBLIP, _INSTRUCT_BLIP_DEFAULT_PROMPT\nfrom src.caption.utils import postprocess_caption\nfrom src.utils.pytorch import torch_int_div\n\nfrom src.caption.lavis.models import load_model_and_preprocess\n\n\nclass EntropyThresholdBLIP2Engine(BLIP2CaptionEngine):\n    def __init__(\n            self,\n            model_name: str = \"Salesforce/blip2-opt-2.7b-coco\",\n            device: Optional[str] = None,\n            threshold_type: Literal['entropy', 'vocab'] = 'entropy',\n            distribution_type: Literal['MI', 'CAD'] = 'MI',\n            threshold: Optional[float] = 1.0,\n            vocab_label_file: Optional[str] = None,\n            pure_llm: Optional[bool] = False,\n\n            # Distribution modification parameters\n            alpha: Optional[float] = 1.0,\n            topk: Optional[int] = -1,\n            renormalize: Optional[bool] = False,\n\n            **kwargs\n    ):\n        self.device = device or \"cpu\"\n        self.model = EntropyThresholdBLIP2Model(\n            model_name,\n            self.device,\n            threshold_type = threshold_type,\n            distribution_type = distribution_type,\n            threshold = threshold,\n            alpha = alpha,\n            topk = topk,\n            renormalize = renormalize,\n            pure_llm = pure_llm,\n            vocab_label_file = vocab_label_file\n        )\n        self.prompt = _BLIP_DEFAULT_PROMPT\n        self.processor = AutoProcessor.from_pretrained(model_name)\n        self.processor_kwargs = {\"return_tensors\": \"pt\", \"text\": self.prompt}\n\n        self.tokenizer = self.processor.tokenizer\n        self.threshold_type = threshold_type\n        self.distribution_type = distribution_type\n        self.vocab_size = self.tokenizer.vocab_size\n        self.threshold = threshold\n        self.alpha = alpha\n        self.topk = topk\n        self.pure_llm = pure_llm\n\n\n    def _get_captioner_class(self):\n        raise NotImplementedError\n\n    def _disable_cross_attention(self):\n        self.model._disable_cross_attention()\n\n    def _enable_cross_attention(self):\n        self.model._enable_cross_attention()\n\n    def _preprocess_image(self, raw_image: Image.Image, prompt: Optional[str] = None) -> torch.Tensor:\n        return self.processor(raw_image, text=prompt, return_tensors=\"pt\").to(self.device, torch.float16)\n\n    def get_caption_distributions(\n        self,\n        raw_image: Image,\n        force_caption: str,\n        remove_punctuation: bool = False,\n    ) -> Dict[str, Any]:\n\n        vocab_size = self.tokenizer.vocab_size\n        if remove_punctuation:\n            force_caption = force_caption.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n        encoded = self.get_encoded_caption(force_caption) # Size = 1 x sequence length\n        tokens_decoded = self.tokenizer.convert_ids_to_tokens(encoded[0])[1:] # remove BOS\n\n        full_distributions = self.get_forced_output_distributions(raw_image, encoded, vocab_size, language_only=False)\n\n        return {\n            'caption': force_caption,\n            'encoded_caption': encoded.cpu(),\n            'tokens_decoded': tokens_decoded,\n            'full_distributions': full_distributions.cpu(),\n            'language_distributions': None\n        }\n\n\nclass EntropyThresholdInstructBLIPEngine(InstructBLIP):\n    def __init__(\n            self,\n            model_name: str = \"instruct-blip\",\n            device: Optional[str] = None,\n            threshold_type: Literal['entropy', 'vocab'] = 'entropy',\n            distribution_type: Literal['MI', 'CAD'] = 'MI',\n            threshold: Optional[float] = 1.0,\n            vocab_label_file: Optional[str] = None,\n            pure_llm: Optional[bool] = False,\n\n            # Distribution modification parameters\n            alpha: Optional[float] = 1.0,\n            topk: Optional[int] = -1,\n            renormalize: Optional[bool] = False,\n\n            **kwargs\n    ):\n        self.device = device or \"cpu\"\n        self.model = EntropyThresholdInstructBLIPModel(\n            self.device,threshold = threshold, alpha = alpha, topk = topk, renormalize = renormalize, pure_llm = pure_llm,\n            vocab_label_file = vocab_label_file, threshold_type = threshold_type, distribution_type = distribution_type\n        )\n        self.vis_processors = self.model.vis_processors\n        self.tokenizer = self.model.model.llm_tokenizer\n        self.prompt = _INSTRUCT_BLIP_DEFAULT_PROMPT\n        self.threshold = threshold\n        self.alpha = alpha\n        self.topk = topk\n        self.pure_llm = pure_llm\n\n    def _get_captioner_class(self):\n        raise NotImplementedError\n\n    def get_caption_distributions(\n        self,\n        raw_image: Image,\n        force_caption: str,\n        remove_punctuation: bool = False,\n    ) -> Dict[str, Any]:\n\n        vocab_size = self.tokenizer.vocab_size\n        if remove_punctuation:\n            force_caption = force_caption.translate(str.maketrans(\"\", \"\", string.punctuation)).lower()\n        encoded = self.get_encoded_caption(force_caption) # Size = 1 x sequence length\n        tokens_decoded = self.tokenizer.convert_ids_to_tokens(encoded[0])[1:] # remove BOS\n\n        full_distributions = self.get_forced_output_distributions(raw_image, encoded, vocab_size, language_only=False)\n\n        return {\n            'caption': force_caption,\n            'encoded_caption': encoded.cpu(),\n            'tokens_decoded': tokens_decoded,\n            'full_distributions': full_distributions.cpu(),\n            'language_distributions': None\n        }\n\n\nclass ThresholdModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def _find_modify_indices(self, next_token_logits, log_softmax):\n        raise NotImplementedError\n\n    def _compute_modified_distribution(self, modify_indices, next_token_logits, log_softmax, language_next_token_logits):\n        raise NotImplementedError\n\n    def _forward(\n        self,\n            extra_language_qformer_output=None,\n        extra_qformer_input_ids=None,\n        extra_running_input_ids=None,\n        **kwargs\n    ) -> ModelOutput:\n        # Compute logits for the original model\n        outputs = self.model.language_model.original_forward(**kwargs)\n\n        # Moved here, from beam search\n        next_token_logits = outputs.logits[:, -1, :]\n        next_token_logits = self.model.language_model.adjust_logits_during_generation(\n            next_token_logits, cur_len=extra_running_input_ids.shape[1]\n        )\n        log_softmax = nn.functional.log_softmax(next_token_logits, dim=-1) # (batch_size * num_beams, vocab_size)\n\n        modify_indices = self._find_modify_indices(next_token_logits, log_softmax)\n\n        if len(modify_indices) > 0:\n            # Prepare inputs\n            llm_input_ids = extra_running_input_ids[modify_indices] if self.pure_llm else torch.cat(\n                    (extra_qformer_input_ids[modify_indices], extra_running_input_ids[modify_indices]), dim=1\n            )\n            updated_inputs = self._get_language_model_inputs(\n                llm_input_ids,\n                None,\n                query_output=None if self.pure_llm else extra_language_qformer_output[modify_indices],\n            )\n\n            # Use non-cached forward pass for language-only model\n            kwargs['use_cache'] = False\n            kwargs.update(updated_inputs)\n            kwargs.pop('input_ids', None)\n            kwargs.pop('past_key_values', None)\n\n            model_inputs = self.model.language_model.prepare_inputs_for_generation(llm_input_ids, **kwargs)\n            model_inputs.pop('extra_running_input_ids', None)\n\n            # Compute logits without image conditioning\n            language_outputs = self.model.language_model.original_forward(**model_inputs)\n            language_next_token_logits = language_outputs.logits[:, -1, :]\n\n            modified = self._compute_modified_distribution(modify_indices, next_token_logits, log_softmax, language_next_token_logits)\n\n            # Make everything past the original distribution top-k logits a minimum value\n            topk = self.topk\n            if topk > -1:\n                k = language_next_token_logits.shape[-1] - topk\n                bottom_k_values, bottom_k_indices = torch.topk(log_softmax[modify_indices], k=k, dim=-1, largest=False)\n                bottom_k_mask = torch.zeros_like(log_softmax[modify_indices], dtype=torch.bool)\n                bottom_k_mask.scatter_(1, bottom_k_indices, True)\n                modified[bottom_k_mask] = -torch.inf\n\n            if self.renormalize:\n                modified = torch.nn.functional.log_softmax(modified, dim=-1)\n            modified = modified.type(outputs.logits.dtype)\n\n        outputs.logits[:, -1, :] = log_softmax\n        if len(modify_indices) > 0:\n            outputs.logits[modify_indices, -1, :] = modified\n\n        return outputs\n\n\n    def _find_modify_indices_entropy(self, next_token_logits, log_softmax) -> torch.Tensor:\n\n        # Compute entropy of full distribution\n        softmax = nn.functional.softmax(next_token_logits, dim=-1)\n        log_softmax = nn.functional.log_softmax(next_token_logits, dim=-1) # (batch_size * num_beams, vocab_size)\n        entropy = -torch.sum(softmax * log_softmax, dim=-1)\n        max_entropy = torch.log(torch.tensor(softmax.shape[-1], dtype=torch.float16, device=softmax.device))\n        entropy_ratio = entropy / max_entropy\n\n        # If entropy_ratio is < threshold, use full distribution. Else, compute `full - alpha * lang`,\n        # where lang is the output distribution from the language-only model.\n        modify_indices = torch.where(entropy_ratio >= self.threshold)[0]\n\n        return modify_indices\n\n    def _find_modify_indices_vocab(self, next_token_logits, log_softmax) -> torch.Tensor:\n\n        # Compute groundedness score and compare to threshold\n        softmax = nn.functional.softmax(next_token_logits, dim=-1)\n        groundedness = (self.binary_grounding * softmax[:, :-1]).sum(dim=-1) # Remove extra last column to match vocab size\n\n        # If groundedness is < threshold, use full distribution. Else, compute `full - alpha * lang`,\n        # where lang is the output distribution from the language-only model.\n        modify_indices = torch.where(groundedness >= self.threshold)[0]\n\n        return modify_indices\n\n    def _compute_modified_distribution_MI(self, modify_indices, next_token_logits, log_softmax, language_next_token_logits):\n        language_log_softmax = nn.functional.log_softmax(language_next_token_logits, dim=-1)\n        return log_softmax[modify_indices] - self.alpha * language_log_softmax\n\n    def _compute_modified_distribution_CAD(self, modify_indices, next_token_logits, log_softmax, language_next_token_logits):\n        cad = (1 + self.alpha) * next_token_logits[modify_indices] - self.alpha * language_next_token_logits\n        return nn.functional.log_softmax(cad, dim=-1)\n\n    def _beam_search(\n        self,\n        input_ids: torch.LongTensor,\n        beam_scorer: BeamScorer,\n        logits_processor: Optional[LogitsProcessorList] = None,\n        stopping_criteria: Optional[StoppingCriteriaList] = None,\n        max_length: Optional[int] = None,\n        pad_token_id: Optional[int] = None,\n        eos_token_id: Optional[Union[int, List[int]]] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        output_scores: Optional[bool] = None,\n        return_dict_in_generate: Optional[bool] = None,\n        synced_gpus: Optional[bool] = False,\n        **model_kwargs,\n    ) -> Union[BeamSearchOutput, torch.LongTensor]:\n        \"\"\"\n        Most of code is copied directly from transformers.generation.utils.GenerateMixin.beam_search.\n        Small modification: Original beam search always ran a log_softmax on the logits. We don't want to do this if the\n        entropy objective is used. Instead, rely on language model forward to do log_softmax.\n        \"\"\"\n        # init values\n        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n        if max_length is not None:\n            warnings.warn(\n                \"`max_length` is deprecated in this function, use\"\n                \" `stopping_criteria=StoppingCriteriaList(MaxLengthCriteria(max_length=max_length))` instead.\",\n                UserWarning,\n            )\n            stopping_criteria = validate_stopping_criteria(stopping_criteria, max_length)\n        if len(stopping_criteria) == 0:\n            warnings.warn(\"You don't have defined any stopping_criteria, this will likely loop forever\", UserWarning)\n        pad_token_id = pad_token_id if pad_token_id is not None else self.model.language_model.generation_config.pad_token_id\n        eos_token_id = eos_token_id if eos_token_id is not None else self.model.language_model.generation_config.eos_token_id\n        if isinstance(eos_token_id, int):\n            eos_token_id = [eos_token_id]\n        output_scores = output_scores if output_scores is not None else self.model.language_model.generation_config.output_scores\n        output_attentions = (\n            output_attentions if output_attentions is not None else self.model.language_model.generation_config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.model.language_model.generation_config.output_hidden_states\n        )\n        return_dict_in_generate = (\n            return_dict_in_generate\n            if return_dict_in_generate is not None\n            else self.model.language_model.generation_config.return_dict_in_generate\n        )\n\n        batch_size = len(beam_scorer._beam_hyps)\n        num_beams = beam_scorer.num_beams\n\n        batch_beam_size, cur_len = input_ids.shape\n\n        if num_beams * batch_size != batch_beam_size:\n            raise ValueError(\n                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n            )\n\n        # init attention / hidden states / scores tuples\n        scores = () if (return_dict_in_generate and output_scores) else None\n        beam_indices = (\n            tuple(() for _ in range(batch_beam_size)) if (return_dict_in_generate and output_scores) else None\n        )\n        decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n        cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n        decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n\n        # if model is an encoder-decoder, retrieve encoder attention weights and hidden states\n        if return_dict_in_generate and self.model.language_model.config.is_encoder_decoder:\n            encoder_attentions = model_kwargs[\"encoder_outputs\"].get(\"attentions\") if output_attentions else None\n            encoder_hidden_states = (\n                model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n            )\n\n        # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n        # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n        beam_scores[:, 1:] = -1e9\n        beam_scores = beam_scores.view((batch_size * num_beams,))\n\n        this_peer_finished = False  # used by synced_gpus only\n        while True:\n            if synced_gpus:\n                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n                # The following logic allows an early break if all peers finished generating their sequence\n                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n                # send 0.0 if we finished, 1.0 otherwise\n                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n                # did all peers finish? the reduced sum will be 0.0 then\n                if this_peer_finished_flag.item() == 0.0:\n                    break\n\n            model_inputs = self.model.language_model.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\n            outputs = self.model.language_model(\n                **model_inputs,\n                return_dict=True,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n            )\n\n            if synced_gpus and this_peer_finished:\n                cur_len = cur_len + 1\n                continue  # don't waste resources running the code we don't need\n\n            next_token_scores = outputs.logits[:, -1, :] # Already post log-softmax, if that's being used.\n            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(next_token_scores)\n\n            # Store scores, attentions and hidden_states when required\n            if return_dict_in_generate:\n                if output_scores:\n                    scores += (next_token_scores_processed,)\n                if output_attentions:\n                    decoder_attentions += (\n                        (outputs.decoder_attentions,) if self.model.language_model.config.is_encoder_decoder else (outputs.attentions,)\n                    )\n                    if self.model.language_model.config.is_encoder_decoder:\n                        cross_attentions += (outputs.cross_attentions,)\n\n                if output_hidden_states:\n                    decoder_hidden_states += (\n                        (outputs.decoder_hidden_states,)\n                        if self.model.language_model.config.is_encoder_decoder\n                        else (outputs.hidden_states,)\n                    )\n\n            # reshape for beam search\n            vocab_size = next_token_scores.shape[-1]\n            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n\n            # Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)\n            next_token_scores, next_tokens = torch.topk(\n                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n            )\n\n            next_indices = torch_int_div(next_tokens, vocab_size)\n            next_tokens = next_tokens % vocab_size\n\n            # stateless\n            beam_outputs = beam_scorer.process(\n                input_ids,\n                next_token_scores,\n                next_tokens,\n                next_indices,\n                pad_token_id=pad_token_id,\n                eos_token_id=eos_token_id,\n                beam_indices=beam_indices,\n            )\n\n            beam_scores = beam_outputs[\"next_beam_scores\"]\n            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n            beam_idx = beam_outputs[\"next_beam_indices\"]\n\n            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n\n            model_kwargs = self.model.language_model._update_model_kwargs_for_generation(\n                outputs, model_kwargs, is_encoder_decoder=self.model.language_model.config.is_encoder_decoder\n            )\n            if model_kwargs[\"past_key_values\"] is not None:\n                model_kwargs[\"past_key_values\"] = self.model.language_model._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n\n            if return_dict_in_generate and output_scores:\n                beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))\n\n            # increase cur_len\n            cur_len = cur_len + 1\n\n            if beam_scorer.is_done or stopping_criteria(input_ids, scores):\n                if not synced_gpus:\n                    break\n                else:\n                    this_peer_finished = True\n\n        sequence_outputs = beam_scorer.finalize(\n            input_ids,\n            beam_scores,\n            next_tokens,\n            next_indices,\n            pad_token_id=pad_token_id,\n            eos_token_id=eos_token_id,\n            max_length=stopping_criteria.max_length,\n            beam_indices=beam_indices,\n        )\n\n        if return_dict_in_generate:\n            if not output_scores:\n                sequence_outputs[\"sequence_scores\"] = None\n\n            if self.model.language_model.config.is_encoder_decoder:\n                return BeamSearchEncoderDecoderOutput(\n                    sequences=sequence_outputs[\"sequences\"],\n                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n                    scores=scores,\n                    beam_indices=sequence_outputs[\"beam_indices\"],\n                    encoder_attentions=encoder_attentions,\n                    encoder_hidden_states=encoder_hidden_states,\n                    decoder_attentions=decoder_attentions,\n                    cross_attentions=cross_attentions,\n                    decoder_hidden_states=decoder_hidden_states,\n                )\n            else:\n                return BeamSearchDecoderOnlyOutput(\n                    sequences=sequence_outputs[\"sequences\"],\n                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n                    scores=scores,\n                    beam_indices=sequence_outputs[\"beam_indices\"],\n                    attentions=decoder_attentions,\n                    hidden_states=decoder_hidden_states,\n                )\n        else:\n            return sequence_outputs[\"sequences\"]\n\n\n\n\nclass EntropyThresholdBLIP2Model(ThresholdModel):\n    def __init__(\n            self,\n            model_name: str,\n            device: str,\n            threshold_type: Literal['entropy', 'vocab'] = 'entropy',\n            distribution_type: Literal['MI', 'CAD'] = 'MI',\n            alpha: float = 1.0,\n            topk: int = -1,\n            renormalize: bool = False,\n            threshold: float = 1.0,\n            vocab_label_file: Optional[str] = None,\n            pure_llm: bool = False\n    ):\n        super().__init__()\n        self.model = Blip2ForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16)\n        # if not torch.distributed.is_initialized():\n        self.model.to(device).eval()\n        self.model.language_model.original_forward = self.model.language_model.forward\n        self.model.language_model.forward = self._forward\n\n        if threshold_type == 'entropy':\n            self._find_modify_indices = self._find_modify_indices_entropy\n        elif threshold_type == 'vocab':\n            self._find_modify_indices = self._find_modify_indices_vocab\n            self.grounding_labels = torch.load(vocab_label_file)\n            binary_grounding = [1 if self.grounding_labels[i] == 'grounded' else 0 for i in range(len(self.grounding_labels))]\n            self.binary_grounding = torch.Tensor(binary_grounding).to(device)\n\n        if distribution_type == 'MI':\n            self._compute_modified_distribution = self._compute_modified_distribution_MI\n        elif distribution_type == 'CAD':\n            self._compute_modified_distribution = self._compute_modified_distribution_CAD\n\n\n        self.model.language_model.prepare_inputs_for_generation = self._prepare_inputs_for_generation\n        self.model.language_model.beam_search = self._beam_search\n        self.alpha = alpha\n        self.threshold = threshold\n        self.topk = topk\n        self.renormalize = renormalize\n        self.pure_llm = pure_llm\n        self._init_cross_attention()\n\n\n    def _init_cross_attention(self):\n        \"\"\"Save original cross-attention settings, in case of turning off cross-attention.\"\"\"\n        self.layer_idx2original_cross_attention = {}\n        for idx, layer in enumerate(self.model.qformer.encoder.layer):\n            self.layer_idx2original_cross_attention[idx] = layer.has_cross_attention\n\n    @torch.no_grad()\n    def generate(\n        self,\n        pixel_values: torch.FloatTensor,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.LongTensor] = None,\n        **generate_kwargs,\n    ) -> torch.LongTensor:\n        if hasattr(self, \"hf_device_map\"):\n            # preprocess for `accelerate`\n            self.model._preprocess_accelerate()\n\n        # 1. Get image embeddings\n        batch_size = pixel_values.shape[0]\n        image_embeds = self.model.vision_model(pixel_values, return_dict=True).last_hidden_state\n        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n\n        # 2. Get language model inputs with full QFormer\n        query_output = self._get_qformer_output(image_embeds, image_attention_mask)\n        language_inputs = self._get_language_model_inputs(\n            input_ids,\n            attention_mask,\n            query_output=query_output,\n        )\n        inputs_embeds, attention_mask = language_inputs[\"inputs_embeds\"], language_inputs[\"attention_mask\"]\n\n        # 3. Precompute QFormer output with language-only QFormer\n        self._disable_cross_attention()\n        language_query_outputs = self._get_qformer_output(image_embeds, image_attention_mask)\n        self._enable_cross_attention()\n\n        # 4. Create dict of extra tensors needed for getting language-only logits during decoding\n        extra_kwargs = {\n            \"extra_language_qformer_output\": language_query_outputs,\n            \"extra_qformer_input_ids\": input_ids,\n            \"extra_running_input_ids\": None,\n        }\n\n        # 5. Generate with custom language model\n        outputs = self.model.language_model.generate(\n            inputs_embeds=inputs_embeds,\n            attention_mask=attention_mask,\n            **generate_kwargs,\n            **extra_kwargs,\n        )\n        return outputs\n\n    def _prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        extra_kwargs = {k:v for k,v in kwargs.items() if k.startswith('extra_')}\n        extra_kwargs['extra_running_input_ids'] = input_ids\n\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        model_inputs.update(extra_kwargs)\n\n        return model_inputs\n\n    def _get_qformer_output(self, image_embeds, image_attention_mask):\n        query_tokens = self.model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n        query_outputs = self.model.qformer(\n            query_embeds=query_tokens,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_attention_mask,\n            return_dict=True,\n        )\n        query_output = query_outputs.last_hidden_state\n        query_output = self.model.language_projection(query_output)\n        return query_output\n\n    def _get_language_model_inputs(self, input_ids, attention_mask, query_output=None):\n        if query_output is None:\n            raise NotImplementedError\n        #language_model_inputs = self.model.language_projection(query_output)\n        language_attention_mask = torch.ones(\n            query_output.size()[:-1], dtype=torch.long, device=query_output.device\n        )\n        if input_ids is None:\n            input_ids = (\n                torch.LongTensor([[self.model.config.text_config.bos_token_id]])\n                .repeat(query_output.shape[0], 1)\n                .to(language_attention_mask.device)\n            )\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        attention_mask = torch.cat([language_attention_mask, attention_mask.to(language_attention_mask.device)], dim=1)\n\n        # Concatenate query embeddings with prompt/partially generated output embeddings\n        inputs_embeds = self.model.get_input_embeddings()(input_ids)\n        inputs_embeds = torch.cat([query_output, inputs_embeds.to(query_output.device)], dim=1)\n        return {'inputs_embeds': inputs_embeds, 'attention_mask': attention_mask}\n\n    def _disable_cross_attention(self):\n        \"\"\"\n        Turn off cross-attention in model QFormer layers. Used to obtain a caption conditioned only on language, not the image.\n        Modifies self.model in-place.\n        \"\"\"\n        for layer in self.model.qformer.encoder.layer:\n            layer.has_cross_attention = False\n\n    def _enable_cross_attention(self):\n        \"\"\"\n        Retores cross-attention in model QFormer layers to the original settings.\n        Modifies self.model in-place.\n        \"\"\"\n        for idx, layer in enumerate(self.model.qformer.encoder.layer):\n            layer.has_cross_attention = self.layer_idx2original_cross_attention[idx]\n\n\n\nclass EntropyThresholdBLIP2COCOBase(EntropyThresholdBLIP2Engine):\n    def __init__(self, model_name = \"Salesforce/blip2-opt-2.7b-coco\", **kwargs):\n        super().__init__(model_name, **kwargs)\n\nclass EntropyThresholdBLIP2Base(EntropyThresholdBLIP2Engine):\n    def __init__(self, model_name = \"Salesforce/blip2-opt-2.7b\", **kwargs):\n        super().__init__(model_name, **kwargs)\n\n\nclass EntropyThresholdInstructBLIPModel(ThresholdModel):\n    def __init__(\n            self,\n            device: str,\n            threshold_type: Literal['entropy', 'vocab'] = 'entropy',\n            distribution_type: Literal['MI', 'CAD'] = 'MI',\n            vocab_label_file: Optional[str] = None,\n            alpha: float = 1.0,\n            topk: int = -1,\n            renormalize: bool = False,\n            threshold: float = 1.0,\n            pure_llm: bool = False\n    ):\n        super().__init__()\n\n        model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n        self.model = model\n        self.vis_processors = vis_processors\n\n        if not torch.distributed.is_initialized():\n            self.model.to(device).eval()\n\n        self.model.language_model = self.model.llm_model\n        self.model.language_model.original_forward = self.model.language_model.forward\n        self.model.language_model.forward = self._forward\n        if threshold_type == 'entropy':\n            self._find_modify_indices = self._find_modify_indices_entropy\n        elif threshold_type == 'vocab':\n            self._find_modify_indices = self._find_modify_indices_vocab\n            self.grounding_labels = torch.load(vocab_label_file)\n            binary_grounding = [1 if self.grounding_labels[i] == 'grounded' else 0 for i in range(len(self.grounding_labels))]\n            self.binary_grounding = torch.Tensor(binary_grounding).to(device)\n\n        if distribution_type == 'MI':\n            self._compute_modified_distribution = self._compute_modified_distribution_MI\n        elif distribution_type == 'CAD':\n            self._compute_modified_distribution = self._compute_modified_distribution_CAD\n\n        self.model.language_model.prepare_inputs_for_generation = self._prepare_inputs_for_generation\n        self.model.language_model.beam_search = self._beam_search\n        self.alpha = alpha\n        self.threshold = threshold\n        self.topk = topk\n        self.renormalize = renormalize\n        self.pure_llm = pure_llm\n        self._init_cross_attention()\n\n    def _init_cross_attention(self):\n        \"\"\"Save original cross-attention settings, in case of turning off cross-attention.\"\"\"\n        self.layer_idx2original_cross_attention = {}\n        for idx, layer in enumerate(self.model.Qformer.bert.encoder.layer):\n            self.layer_idx2original_cross_attention[idx] = layer.has_cross_attention\n\n    def _disable_cross_attention(self):\n        for layer in self.model.Qformer.bert.encoder.layer:\n            layer.has_cross_attention = False\n\n    def _enable_cross_attention(self):\n        for idx, layer in enumerate(self.model.Qformer.bert.encoder.layer):\n            layer.has_cross_attention = self.layer_idx2original_cross_attention[idx]\n\n    @torch.no_grad()\n    def generate(\n        self,\n        samples,\n        use_nucleus_sampling=False,\n        num_beams=5,\n        max_length=256,\n        min_length=1,\n        top_p=0.9,\n        repetition_penalty=1.5,\n        length_penalty=1,\n        num_captions=1,\n        temperature=1,\n        return_dict=False,\n        prefix_allowed_tokens_fn=None,\n        **generate_kwargs,\n    ) -> torch.LongTensor:\n        self.model.llm_tokenizer.padding_side = \"left\"\n\n        if type(samples) == list:\n            # Hack - using batch eval makes a list of samples. Convert to single dictionary with\n            # stacked values.\n            combined = {}\n            for k,v in samples[0].items():\n                if type(v) == torch.Tensor:\n                    combined[k] = torch.cat([s[k] for s in samples], dim=0)\n                else:\n                    combined[k] = [s[k] for s in samples]\n            samples = combined\n\n        if \"prompt\" in samples.keys():\n            prompt = samples[\"prompt\"]\n        else:\n            prompt = self.model.prompt\n\n        image = samples[\"image\"]\n\n        bs = image.size(0)\n\n        if isinstance(prompt, str):\n            prompt = [prompt] * bs\n        else:\n            assert len(prompt) == bs, \"The number of prompts must be equal to the batch size.\"\n\n        # For TextCaps\n        if \"ocr_tokens\" in samples.keys() and \"{}\" in prompt[0]:\n            prompt = [p.format(', '.join(samples['ocr_tokens'][i][:30])) for i, p in enumerate(prompt)]\n\n        query_tokens = self.model.query_tokens.expand(bs, -1, -1)\n        if self.model.qformer_text_input:\n            # remove ocr tokens in q_former (for eval textvqa)\n            # qformer_prompt = prompt\n            # qformer_prompt = ['Question: ' + qp.split(' Question: ')[1] for qp in qformer_prompt]\n\n            text_Qformer = self.model.tokenizer(\n                prompt,\n                padding='longest',\n                truncation=True,\n                max_length=self.model.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(image.device)\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n        # For video data\n        if image.dim() == 5:\n            raise NotImplementedError(\"Video data is not supported yet.\")\n        else:\n            with self.model.maybe_autocast():\n                image_embeds = self.model.ln_vision(self.model.visual_encoder(image))\n            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image.device)\n\n            query_output = self._get_qformer_output(\n                text_Qformer, Qformer_atts, query_tokens, image_embeds, image_atts\n            )\n\n            self._disable_cross_attention()\n            language_query_output = self._get_qformer_output(\n                text_Qformer, Qformer_atts, query_tokens, image_embeds, image_atts\n            )\n            self._enable_cross_attention()\n\n            atts_llm = torch.ones(query_output.size()[:-1], dtype=torch.long).to(image.device)\n\n        llm_tokens = self.model.llm_tokenizer(\n            prompt,\n            padding=\"longest\",\n            return_tensors=\"pt\"\n        ).to(image.device)\n\n        extra_kwargs = {\n            \"extra_language_qformer_output\": language_query_output,\n            \"extra_qformer_input_ids\": llm_tokens.input_ids,\n            \"extra_running_input_ids\": None,\n        }\n\n        with self.model.maybe_autocast():\n            inputs_embeds = self.model.llm_model.get_input_embeddings()(llm_tokens.input_ids)\n            inputs_embeds = torch.cat([query_output, inputs_embeds], dim=1)\n            attention_mask = torch.cat([atts_llm, llm_tokens.attention_mask], dim=1)\n\n            outputs = self.model.language_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                do_sample=use_nucleus_sampling,\n                top_p=top_p,\n                temperature=temperature,\n                num_beams=num_beams,\n                max_length=max_length,\n                min_length=min_length,\n                # eos_token_id=self.model.eos_token_id,\n                repetition_penalty=repetition_penalty,\n                length_penalty=length_penalty,\n                num_return_sequences=num_captions,\n                return_dict_in_generate=return_dict,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                output_scores=True,\n                **extra_kwargs,\n            )\n\n        tokens = outputs[0] if return_dict else outputs\n\n        tokens[tokens == 0] = 2 # convert output id 0 to 2 (eos_token_id)\n        output_text = self.model.llm_tokenizer.batch_decode(tokens, skip_special_tokens=True)\n        output_text = [text.strip() for text in output_text]\n\n        if return_dict:\n            return output_text, outputs\n        return output_text\n\n\n    def _prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        extra_kwargs = {k:v for k,v in kwargs.items() if k.startswith('extra_')}\n        extra_kwargs['extra_running_input_ids'] = input_ids\n\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        model_inputs.update(extra_kwargs)\n        return model_inputs\n\n    def _get_qformer_output(self, text_Qformer, Qformer_atts, query_tokens, image_embeds, image_atts):\n        if self.model.qformer_text_input:\n            query_output = self.model.Qformer.bert(\n                text_Qformer.input_ids,\n                attention_mask=Qformer_atts,\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        else:\n            query_output = self.model.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        inputs_llm = self.model.llm_proj(query_output.last_hidden_state[:,:query_tokens.size(1),:])\n        return inputs_llm\n\n    def _get_language_model_inputs(self, input_ids, attention_mask, query_output=None):\n        if input_ids is None:\n            breakpoint()\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n\n        # Concatenate query embeddings with prompt/partially generated output embeddings\n        inputs_embeds = self.model.llm_model.get_input_embeddings()(input_ids)\n\n        if query_output is not None:\n            inputs_embeds = torch.cat([query_output, inputs_embeds], dim=1)\n            atts_llm = torch.ones(query_output.size()[:-1], dtype=torch.long).to(query_output.device)\n            attention_mask = torch.cat([atts_llm, attention_mask], dim=1)\n\n        # create position_ids on the fly for batch generation\n        position_ids = attention_mask.long().cumsum(-1) - 1\n        position_ids.masked_fill_(attention_mask == 0, 1)\n\n        return {'inputs_embeds': inputs_embeds, 'attention_mask': attention_mask, 'position_ids': position_ids}\n\n\n"}
{"type": "source_file", "path": "methods/eval_utils.py", "content": "import torch\n\n\ndef compute_eval_diffs(results_collection, coco_classes):\n    h_before_total = 0\n    recall_before_total = 0\n    h_after_total = 0\n    recall_after_total = 0\n    h_intersect_total = 0\n    recall_intersect_total = 0\n    classes_to_count = dict()\n    recall_lost_total = 0\n    new_h_total = 0\n    lost_h_total = 0\n    recall_gained_total = 0\n    h_free_images = [0, 0]\n\n    for coco_class in coco_classes:\n        classes_to_count[coco_class] = {\n            \"h_before\": 0,\n            \"h_after\": 0,\n            \"recall_before\": 0,\n            \"recall_after\": 0,\n        }\n\n    examples_good = []\n    count = 0\n    for coco_img in results_collection:\n        count += 1\n        h_before, recall_before, h_after, recall_after = results_collection[coco_img]\n        for caption_word, coco_class in h_before:\n            classes_to_count[coco_class][\"h_before\"] += 1\n        for caption_word, coco_class in h_after:\n            classes_to_count[coco_class][\"h_after\"] += 1\n\n        for caption_word, coco_class in recall_before:\n            classes_to_count[coco_class][\"recall_before\"] += 1\n        for caption_word, coco_class in recall_after:\n            classes_to_count[coco_class][\"recall_after\"] += 1\n\n        recall_after_total += len(recall_after)\n        recall_before_total += len(recall_before)\n        h_before_total += len(h_before)\n        h_after_total += len(h_after)\n\n        # if len(h_before) > len(h_after):\n        #   check_results.append(coco_img)\n\n        # Calculate hallucination and recall intersections\n        h_before_classes = set([ele[1] for ele in h_before])\n        h_after_classes = set([ele[1] for ele in h_after])\n        recall_before_classes = set([ele[1] for ele in recall_before])\n        recall_after_classes = set([ele[1] for ele in recall_after])\n        h_intersect_total += len(h_before_classes.intersection(h_after_classes))\n        recall_intersect_total += len(\n            recall_before_classes.intersection(recall_after_classes)\n        )\n\n        # if len(recall_before_classes) - len(recall_after_classes.intersection(recall_before_classes)) > 0:\n        if (\n            len(h_after_classes) - len(h_before_classes) < 0\n            and len(recall_after_classes) >= len(recall_before_classes)\n            and len(h_after_classes)\n            - len(h_after_classes.intersection(h_before_classes))\n            == 0\n        ):\n            examples_good.append(\n                (\n                    coco_img,\n                    h_after_classes - h_before_classes.intersection(h_after_classes),\n                )\n            )\n\n        new_h_total += len(h_after_classes) - len(\n            h_before_classes.intersection(h_after_classes)\n        )\n        lost_h_total += len(h_before_classes) - len(\n            h_before_classes.intersection(h_after_classes)\n        )\n\n        if len(h_after) == 0:\n            h_free_images[1] += 1\n        if len(h_before) == 0:\n            h_free_images[0] += 1\n\n        # Recall lost - describes the ground truth words that no longer show up as a result of this intervention method. We want this number to be as high as possible\n        recall_lost_total += len(recall_before_classes) - len(\n            recall_after_classes.intersection(recall_before_classes)\n        )\n        recall_gained_total += len(recall_after_classes) - len(\n            recall_after_classes.intersection(recall_before_classes)\n        )\n    return dict(\n        h_free_images=h_free_images,\n        hallucinations=[\n            h_before_total,\n            h_after_total,\n            h_before_total / (h_before_total + recall_before_total),\n            h_after_total / (h_after_total + recall_after_total),\n        ],\n        recall=[recall_before_total, recall_after_total],\n        hallucinations_gained=new_h_total,\n        hallucinations_lost=lost_h_total,\n        recall_lost=recall_lost_total,\n        recall_gained=recall_gained_total,\n        examples_good=examples_good,\n        classes_to_count=classes_to_count,\n    )\n"}
{"type": "source_file", "path": "src/caption/blip_engine.py", "content": "from typing import Dict, List, Literal, Optional, Union, Any\nimport logging\n\nimport torch\nimport transformers\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nfrom transformers.image_processing_utils import BatchFeature\n\nfrom packaging import version\nfrom PIL import Image\nimport string\n\nfrom src.caption.base import CaptionEngine\nfrom src.caption.utils import postprocess_caption\nfrom src.utils.pytorch import select_device\n\nfrom src.caption.lavis.models import load_model_and_preprocess\n\n\n_BLIP_DEFAULT_PROMPT = \"this is a picture of\"\n# _INSTRUCT_BLIP_DEFAULT_PROMPT = \"Describe this image in detail.\"\n_INSTRUCT_BLIP_DEFAULT_PROMPT = \"Write a short description for the image.\"\n\n\nclass BLIP2CaptionEngine(CaptionEngine):\n    def __init__(\n        self,\n        model: str = \"Salesforce/blip2-opt-2.7b-coco\",\n        device: Optional[str] = None,\n        prompt: str = _BLIP_DEFAULT_PROMPT,\n        **kwargs,\n    ):\n        logging.info(f\"Using BLIP2 model {model}\")\n        self.vision_only = kwargs.get(\"vision_only\", False)\n\n        if model == \"Salesforce/blip2-opt-2.7b\":\n            model_type = \"pretrain_opt2.7b\"\n            model_name = \"blip2_opt\"\n            tokenizer = \"self.model.opt_tokenizer\"\n            start_token = 2\n        elif model == \"Salesforce/blip2-opt-2.7b-coco\":\n            model_type = \"caption_coco_opt2.7b\"\n            model_name = \"blip2_opt\"\n            tokenizer = \"self.model.opt_tokenizer\"\n            start_token = 2\n        elif model == \"Salesforce/blip2-opt-6.7b\":\n            model_type = \"pretrain_opt6.7b\"\n            model_name = \"blip2_opt\"\n            tokenizer = \"self.model.opt_tokenizer\"\n            start_token = 2\n        elif model == \"Salesforce/blip2-opt-6.7b-coco\":\n            model_type = \"caption_coco_opt6.7b\"\n            model_name = \"blip2_opt\"\n            tokenizer = \"self.model.opt_tokenizer\"\n            start_token = 2\n        else:\n            raise ValueError(f\"Unknown BLIP2 model {model}\")\n\n        model, vis_processors, _ = load_model_and_preprocess(\n            name=model_name,\n            model_type=model_type,\n            is_eval=True,\n            device=device,\n            vision_only=self.vision_only,\n        )\n        self.model = model\n        self.vis_processors = vis_processors\n        self.tokenizer = eval(tokenizer)\n\n        # if not torch.distributed.is_initialized():\n        self.model.to(device or \"cpu\").eval()\n        self.prompt = prompt\n        self.device = device or \"cpu\"\n        self.pure_llm = kwargs.get(\"pure_llm\", False)\n        self._init_cross_attention()\n        self.vision_only = kwargs.get(\"vision_only\", False)\n        self.start_token = start_token\n\n    def _init_cross_attention(self):\n        \"\"\"Save original cross-attention settings, in case of turning off cross-attention.\"\"\"\n        self.layer_idx2original_cross_attention = {}\n        # For loading model from Blip2ForConditionalGeneration.from_pretrained\n        # for idx, layer in enumerate(self.model.qformer.encoder.layer):\n\n        # For loading OPT model from load_model_and_preprocess\n        for idx, layer in enumerate(self.model.Qformer.bert.encoder.layer):\n            self.layer_idx2original_cross_attention[idx] = layer.has_cross_attention\n\n    def processor(self, image, prompt=None):\n        if prompt is None:\n            prompt = self.prompt\n        inputs = {\"image\": self._preprocess_image(image), \"prompt\": prompt}\n        return inputs\n\n    def _preprocess_image(\n        self, raw_image: Image.Image, prompt: Optional[str] = None\n    ) -> torch.Tensor:\n        return self.vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n\n    def get_baseline_caption(\n        self,\n        inputs,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        topp=-1,\n        return_embeds=False,\n        return_tokens=False,\n    ) -> List[str]:\n\n        out = self.model.generate(  # type: ignore\n            inputs,\n            num_beams=num_beams,\n            temperature=temperature,\n            max_length=max_length,\n            top_p=topp,\n            use_nucleus_sampling=topp > 0,\n            return_embeds=return_embeds,\n        )\n\n        if return_embeds:\n            baseline_caption, inputs_embeds, inputs_query, outputs = out\n        else:\n            baseline_caption = out\n\n        baseline_caption = [postprocess_caption(b.strip()) for b in baseline_caption]\n\n        if return_embeds:\n            return baseline_caption, inputs_embeds, inputs_query, outputs\n        if return_tokens:\n            return baseline_caption, out[1]\n        return baseline_caption\n\n    def get_baseline_gen(self, force_caption: Optional[str], raw_image: Image.Image):\n        if force_caption is not None:\n            # Lowercase & remove punctuation\n            force_caption = force_caption.translate(\n                str.maketrans(\"\", \"\", string.punctuation)\n            ).lower()\n            encoded = self.get_encoded_caption(force_caption)\n            max_length = encoded.shape[1]\n            num_beams = 1\n            no_repeat_ngram_size = 0\n\n            def prefix_allowed_tokens_fn(batch_id, sent):\n                next_tok_id = len(sent)\n                return encoded[batch_id][next_tok_id].tolist()\n\n        else:\n            num_beams = 16\n            no_repeat_ngram_size = 3\n            prefix_allowed_tokens_fn = None\n            max_length = 256\n\n        inputs = self._preprocess_image(raw_image)\n\n        baseline_gen = self.model.generate(  # type: ignore\n            **inputs,\n            num_beams=num_beams,\n            no_repeat_ngram_size=no_repeat_ngram_size,\n            max_length=max_length,\n            return_dict_in_generate=True,\n            output_attentions=True,\n            output_scores=True,\n            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n        )\n\n        baseline_caption = self.tokenizer.batch_decode(\n            baseline_gen.sequences, skip_special_tokens=True\n        )\n        baseline_caption = postprocess_caption(baseline_caption[0].strip())\n        return baseline_gen, baseline_caption\n\n    def _get_generated_attention(self, gen):\n        # Generation object is a type of [X]DecoderOnlyOutput (e.g., GreedyDecoderOnlyOutput),\n        # which only has self attention.\n        return {\"self_attention\": gen.attentions}\n\n    def __call__(\n        self,\n        raw_image: Image.Image,\n        n_captions: int = 1,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        prompt=None,\n        topp=-1,\n        return_embeds=False,\n        generation_type=\"normal\",\n    ) -> List[str]:\n\n        inputs = self.processor(raw_image, prompt)\n\n        return self.get_baseline_caption(\n            inputs,\n            do_sample=do_sample,\n            num_beams=num_beams,\n            max_length=max_length,\n            temperature=temperature,\n            topp=topp,\n            return_embeds=return_embeds,\n        )\n\n    def get_forced_output_distributions(\n        self,\n        raw_image: Image,\n        encoded_caption: torch.Tensor,\n        vocab_size: int,\n        prompt: Optional[str] = None,\n        language_only: bool = False,\n        pure_llm: bool = False,\n    ) -> torch.Tensor:\n        distributions = (\n            []\n        )  # Will be list of len(encoded_caption shape - 1), each entry vocab_size\n        inputs = self.processor(raw_image, prompt)\n\n        if language_only:\n            self._disable_cross_attention()\n\n        for i in range(1, encoded_caption.shape[1]):\n\n            def prefix_allowed_tokens_fn(batch_id, sent):\n                if sent.shape[0] < i:\n                    tokens = encoded_caption[batch_id][sent.shape[0]].tolist()\n                else:\n                    tokens = None\n                return tokens\n\n            gen = self.model.generate(  # type: ignore\n                inputs,\n                num_beams=1,\n                # max_length=encoded_caption.shape[1] + 1,\n                max_length=i + 1,\n                return_dict=True,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                pure_llm=False if not language_only else self.pure_llm,\n            )\n\n            distributions.append(gen[1].scores[i - 1][0])\n\n        if language_only:\n            self._enable_cross_attention()\n\n        return torch.stack(distributions)\n\n    def _disable_cross_attention(self):\n        \"\"\"\n        Turn off cross-attention in model QFormer layers. Used to obtain a caption conditioned only on language, not the image.\n        Modifies self.model in-place.\n        \"\"\"\n        # See notes in self._init_cross_attention()\n        # for layer in self.model.qformer.encoder.layer:\n        for layer in self.model.Qformer.bert.encoder.layer:\n            layer.has_cross_attention = False\n\n    def _enable_cross_attention(self):\n        \"\"\"\n        Retores cross-attention in model QFormer layers to the original settings.\n        Modifies self.model in-place.\n        \"\"\"\n        # See notes in self._init_cross_attention()\n        # for idx, layer in enumerate(self.model.qformer.encoder.layer):\n        for idx, layer in enumerate(self.model.Qformer.bert.encoder.layer):\n            layer.has_cross_attention = self.layer_idx2original_cross_attention[idx]\n\n\nclass BLIP2COCOLarge(BLIP2CaptionEngine):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(\n            model=\"Salesforce/blip2-opt-6.7b-coco\", device=device, **kwargs\n        )\n\n\nclass BLIP2COCOBase(BLIP2CaptionEngine):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(\n            model=\"Salesforce/blip2-opt-2.7b-coco\", device=device, **kwargs\n        )\n\n\nclass BLIP2COCOT5Large(BLIP2CaptionEngine):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(\n            model=\"Salesforce/blip2-flan-t5-xl-coco\", device=device, **kwargs\n        )\n\n\nclass BLIP2Large(BLIP2CaptionEngine):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"Salesforce/blip2-opt-6.7b\", device=device, **kwargs)\n\n\nclass BLIP2Base(BLIP2CaptionEngine):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"Salesforce/blip2-opt-2.7b\", device=device, **kwargs)\n\n\nclass BLIP2T5Large(BLIP2CaptionEngine):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"Salesforce/blip2-flan-t5-xl\", device=device, **kwargs)\n\n\n# class InstructBLIP(BLIP2CaptionEngine):\n#     def __init__(\n#         self, model: str = \"instruct-blip\", device: Optional[str] = None, prompt: str = _INSTRUCT_BLIP_DEFAULT_PROMPT\n#     ):\n#         logging.info('Using InstructBLIP model')\n#         model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_vicuna_instruct\", model_type=\"vicuna7b\", is_eval=True, device=device)\n#         self.model = model\n#         self.vis_processors = vis_processors\n#         breakpoint()\n#         self.tokenizer = self._processor.tokenizer\n#         self.model = Blip2ForConditionalGeneration.from_pretrained(model, torch_dtype=torch.float16)\n#         if not torch.distributed.is_initialized():\n#             self.model.to(device or \"cpu\").eval()\n#         self.prompt = prompt\n#         self.device = device or \"cpu\"\n\n\n#     def processor(self, image, prompt=None):\n#         if prompt is None:\n#             prompt = self.prompt\n#         kwargs = {\"return_tensors\": \"pt\", \"text\": prompt}\n#         return self._processor(image, **kwargs)\n\n#     def _preprocess_image(self, raw_image: Image.Image, prompt: Optional[str] = None) -> torch.Tensor:\n#         return self.vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup\n\nsetup()\n"}
{"type": "source_file", "path": "methods/llava_next_utils.py", "content": "import torch\nfrom PIL import Image\nimport requests\nfrom tqdm import tqdm\nfrom transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n\ndef get_logits(model, output):\n    \"\"\"Extract and process logits from model output.\"\"\"\n    x = torch.stack(output.hidden_states[0]).max(dim=1).values\n\n    logits = []\n    for i in tqdm(range(x.shape[0])):\n        with torch.no_grad():\n            logits.append(model.language_model.lm_head(x[i, :, :]).detach().cpu())\n    \n    logit_lens = torch.stack(logits)\n    with torch.no_grad():\n        probs = torch.softmax(logit_lens, dim=-1)\n    \n    raw_probs = probs.permute(2, 0, 1)\n    raw_probs = raw_probs[:, :, 5:-14]\n    probs = torch.max(raw_probs, dim=1).values\n    probs = torch.max(probs, dim=1).values\n    return raw_probs, probs\n\ndef get_generated_logits(model, output):\n    \"\"\"Get logits from generated output.\"\"\"\n    logits = []\n    for i in range(1, len(output.hidden_states)):\n        logits.append(torch.stack(output.hidden_states[i]).max(dim=1).values)\n    \n    logits = torch.stack(logits)\n    logits = logits.squeeze(2)[:, -1, :]\n\n    real_logits = model.language_model.lm_head(logits).detach().cpu()\n    softmax_logits = torch.softmax(real_logits, dim=-1)\n\n    return softmax_logits.permute(1, 0)\n\ndef get_text_hidden_state_at_layer(text, model, processor, layer):\n    \"\"\"Get hidden state representation for text at specified layer.\"\"\"\n    token_ids = processor.tokenizer.encode(text)\n    input_ids = torch.tensor(token_ids).unsqueeze(0).to(\"cuda\")\n\n    with torch.inference_mode():\n        outputs = model.language_model(\n            input_ids=input_ids,\n            output_hidden_states=True,\n            return_dict=True,\n            max_new_tokens=5,\n        )\n\n    hidden_states = outputs.hidden_states\n    return hidden_states[layer][0, -1, :].unsqueeze(0)\n\ndef projection(image_embeddings, text_embedding):\n    \"\"\"Calculate projection of image embeddings onto text embedding.\"\"\"\n    return (image_embeddings @ text_embedding.T)[0, :, 0] / (\n        text_embedding @ text_embedding.T\n    ).squeeze()\n\ndef subtract_projection(image_embeddings, text_embedding, weight=1):\n    \"\"\"Subtract text projection from image embeddings.\"\"\"\n    image_embeddings = image_embeddings.clone()\n    proj = projection(image_embeddings, text_embedding)\n    for i in range(image_embeddings.shape[1]):\n        if proj[i] > 0:\n            image_embeddings[:, i] -= weight * proj[i] * text_embedding\n    return image_embeddings\n\ndef subtract_projections(image_embeddings, text_embeddings, weight=1):\n    \"\"\"Subtract multiple text projections from image embeddings.\"\"\"\n    img_embeddings = image_embeddings.clone()\n    for text_embedding in text_embeddings:\n        img_embeddings = subtract_projection(img_embeddings, text_embedding, weight)\n    return img_embeddings\n\ndef generate_mass_edit_hook(\n    text_embeddings, start_edit_index, end_edit_index, layer, weight=1, minimum_size=32\n):\n    \"\"\"Generate hook for editing embeddings during forward pass.\"\"\"\n    def edit_embeddings(module, input, output):\n        new_output = list(output)\n        if new_output[0].shape[1] > minimum_size:\n            print(f\"Editing layer {layer}\")\n            new_output[0][:, start_edit_index:end_edit_index] = subtract_projections(\n                new_output[0][:, start_edit_index:end_edit_index],\n                text_embeddings,\n                weight=weight,\n            )\n        return tuple(new_output)\n\n    return edit_embeddings\n\ndef generate_caption(model, processor, image_path, prompt=None):\n    \"\"\"Generate caption for an image.\"\"\"\n    if isinstance(image_path, str):\n        if image_path.startswith('http'):\n            image = Image.open(requests.get(image_path, stream=True).raw)\n        else:\n            image = Image.open(image_path)\n    else:\n        image = image_path\n\n    if prompt is None:\n        prompt = \"Please describe this image in detail.\"\n\n    conversation = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": prompt},\n            ],\n        },\n    ]\n    \n    formatted_prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n    inputs = processor(images=image, text=formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    with torch.inference_mode():\n        output = model.generate(\n            **inputs,\n            max_new_tokens=512,\n            return_dict_in_generate=True,\n            output_hidden_states=True,\n            temperature=1.0,\n            num_beams=5\n        )\n    \n    caption = processor.decode(output.sequences[0], skip_special_tokens=True)\n    return caption, output\n\ndef retrieve_logit_lens_llava_next(state, img_path):\n    \"\"\"Get logit lens analysis for an image using LLaVA-Next.\"\"\"\n    device = state[\"model\"].device\n    \n    if isinstance(img_path, str):\n        if img_path.startswith('http'):\n            image = Image.open(requests.get(img_path, stream=True).raw)\n        else:\n            image = Image.open(img_path)\n    else:\n        image = img_path\n\n    conversation = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image\"},\n                {\"type\": \"text\", \"text\": \"Write a detailed description.\"},\n            ],\n        },\n    ]\n    \n    prompt = state[\"processor\"].apply_chat_template(conversation, add_generation_prompt=True)\n    inputs = state[\"processor\"](images=image, text=prompt, return_tensors=\"pt\").to(device)\n\n    with torch.inference_mode():\n        output = state[\"model\"].generate(\n            **inputs,\n            max_new_tokens=5,\n            return_dict_in_generate=True,\n            output_hidden_states=True,\n            temperature=1.0,\n            num_beams=5\n        )\n\n    caption = state[\"processor\"].decode(output.sequences[0], skip_special_tokens=True).strip()\n\n    x = torch.stack(output.hidden_states[0]).max(dim=1).values.to(device)\n\n    logits = []\n    for i in range(x.shape[0]):\n        with torch.no_grad():\n            logits.append(state[\"model\"].language_model.lm_head(x[i, :, :]).detach().cpu())\n    \n    logit_lens = torch.stack(logits)\n    \n    with torch.no_grad():\n        softmax_probs = torch.softmax(logit_lens, dim=-1)\n    \n    softmax_probs = softmax_probs.permute(2, 0, 1)\n    \n    try:\n        image_token_region = softmax_probs[:, :, 5:-14]\n    except Exception as e:\n        print(f\"Warning: Error slicing token region: {e}\")\n        print(f\"Softmax probs shape: {softmax_probs.shape}\")\n        image_token_region = softmax_probs\n    \n    image_token_region = image_token_region.detach().cpu().numpy()\n    return caption, image_token_region\n\ndef load_llava_next_state(model_path=\"llava-hf/llava-v1.6-vicuna-7b-hf\", device=\"cuda\"):\n    processor = LlavaNextProcessor.from_pretrained(model_path)\n    model = LlavaNextForConditionalGeneration.from_pretrained(\n        model_path, \n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True,\n        device_map=\"auto\"\n    )\n    \n    vocabulary = processor.tokenizer.get_vocab()\n    vocab_embeddings = model.get_input_embeddings()(\n        torch.tensor(list(vocabulary.values()), dtype=torch.long).unsqueeze(0).to(device)\n    )\n\n    execute_model = lambda img_path, text_prompt=None: generate_caption(\n        model, processor, img_path, text_prompt\n    )[0]\n    \n    register_hook = lambda hook, layer: model.language_model.model.layers[layer].register_forward_hook(hook)\n    register_pre_hook = lambda pre_hook, layer: model.language_model.model.layers[layer].register_forward_pre_hook(pre_hook)\n    hidden_layer_embedding = lambda text, layer: get_text_hidden_state_at_layer(\n        text, model, processor, layer\n    )\n\n    return {\n        \"vocabulary\": vocabulary,\n        \"vocab_embeddings\": vocab_embeddings,\n        \"tokenizer\": processor.tokenizer,\n        \"execute_model\": execute_model,\n        \"register_hook\": register_hook,\n        \"register_pre_hook\": register_pre_hook,\n        \"hidden_layer_embedding\": hidden_layer_embedding,\n        \"model\": model,\n        \"model_name\": model_path,\n        \"processor\": processor,\n    }\n"}
{"type": "source_file", "path": "src/caption/base.py", "content": "from abc import ABC, abstractmethod\nfrom typing import List, Optional, Dict, Any, Tuple, Callable, Union\nfrom collections import defaultdict\nimport logging\nfrom PIL import Image\nimport torch\nimport string\nfrom torchvision import transforms\nimport numpy as np\nfrom scipy.stats import entropy\n\n\nclass CaptionEngineAbstract(ABC):\n    @abstractmethod\n    def __init__(self, device: Optional[str] = None):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def __call__(\n        self,\n        raw_image: Image,\n        n_captions: int = 1,\n        temperature: Optional[float] = None,\n        prompt: Optional[str] = None,\n    ) -> List[str]:\n        # Takes an RGB image and returns a list of captions\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_baseline_caption(self, inputs: Any) -> List[str]:\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_caption_hallucination_mode(\n        self,\n        raw_image: Image.Image,\n        force_caption: Optional[str] = None,\n        hc_confs: List[str] = None,\n        return_attentions: bool = False,\n    ) -> List[str]:\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_encoded_caption(self, caption: str) -> torch.Tensor:\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_forced_output_distributions(\n        self,\n        raw_image: Image.Image,\n        encoded_caption: torch.Tensor,\n        vocab_size: int,\n        language_only: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"\n        Output the logit distributions for a teacher-forced caption.\n        args:\n            raw_image: PIL Image input\n            encoded_caption: 1 x seq_len+1 Tensor of encoded caption, includes BOS & EOS\n            vocab_size: int, size of vocabulary\n            language_only: bool, if True, return the distribution of logits that are conditioned only on the language prefix and not the image at each time step.\n\n        return seq_len x vocab_size Tensor of logit distributions at each time step of teacher-forced encoded_caption. Includes EOS but not BOS.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass CaptionEngine(CaptionEngineAbstract):\n    def __init__(self, *args, **kwargs):\n        self.start_token = None\n        self.verifier_initialized = False\n        self.verification_threshold = None\n\n    def initialize_verifier(self, verifier_type, verification_threshold):\n        logging.info(\n            f\"Initializing verifier {verifier_type} with threshold {verification_threshold} ...\"\n        )\n        assert verification_threshold is not None, \"Must provide verification threshold\"\n        if verifier_type == \"openclip-ViTG\":\n            import open_clip\n\n            model_type, pretrained = (\"ViT-bigG-14\", \"laion2b_s39b_b160k\")\n            model, _, preprocess = open_clip.create_model_and_transforms(\n                model_type, pretrained=pretrained\n            )\n            tokenizer = open_clip.get_tokenizer(model_type)\n            model.to(self.device)\n            self.verifier = model\n            self.verifier_type = verifier_type\n            self.verification_threshold = verification_threshold\n            self.verifier_preprocess = preprocess\n            self.verifier_tokenizer = tokenizer\n            self.verifier_initialized = True\n        else:\n            raise NotImplementedError(\n                f\"Verifier type {verifier_type} not implemented for initialize_verifier.\"\n            )\n\n    def verify_caption(self, raw_image: Image.Image, sentence_list: List[str]):\n        \"\"\"\n        Given list of sentences, return list of same length of 0s and 1s, where 0 = reject, 1 = pass.\n        \"\"\"\n        if \"openclip\" in self.verifier_type:\n            # Get image embedding\n            image_input = (\n                self.verifier_preprocess(raw_image).unsqueeze(0).to(self.device)\n            )\n            with torch.no_grad():\n                image_features = self.verifier.encode_image(image_input)\n            image_features /= image_features.norm(dim=-1, keepdim=True)\n\n            # Get text embeddings\n            text_tokenized = self.verifier_tokenizer(sentence_list).to(self.device)\n            with torch.no_grad():\n                text_features = self.verifier.encode_text(text_tokenized)\n            text_features /= text_features.norm(dim=-1, keepdim=True)\n\n            # Compute similarities and threshold for decisions\n            similarities = (100.0 * image_features @ text_features.T)[\n                0\n            ]  # Length of sentence_list\n            similarities = similarities.cpu()\n            decisions = (similarities > self.verification_threshold).int().tolist()\n            print(sentence_list)\n            print(decisions)\n\n        else:\n            raise NotImplementedError(\n                f\"Verifier type {self.verifier_type} not implemented for verify_caption.\"\n            )\n\n        return decisions\n\n    def get_caption_iterative_filtering(\n        self,\n        inputs,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        raw_image=None,\n    ) -> List[str]:\n        assert (\n            self.verifier_initialized\n        ), \"Must initialize verifier before using iterative filtering.\"\n\n        caption, inputs_embeds, inputs_query, outputs = self.get_baseline_caption(\n            inputs,\n            do_sample=do_sample,\n            num_beams=num_beams,\n            max_length=max_length,\n            temperature=temperature,\n            return_embeds=True,\n        )\n        tokens = outputs[0]\n        stop_idxs = torch.where(tokens == self.stop_token)[0]  # Indices of stop tokens\n        current_start = 0\n        sentences = []\n        for stop_idx in stop_idxs:\n            sentences.append(tokens[current_start : stop_idx + 1])\n            current_start = stop_idx + 1\n        decoded_sentences = [\n            self.tokenizer.decode(sentence) for sentence in sentences\n        ]  # List[str] of sentences\n\n        # Get reject/accept decisions for each sentence (0 = reject, 1 = accept)\n        decisions = self.verify_caption(raw_image, decoded_sentences)\n\n        return caption\n\n    def get_encoded_caption(self, caption: str) -> torch.Tensor:\n        return self.tokenizer.encode(caption, return_tensors=\"pt\").to(self.device)\n\n    def compute_sparse_distribution_metrics(\n        self,\n        full_distributions: torch.Tensor,\n        language_distributions: torch.Tensor,\n        encoded_caption: torch.Tensor,\n    ) -> Dict[str, Any]:\n\n        \"\"\"\n        N = sequence length\n        full_distributions: N x vocab size\n        language_distributions: N x vocab size\n        encoded_caption: N\n        \"\"\"\n        N = full_distributions.shape[0]\n        assert language_distributions.shape[0] == N\n        assert encoded_caption.shape[0] == N\n\n        full_logits = [\n            d[token].item() for d, token in zip(full_distributions, encoded_caption)\n        ]\n        language_logits = [\n            d[token].item() for d, token in zip(language_distributions, encoded_caption)\n        ]\n\n        full_softmax_scores = torch.softmax(full_distributions.float(), dim=-1)\n        language_softmax_scores = torch.softmax(language_distributions.float(), dim=-1)\n\n        full_entropies = [entropy(d) for d in full_softmax_scores]\n        language_entropies = [entropy(d) for d in language_softmax_scores]\n        kl_divs_qk_lang = [\n            entropy(VL, qk=L)\n            for VL, L in zip(full_softmax_scores, language_softmax_scores)\n        ]\n        kl_divs_qk_full = [\n            entropy(L, qk=VL)\n            for VL, L in zip(full_softmax_scores, language_softmax_scores)\n        ]\n\n        out = {\n            \"full_logit\": full_logits,\n            \"language_logit\": language_logits,\n            \"full_entropy\": full_entropies,\n            \"language_entropy\": language_entropies,\n            \"kl_div_qk_lang\": kl_divs_qk_lang,\n            \"kl_div_qk_full\": kl_divs_qk_full,\n        }\n        return out\n\n    def get_caption_distributions(\n        self,\n        raw_image: Image,\n        force_caption: str,\n        tokens: Optional[List[int]] = None,\n        prompt: Optional[str] = None,\n        remove_punctuation=False,\n        sparse=False,\n    ) -> Dict[str, Any]:\n\n        vocab_size = self.tokenizer.vocab_size\n\n        if tokens is None:\n            if remove_punctuation:\n                force_caption = force_caption.translate(\n                    str.maketrans(\"\", \"\", string.punctuation)\n                ).lower()\n            encoded = self.get_encoded_caption(\n                force_caption\n            )  # Size = 1 x sequence length\n            if self.start_token is not None and encoded[0][0] != self.start_token:\n                # Add start_token to beginning of tokens\n                encoded = torch.cat(\n                    (torch.tensor([[self.start_token]]).to(self.device), encoded), dim=1\n                )\n            tokens_decoded = self.tokenizer.convert_ids_to_tokens(encoded[0])\n        else:\n            if self.start_token is not None and tokens[0] != self.start_token:\n                # Add start_token to beginning of tokens\n                tokens = [self.start_token] + tokens\n            encoded = torch.tensor(tokens).long().unsqueeze(0).to(self.device)\n            tokens_decoded = self.tokenizer.convert_ids_to_tokens(tokens)\n\n        # encoded begins with self.start_token (and tokens_decoded starts with decoded version)\n\n        full_distributions = self.get_forced_output_distributions(\n            raw_image, encoded, vocab_size, language_only=False, prompt=prompt\n        )\n        language_distributions = self.get_forced_output_distributions(\n            raw_image, encoded, vocab_size, language_only=True, prompt=prompt\n        )\n\n        full_distributions = full_distributions.cpu()\n        language_distributions = language_distributions.cpu()\n        encoded = encoded.cpu()[0]\n\n        if self.start_token is not None:\n            encoded = encoded[1:]  # Remove start_token\n\n        assert (\n            full_distributions.shape == language_distributions.shape\n        ), f\"Shape mismatch. full_distributions.shape: {full_distributions.shape}, language_distributions.shape: {language_distributions.shape}\"\n\n        sparse_metrics = self.compute_sparse_distribution_metrics(\n            full_distributions, language_distributions, encoded\n        )\n\n        out = {\n            \"caption\": force_caption,\n            \"encoded_caption\": encoded,\n            \"tokens_decoded\": tokens_decoded,\n        }\n        out.update(sparse_metrics)\n        if not sparse:\n            out.update(\n                {\n                    \"full_distributions\": full_distributions,\n                    \"language_distributions\": language_distributions,\n                }\n            )\n\n        return out\n\n    def get_caption_hallucination_mode(\n        self,\n        raw_image: Image,\n        force_caption: Optional[str] = None,\n        hc_confs: List[str] = [\"logit\"],\n        return_attentions: Optional[bool] = False,\n    ) -> Dict[str, Any]:\n        assert len(hc_confs) > 0, \"hc_confs should not be empty\"\n\n        baseline_gen, baseline_caption = self.get_baseline_gen(force_caption, raw_image)\n\n        # Get word confidences for each hc_confs method.\n        # For now, assume the aggregation is \"mean\" over multiple tokens that make up one word.\n        # confidence_data: Dict[str, Dict[str, List[float]]]. maps hc_conf to dictionary of format:\n        #  {'all_confidences': List[float], 'word_confidences': List[List[float]], 'word_confidences_aggregated': List[float]}\n        confidence_data = {}\n        for hc_conf in hc_confs:\n            confidence_data[hc_conf] = {\n                \"all_confidences\": [],\n                \"word_confidences\": [],\n                \"word_confidences_aggregated\": [],\n            }\n        tokens = baseline_gen.sequences[0]  # includes BOS\n        attentions = self._get_generated_attention(baseline_gen)\n        word_indices = self.tokens_to_word_indices(tokens)\n\n        vocab_size = self.tokenizer.vocab_size\n        encoded = self.get_encoded_caption(force_caption)  # Size = 1 x sequence length\n        full_logit_distributions = self.get_forced_output_distributions(\n            raw_image, encoded, vocab_size\n        )  # no BOS, includes EOS\n        tokens_decoded = self.tokenizer.convert_ids_to_tokens(tokens)[1:]  # remove BOS\n\n        # Filter tokens for those that correspond to words.\n        tokens = tokens[word_indices >= 0]\n\n        # Only take values that correspond to words, to align tensors with `tokens`.\n        # Don't use the BOS in word_indices.\n        logit_distributions = [\n            full_logit_distributions[i]\n            for i in range(len(word_indices) - 1)\n            if word_indices[i + 1] >= 0\n        ]\n        if return_attentions:\n            for attention_type, attention in attentions.items():\n                attention = [\n                    attention[i]\n                    for i in range(len(word_indices) - 1)\n                    if word_indices[i + 1] >= 0\n                ]\n                attentions[attention_type] = attention\n\n        # Align word indices with `tokens` (removes `-1` entries, e.g. for EOS).\n        word_indices = word_indices[word_indices >= 0]\n\n        # Remove punctuation and split caption into words.\n        remove_chars = string.punctuation\n        word_list = baseline_caption.translate(\n            str.maketrans(\"\", \"\", remove_chars)\n        ).split()\n\n        if return_attentions:\n            word_to_attentions = {}\n            for attention_type, attention in attentions.items():\n                word_to_att = self._get_word_to_attentions(\n                    attention, tokens, word_list, word_indices\n                )\n                word_to_attentions[attention_type] = word_to_att\n        else:\n            word_to_attentions = None\n\n        for hc_conf in hc_confs:\n            confs = self._get_confidences(hc_conf, tokens, logit_distributions)\n            (\n                word_confidences_lists,\n                word_confidences_aggregated,\n            ) = self.aggregate_confidences_by_words(confs, word_indices)\n\n            confidence_data[hc_conf][\"all_confidences\"] = confs\n            confidence_data[hc_conf][\"word_confidences\"] = word_confidences_lists\n            confidence_data[hc_conf][\n                \"word_confidences_aggregated\"\n            ] = word_confidences_aggregated\n\n        return {\n            \"baseline_caption\": baseline_caption,\n            \"word_list\": word_list,\n            \"confidence_data\": confidence_data,\n            \"word_to_attentions\": word_to_attentions,\n            \"logit_distributions\": full_logit_distributions,\n            \"encoded_caption\": encoded,\n            \"tokens_decoded\": tokens_decoded,\n        }\n\n    def _get_confidences(self, hc_conf, tokens, logits):\n        softmax_scores = [\n            torch.nn.functional.softmax(logits[i], dim=-1) for i in range(len(tokens))\n        ]\n        if hc_conf == \"logit\":\n            confs = [logits[i][tokens[i]].item() for i in range(len(tokens))]\n        elif hc_conf == \"softmax\":\n            confs = [softmax_scores[i][tokens[i]].item() for i in range(len(tokens))]\n        elif hc_conf == \"entropy\":\n            confs = [\n                float(-entropy(softmax_scores[i].cpu())) for i in range(len(tokens))\n            ]\n        else:\n            raise ValueError(f\"Unknown hc_conf: {hc_conf}\")\n        return confs\n\n    def _get_word_to_attentions(\n        self, attention, tokens, word_list, word_indices\n    ) -> Dict[str, List[Any]]:\n        \"\"\"\n        Returns a dictionary mapping each word to a list of attentions of len(# tokens in word), where each element\n               is a tuple of len(# layers), and each element is a tensor of size:\n                (num_heads x gen_len x num_enc_tokens) for cross-attention, or\n                (num_heads x gen_len x gen_len) for self-attention.\n        \"\"\"\n        word_to_attentions = {(i, w): [] for (i, w) in enumerate(word_list)}\n        index_to_word = {i: w for i, w in enumerate(word_list)}\n        for i in range(len(tokens)):\n            word_index = word_indices[i].item()\n            word = index_to_word[word_index]\n            att = attention[i]  # tuple of len(# layers)\n            att = [a[0].detach().cpu() for a in att]\n            word_to_attentions[(word_index, word)].append(att)\n\n        return word_to_attentions\n\n    def aggregate_confidences_by_words(\n        self,\n        confidences: torch.Tensor,\n        word_indices: torch.Tensor,\n        agg_fn: Callable[[torch.Tensor], float] = lambda x: x.mean().item(),\n    ) -> List[float]:\n        \"\"\"\n        Aggregates confidences by word, using the given aggregation function.\n\n        Args:\n            confidences (torch.Tensor[float]): A tensor of confidences, of shape (sequence_length,).\n            word_indices (torch.Tensor[long]): A tensor of word indices, of shape (sequence_length,).\n            agg_fn (Callable[[torch.Tensor], float]): An aggregation function that takes in a tensor and returns a single value (e.g., mean).\n        Returns:\n            word_confidences (List[List[float]]): A list of lists of unaggregated word confidences, of length num_words.\n            word_confidences_aggregated (List[float]): A list of aggregated word confidences, of length num_words.\n        \"\"\"\n        grouped = defaultdict(list)\n        for word_index, conf in zip(word_indices, confidences):\n            grouped[word_index.item()].append(conf)\n\n        word_confidences = [grouped[idx] for idx in sorted(grouped.keys())]\n\n        # Convert each list of confidences to a tensor, and apply the aggregation function.\n        word_confidences_aggregated = [\n            agg_fn(torch.Tensor(confidences)) for confidences in grouped.values()\n        ]\n\n        if -float(\"inf\") in word_confidences_aggregated:\n            breakpoint()\n\n        return word_confidences, word_confidences_aggregated\n\n    def tokens_to_word_indices(self, tokens: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Maps a list of token IDs to a list of integers, where each integer corresponds to the index of the word that the token belongs to.\n        Ignored tokens are mapped to -1, which are punctuation, BOS, EOS, or PAD.\n\n        Args:\n            tokens (torch.Tensor[int]): A tensor of token IDs, of shape (sequence_length,).\n        Returns:\n            word_indices (torch.Tensor[long]): A tensor of word indices, of shape (sequence_length,).\n        \"\"\"\n        ignore = [\n            self.tokenizer.bos_token,\n            self.tokenizer.eos_token,\n            self.tokenizer.pad_token,\n            \"\",  # 'space' token, don't count if it appears on its own (happens rarely)\n        ]\n        ignore.extend(list(string.punctuation))\n\n        tokens_decoded = self.tokenizer.convert_ids_to_tokens(tokens)\n        # tokens_decoded = [self.tokenizer.decode(torch.Tensor([t])) for t in tokens][1:]\n\n        current_word_index = 0\n        word_indices = []\n\n        for token in tokens_decoded:\n            if token in ignore:\n                word_indices.append(-1)\n            else:\n                if token.startswith(\"\"):\n                    current_word_index += 1\n                word_indices.append(current_word_index)\n\n        return torch.Tensor(word_indices).long().to(tokens.device)\n"}
{"type": "source_file", "path": "methods/llava_utils.py", "content": "import requests\nfrom PIL import Image\nfrom io import BytesIO\nimport torch\n\nfrom llava.constants import (\n    IMAGE_TOKEN_INDEX,\n    DEFAULT_IMAGE_TOKEN,\n    DEFAULT_IM_START_TOKEN,\n    DEFAULT_IM_END_TOKEN,\n    IMAGE_PLACEHOLDER,\n)\nfrom llava.conversation import conv_templates, SeparatorStyle\nfrom llava.model.builder import load_pretrained_model\nfrom llava.utils import disable_torch_init\nfrom llava.mm_utils import (\n    process_images,\n    tokenizer_image_token,\n    get_model_name_from_path,\n    KeywordsStoppingCriteria,\n)\n\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport re\nimport numpy as np\nfrom methods.utils import load_images, string_to_token_ids\n\nfrom transformers.generation.logits_process import TopKLogitsWarper\nfrom transformers.generation.logits_process import LogitsProcessorList\n\n\n# Get the token embeddings from LLaVA\ndef get_vocab_embeddings_llava(llm_model, tokenizer, device=\"cuda\"):\n    vocab = tokenizer.get_vocab()\n    llm_tokens = (\n        torch.tensor(list(vocab.values()), dtype=torch.long).unsqueeze(0).to(device)\n    )\n    token_embeddings = llm_model.get_input_embeddings()(llm_tokens)\n    return token_embeddings\n\n\n# Weaves in the image token placeholders into the provided text prompt\ndef generate_text_prompt(model, model_name, text_prompt):\n    qs = text_prompt\n    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n    if IMAGE_PLACEHOLDER in qs:\n        if model.config.mm_use_im_start_end:\n            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n        else:\n            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n    else:\n        if model.config.mm_use_im_start_end:\n            qs = image_token_se + \"\\n\" + qs\n        else:\n            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n\n    if \"llama-2\" in model_name.lower():\n        conv_mode = \"llava_llama_2\"\n    elif \"v1\" in model_name.lower():\n        conv_mode = \"llava_v1\"\n    elif \"mpt\" in model_name.lower():\n        conv_mode = \"mpt\"\n    else:\n        conv_mode = \"llava_v0\"\n\n    conv = conv_templates[conv_mode].copy()\n    conv.append_message(conv.roles[0], qs)\n    conv.append_message(conv.roles[1], None)\n\n    return conv\n\n\ndef generate_images_tensor(model, img_path, image_processor):\n    image_files = [img_path]\n    images = load_images(image_files)\n    image_sizes = [x.size for x in images]\n\n    images_tensor = process_images(images, image_processor, model.config).to(\n        model.device, dtype=torch.float16\n    )\n\n    return images_tensor, images, image_sizes\n\n\ndef prompt_to_img_input_ids(prompt, tokenizer):\n    input_ids = (\n        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n        .unsqueeze(0)\n        .cuda()\n    )\n    return input_ids\n\n\ndef run_llava_model(\n    model,\n    model_name,\n    images_tensor,\n    image_sizes,\n    tokenizer,\n    text_prompt=None,\n    hidden_states=False,\n):\n    if text_prompt == None:\n        text_prompt = \"Write a detailed description.\"\n\n    conv = generate_text_prompt(model, model_name, text_prompt)\n    input_ids = prompt_to_img_input_ids(conv.get_prompt(), tokenizer)\n\n    # Model parameters\n    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n    keywords = [stop_str]\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n    with torch.inference_mode():\n        output = model.generate(\n            input_ids,\n            images=images_tensor,\n            temperature=1.0,\n            num_beams=5,\n            max_new_tokens=512,\n            # use_cache=True,\n            use_cache=True,\n            stopping_criteria=[stopping_criteria],\n            output_hidden_states=hidden_states,\n            return_dict_in_generate=True,\n            image_sizes=image_sizes,\n        )\n\n    if hidden_states:\n        return input_ids, output\n\n    outputs = tokenizer.batch_decode(output.sequences, skip_special_tokens=True)[\n        0\n    ].strip()\n\n    return outputs\n\n\ndef retrieve_logit_lens_llava(state, img_path, text_prompt = None):\n    images_tensor, images, image_sizes = generate_images_tensor(\n        state[\"model\"], img_path, state[\"image_processor\"]\n    )\n    input_ids, output = run_llava_model(\n        state[\"model\"],\n        state[\"model_name\"],\n        images_tensor,\n        image_sizes,\n        state[\"tokenizer\"],\n        hidden_states=True,\n        text_prompt=text_prompt\n    )\n\n    output_ids = output.sequences\n    o = state[\"tokenizer\"].batch_decode(\n        output_ids, skip_special_tokens=True\n    )[0]\n    caption = o.strip()\n\n    hidden_states = torch.stack(output.hidden_states[0])\n\n    logits_warper = TopKLogitsWarper(top_k=50, filter_value=float(\"-inf\"))\n    logits_processor = LogitsProcessorList([])\n\n    with torch.inference_mode():\n        curr_layer_logits = state[\"model\"].lm_head(hidden_states).cpu().float()\n        logit_scores = torch.nn.functional.log_softmax(curr_layer_logits, dim=-1)\n        logit_scores_processed = logits_processor(input_ids, logit_scores)\n        logit_scores = logits_warper(input_ids, logit_scores_processed)\n        softmax_probs = torch.nn.functional.softmax(logit_scores, dim=-1)\n\n    softmax_probs = softmax_probs.detach().cpu().numpy()\n\n    image_token_index = input_ids.tolist()[0].index(-200)\n    softmax_probs = softmax_probs[\n        :, :, image_token_index : image_token_index + (24 * 24)\n    ]\n\n    # transpose to (vocab_dim, num_layers, num_tokens, num_beams)\n    softmax_probs = softmax_probs.transpose(3, 0, 2, 1)\n\n    # maximum over all beams\n    softmax_probs = softmax_probs.max(axis=3)\n    return caption, softmax_probs\n\n\ndef reshape_llava_prompt_hidden_layers(hidden_states):\n    prompt_hidden_states = hidden_states[\n        0\n    ]  # shape is (# layers, # beams, # prompt tokens, # dim size)\n    first_beam_layers = torch.stack(list(prompt_hidden_states), dim=0)[:, 0]\n    return first_beam_layers\n\n\ndef get_hidden_text_embedding(\n    target_word, model, vocab_embeddings, tokenizer, layer=5, device=\"cuda\"\n):\n    # Tokenize the target word into input ids\n    token_ids = string_to_token_ids(target_word, tokenizer)\n    input_ids = torch.tensor(token_ids).unsqueeze(0).to(device)\n\n    # Model parameters\n    stop_str = target_word\n    keywords = [stop_str]\n    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)\n\n    with torch.inference_mode():\n        output = model.generate(\n            input_ids,\n            temperature=1.0,\n            num_beams=5,\n            max_new_tokens=10,  # can be small because we only care about image representations\n            return_dict_in_generate=True,\n            output_hidden_states=True,\n            use_cache=False,\n            stopping_criteria=[stopping_criteria],\n        )\n\n    hidden_states = reshape_llava_prompt_hidden_layers(output[\"hidden_states\"])\n\n    dist = torch.norm(\n        hidden_states[0, len(token_ids) - 1]\n        - vocab_embeddings[0, token_ids[len(token_ids) - 1]]\n    )\n    if dist > 0.1:\n        print(\n            f\"Validation check failed: caption word {target_word} didn't match: {dist}\"\n        )\n\n    return hidden_states[layer, len(token_ids) - 1].unsqueeze(0)\n\n\ndef get_caption_from_llava(\n    img_path, model, model_name, tokenizer, image_processor, text_prompt=None\n):\n    images_tensor, images, image_sizes = generate_images_tensor(\n        model, img_path, image_processor\n    )\n\n    # Generate the new caption\n    new_caption = run_llava_model(\n        model,\n        model_name,\n        images_tensor,\n        image_sizes,\n        tokenizer,\n        text_prompt=text_prompt,\n    )\n\n    return new_caption\n\n\ndef load_llava_state(device=\"cuda\"):\n    model_path = \"liuhaotian/llava-v1.5-7b\"\n    model_name = get_model_name_from_path(model_path)\n    tokenizer, model, image_processor, context_len = load_pretrained_model(\n        model_path, None, model_name, device=device\n    )\n\n    vocabulary = tokenizer.get_vocab()\n    vocab_embeddings = get_vocab_embeddings_llava(model, tokenizer, device=device)\n\n    execute_model = lambda img_path, text_prompt=None, image_embeddings=None: get_caption_from_llava(\n        img_path, model, model_name, tokenizer, image_processor, text_prompt=text_prompt\n    )\n    register_hook = (\n        lambda hook, layer: model.get_model().layers[layer].register_forward_hook(hook)\n    )\n    register_pre_hook = (\n        lambda pre_hook, layer: model.get_model()\n        .layers[layer]\n        .register_forward_pre_hook(pre_hook)\n    )\n    hidden_layer_embedding = lambda text, layer: get_hidden_text_embedding(\n        text, model, vocab_embeddings, tokenizer, layer, device=device\n    )\n\n    return {\n        \"vocabulary\": vocabulary,\n        \"vocab_embeddings\": vocab_embeddings,\n        \"tokenizer\": tokenizer,\n        \"execute_model\": execute_model,\n        \"register_hook\": register_hook,\n        \"register_pre_hook\": register_pre_hook,\n        \"hidden_layer_embedding\": hidden_layer_embedding,\n        \"model\": model,\n        \"model_name\": model_name,\n        \"image_processor\": image_processor,\n    }\n"}
{"type": "source_file", "path": "src/caption/lavis/common/__init__.py", "content": ""}
{"type": "source_file", "path": "src/caption/lavis/configs/__init__.py", "content": ""}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/Qformer.py", "content": "\"\"\"\n * Copyright (c) 2023, salesforce.com, inc.\n * All rights reserved.\n * SPDX-License-Identifier: BSD-3-Clause\n * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n * By Junnan Li\n * Based on huggingface code base\n * https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert\n\"\"\"\n\nimport math\nimport os\nimport warnings\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Dict, Any\n\nimport torch\nfrom torch import Tensor, device, dtype, nn\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn.functional as F\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    ModelOutput,\n)\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    BaseModelOutputWithPoolingAndCrossAttentions,\n    CausalLMOutputWithCrossAttentions,\n    MaskedLMOutput,\n    MultipleChoiceModelOutput,\n    NextSentencePredictorOutput,\n    QuestionAnsweringModelOutput,\n    SequenceClassifierOutput,\n    TokenClassifierOutput,\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    apply_chunking_to_forward,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import logging\nfrom transformers.models.bert.configuration_bert import BertConfig\n\nlogger = logging.get_logger(__name__)\n\n\nclass BertEmbeddings(nn.Module):\n    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.word_embeddings = nn.Embedding(\n            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id\n        )\n        self.position_embeddings = nn.Embedding(\n            config.max_position_embeddings, config.hidden_size\n        )\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n        self.register_buffer(\n            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1))\n        )\n        self.position_embedding_type = getattr(\n            config, \"position_embedding_type\", \"absolute\"\n        )\n\n        self.config = config\n\n    def forward(\n        self,\n        input_ids=None,\n        position_ids=None,\n        query_embeds=None,\n        past_key_values_length=0,\n    ):\n        if input_ids is not None:\n            seq_length = input_ids.size()[1]\n        else:\n            seq_length = 0\n\n        if position_ids is None:\n            position_ids = self.position_ids[\n                :, past_key_values_length : seq_length + past_key_values_length\n            ].clone()\n\n        if input_ids is not None:\n            embeddings = self.word_embeddings(input_ids)\n            if self.position_embedding_type == \"absolute\":\n                position_embeddings = self.position_embeddings(position_ids)\n                embeddings = embeddings + position_embeddings\n\n            if query_embeds is not None:\n                embeddings = torch.cat((query_embeds, embeddings), dim=1)\n        else:\n            embeddings = query_embeds\n\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config, is_cross_attention):\n        super().__init__()\n        self.config = config\n        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(\n            config, \"embedding_size\"\n        ):\n            raise ValueError(\n                \"The hidden size (%d) is not a multiple of the number of attention \"\n                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n            )\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        if is_cross_attention:\n            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n        else:\n            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.position_embedding_type = getattr(\n            config, \"position_embedding_type\", \"absolute\"\n        )\n        if (\n            self.position_embedding_type == \"relative_key\"\n            or self.position_embedding_type == \"relative_key_query\"\n        ):\n            self.max_position_embeddings = config.max_position_embeddings\n            self.distance_embedding = nn.Embedding(\n                2 * config.max_position_embeddings - 1, self.attention_head_size\n            )\n        self.save_attention = False\n\n    def save_attn_gradients(self, attn_gradients):\n        self.attn_gradients = attn_gradients\n\n    def get_attn_gradients(self):\n        return self.attn_gradients\n\n    def save_attention_map(self, attention_map):\n        self.attention_map = attention_map\n\n    def get_attention_map(self):\n        return self.attention_map\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (\n            self.num_attention_heads,\n            self.attention_head_size,\n        )\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n\n        # If this is instantiated as a cross-attention module, the keys\n        # and values come from an encoder; the attention mask needs to be\n        # such that the encoder's padding tokens are not attended to.\n        is_cross_attention = encoder_hidden_states is not None\n\n        if is_cross_attention:\n            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n            attention_mask = encoder_attention_mask\n        elif past_key_value is not None:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n        else:\n            key_layer = self.transpose_for_scores(self.key(hidden_states))\n            value_layer = self.transpose_for_scores(self.value(hidden_states))\n\n        mixed_query_layer = self.query(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        past_key_value = (key_layer, value_layer)\n\n        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n\n        if (\n            self.position_embedding_type == \"relative_key\"\n            or self.position_embedding_type == \"relative_key_query\"\n        ):\n            seq_length = hidden_states.size()[1]\n            position_ids_l = torch.arange(\n                seq_length, dtype=torch.long, device=hidden_states.device\n            ).view(-1, 1)\n            position_ids_r = torch.arange(\n                seq_length, dtype=torch.long, device=hidden_states.device\n            ).view(1, -1)\n            distance = position_ids_l - position_ids_r\n            positional_embedding = self.distance_embedding(\n                distance + self.max_position_embeddings - 1\n            )\n            positional_embedding = positional_embedding.to(\n                dtype=query_layer.dtype\n            )  # fp16 compatibility\n\n            if self.position_embedding_type == \"relative_key\":\n                relative_position_scores = torch.einsum(\n                    \"bhld,lrd->bhlr\", query_layer, positional_embedding\n                )\n                attention_scores = attention_scores + relative_position_scores\n            elif self.position_embedding_type == \"relative_key_query\":\n                relative_position_scores_query = torch.einsum(\n                    \"bhld,lrd->bhlr\", query_layer, positional_embedding\n                )\n                relative_position_scores_key = torch.einsum(\n                    \"bhrd,lrd->bhlr\", key_layer, positional_embedding\n                )\n                attention_scores = (\n                    attention_scores\n                    + relative_position_scores_query\n                    + relative_position_scores_key\n                )\n\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        if is_cross_attention and self.save_attention:\n            self.save_attention_map(attention_probs)\n            attention_probs.register_hook(self.save_attn_gradients)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs_dropped = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs_dropped = attention_probs_dropped * head_mask\n\n        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (\n            (context_layer, attention_probs) if output_attentions else (context_layer,)\n        )\n\n        outputs = outputs + (past_key_value,)\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config, is_cross_attention=False):\n        super().__init__()\n        self.self = BertSelfAttention(config, is_cross_attention)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads,\n            self.self.num_attention_heads,\n            self.self.attention_head_size,\n            self.pruned_heads,\n        )\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = (\n            self.self.attention_head_size * self.self.num_attention_heads\n        )\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n    ):\n        self_outputs = self.self(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            encoder_hidden_states,\n            encoder_attention_mask,\n            past_key_value,\n            output_attentions,\n        )\n        attention_output = self.output(self_outputs[0], hidden_states)\n\n        outputs = (attention_output,) + self_outputs[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config, layer_num):\n        super().__init__()\n        self.config = config\n        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n        self.seq_len_dim = 1\n        self.attention = BertAttention(config)\n        self.layer_num = layer_num\n        if (\n            self.config.add_cross_attention\n            and layer_num % self.config.cross_attention_freq == 0\n        ):\n            self.crossattention = BertAttention(\n                config, is_cross_attention=self.config.add_cross_attention\n            )\n            self.has_cross_attention = True\n        else:\n            self.has_cross_attention = False\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n        self.intermediate_query = BertIntermediate(config)\n        self.output_query = BertOutput(config)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_value=None,\n        output_attentions=False,\n        query_length=0,\n    ):\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = (\n            past_key_value[:2] if past_key_value is not None else None\n        )\n        self_attention_outputs = self.attention(\n            hidden_states,\n            attention_mask,\n            head_mask,\n            output_attentions=output_attentions,\n            past_key_value=self_attn_past_key_value,\n        )\n        attention_output = self_attention_outputs[0]\n        outputs = self_attention_outputs[1:-1]\n\n        present_key_value = self_attention_outputs[-1]\n\n        if query_length > 0:\n            query_attention_output = attention_output[:, :query_length, :]\n\n            if self.has_cross_attention:\n                assert (\n                    encoder_hidden_states is not None\n                ), \"encoder_hidden_states must be given for cross-attention layers\"\n                cross_attention_outputs = self.crossattention(\n                    query_attention_output,\n                    attention_mask,\n                    head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    output_attentions=output_attentions,\n                )\n                query_attention_output = cross_attention_outputs[0]\n                outputs = (\n                    outputs + cross_attention_outputs[1:-1]\n                )  # add cross attentions if we output attention weights\n\n            layer_output = apply_chunking_to_forward(\n                self.feed_forward_chunk_query,\n                self.chunk_size_feed_forward,\n                self.seq_len_dim,\n                query_attention_output,\n            )\n            if attention_output.shape[1] > query_length:\n                layer_output_text = apply_chunking_to_forward(\n                    self.feed_forward_chunk,\n                    self.chunk_size_feed_forward,\n                    self.seq_len_dim,\n                    attention_output[:, query_length:, :],\n                )\n                layer_output = torch.cat([layer_output, layer_output_text], dim=1)\n        else:\n            layer_output = apply_chunking_to_forward(\n                self.feed_forward_chunk,\n                self.chunk_size_feed_forward,\n                self.seq_len_dim,\n                attention_output,\n            )\n        outputs = (layer_output,) + outputs\n\n        outputs = outputs + (present_key_value,)\n\n        return outputs\n\n    def feed_forward_chunk(self, attention_output):\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        return layer_output\n\n    def feed_forward_chunk_query(self, attention_output):\n        intermediate_output = self.intermediate_query(attention_output)\n        layer_output = self.output_query(intermediate_output, attention_output)\n        return layer_output\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.layer = nn.ModuleList(\n            [BertLayer(config, i) for i in range(config.num_hidden_layers)]\n        )\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        head_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n        query_length=0,\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n        all_cross_attentions = (\n            () if output_attentions and self.config.add_cross_attention else None\n        )\n\n        next_decoder_cache = () if use_cache else None\n\n        for i in range(self.config.num_hidden_layers):\n            layer_module = self.layer[i]\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_head_mask = head_mask[i] if head_mask is not None else None\n            past_key_value = past_key_values[i] if past_key_values is not None else None\n\n            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n\n                if use_cache:\n                    logger.warn(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return module(\n                            *inputs, past_key_value, output_attentions, query_length\n                        )\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask,\n                    layer_head_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    past_key_value,\n                    output_attentions,\n                    query_length,\n                )\n\n            hidden_states = layer_outputs[0]\n            if use_cache:\n                next_decoder_cache += (layer_outputs[-1],)\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n                all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    next_decoder_cache,\n                    all_hidden_states,\n                    all_self_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_decoder_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We \"pool\" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n        self.decoder.bias = self.bias\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states)\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = BertConfig\n    base_model_prefix = \"bert\"\n    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nclass BertModel(BertPreTrainedModel):\n    \"\"\"\n    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n    input to the forward pass.\n    \"\"\"\n\n    def __init__(self, config, add_pooling_layer=False):\n        super().__init__(config)\n        self.config = config\n\n        self.embeddings = BertEmbeddings(config)\n\n        self.encoder = BertEncoder(config)\n\n        self.pooler = BertPooler(config) if add_pooling_layer else None\n\n        self.init_weights()\n\n    def get_input_embeddings(self):\n        return self.embeddings.word_embeddings\n\n    def set_input_embeddings(self, value):\n        self.embeddings.word_embeddings = value\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def get_extended_attention_mask(\n        self,\n        attention_mask: Tensor,\n        input_shape: Tuple[int],\n        device: device,\n        is_decoder: bool,\n        has_query: bool = False,\n    ) -> Tensor:\n        \"\"\"\n        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n\n        Arguments:\n            attention_mask (:obj:`torch.Tensor`):\n                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n            input_shape (:obj:`Tuple[int]`):\n                The shape of the input to the model.\n            device: (:obj:`torch.device`):\n                The device of the input to the model.\n\n        Returns:\n            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n        \"\"\"\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if attention_mask.dim() == 3:\n            extended_attention_mask = attention_mask[:, None, :, :]\n        elif attention_mask.dim() == 2:\n            # Provided a padding mask of dimensions [batch_size, seq_length]\n            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n            if is_decoder:\n                batch_size, seq_length = input_shape\n\n                seq_ids = torch.arange(seq_length, device=device)\n                causal_mask = (\n                    seq_ids[None, None, :].repeat(batch_size, seq_length, 1)\n                    <= seq_ids[None, :, None]\n                )\n\n                # add a prefix ones mask to the causal mask\n                # causal and attention masks must have same type with pytorch version < 1.3\n                causal_mask = causal_mask.to(attention_mask.dtype)\n\n                if causal_mask.shape[1] < attention_mask.shape[1]:\n                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n                    if has_query:  # UniLM style attention mask\n                        causal_mask = torch.cat(\n                            [\n                                torch.zeros(\n                                    (batch_size, prefix_seq_len, seq_length),\n                                    device=device,\n                                    dtype=causal_mask.dtype,\n                                ),\n                                causal_mask,\n                            ],\n                            axis=1,\n                        )\n                    causal_mask = torch.cat(\n                        [\n                            torch.ones(\n                                (batch_size, causal_mask.shape[1], prefix_seq_len),\n                                device=device,\n                                dtype=causal_mask.dtype,\n                            ),\n                            causal_mask,\n                        ],\n                        axis=-1,\n                    )\n                extended_attention_mask = (\n                    causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n                )\n            else:\n                extended_attention_mask = attention_mask[:, None, None, :]\n        else:\n            raise ValueError(\n                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n                    input_shape, attention_mask.shape\n                )\n            )\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(\n            dtype=self.dtype\n        )  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n        return extended_attention_mask\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        query_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        is_decoder=False,\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        \"\"\"\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        # use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        if input_ids is None:\n            assert (\n                query_embeds is not None\n            ), \"You have to specify query_embeds when input_ids is None\"\n\n        # past_key_values_length\n        past_key_values_length = (\n            past_key_values[0][0].shape[2] - self.config.query_length\n            if past_key_values is not None\n            else 0\n        )\n\n        query_length = query_embeds.shape[1] if query_embeds is not None else 0\n\n        embedding_output = self.embeddings(\n            input_ids=input_ids,\n            position_ids=position_ids,\n            query_embeds=query_embeds,\n            past_key_values_length=past_key_values_length,\n        )\n\n        input_shape = embedding_output.size()[:-1]\n        batch_size, seq_length = input_shape\n        device = embedding_output.device\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                ((batch_size, seq_length + past_key_values_length)), device=device\n            )\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        if is_decoder:\n            extended_attention_mask = self.get_extended_attention_mask(\n                attention_mask,\n                input_ids.shape,\n                device,\n                is_decoder,\n                has_query=(query_embeds is not None),\n            )\n        else:\n            extended_attention_mask = self.get_extended_attention_mask(\n                attention_mask, input_shape, device, is_decoder\n            )\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if encoder_hidden_states is not None:\n            if type(encoder_hidden_states) == list:\n                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[\n                    0\n                ].size()\n            else:\n                (\n                    encoder_batch_size,\n                    encoder_sequence_length,\n                    _,\n                ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n\n            if type(encoder_attention_mask) == list:\n                encoder_extended_attention_mask = [\n                    self.invert_attention_mask(mask) for mask in encoder_attention_mask\n                ]\n            elif encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n                encoder_extended_attention_mask = self.invert_attention_mask(\n                    encoder_attention_mask\n                )\n            else:\n                encoder_extended_attention_mask = self.invert_attention_mask(\n                    encoder_attention_mask\n                )\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n\n        encoder_outputs = self.encoder(\n            embedding_output,\n            attention_mask=extended_attention_mask,\n            head_mask=head_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_extended_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            query_length=query_length,\n        )\n        sequence_output = encoder_outputs[0]\n        pooled_output = (\n            self.pooler(sequence_output) if self.pooler is not None else None\n        )\n\n        if not return_dict:\n            return (sequence_output, pooled_output) + encoder_outputs[1:]\n\n        return BaseModelOutputWithPoolingAndCrossAttentions(\n            last_hidden_state=sequence_output,\n            pooler_output=pooled_output,\n            past_key_values=encoder_outputs.past_key_values,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n            cross_attentions=encoder_outputs.cross_attentions,\n        )\n\n\nclass BertLMHeadModel(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        query_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        past_key_values=None,\n        use_cache=True,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        return_logits=False,\n        is_decoder=True,\n        reduction=\"mean\",\n    ):\n        r\"\"\"\n        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n            the model is configured as a decoder.\n        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n        use_cache (:obj:`bool`, `optional`):\n            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n            decoding (see :obj:`past_key_values`).\n        Returns:\n        Example::\n            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n            >>> import torch\n            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n            >>> outputs = model(**inputs)\n            >>> prediction_logits = outputs.logits\n        \"\"\"\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n        if labels is not None:\n            use_cache = False\n        if past_key_values is not None:\n            query_embeds = None\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            query_embeds=query_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n        )\n\n        sequence_output = outputs[0]\n        if query_embeds is not None:\n            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]\n\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores[:, :-1, :].contiguous()\n\n        lm_loss = None\n        if labels is not None:\n            # we are doing next-token prediction; shift prediction scores and input ids by one\n            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n            labels = labels[:, 1:].contiguous()\n            loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1)\n            lm_loss = loss_fct(\n                shifted_prediction_scores.view(-1, self.config.vocab_size),\n                labels.view(-1),\n            )\n            if reduction == \"none\":\n                lm_loss = lm_loss.view(prediction_scores.size(0), -1).sum(1)\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return ((lm_loss,) + output) if lm_loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=lm_loss,\n            logits=prediction_scores,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n            cross_attentions=outputs.cross_attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, query_embeds, past=None, attention_mask=None, **model_kwargs\n    ):\n        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n        if attention_mask is None:\n            attention_mask = input_ids.new_ones(input_ids.shape)\n        query_mask = input_ids.new_ones(query_embeds.shape[:-1])\n        attention_mask = torch.cat([query_mask, attention_mask], dim=-1)\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"input_ids\": input_ids,\n            \"query_embeds\": query_embeds,\n            \"attention_mask\": attention_mask,\n            \"past_key_values\": past,\n            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n            \"is_decoder\": True,\n        }\n\n    def _reorder_cache(self, past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (\n                tuple(\n                    past_state.index_select(0, beam_idx) for past_state in layer_past\n                ),\n            )\n        return reordered_past\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n\n    def get_output_embeddings(self):\n        return self.cls.predictions.decoder\n\n    def set_output_embeddings(self, new_embeddings):\n        self.cls.predictions.decoder = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        position_ids=None,\n        head_mask=None,\n        query_embeds=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        labels=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n        return_logits=False,\n        is_decoder=False,\n    ):\n        r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"\n\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            query_embeds=query_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n            is_decoder=is_decoder,\n        )\n\n        if query_embeds is not None:\n            sequence_output = outputs[0][:, query_embeds.shape[1] :, :]\n        prediction_scores = self.cls(sequence_output)\n\n        if return_logits:\n            return prediction_scores\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)\n            )\n\n        if not return_dict:\n            output = (prediction_scores,) + outputs[2:]\n            return (\n                ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n            )\n\n        return MaskedLMOutput(\n            loss=masked_lm_loss,\n            logits=prediction_scores,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n"}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/blip2.py", "content": "\"\"\"\n Copyright (c) 2023, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\nimport contextlib\nimport logging\nimport os\nimport time\nimport datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nimport torch.nn.functional as F\n\nimport src.caption.lavis.common.dist_utils as dist_utils\nfrom src.caption.lavis.common.dist_utils import download_cached_file\nfrom src.caption.lavis.common.utils import is_url\nfrom src.caption.lavis.common.logger import MetricLogger\nfrom src.caption.lavis.models.base_model import BaseModel\nfrom src.caption.lavis.models.blip2_models.Qformer import BertConfig, BertLMHeadModel\nfrom src.caption.lavis.models.eva_vit import create_eva_vit_g\nfrom src.caption.lavis.models.clip_vit import create_clip_vit_L\nfrom transformers import BertTokenizer\n\n\nclass Blip2Base(BaseModel):\n    @classmethod\n    def init_tokenizer(cls, truncation_side=\"right\"):\n        tokenizer = BertTokenizer.from_pretrained(\n            \"bert-base-uncased\", truncation_side=truncation_side\n        )\n        tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n        return tokenizer\n\n    def maybe_autocast(self, dtype=torch.float16):\n        # if on cpu, don't use autocast\n        # if on gpu, use autocast with dtype if provided, otherwise use torch.float16\n        enable_autocast = self.device != torch.device(\"cpu\")\n\n        if enable_autocast:\n            return torch.cuda.amp.autocast(dtype=dtype)\n        else:\n            return contextlib.nullcontext()\n\n    @classmethod\n    def init_Qformer(cls, num_query_token, vision_width, cross_attention_freq=2):\n        encoder_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n        encoder_config.encoder_width = vision_width\n        # insert cross-attention layer every other block\n        encoder_config.add_cross_attention = True\n        encoder_config.cross_attention_freq = cross_attention_freq\n        encoder_config.query_length = num_query_token\n        Qformer = BertLMHeadModel.from_pretrained(\n            \"bert-base-uncased\", config=encoder_config\n        )\n        query_tokens = nn.Parameter(\n            torch.zeros(1, num_query_token, encoder_config.hidden_size)\n        )\n        query_tokens.data.normal_(mean=0.0, std=encoder_config.initializer_range)\n        return Qformer, query_tokens\n\n    def init_vision_encoder(\n        self, model_name, img_size, drop_path_rate, use_grad_checkpoint, precision\n    ):\n        assert model_name in [\n            \"eva_clip_g\",\n            \"eva2_clip_L\",\n            \"clip_L\",\n        ], \"vit model must be eva_clip_g, eva2_clip_L or clip_L\"\n        if model_name == \"eva_clip_g\":\n            visual_encoder = create_eva_vit_g(\n                img_size, drop_path_rate, use_grad_checkpoint, precision\n            )\n        #         elif model_name == \"eva2_clip_L\":\n        #             visual_encoder = create_eva2_vit_L(\n        #                 img_size, drop_path_rate, use_grad_checkpoint, precision\n        #             )\n        elif model_name == \"clip_L\":\n            visual_encoder = create_clip_vit_L(img_size, use_grad_checkpoint, precision)\n        ln_vision = LayerNorm(visual_encoder.num_features)\n        self.vit_name = model_name\n        return visual_encoder, ln_vision\n\n    def load_from_pretrained(self, url_or_filename):\n        if is_url(url_or_filename):\n            cached_file = download_cached_file(\n                url_or_filename, check_hash=False, progress=True\n            )\n            checkpoint = torch.load(cached_file, map_location=\"cpu\")\n        elif os.path.isfile(url_or_filename):\n            checkpoint = torch.load(url_or_filename, map_location=\"cpu\")\n        else:\n            raise RuntimeError(\"checkpoint url or path is invalid\")\n\n        state_dict = checkpoint[\"model\"]\n\n        msg = self.load_state_dict(state_dict, strict=False)\n\n        # logging.info(\"Missing keys {}\".format(msg.missing_keys))\n        logging.info(\"load checkpoint from %s\" % url_or_filename)\n\n        return msg\n\n    def get_optimizer_params(self, weight_decay, lr_scale=1):\n\n        vit_num_layers = self.visual_encoder.get_num_layer()\n        lr_scales = list(\n            lr_scale ** (vit_num_layers + 1 - i) for i in range(vit_num_layers + 2)\n        )\n\n        parameter_group_names = {}\n        parameter_group_vars = {}\n\n        for name, param in self.named_parameters():\n            if not param.requires_grad:\n                continue  # frozen weights\n            if len(param.shape) == 1 or name.endswith(\".bias\"):\n                group_name = \"no_decay\"\n                this_weight_decay = 0.0\n            else:\n                group_name = \"decay\"\n                this_weight_decay = weight_decay\n            if \"visual_encoder\" in name:\n                layer_id = self.visual_encoder.get_num_layer(\n                    name.replace(\"visual_encoder.\", \"\")\n                )\n                group_name = \"vit_layer_%d_%s\" % (layer_id, group_name)\n            else:\n                layer_id = None\n\n            if group_name not in parameter_group_names:\n                if layer_id is not None:\n                    scale = lr_scales[layer_id]\n                else:\n                    scale = 1\n                parameter_group_names[group_name] = {\n                    \"weight_decay\": this_weight_decay,\n                    \"params\": [],\n                    \"lr_scale\": scale,\n                }\n                parameter_group_vars[group_name] = {\n                    \"weight_decay\": this_weight_decay,\n                    \"params\": [],\n                    \"lr_scale\": scale,\n                }\n            parameter_group_vars[group_name][\"params\"].append(param)\n            parameter_group_names[group_name][\"params\"].append(name)\n        # import json\n        # print(\"Param groups = %s\" % json.dumps(parameter_group_names, indent=2))\n        optim_params = list(parameter_group_vars.values())\n        return optim_params\n\n    def _lemmatize(self, answers):\n        def apply(answer):\n            doc = self.lemmatizer(answer)\n\n            words = []\n            for token in doc:\n                if token.pos_ in [\"NOUN\", \"VERB\"]:\n                    words.append(token.lemma_)\n                else:\n                    words.append(token.text)\n            answer = \" \".join(words)\n\n            return answer\n\n        return [apply(answer) for answer in answers]\n\n    @property\n    def lemmatizer(self):\n        if self._lemmatizer is None:\n            try:\n                import spacy\n\n                self._lemmatizer = spacy.load(\"en_core_web_sm\")\n            except ImportError:\n                logging.error(\n                    \"\"\"\n                    Please install spacy and en_core_web_sm model to apply lemmatization.\n                    python -m spacy download en_core_web_sm\n                    OR\n                    import spacy.cli\n                    spacy.cli.download(\"en_core_web_sm\")\n                    \"\"\"\n                )\n                exit(1)\n\n        return self._lemmatizer\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\ndef compute_sim_matrix(model, data_loader, **kwargs):\n    k_test = kwargs.pop(\"k_test\")\n\n    metric_logger = MetricLogger(delimiter=\"  \")\n    header = \"Evaluation:\"\n\n    logging.info(\"Computing features for evaluation...\")\n    start_time = time.time()\n\n    texts = data_loader.dataset.text\n    num_text = len(texts)\n    text_bs = 256\n    text_ids = []\n    text_embeds = []\n    text_atts = []\n    for i in range(0, num_text, text_bs):\n        text = texts[i : min(num_text, i + text_bs)]\n        text_input = model.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=35,\n            return_tensors=\"pt\",\n        ).to(model.device)\n        text_feat = model.forward_text(text_input)\n        text_embed = F.normalize(model.text_proj(text_feat))\n        text_embeds.append(text_embed)\n        text_ids.append(text_input.input_ids)\n        text_atts.append(text_input.attention_mask)\n\n    text_embeds = torch.cat(text_embeds, dim=0)\n    text_ids = torch.cat(text_ids, dim=0)\n    text_atts = torch.cat(text_atts, dim=0)\n\n    vit_feats = []\n    image_embeds = []\n    for samples in data_loader:\n        image = samples[\"image\"]\n\n        image = image.to(model.device)\n        image_feat, vit_feat = model.forward_image(image)\n        image_embed = model.vision_proj(image_feat)\n        image_embed = F.normalize(image_embed, dim=-1)\n\n        vit_feats.append(vit_feat.cpu())\n        image_embeds.append(image_embed)\n\n    vit_feats = torch.cat(vit_feats, dim=0)\n    image_embeds = torch.cat(image_embeds, dim=0)\n\n    sims_matrix = []\n    for image_embed in image_embeds:\n        sim_q2t = image_embed @ text_embeds.t()\n        sim_i2t, _ = sim_q2t.max(0)\n        sims_matrix.append(sim_i2t)\n    sims_matrix = torch.stack(sims_matrix, dim=0)\n\n    score_matrix_i2t = torch.full(\n        (len(data_loader.dataset.image), len(texts)), -100.0\n    ).to(model.device)\n\n    num_tasks = dist_utils.get_world_size()\n    rank = dist_utils.get_rank()\n    step = sims_matrix.size(0) // num_tasks + 1\n    start = rank * step\n    end = min(sims_matrix.size(0), start + step)\n\n    for i, sims in enumerate(\n        metric_logger.log_every(sims_matrix[start:end], 50, header)\n    ):\n        topk_sim, topk_idx = sims.topk(k=k_test, dim=0)\n        image_inputs = vit_feats[start + i].repeat(k_test, 1, 1).to(model.device)\n        score = model.compute_itm(\n            image_inputs=image_inputs,\n            text_ids=text_ids[topk_idx],\n            text_atts=text_atts[topk_idx],\n        ).float()\n        score_matrix_i2t[start + i, topk_idx] = score + topk_sim\n\n    sims_matrix = sims_matrix.t()\n    score_matrix_t2i = torch.full(\n        (len(texts), len(data_loader.dataset.image)), -100.0\n    ).to(model.device)\n\n    step = sims_matrix.size(0) // num_tasks + 1\n    start = rank * step\n    end = min(sims_matrix.size(0), start + step)\n\n    for i, sims in enumerate(\n        metric_logger.log_every(sims_matrix[start:end], 50, header)\n    ):\n        topk_sim, topk_idx = sims.topk(k=k_test, dim=0)\n        image_inputs = vit_feats[topk_idx.cpu()].to(model.device)\n        score = model.compute_itm(\n            image_inputs=image_inputs,\n            text_ids=text_ids[start + i].repeat(k_test, 1),\n            text_atts=text_atts[start + i].repeat(k_test, 1),\n        ).float()\n        score_matrix_t2i[start + i, topk_idx] = score + topk_sim\n\n    if dist_utils.is_dist_avail_and_initialized():\n        dist.barrier()\n        torch.distributed.all_reduce(\n            score_matrix_i2t, op=torch.distributed.ReduceOp.SUM\n        )\n        torch.distributed.all_reduce(\n            score_matrix_t2i, op=torch.distributed.ReduceOp.SUM\n        )\n\n    total_time = time.time() - start_time\n    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n    logging.info(\"Evaluation time {}\".format(total_time_str))\n\n    return score_matrix_i2t.cpu().numpy(), score_matrix_t2i.cpu().numpy()\n"}
{"type": "source_file", "path": "src/caption/lavis/__init__.py", "content": ""}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/blip2_t5_instruct.py", "content": "\"\"\"\n Copyright (c) 2023, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\nimport logging\nimport string\nimport random\nimport copy\n\nimport torch\nimport torch.nn as nn\nfrom torch.cuda.amp import autocast as autocast\nfrom transformers import T5TokenizerFast\n\n# from src.caption.lavis.common.registry import registry\nfrom src.caption.lavis.models.blip2_models.blip2 import Blip2Base, disabled_train\nfrom src.caption.lavis.models.blip2_models.modeling_t5 import (\n    T5Config,\n    T5ForConditionalGeneration,\n)\nfrom transformers.modeling_outputs import BaseModelOutput\n\n\n# @registry.register_model(\"blip2_t5_instruct\")\nclass Blip2T5Instruct(Blip2Base):\n    \"\"\"\n    BLIP2 T5 model.\n    Supported model types:\n        - flant5xl\n        - flant5xxl\n    Usage:\n        >>> from lavis.models import load_model\n        >>> model = load_model(\"blip2_t5_instruct\", \"flant5xl\")\n    \"\"\"\n\n    PRETRAINED_MODEL_CONFIG_DICT = {\n        \"flant5xl\": \"src/caption/lavis/configs/blip2_instruct_flant5xl.yaml\",\n        \"flant5xxl\": \"src/caption/lavis/configs/blip2_instruct_flant5xxl.yaml\",\n    }\n\n    def __init__(\n        self,\n        vit_model=\"eva_clip_g\",\n        img_size=224,\n        drop_path_rate=0,\n        use_grad_checkpoint=False,\n        vit_precision=\"fp16\",\n        freeze_vit=True,\n        num_query_token=32,\n        t5_model=\"google/flan-t5-xl\",\n        prompt=\"\",\n        max_txt_len=128,\n        max_output_txt_len=256,\n        apply_lemmatizer=False,\n        num_few_shot_examples=0,\n        few_shot_prob=0,\n        qformer_text_input=True,\n    ):\n        \"\"\"\n        apply_lemmatizer: when set to True, postprocess predict_answers() result with lemmas.\n        \"\"\"\n        super().__init__()\n\n        self.tokenizer = self.init_tokenizer(truncation_side=\"left\")\n\n        self.visual_encoder, self.ln_vision = self.init_vision_encoder(\n            vit_model, img_size, drop_path_rate, use_grad_checkpoint, vit_precision\n        )\n        if freeze_vit:\n            for name, param in self.visual_encoder.named_parameters():\n                param.requires_grad = False\n            self.visual_encoder = self.visual_encoder.eval()\n            self.visual_encoder.train = disabled_train\n            logging.info(\"freeze vision encoder\")\n\n        self.Qformer, self.query_tokens = self.init_Qformer(\n            num_query_token, self.visual_encoder.num_features\n        )\n\n        if not qformer_text_input:\n            self.Qformer.bert.embeddings.word_embeddings = None\n            self.Qformer.bert.embeddings.position_embeddings = None\n            for layer in self.Qformer.bert.encoder.layer:\n                layer.output = None\n                layer.intermediate = None\n        else:\n            self.Qformer.resize_token_embeddings(len(self.tokenizer))\n        self.Qformer.cls = None\n\n        self.t5_tokenizer = T5TokenizerFast.from_pretrained(\n            t5_model, truncation_side=\"left\"\n        )\n        self.t5_output_tokenizer = T5TokenizerFast.from_pretrained(\n            t5_model, truncation_side=\"right\"\n        )\n\n        t5_config = T5Config.from_pretrained(t5_model)\n        t5_config.dense_act_fn = \"gelu\"\n        self.t5_model = T5ForConditionalGeneration.from_pretrained(\n            t5_model, config=t5_config\n        )\n\n        # Changed parameters & autocasts in file from torch.bfloat16, since GPUs did not support it.\n        for name, param in self.t5_model.named_parameters():\n            param.requires_grad = False\n            param.data = param.data.float()\n\n        self.t5_proj = nn.Linear(\n            self.Qformer.config.hidden_size, self.t5_model.config.hidden_size\n        )\n\n        self.max_txt_len = max_txt_len\n        self.max_output_txt_len = max_output_txt_len\n        self.prompt = prompt\n\n        self._apply_lemmatizer = apply_lemmatizer\n        self._lemmatizer = None\n\n        self.num_few_shot_examples = num_few_shot_examples\n        self.few_shot_prob = few_shot_prob\n\n        self.qformer_text_input = qformer_text_input\n\n    def forward(self, samples):\n        # print('-----------------')\n        # print(samples[\"text_input\"])\n        # print(samples[\"text_output\"])\n        # print('-----------------')\n\n        image = samples[\"image\"]\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n            image.device\n        )\n\n        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n        if self.qformer_text_input:\n            text_Qformer = self.tokenizer(\n                samples[\"text_input\"],\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n            query_output = self.Qformer.bert(\n                text_Qformer.input_ids,\n                attention_mask=Qformer_atts,\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        else:\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n\n        inputs_t5 = self.t5_proj(\n            query_output.last_hidden_state[:, : query_tokens.size(1), :]\n        )\n        atts_t5 = torch.ones(inputs_t5.size()[:-1], dtype=torch.long).to(image.device)\n\n        fs_embeds, fs_atts = None, None\n        if self.few_shot_prob > 0 and \"few_shot_samples\" in samples.keys():\n            fs_embeds, fs_atts = self.prepare_few_shot_embeds(\n                samples[\"few_shot_samples\"]\n            )\n\n        with self.maybe_autocast(dtype=torch.float):\n            input_tokens = self.t5_tokenizer(\n                samples[\"text_input\"],\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            output_tokens = self.t5_output_tokenizer(\n                samples[\"text_output\"],\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_output_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n\n            encoder_atts = torch.cat([atts_t5, input_tokens.attention_mask], dim=1)\n\n            targets = output_tokens.input_ids.masked_fill(\n                output_tokens.input_ids == self.t5_tokenizer.pad_token_id, -100\n            )\n\n            inputs_embeds = self.t5_model.encoder.embed_tokens(input_tokens.input_ids)\n            inputs_embeds = torch.cat([inputs_t5, inputs_embeds], dim=1)\n\n            if fs_embeds is not None:\n                inputs_embeds = torch.cat([fs_embeds, inputs_embeds], dim=1)\n                encoder_atts = torch.cat([fs_atts, encoder_atts], dim=1)\n\n            outputs = self.t5_model(\n                inputs_embeds=inputs_embeds,\n                attention_mask=encoder_atts,\n                decoder_attention_mask=output_tokens.attention_mask,\n                return_dict=True,\n                labels=targets,\n            )\n            loss = outputs.loss\n\n            return {\"loss\": loss}\n\n    def prepare_few_shot_embeds(self, samples):\n        this_n_fs = random.choices(\n            list(range(self.num_few_shot_examples + 1)),\n            weights=[1 - self.few_shot_prob]\n            + [self.few_shot_prob / self.num_few_shot_examples]\n            * self.num_few_shot_examples,\n        )[0]\n\n        if this_n_fs == 0:\n            return None, None\n\n        images = []\n        text_input = []\n        for sample in samples:\n            for n in range(this_n_fs):\n                images.append(sample[\"image\"][n])\n                text_input.append(sample[\"text_input\"][n])\n        images = torch.stack(images, dim=0)\n\n        image = images\n\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n            image.device\n        )\n\n        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n        if self.qformer_text_input:\n            text_Qformer = self.tokenizer(\n                text_input,\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n            query_output = self.Qformer.bert(\n                text_Qformer.input_ids,\n                attention_mask=Qformer_atts,\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        else:\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n\n        inputs_t5 = self.t5_proj(\n            query_output.last_hidden_state[:, : query_tokens.size(1), :]\n        )\n        atts_t5 = torch.ones(inputs_t5.size()[:-1], dtype=torch.long).to(image.device)\n\n        with self.maybe_autocast(dtype=torch.float):\n            input_tokens = self.t5_tokenizer(\n                text_input,\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n\n            encoder_atts = torch.cat([atts_t5, input_tokens.attention_mask], dim=1)\n\n            inputs_embeds = self.t5_model.encoder.embed_tokens(input_tokens.input_ids)\n            inputs_embeds = torch.cat([inputs_t5, inputs_embeds], dim=1)\n\n        if this_n_fs > 1:\n            encoder_atts = encoder_atts.reshape(\n                encoder_atts.size(0) // this_n_fs, encoder_atts.size(1) * this_n_fs\n            )\n            inputs_embeds = inputs_embeds.reshape(\n                inputs_embeds.size(0) // this_n_fs,\n                inputs_embeds.size(1) * this_n_fs,\n                inputs_embeds.size(2),\n            )\n\n        return inputs_embeds, encoder_atts\n\n    def _compute_vision_outputs(self, image, input_ids, Qformer_atts, query_tokens):\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n            image.device\n        )\n\n        if self.qformer_text_input:\n            query_output = self.Qformer.bert(\n                input_ids,\n                attention_mask=Qformer_atts,\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        else:\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n\n        inputs_t5 = self.t5_proj(\n            query_output.last_hidden_state[:, : query_tokens.size(1), :]\n        )\n        atts_t5 = torch.ones(inputs_t5.size()[:-1], dtype=torch.long).to(image.device)\n        return inputs_t5, atts_t5\n\n    @torch.no_grad()\n    def generate(\n        self,\n        samples,\n        use_nucleus_sampling=False,\n        num_beams=5,\n        max_length=256,\n        min_length=1,\n        top_p=0.9,\n        repetition_penalty=1.5,\n        length_penalty=1.0,\n        num_captions=1,\n        temperature=1,\n        return_dict=False,\n        prefix_allowed_tokens_fn=None,\n        pure_llm=False,\n        return_embeds=False,\n        inputs_vision=None,\n        atts_vision=None,\n    ):\n        if \"prompt\" in samples.keys():\n            prompt = samples[\"prompt\"]\n        else:\n            prompt = self.prompt\n\n        image = samples[\"image\"]\n\n        bs = image.size(0)\n\n        if isinstance(prompt, str):\n            prompt = [prompt] * bs\n        else:\n            assert (\n                len(prompt) == bs\n            ), \"The number of prompts must be equal to the batch size.\"\n\n        # For TextCaps\n        if \"ocr_tokens\" in samples.keys() and \"{}\" in prompt[0]:\n            prompt = [\n                p.format(\", \".join(samples[\"ocr_tokens\"][i][:30]))\n                for i, p in enumerate(prompt)\n            ]\n\n        query_tokens = self.query_tokens.expand(bs, -1, -1)\n        if self.qformer_text_input:\n            # remove ocr tokens in q_former (for eval textvqa)\n            # qformer_prompt = prompt\n            # qformer_prompt = ['Question: ' + qp.split(' Question: ')[1] for qp in qformer_prompt]\n\n            text_Qformer = self.tokenizer(\n                prompt,\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n        if inputs_vision is None and atts_vision is None:\n            inputs_t5, atts_t5 = self._compute_vision_outputs(\n                image, text_Qformer.input_ids, Qformer_atts, query_tokens\n            )\n        else:\n            inputs_t5, atts_t5 = inputs_vision, atts_vision\n\n        input_tokens = self.t5_tokenizer(\n            prompt, padding=\"longest\", return_tensors=\"pt\"\n        ).to(image.device)\n\n        encoder_atts = torch.cat([atts_t5, input_tokens.attention_mask], dim=1)\n\n        with self.maybe_autocast(dtype=torch.float):\n            inputs_embeds = self.t5_model.encoder.embed_tokens(input_tokens.input_ids)\n            inputs_embeds = torch.cat([inputs_t5, inputs_embeds], dim=1)\n\n            outputs = self.t5_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=encoder_atts,\n                do_sample=use_nucleus_sampling,\n                top_p=top_p,\n                temperature=temperature,\n                num_beams=num_beams,\n                max_new_tokens=max_length,\n                min_length=min_length,\n                repetition_penalty=repetition_penalty,\n                length_penalty=length_penalty,\n                num_return_sequences=num_captions,\n                return_dict_in_generate=return_dict,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                output_scores=True,\n                output_attentions=True,\n            )\n            tokens = outputs[0] if return_dict else outputs\n\n            output_text = self.t5_tokenizer.batch_decode(\n                tokens, skip_special_tokens=True\n            )\n\n        if return_embeds:\n            return output_text, inputs_embeds, inputs_t5, atts_t5, outputs\n        if return_dict:\n            return output_text, outputs\n\n        return output_text\n\n    def predict_answers(\n        self,\n        samples,\n        num_beams=5,\n        inference_method=\"generate\",\n        max_len=10,\n        min_len=1,\n        num_ans_candidates=128,\n        answer_list=None,\n        prompt=\"\",\n        length_penalty=-1,\n        **kwargs,\n    ):\n        if isinstance(samples[\"text_input\"], str):\n            samples[\"text_input\"] = [samples[\"text_input\"]]\n\n        if prompt:\n            if prompt.count(\"{}\") == 2:\n                if \"ocr_tokens\" in samples:\n                    text_input = [\n                        prompt.format(\n                            \", \".join(samples[\"ocr_tokens\"][i][:30]),\n                            samples[\"text_input\"][i],\n                        )\n                        for i in range(len(samples[\"text_input\"]))\n                    ]\n                elif \"choices\" in samples:\n                    text_input = []\n                    for i in range(len(samples[\"text_input\"])):\n                        this_choices = [\n                            f\"({string.ascii_lowercase[j]}) {ch}\"\n                            for j, ch in enumerate(samples[\"choices\"][i])\n                        ]\n                        this_choices = \" \".join(this_choices)\n                        text_input.append(\n                            prompt.format(samples[\"text_input\"][i], this_choices)\n                        )\n            else:\n                text_input = [\n                    prompt.format(question) for question in samples[\"text_input\"]\n                ]\n        else:\n            text_input = samples[\"text_input\"]\n\n        samples[\"prompt\"] = text_input\n\n        output_text = self.generate(\n            samples,\n            num_beams=num_beams,\n            max_length=max_len,\n            min_length=min_len,\n            length_penalty=length_penalty,\n        )\n\n        if self._apply_lemmatizer or (\n            \"apply_lemmatizer\" in samples.keys() and samples[\"apply_lemmatizer\"]\n        ):\n            output_text = self._lemmatize(output_text)\n\n        return output_text\n\n    def predict_class(\n        self,\n        samples,\n        candidates,\n        n_segments=1,\n    ):\n        # If candidates is a list of lists, each sample has its candidates, then we need to iterate one by one\n        if type(candidates[0]) == list:\n            results = []\n\n            for i in range(samples[\"image\"].size(0)):\n                this_sample = {\n                    \"image\": samples[\"image\"][i].unsqueeze(0),\n                    \"prompt\": samples[\"prompt\"],\n                }\n\n                if \"text_input\" in samples.keys():\n                    this_sample[\"text_input\"] = [samples[\"text_input\"][i]]\n\n                if \"context\" in samples.keys():\n                    this_sample[\"context\"] = [samples[\"context\"][i]]\n\n                if \"history\" in samples.keys():\n                    this_sample[\"history\"] = [samples[\"history\"][i]]\n\n                if \"caption\" in samples.keys():\n                    this_sample[\"caption\"] = [samples[\"caption\"][i]]\n\n                this_result = self._predict_class(\n                    this_sample, candidates[i], n_segments\n                )\n                results.append(this_result)\n\n            try:\n                results = torch.cat(results, dim=0)\n            except:\n                results = [res.tolist()[0] for res in results]\n\n            return results\n\n        return self._predict_class(samples, candidates, n_segments)\n\n    def _predict_class(\n        self,\n        samples,\n        candidates,\n        n_segments=1,\n    ):\n        \"\"\"\n        Args:\n            samples (dict): A dictionary containing the following keys:\n                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n                - prompt: the instruction\n            candidates:\n                (list): A list of candidate class names;\n            n_segments:\n                (int): Split the candidates into n_segments and predict one by one. This is useful when the number of candidates is too large.\n        Returns:\n            output_class: predicted class index\n        \"\"\"\n\n        image = samples[\"image\"]\n        prompt = samples[\"prompt\"]\n\n        bs = image.size(0)\n\n        if isinstance(prompt, str):\n            prompt = [prompt] * bs\n        else:\n            assert (\n                len(prompt) == bs\n            ), \"The number of prompts must be equal to the batch size.\"\n\n        if \"text_input\" in samples.keys():\n            if type(samples[\"text_input\"][0]) == list:\n                prompt = [\n                    prompt[i].format(*samples[\"text_input\"][i])\n                    for i in range(len(prompt))\n                ]\n            else:\n                prompt = [\n                    prompt[i].format(samples[\"text_input\"][i])\n                    for i in range(len(prompt))\n                ]\n\n        # scienceqa\n        if \"context\" in samples.keys() and samples[\"context\"] != \"\":\n            prompt = [\n                f'context: {samples[\"context\"][i]}. {prompt[i]}'\n                for i in range(len(prompt))\n            ]\n\n        # visual dialog\n        if \"history\" in samples.keys() and samples[\"history\"][0] != \"\":\n            prompt = [\n                f'dialog history: {samples[\"history\"][i]}\\n{prompt[i]}'\n                for i in range(len(prompt))\n            ]\n\n        if \"caption\" in samples.keys() and samples[\"caption\"][0] != \"\":\n            prompt = [\n                f'This image has the caption \"{samples[\"caption\"][i]}\". {prompt[i]}'\n                for i in range(len(prompt))\n            ]\n\n        query_tokens = self.query_tokens.expand(bs, -1, -1)\n        if self.qformer_text_input:\n            text_Qformer = self.tokenizer(\n                prompt,\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n        if image.dim() == 5:\n            inputs_t5, atts_t5 = [], []\n            for j in range(image.size(2)):\n                this_frame = image[:, :, j, :, :]\n                with self.maybe_autocast():\n                    frame_embeds = self.ln_vision(self.visual_encoder(this_frame))\n                    frame_atts = torch.ones(\n                        frame_embeds.size()[:-1], dtype=torch.long\n                    ).to(image.device)\n\n                if self.qformer_text_input:\n                    frame_query_output = self.Qformer.bert(\n                        text_Qformer.input_ids,\n                        attention_mask=Qformer_atts,\n                        query_embeds=query_tokens,\n                        encoder_hidden_states=frame_embeds,\n                        encoder_attention_mask=frame_atts,\n                        return_dict=True,\n                    )\n                else:\n                    frame_query_output = self.Qformer.bert(\n                        query_embeds=query_tokens,\n                        encoder_hidden_states=frame_embeds,\n                        encoder_attention_mask=frame_atts,\n                        return_dict=True,\n                    )\n\n                frame_inputs_t5 = self.t5_proj(\n                    frame_query_output.last_hidden_state[:, : query_tokens.size(1), :]\n                )\n                frame_atts_t5 = torch.ones(\n                    frame_inputs_t5.size()[:-1], dtype=torch.long\n                ).to(image.device)\n                inputs_t5.append(frame_inputs_t5)\n                atts_t5.append(frame_atts_t5)\n            inputs_t5 = torch.cat(inputs_t5, dim=1)\n            atts_t5 = torch.cat(atts_t5, dim=1)\n        else:\n            with self.maybe_autocast():\n                image_embeds = self.ln_vision(self.visual_encoder(image))\n            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n            if self.qformer_text_input:\n                query_output = self.Qformer.bert(\n                    text_Qformer.input_ids,\n                    attention_mask=Qformer_atts,\n                    query_embeds=query_tokens,\n                    encoder_hidden_states=image_embeds,\n                    encoder_attention_mask=image_atts,\n                    return_dict=True,\n                )\n            else:\n                query_output = self.Qformer.bert(\n                    query_embeds=query_tokens,\n                    encoder_hidden_states=image_embeds,\n                    encoder_attention_mask=image_atts,\n                    return_dict=True,\n                )\n\n            inputs_t5 = self.t5_proj(\n                query_output.last_hidden_state[:, : query_tokens.size(1), :]\n            )\n            atts_t5 = torch.ones(inputs_t5.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n        input_tokens = self.t5_tokenizer(\n            prompt, padding=\"longest\", return_tensors=\"pt\"\n        ).to(image.device)\n        output_tokens = self.t5_tokenizer(\n            candidates, padding=\"longest\", return_tensors=\"pt\"\n        ).to(image.device)\n\n        encoder_atts = torch.cat([atts_t5, input_tokens.attention_mask], dim=1)\n\n        n_cands = len(candidates)\n\n        with self.maybe_autocast(dtype=torch.float):\n            inputs_embeds = self.t5_model.encoder.embed_tokens(input_tokens.input_ids)\n            inputs_embeds = torch.cat([inputs_t5, inputs_embeds], dim=1)\n\n            encoder_outputs = self.t5_model.encoder(\n                inputs_embeds=inputs_embeds,\n                attention_mask=encoder_atts,\n            )\n\n            all_losses = []\n            for n in range(n_segments):\n                seg_len = n_cands // n_segments\n                if n == (n_segments - 1):\n                    seg_len = n_cands - seg_len * (n_segments - 1)\n\n                # this_encoder_outputs = copy.deepcopy(encoder_outputs)\n                this_encoder_outputs = BaseModelOutput(\n                    last_hidden_state=encoder_outputs[0].clone(),\n                )\n\n                this_encoder_outputs[\"last_hidden_state\"] = this_encoder_outputs[\n                    0\n                ].repeat_interleave(seg_len, dim=0)\n                this_encoder_atts = encoder_atts.repeat_interleave(seg_len, dim=0)\n\n                start_i = n * (n_cands // n_segments)\n                end_i = start_i + seg_len\n                this_output_tokens_ids = output_tokens.input_ids[start_i:end_i].repeat(\n                    bs, 1\n                )\n                this_output_tokens_atts = output_tokens.attention_mask[\n                    start_i:end_i\n                ].repeat(bs, 1)\n\n                this_targets = this_output_tokens_ids.masked_fill(\n                    this_output_tokens_ids == self.t5_tokenizer.pad_token_id, -100\n                )\n\n                outputs = self.t5_model(\n                    encoder_outputs=this_encoder_outputs,\n                    attention_mask=this_encoder_atts,\n                    decoder_attention_mask=this_output_tokens_atts,\n                    return_dict=True,\n                    labels=this_targets,\n                    reduction=\"none\",\n                )\n                loss = outputs.loss\n\n                loss = loss.reshape(bs, seg_len)\n                # output_class_ranks = torch.argsort(loss, dim=-1)\n                all_losses.append(loss)\n\n            all_losses = torch.cat(all_losses, dim=-1)\n            output_class_ranks = torch.argsort(all_losses, dim=-1)\n\n            # encoder_outputs['last_hidden_state'] = encoder_outputs[0].repeat_interleave(n_cands, dim=0)\n            # encoder_atts = encoder_atts.repeat_interleave(n_cands, dim=0)\n            # output_tokens.input_ids = output_tokens.input_ids.repeat(bs, 1)\n            # output_tokens.attention_mask = output_tokens.attention_mask.repeat(bs, 1)\n\n            # # compute the LM loss for each candidate (sum logprob across all tokens) and select the highest\n            # targets = output_tokens.input_ids.masked_fill(output_tokens.input_ids == self.t5_tokenizer.pad_token_id, -100)\n\n            # outputs = self.t5_model(\n            #     encoder_outputs=encoder_outputs,\n            #     attention_mask=encoder_atts,\n            #     decoder_attention_mask=output_tokens.attention_mask,\n            #     return_dict=True,\n            #     labels=targets,\n            #     reduction=\"none\",\n            # )\n            # loss = outputs.loss\n\n            # loss = loss.reshape(bs, n_cands)\n            # output_class_ranks = torch.argsort(loss, dim=-1) # (bs, num_candidates)\n\n        return output_class_ranks\n\n    def _lemmatize(self, answers):\n        def apply(answer):\n            doc = self.lemmatizer(answer)\n\n            words = []\n            for token in doc:\n                if token.pos_ in [\"NOUN\", \"VERB\"]:\n                    words.append(token.lemma_)\n                else:\n                    words.append(token.text)\n            answer = \" \".join(words)\n\n            return answer\n\n        return [apply(answer) for answer in answers]\n\n    @property\n    def lemmatizer(self):\n        if self._lemmatizer is None:\n            try:\n                import spacy\n\n                self._lemmatizer = spacy.load(\"en_core_web_sm\")\n            except ImportError:\n                logging.error(\n                    \"\"\"\n                    Please install spacy and en_core_web_sm model to apply lemmatization.\n                    python -m spacy download en_core_web_sm\n                    OR\n                    import spacy.cli\n                    spacy.cli.download(\"en_core_web_sm\")\n                    \"\"\"\n                )\n                exit(1)\n\n        return self._lemmatizer\n\n    @classmethod\n    def from_config(cls, cfg):\n        vit_model = cfg.get(\"vit_model\", \"eva_clip_g\")\n        img_size = cfg.get(\"image_size\")\n        num_query_token = cfg.get(\"num_query_token\")\n        t5_model = cfg.get(\"t5_model\")\n\n        drop_path_rate = cfg.get(\"drop_path_rate\", 0)\n        use_grad_checkpoint = cfg.get(\"use_grad_checkpoint\", False)\n        vit_precision = cfg.get(\"vit_precision\", \"fp16\")\n        freeze_vit = cfg.get(\"freeze_vit\", True)\n\n        prompt = cfg.get(\"prompt\", \"\")\n        max_txt_len = cfg.get(\"max_txt_len\", 128)\n        max_output_txt_len = cfg.get(\"max_output_txt_len\", 256)\n\n        apply_lemmatizer = cfg.get(\"apply_lemmatizer\", False)\n\n        num_few_shot_examples = cfg.get(\"num_few_shot_examples\", 0)\n        few_shot_prob = cfg.get(\"few_shot_prob\", 0.0)\n\n        qformer_text_input = cfg.get(\"qformer_text_input\", True)\n\n        model = cls(\n            vit_model=vit_model,\n            img_size=img_size,\n            drop_path_rate=drop_path_rate,\n            use_grad_checkpoint=use_grad_checkpoint,\n            vit_precision=vit_precision,\n            freeze_vit=freeze_vit,\n            num_query_token=num_query_token,\n            t5_model=t5_model,\n            prompt=prompt,\n            max_txt_len=max_txt_len,\n            max_output_txt_len=max_output_txt_len,\n            apply_lemmatizer=apply_lemmatizer,\n            num_few_shot_examples=num_few_shot_examples,\n            few_shot_prob=few_shot_prob,\n            qformer_text_input=qformer_text_input,\n        )\n\n        # if qformer_text_input:\n        #     # Hard-coded to load from BLIP-2 stage-1 pre-trained model (not ideal)\n        #     model.load_from_pretrained(\n        #         url_or_filename=\"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth\"\n        #     )\n\n        model.load_checkpoint_from_config(cfg)\n\n        return model\n"}
{"type": "source_file", "path": "src/caption/lavis/models/__init__.py", "content": "import logging\nimport torch\nfrom omegaconf import OmegaConf\n\nfrom src.caption.lavis.models.blip2_models.blip2_vicuna_instruct import (\n    Blip2VicunaInstruct,\n)\nfrom src.caption.lavis.models.blip2_models.blip2_t5_instruct import Blip2T5Instruct\nfrom src.caption.lavis.models.blip2_models.blip2_opt import Blip2OPT\n\nfrom src.caption.lavis.processors.base_processor import BaseProcessor\nfrom src.caption.lavis.processors.blip_processors import (\n    Blip2ImageTrainProcessor,\n    BlipImageTrainProcessor,\n    BlipImageEvalProcessor,\n    BlipCaptionProcessor,\n)\n\nLOCAL_MODEL_REGISTRY = {\n    \"blip2_opt\": Blip2OPT,\n    \"blip2_vicuna_instruct\": Blip2VicunaInstruct,\n    \"blip2_t5_instruct\": Blip2T5Instruct,\n}\n\nLOCAL_PROCESSOR_REGISTRY = {\n    \"blip2_image_train\": Blip2ImageTrainProcessor,\n    \"blip_image_train\": BlipImageTrainProcessor,\n    \"blip_image_eval\": BlipImageEvalProcessor,\n    \"blip_caption\": BlipCaptionProcessor,\n}\n\n\ndef load_preprocess(config):\n    \"\"\"\n    Load preprocessor configs and construct preprocessors.\n\n    If no preprocessor is specified, return BaseProcessor, which does not do any preprocessing.\n\n    Args:\n        config (dict): preprocessor configs.\n\n    Returns:\n        vis_processors (dict): preprocessors for visual inputs.\n        txt_processors (dict): preprocessors for text inputs.\n\n        Key is \"train\" or \"eval\" for processors used in training and evaluation respectively.\n    \"\"\"\n\n    def _build_proc_from_cfg(cfg):\n        if cfg.name not in LOCAL_PROCESSOR_REGISTRY:\n            raise KeyError(\"Unknown processor:\", cfg.name)\n        return (\n            # registry.get_processor_class(cfg.name).from_config(cfg)\n            LOCAL_PROCESSOR_REGISTRY[cfg.name].from_config(cfg)\n            if cfg is not None\n            else BaseProcessor()\n        )\n\n    vis_processors = dict()\n    txt_processors = dict()\n\n    vis_proc_cfg = config.get(\"vis_processor\")\n    txt_proc_cfg = config.get(\"text_processor\")\n\n    if vis_proc_cfg is not None:\n        vis_train_cfg = vis_proc_cfg.get(\"train\")\n        vis_eval_cfg = vis_proc_cfg.get(\"eval\")\n    else:\n        vis_train_cfg = None\n        vis_eval_cfg = None\n\n    vis_processors[\"train\"] = _build_proc_from_cfg(vis_train_cfg)\n    vis_processors[\"eval\"] = _build_proc_from_cfg(vis_eval_cfg)\n\n    if txt_proc_cfg is not None:\n        txt_train_cfg = txt_proc_cfg.get(\"train\")\n        txt_eval_cfg = txt_proc_cfg.get(\"eval\")\n    else:\n        txt_train_cfg = None\n        txt_eval_cfg = None\n\n    txt_processors[\"train\"] = _build_proc_from_cfg(txt_train_cfg)\n    txt_processors[\"eval\"] = _build_proc_from_cfg(txt_eval_cfg)\n\n    return vis_processors, txt_processors\n\n\ndef load_model_and_preprocess(\n    name, model_type, is_eval=False, device=\"cpu\", vision_only=False\n):\n    \"\"\"\n    Load model and its related preprocessors.\n\n    List all available models and types in registry:\n    >>> from lavis.models import model_zoo\n    >>> print(model_zoo)\n\n    Args:\n        name (str): name of the model.\n        model_type (str): type of the model.\n        is_eval (bool): whether the model is in eval mode. Default: False.\n        device (str): device to use. Default: \"cpu\".\n\n    Returns:\n        model (torch.nn.Module): model.\n        vis_processors (dict): preprocessors for visual inputs.\n        txt_processors (dict): preprocessors for text inputs.\n    \"\"\"\n    if name not in LOCAL_MODEL_REGISTRY:\n        raise KeyError(\"Unknown model:\", name)\n\n    model_cls = LOCAL_MODEL_REGISTRY[name]\n\n    # load model\n    model = model_cls.from_pretrained(model_type=model_type, vision_only=vision_only)\n\n    if is_eval:\n        model.eval()\n\n    # load preprocess\n    cfg = OmegaConf.load(model_cls.default_config_path(model_type))\n    if cfg is not None:\n        preprocess_cfg = cfg.preprocess\n\n        vis_processors, txt_processors = load_preprocess(preprocess_cfg)\n    else:\n        vis_processors, txt_processors = None, None\n        logging.info(\n            f\"\"\"No default preprocess for model {name} ({model_type}).\n                This can happen if the model is not finetuned on downstream datasets,\n                or it is not intended for direct use without finetuning.\n            \"\"\"\n        )\n\n    if device == \"cpu\" or device == torch.device(\"cpu\"):\n        model = model.float()\n\n    return model.to(device), vis_processors, txt_processors\n"}
{"type": "source_file", "path": "src/caption/lavis/common/dist_utils.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport datetime\nimport functools\nimport os\n\nimport torch\nimport torch.distributed as dist\nimport timm.models.hub as timm_hub\n\n\ndef setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n\n    builtin_print = __builtin__.print\n\n    def print(*args, **kwargs):\n        force = kwargs.pop(\"force\", False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)\n\n    __builtin__.print = print\n\n\ndef is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\n\n\ndef get_world_size():\n    if not is_dist_avail_and_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not is_dist_avail_and_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef init_distributed_mode(args):\n    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n        args.rank = int(os.environ[\"RANK\"])\n        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n        args.gpu = int(os.environ[\"LOCAL_RANK\"])\n    elif \"SLURM_PROCID\" in os.environ:\n        args.rank = int(os.environ[\"SLURM_PROCID\"])\n        args.gpu = args.rank % torch.cuda.device_count()\n    else:\n        print(\"Not using distributed mode\")\n        args.distributed = False\n        return\n\n    args.distributed = True\n\n    torch.cuda.set_device(args.gpu)\n    args.dist_backend = \"nccl\"\n    print(\n        \"| distributed init (rank {}, world {}): {}\".format(\n            args.rank, args.world_size, args.dist_url\n        ),\n        flush=True,\n    )\n    torch.distributed.init_process_group(\n        backend=args.dist_backend,\n        init_method=args.dist_url,\n        world_size=args.world_size,\n        rank=args.rank,\n        timeout=datetime.timedelta(\n            days=365\n        ),  # allow auto-downloading and de-compressing\n    )\n    torch.distributed.barrier()\n    setup_for_distributed(args.rank == 0)\n\n\ndef get_dist_info():\n    if torch.__version__ < \"1.0\":\n        initialized = dist._initialized\n    else:\n        initialized = dist.is_initialized()\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:  # non-distributed training\n        rank = 0\n        world_size = 1\n    return rank, world_size\n\n\ndef main_process(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        rank, _ = get_dist_info()\n        if rank == 0:\n            return func(*args, **kwargs)\n\n    return wrapper\n\n\ndef download_cached_file(url, check_hash=True, progress=False):\n    \"\"\"\n    Download a file from a URL and cache it locally. If the file already exists, it is not downloaded again.\n    If distributed, only the main process downloads the file, and the other processes wait for the file to be downloaded.\n    \"\"\"\n\n    def get_cached_file_path():\n        # a hack to sync the file path across processes\n        parts = torch.hub.urlparse(url)\n        filename = os.path.basename(parts.path)\n        cached_file = os.path.join(timm_hub.get_cache_dir(), filename)\n\n        return cached_file\n\n    if is_main_process():\n        timm_hub.download_cached_file(url, check_hash, progress)\n\n    if is_dist_avail_and_initialized():\n        dist.barrier()\n\n    return get_cached_file_path()\n"}
{"type": "source_file", "path": "src/caption/lavis/models/clip_vit.py", "content": "from collections import OrderedDict\nfrom itertools import repeat\nimport collections.abc\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper\n\nfrom src.caption.lavis.models.eva_vit import convert_weights_to_fp16\nfrom src.caption.lavis.common.dist_utils import download_cached_file\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1):\n        super().__init__()\n\n        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1\n        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu1 = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu2 = nn.ReLU(inplace=True)\n\n        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        self.downsample = None\n        self.stride = stride\n\n        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1\n            self.downsample = nn.Sequential(OrderedDict([\n                (\"-1\", nn.AvgPool2d(stride)),\n                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n            ]))\n\n    def forward(self, x: torch.Tensor):\n        identity = x\n\n        out = self.relu1(self.bn1(self.conv1(x)))\n        out = self.relu2(self.bn2(self.conv2(out)))\n        out = self.avgpool(out)\n        out = self.bn3(self.conv3(out))\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu3(out)\n        return out\n\n\nclass AttentionPool2d(nn.Module):\n    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):\n        super().__init__()\n        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n        self.num_heads = num_heads\n\n    def forward(self, x):\n        x = x.reshape(x.shape[0], x.shape[1], x.shape[2] * x.shape[3]).permute(2, 0, 1)  # NCHW -> (HW)NC\n        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC\n        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC\n        x, _ = F.multi_head_attention_forward(\n            query=x, key=x, value=x,\n            embed_dim_to_check=x.shape[-1],\n            num_heads=self.num_heads,\n            q_proj_weight=self.q_proj.weight,\n            k_proj_weight=self.k_proj.weight,\n            v_proj_weight=self.v_proj.weight,\n            in_proj_weight=None,\n            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),\n            bias_k=None,\n            bias_v=None,\n            add_zero_attn=False,\n            dropout_p=0,\n            out_proj_weight=self.c_proj.weight,\n            out_proj_bias=self.c_proj.bias,\n            use_separate_proj_weight=True,\n            training=self.training,\n            need_weights=False\n        )\n\n        return x[0]\n    \n\nclass LayerNorm(nn.LayerNorm):\n    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n\n    def forward(self, x: torch.Tensor):\n        orig_type = x.dtype\n        ret = super().forward(x.type(torch.float32))\n        return ret.type(orig_type)\n\n\nclass QuickGELU(nn.Module):\n    def forward(self, x: torch.Tensor):\n        return x * torch.sigmoid(1.702 * x)\n\n\nclass ResidualAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None, use_grad_checkpointing=False):\n        super().__init__()\n\n        self.attn = nn.MultiheadAttention(d_model, n_head)\n        self.ln_1 = LayerNorm(d_model)\n        self.mlp = nn.Sequential(OrderedDict([\n            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n            (\"gelu\", QuickGELU()),\n            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n        ]))\n        self.ln_2 = LayerNorm(d_model)\n        self.attn_mask = attn_mask\n\n        if use_grad_checkpointing:\n            self.attn = checkpoint_wrapper(self.attn)\n            self.mlp = checkpoint_wrapper(self.mlp)\n            \n    def attention(self, x: torch.Tensor):\n        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n\n    def forward(self, x: torch.Tensor):\n        x = x + self.attention(self.ln_1(x))\n        x = x + self.mlp(self.ln_2(x))\n        return x\n\n\nclass Transformer(nn.Module):\n    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None, use_grad_checkpointing=False):\n        super().__init__()\n        self.width = width\n        self.layers = layers\n        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask, use_grad_checkpointing and i>12) for i in range(layers)])\n\n    def forward(self, x: torch.Tensor):\n        return self.resblocks(x)\n\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, use_grad_checkpointing: bool):\n        super().__init__()\n        self.input_resolution = input_resolution\n        self.num_features = width\n        self.num_heads = heads\n        self.num_patches = (input_resolution // patch_size) ** 2\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n\n        scale = width ** -0.5\n        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n        self.positional_embedding = nn.Parameter(scale * torch.randn(self.num_patches + 1, width))\n        self.ln_pre = LayerNorm(width)\n        \n        self.transformer = Transformer(width, layers, heads, use_grad_checkpointing=use_grad_checkpointing)\n           \n#         self.ln_final = LayerNorm(width)\n\n    def forward(self, x: torch.Tensor):\n\n        x = self.conv1(x)  # shape = [*, width, grid, grid]\n        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n        x = x + self.positional_embedding.to(x.dtype)\n        x = self.ln_pre(x)\n\n        x = x.permute(1, 0, 2)  # NLD -> LND\n        x = self.transformer(x)\n        x = x.permute(1, 0, 2)  # LND -> NLD\n        \n#         x = self.ln_final(x)\n        return x\n    \n    def get_num_layer(self, var_name=\"\"):\n        if var_name in (\"class_embedding\", \"positional_embedding\", \"conv1\", \"ln_pre\"):\n            return 0\n        elif var_name.startswith(\"transformer.resblocks\"):\n            layer_id = int(var_name.split('.')[2])\n            return layer_id + 1\n        else:\n            return len(self.transformer.resblocks)    \n            \n            \n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\nto_2tuple = _ntuple(2)    \n    \ndef interpolate_pos_embed(model, state_dict, interpolation: str = 'bicubic', seq_dim=1):\n    # Rescale the grid of position embeddings when loading from state_dict\n    old_pos_embed = state_dict.get('positional_embedding', None)\n    \n    grid_size = round((model.positional_embedding.shape[0] - 1) ** 0.5)\n    if old_pos_embed is None:\n        return\n    grid_size = to_2tuple(grid_size)\n    extra_tokens = 1  # FIXME detect different token configs (ie no class token, or more)\n    new_seq_len = grid_size[0] * grid_size[1] + extra_tokens\n    if new_seq_len == old_pos_embed.shape[0]:\n        return\n\n    if extra_tokens:\n        pos_emb_tok, pos_emb_img = old_pos_embed[:extra_tokens], old_pos_embed[extra_tokens:]\n    else:\n        pos_emb_tok, pos_emb_img = None, old_pos_embed\n        \n    old_grid_size = to_2tuple(int(math.sqrt(len(pos_emb_img))))\n\n    print('Resizing position embedding grid-size from %s to %s', old_grid_size, grid_size)\n    pos_emb_img = pos_emb_img.reshape(1, old_grid_size[0], old_grid_size[1], -1).permute(0, 3, 1, 2)\n    pos_emb_img = F.interpolate(\n        pos_emb_img,\n        size=grid_size,\n        mode=interpolation,\n        align_corners=True,\n    )\n    pos_emb_img = pos_emb_img.permute(0, 2, 3, 1).reshape(1, grid_size[0] * grid_size[1], -1)[0]\n    if pos_emb_tok is not None:\n        new_pos_embed = torch.cat([pos_emb_tok, pos_emb_img], dim=0)\n    else:\n        new_pos_embed = pos_emb_img\n    state_dict['positional_embedding'] = new_pos_embed\n    \n    \ndef create_clip_vit_L(img_size=224,use_checkpoint=False,precision=\"fp16\"):\n    model = VisionTransformer(\n            input_resolution=img_size,\n            patch_size=14,\n            width=1024,\n            layers=23,\n            heads=16,\n            use_grad_checkpointing=use_checkpoint,\n        )         \n    url = \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/clip_vit_L.pth\"\n    cached_file = download_cached_file(\n        url, check_hash=False, progress=True\n    )\n    state_dict = torch.load(cached_file, map_location=\"cpu\")    \n    interpolate_pos_embed(model,state_dict)\n    \n    incompatible_keys = model.load_state_dict(state_dict, strict=False)\n    # print(incompatible_keys)\n    \n    if precision == \"fp16\":\n        convert_weights_to_fp16(model)\n    return model\n"}
{"type": "source_file", "path": "src/caption/lavis/processors/__init__.py", "content": ""}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/modeling_t5.py", "content": "# coding=utf-8\n# Copyright 2018 Mesh TensorFlow authors, T5 Authors and HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch T5 model.\"\"\"\n\n\nimport copy\nimport math\nimport os\nimport warnings\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.checkpoint import checkpoint\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.pytorch_utils import (\n    ALL_LAYERNORM_LAYERS,\n    find_pruneable_heads_and_indices,\n    prune_linear_layer,\n)\nfrom transformers.utils import (\n    DUMMY_INPUTS,\n    DUMMY_MASK,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    is_torch_fx_proxy,\n    logging,\n    replace_return_docstrings,\n)\nfrom transformers.utils.model_parallel_utils import assert_device_map, get_device_map\nfrom transformers.models.t5.configuration_t5 import T5Config\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"T5Config\"\n_TOKENIZER_FOR_DOC = \"T5Tokenizer\"\n_CHECKPOINT_FOR_DOC = \"t5-small\"\n\n####################################################\n# This dict contains ids and associated url\n# for the pretrained weights provided with the models\n####################################################\nT5_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"t5-small\",\n    \"t5-base\",\n    \"t5-large\",\n    \"t5-3b\",\n    \"t5-11b\",\n    # See all T5 models at https://huggingface.co/models?filter=t5\n]\n\n\n####################################################\n# This is a conversion method from TF 1.0 to PyTorch\n# More details: https://medium.com/huggingface/from-tensorflow-to-pytorch-265f40ef2a28\n####################################################\ndef load_tf_weights_in_t5(model, config, tf_checkpoint_path):\n    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n    try:\n        import re\n\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(\n            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n            \"https://www.tensorflow.org/install/ for installation instructions.\"\n        )\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    tf_weights = {}\n    for name, shape in init_vars:\n        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        tf_weights[name] = array\n\n    for txt_name in names:\n        name = txt_name.split(\"/\")\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(\n            n\n            in [\n                \"adam_v\",\n                \"adam_m\",\n                \"AdamWeightDecayOptimizer\",\n                \"AdamWeightDecayOptimizer_1\",\n                \"global_step\",\n            ]\n            for n in name\n        ):\n            logger.info(f\"Skipping {'/'.join(name)}\")\n            tf_weights.pop(txt_name, None)\n            continue\n        if \"_slot_\" in name[-1]:\n            logger.info(f\"Skipping {'/'.join(name)}\")\n            tf_weights.pop(txt_name, None)\n            continue\n        pointer = model\n        array = tf_weights[txt_name]\n\n        for m_name in name:\n            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n                scope_names = re.split(r\"_(\\d+)\", m_name)\n            else:\n                scope_names = [m_name]\n            if scope_names[0] in [\"kernel\", \"scale\", \"embedding\"]:\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"self_attention\":\n                pointer = getattr(pointer, \"layer\")\n                pointer = pointer[0]\n            elif scope_names[0] == \"enc_dec_attention\":\n                pointer = getattr(pointer, \"layer\")\n                pointer = pointer[1]\n            elif scope_names[0] == \"dense_relu_dense\":\n                pointer = getattr(pointer, \"layer\")\n                pointer = pointer[2]\n            elif scope_names[0] == \"rms_norm\":\n                if hasattr(pointer, \"layer_norm\"):\n                    pointer = getattr(pointer, \"layer_norm\")\n                elif hasattr(pointer, \"final_layer_norm\"):\n                    pointer = getattr(pointer, \"final_layer_norm\")\n            elif scope_names[0] == \"scale\":\n                pointer = getattr(pointer, \"weight\")\n            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n                pointer = getattr(pointer, \"bias\")\n            elif scope_names[0] == \"squad\":\n                pointer = getattr(pointer, \"classifier\")\n            elif scope_names[0] == \"decoder\" and name[1] == \"logits\":\n                continue\n            elif scope_names[0] == \"logits\":\n                pointer = getattr(pointer, \"lm_head\")\n            elif (\n                scope_names[0] == \"wi\"\n                and len(scope_names) > 1\n                and scope_names[1].isdigit()\n            ):\n                pointer = getattr(pointer, f\"wi_{scope_names[1]}\")\n                continue\n            else:\n                try:\n                    pointer = getattr(pointer, scope_names[0])\n                except AttributeError:\n                    logger.info(f\"Skipping {'/'.join(name)}\")\n                    continue\n            if len(scope_names) >= 2:\n                num = int(scope_names[1])\n                pointer = pointer[num]\n        if scope_names[0] not in [\"kernel\", \"scale\", \"embedding\"]:\n            pointer = getattr(pointer, \"weight\")\n        if scope_names[0] != \"embedding\":\n            logger.info(f\"Transposing numpy weight of shape {array.shape} for {name}\")\n            array = np.transpose(array)\n        try:\n            assert (\n                pointer.shape == array.shape\n            ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(f\"Initialize PyTorch weight {name}\")\n        pointer.data = torch.from_numpy(array.astype(np.float32))\n        tf_weights.pop(txt_name, None)\n\n    logger.info(f\"Weights not copied to PyTorch model: {', '.join(tf_weights.keys())}.\")\n    return model\n\n\n####################################################\n# PyTorch Models are constructed by sub-classing\n# - torch.nn.Module for the layers and\n# - PreTrainedModel for the models (it-self a sub-class of nn.Module)\n####################################################\nPARALLELIZE_DOCSTRING = r\"\"\"\n    This is an experimental feature and is a subject to change at a moment's notice.\n\n    Uses a device map to distribute attention modules of the model across several devices. If no device map is given,\n    it will evenly distribute blocks across all devices.\n\n    Args:\n        device_map (`Dict[int, list]`, optional, defaults to None):\n            A dictionary that maps attention modules to devices. Note that the embedding module and LMHead are always\n            automatically mapped to the first device (for esoteric reasons). That means that the first device should\n            have fewer attention modules mapped to it than other devices. For reference, the t5 models have the\n            following number of attention modules:\n\n                - t5-small: 6\n                - t5-base: 12\n                - t5-large: 24\n                - t5-3b: 24\n                - t5-11b: 24\n\n    Example:\n\n    ```python\n    # Here is an example of a device map on a machine with 4 GPUs using t5-3b, which has a total of 24 attention modules:\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],\n        3: [17, 18, 19, 20, 21, 22, 23],\n    }\n    model.parallelize(device_map)\n    ```\n\"\"\"\nDEPARALLELIZE_DOCSTRING = r\"\"\"\n    Moves the model to cpu from a model parallel state.\n\n    Example:\n\n    ```python\n    # On a 4 GPU machine with t5-3b:\n    model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")\n    device_map = {\n        0: [0, 1, 2],\n        1: [3, 4, 5, 6, 7, 8, 9],\n        2: [10, 11, 12, 13, 14, 15, 16],\n        3: [17, 18, 19, 20, 21, 22, 23],\n    }\n    model.parallelize(device_map)  # Splits the model across several devices\n    model.deparallelize()  # Put the model back on cpu and cleans memory by calling torch.cuda.empty_cache()\n    ```\n\"\"\"\n\n\nclass T5LayerNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n\n        # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\n        # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\n        # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\n        # half-precision inputs is done in fp32\n\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\ntry:\n    from apex.normalization import FusedRMSNorm\n\n    T5LayerNorm = FusedRMSNorm  # noqa\n\n    logger.info(\n        \"Discovered apex.normalization.FusedRMSNorm - will use it instead of T5LayerNorm\"\n    )\nexcept ImportError:\n    # using the normal T5LayerNorm\n    pass\nexcept Exception:\n    logger.warning(\"discovered apex but it failed to load, falling back to T5LayerNorm\")\n    pass\n\nALL_LAYERNORM_LAYERS.append(T5LayerNorm)\n\n\nclass T5DenseActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n\n    def forward(self, hidden_states):\n        hidden_states = self.wi(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.wo(hidden_states)\n        return hidden_states\n\n\nclass T5DenseGatedActDense(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=False)\n        self.wo = nn.Linear(config.d_ff, config.d_model, bias=False)\n        self.dropout = nn.Dropout(config.dropout_rate)\n        self.act = ACT2FN[config.dense_act_fn]\n\n    def forward(self, hidden_states):\n        hidden_gelu = self.act(self.wi_0(hidden_states))\n        hidden_linear = self.wi_1(hidden_states)\n        hidden_states = hidden_gelu * hidden_linear\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.wo(hidden_states)\n        return hidden_states\n\n\nclass T5LayerFF(nn.Module):\n    def __init__(self, config: T5Config):\n        super().__init__()\n        if config.is_gated_act:\n            self.DenseReluDense = T5DenseGatedActDense(config)\n        else:\n            self.DenseReluDense = T5DenseActDense(config)\n\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(self, hidden_states):\n        forwarded_states = self.layer_norm(hidden_states)\n        forwarded_states = self.DenseReluDense(forwarded_states)\n        hidden_states = hidden_states + self.dropout(forwarded_states)\n        return hidden_states\n\n\nclass T5Attention(nn.Module):\n    def __init__(self, config: T5Config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.has_relative_attention_bias = has_relative_attention_bias\n        self.relative_attention_num_buckets = config.relative_attention_num_buckets\n        self.relative_attention_max_distance = config.relative_attention_max_distance\n        self.d_model = config.d_model\n        self.key_value_proj_dim = config.d_kv\n        self.n_heads = config.num_heads\n        self.dropout = config.dropout_rate\n        self.inner_dim = self.n_heads * self.key_value_proj_dim\n\n        # Mesh TensorFlow initialization to avoid scaling before softmax\n        self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)\n        self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)\n        self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)\n        self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)\n\n        if self.has_relative_attention_bias:\n            self.relative_attention_bias = nn.Embedding(\n                self.relative_attention_num_buckets, self.n_heads\n            )\n        self.pruned_heads = set()\n        self.gradient_checkpointing = False\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        heads, index = find_pruneable_heads_and_indices(\n            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n        )\n        # Prune linear layers\n        self.q = prune_linear_layer(self.q, index)\n        self.k = prune_linear_layer(self.k, index)\n        self.v = prune_linear_layer(self.v, index)\n        self.o = prune_linear_layer(self.o, index, dim=1)\n        # Update hyper params\n        self.n_heads = self.n_heads - len(heads)\n        self.inner_dim = self.key_value_proj_dim * self.n_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    @staticmethod\n    def _relative_position_bucket(\n        relative_position, bidirectional=True, num_buckets=32, max_distance=128\n    ):\n        \"\"\"\n        Adapted from Mesh Tensorflow:\n        https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593\n\n        Translate relative position to a bucket number for relative attention. The relative position is defined as\n        memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to\n        position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for\n        small absolute relative_position and larger buckets for larger absolute relative_positions. All relative\n        positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.\n        This should allow for more graceful generalization to longer sequences than the model has been trained on\n\n        Args:\n            relative_position: an int32 Tensor\n            bidirectional: a boolean - whether the attention is bidirectional\n            num_buckets: an integer\n            max_distance: an integer\n\n        Returns:\n            a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)\n        \"\"\"\n        relative_buckets = 0\n        if bidirectional:\n            num_buckets //= 2\n            relative_buckets += (relative_position > 0).to(torch.long) * num_buckets\n            relative_position = torch.abs(relative_position)\n        else:\n            relative_position = -torch.min(\n                relative_position, torch.zeros_like(relative_position)\n            )\n        # now relative_position is in the range [0, inf)\n\n        # half of the buckets are for exact increments in positions\n        max_exact = num_buckets // 2\n        is_small = relative_position < max_exact\n\n        # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\n        relative_position_if_large = max_exact + (\n            torch.log(relative_position.float() / max_exact)\n            / math.log(max_distance / max_exact)\n            * (num_buckets - max_exact)\n        ).to(torch.long)\n        relative_position_if_large = torch.min(\n            relative_position_if_large,\n            torch.full_like(relative_position_if_large, num_buckets - 1),\n        )\n\n        relative_buckets += torch.where(\n            is_small, relative_position, relative_position_if_large\n        )\n        return relative_buckets\n\n    def compute_bias(self, query_length, key_length, device=None):\n        \"\"\"Compute binned relative position bias\"\"\"\n        if device is None:\n            device = self.relative_attention_bias.weight.device\n        context_position = torch.arange(query_length, dtype=torch.long, device=device)[\n            :, None\n        ]\n        memory_position = torch.arange(key_length, dtype=torch.long, device=device)[\n            None, :\n        ]\n        relative_position = (\n            memory_position - context_position\n        )  # shape (query_length, key_length)\n        relative_position_bucket = self._relative_position_bucket(\n            relative_position,  # shape (query_length, key_length)\n            bidirectional=(not self.is_decoder),\n            num_buckets=self.relative_attention_num_buckets,\n            max_distance=self.relative_attention_max_distance,\n        )\n        values = self.relative_attention_bias(\n            relative_position_bucket\n        )  # shape (query_length, key_length, num_heads)\n        values = values.permute([2, 0, 1]).unsqueeze(\n            0\n        )  # shape (1, num_heads, query_length, key_length)\n        return values\n\n    def forward(\n        self,\n        hidden_states,\n        mask=None,\n        key_value_states=None,\n        position_bias=None,\n        past_key_value=None,\n        layer_head_mask=None,\n        query_length=None,\n        use_cache=False,\n        output_attentions=False,\n    ):\n        \"\"\"\n        Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).\n        \"\"\"\n        # Input is (batch_size, seq_length, dim)\n        # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)\n        # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)\n        batch_size, seq_length = hidden_states.shape[:2]\n\n        real_seq_length = seq_length\n\n        if past_key_value is not None:\n            assert (\n                len(past_key_value) == 2\n            ), f\"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states\"\n            real_seq_length += (\n                past_key_value[0].shape[2] if query_length is None else query_length\n            )\n\n        key_length = (\n            real_seq_length if key_value_states is None else key_value_states.shape[1]\n        )\n\n        def shape(states):\n            \"\"\"projection\"\"\"\n            return states.view(\n                batch_size, -1, self.n_heads, self.key_value_proj_dim\n            ).transpose(1, 2)\n\n        def unshape(states):\n            \"\"\"reshape\"\"\"\n            return (\n                states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)\n            )\n\n        def project(hidden_states, proj_layer, key_value_states, past_key_value):\n            \"\"\"projects hidden states correctly to key/query states\"\"\"\n            if key_value_states is None:\n                # self-attn\n                # (batch_size, n_heads, seq_length, dim_per_head)\n                hidden_states = shape(proj_layer(hidden_states))\n            elif past_key_value is None:\n                # cross-attn\n                # (batch_size, n_heads, seq_length, dim_per_head)\n                hidden_states = shape(proj_layer(key_value_states))\n\n            if past_key_value is not None:\n                if key_value_states is None:\n                    # self-attn\n                    # (batch_size, n_heads, key_length, dim_per_head)\n                    hidden_states = torch.cat([past_key_value, hidden_states], dim=2)\n                else:\n                    # cross-attn\n                    hidden_states = past_key_value\n            return hidden_states\n\n        # get query states\n        query_states = shape(\n            self.q(hidden_states)\n        )  # (batch_size, n_heads, seq_length, dim_per_head)\n\n        # get key/value states\n        key_states = project(\n            hidden_states,\n            self.k,\n            key_value_states,\n            past_key_value[0] if past_key_value is not None else None,\n        )\n        value_states = project(\n            hidden_states,\n            self.v,\n            key_value_states,\n            past_key_value[1] if past_key_value is not None else None,\n        )\n\n        # compute scores\n        scores = torch.matmul(\n            query_states, key_states.transpose(3, 2)\n        )  # equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\n\n        if position_bias is None:\n            if not self.has_relative_attention_bias:\n                position_bias = torch.zeros(\n                    (1, self.n_heads, real_seq_length, key_length),\n                    device=scores.device,\n                    dtype=scores.dtype,\n                )\n                if self.gradient_checkpointing and self.training:\n                    position_bias.requires_grad = True\n            else:\n                position_bias = self.compute_bias(\n                    real_seq_length, key_length, device=scores.device\n                )\n\n            # if key and values are already calculated\n            # we want only the last query position bias\n            if past_key_value is not None:\n                position_bias = position_bias[:, :, -hidden_states.size(1) :, :]\n\n            if mask is not None:\n                position_bias = (\n                    position_bias + mask\n                )  # (batch_size, n_heads, seq_length, key_length)\n\n        if self.pruned_heads:\n            mask = torch.ones(position_bias.shape[1])\n            mask[list(self.pruned_heads)] = 0\n            position_bias_masked = position_bias[:, mask.bool()]\n        else:\n            position_bias_masked = position_bias\n\n        scores += position_bias_masked\n        attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n            scores\n        )  # (batch_size, n_heads, seq_length, key_length)\n        attn_weights = nn.functional.dropout(\n            attn_weights, p=self.dropout, training=self.training\n        )  # (batch_size, n_heads, seq_length, key_length)\n\n        # Mask heads if we want to\n        if layer_head_mask is not None:\n            attn_weights = attn_weights * layer_head_mask\n\n        attn_output = unshape(\n            torch.matmul(attn_weights, value_states)\n        )  # (batch_size, seq_length, dim)\n        attn_output = self.o(attn_output)\n\n        present_key_value_state = (\n            (key_states, value_states) if (self.is_decoder and use_cache) else None\n        )\n        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n\n        if output_attentions:\n            outputs = outputs + (attn_weights,)\n        return outputs\n\n\nclass T5LayerSelfAttention(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.SelfAttention = T5Attention(\n            config, has_relative_attention_bias=has_relative_attention_bias\n        )\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        position_bias=None,\n        layer_head_mask=None,\n        past_key_value=None,\n        use_cache=False,\n        output_attentions=False,\n    ):\n        normed_hidden_states = self.layer_norm(hidden_states)\n        attention_output = self.SelfAttention(\n            normed_hidden_states,\n            mask=attention_mask,\n            position_bias=position_bias,\n            layer_head_mask=layer_head_mask,\n            past_key_value=past_key_value,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n        )\n        hidden_states = hidden_states + self.dropout(attention_output[0])\n        outputs = (hidden_states,) + attention_output[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass T5LayerCrossAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)\n        self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n    def forward(\n        self,\n        hidden_states,\n        key_value_states,\n        attention_mask=None,\n        position_bias=None,\n        layer_head_mask=None,\n        past_key_value=None,\n        use_cache=False,\n        query_length=None,\n        output_attentions=False,\n    ):\n        normed_hidden_states = self.layer_norm(hidden_states)\n        attention_output = self.EncDecAttention(\n            normed_hidden_states,\n            mask=attention_mask,\n            key_value_states=key_value_states,\n            position_bias=position_bias,\n            layer_head_mask=layer_head_mask,\n            past_key_value=past_key_value,\n            use_cache=use_cache,\n            query_length=query_length,\n            output_attentions=output_attentions,\n        )\n        layer_output = hidden_states + self.dropout(attention_output[0])\n        outputs = (layer_output,) + attention_output[\n            1:\n        ]  # add attentions if we output them\n        return outputs\n\n\nclass T5Block(nn.Module):\n    def __init__(self, config, has_relative_attention_bias=False):\n        super().__init__()\n        self.is_decoder = config.is_decoder\n        self.layer = nn.ModuleList()\n        self.layer.append(\n            T5LayerSelfAttention(\n                config, has_relative_attention_bias=has_relative_attention_bias\n            )\n        )\n        if self.is_decoder:\n            self.layer.append(T5LayerCrossAttention(config))\n\n        self.layer.append(T5LayerFF(config))\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        position_bias=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        encoder_decoder_position_bias=None,\n        layer_head_mask=None,\n        cross_attn_layer_head_mask=None,\n        past_key_value=None,\n        use_cache=False,\n        output_attentions=False,\n        return_dict=True,\n    ):\n\n        if past_key_value is not None:\n            if not self.is_decoder:\n                logger.warning(\n                    \"`past_key_values` is passed to the encoder. Please make sure this is intended.\"\n                )\n            expected_num_past_key_values = 2 if encoder_hidden_states is None else 4\n\n            if len(past_key_value) != expected_num_past_key_values:\n                raise ValueError(\n                    f\"There should be {expected_num_past_key_values} past states. \"\n                    f\"{'2 (past / key) for cross attention. ' if expected_num_past_key_values == 4 else ''}\"\n                    f\"Got {len(past_key_value)} past key / value states\"\n                )\n\n            self_attn_past_key_value = past_key_value[:2]\n            cross_attn_past_key_value = past_key_value[2:]\n        else:\n            self_attn_past_key_value, cross_attn_past_key_value = None, None\n\n        self_attention_outputs = self.layer[0](\n            hidden_states,\n            attention_mask=attention_mask,\n            position_bias=position_bias,\n            layer_head_mask=layer_head_mask,\n            past_key_value=self_attn_past_key_value,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n        )\n        hidden_states, present_key_value_state = self_attention_outputs[:2]\n        attention_outputs = self_attention_outputs[\n            2:\n        ]  # Keep self-attention outputs and relative position weights\n\n        # clamp inf values to enable fp16 training\n        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(\n                hidden_states, min=-clamp_value, max=clamp_value\n            )\n\n        do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n        if do_cross_attention:\n            # the actual query length is unknown for cross attention\n            # if using past key value states. Need to inject it here\n            if present_key_value_state is not None:\n                query_length = present_key_value_state[0].shape[2]\n            else:\n                query_length = None\n\n            cross_attention_outputs = self.layer[1](\n                hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                position_bias=encoder_decoder_position_bias,\n                layer_head_mask=cross_attn_layer_head_mask,\n                past_key_value=cross_attn_past_key_value,\n                query_length=query_length,\n                use_cache=use_cache,\n                output_attentions=output_attentions,\n            )\n            hidden_states = cross_attention_outputs[0]\n\n            # clamp inf values to enable fp16 training\n            if (\n                hidden_states.dtype == torch.float16\n                and torch.isinf(hidden_states).any()\n            ):\n                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                hidden_states = torch.clamp(\n                    hidden_states, min=-clamp_value, max=clamp_value\n                )\n\n            # Combine self attn and cross attn key value states\n            if present_key_value_state is not None:\n                present_key_value_state = (\n                    present_key_value_state + cross_attention_outputs[1]\n                )\n\n            # Keep cross-attention outputs and relative position weights\n            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n\n        # Apply Feed Forward layer\n        hidden_states = self.layer[-1](hidden_states)\n\n        # clamp inf values to enable fp16 training\n        if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(\n                hidden_states, min=-clamp_value, max=clamp_value\n            )\n\n        outputs = (hidden_states,)\n\n        if use_cache:\n            outputs = outputs + (present_key_value_state,) + attention_outputs\n        else:\n            outputs = outputs + attention_outputs\n\n        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n\n\nclass T5PreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = T5Config\n    load_tf_weights = load_tf_weights_in_t5\n    base_model_prefix = \"transformer\"\n    is_parallelizable = True\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"T5Block\"]\n\n    @property\n    def dummy_inputs(self):\n        input_ids = torch.tensor(DUMMY_INPUTS)\n        input_mask = torch.tensor(DUMMY_MASK)\n        dummy_inputs = {\n            \"decoder_input_ids\": input_ids,\n            \"input_ids\": input_ids,\n            \"decoder_attention_mask\": input_mask,\n        }\n        return dummy_inputs\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        factor = (\n            self.config.initializer_factor\n        )  # Used for testing weights initialization\n        if isinstance(module, T5LayerNorm):\n            module.weight.data.fill_(factor * 1.0)\n        elif isinstance(module, (T5Model, T5ForConditionalGeneration, T5EncoderModel)):\n            # Mesh TensorFlow embeddings initialization\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n            module.shared.weight.data.normal_(mean=0.0, std=factor * 1.0)\n            if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n                module.lm_head.weight.data.normal_(mean=0.0, std=factor * 1.0)\n        elif isinstance(module, T5DenseActDense):\n            # Mesh TensorFlow FF initialization\n            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n            module.wi.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n            )\n            if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n                module.wi.bias.data.zero_()\n            module.wo.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_ff) ** -0.5)\n            )\n            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n                module.wo.bias.data.zero_()\n        elif isinstance(module, T5DenseGatedActDense):\n            module.wi_0.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n            )\n            if hasattr(module.wi_0, \"bias\") and module.wi_0.bias is not None:\n                module.wi_0.bias.data.zero_()\n            module.wi_1.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_model) ** -0.5)\n            )\n            if hasattr(module.wi_1, \"bias\") and module.wi_1.bias is not None:\n                module.wi_1.bias.data.zero_()\n            module.wo.weight.data.normal_(\n                mean=0.0, std=factor * ((self.config.d_ff) ** -0.5)\n            )\n            if hasattr(module.wo, \"bias\") and module.wo.bias is not None:\n                module.wo.bias.data.zero_()\n        elif isinstance(module, T5Attention):\n            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n            d_model = self.config.d_model\n            key_value_proj_dim = self.config.d_kv\n            n_heads = self.config.num_heads\n            module.q.weight.data.normal_(\n                mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5)\n            )\n            module.k.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n            module.v.weight.data.normal_(mean=0.0, std=factor * (d_model**-0.5))\n            module.o.weight.data.normal_(\n                mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5)\n            )\n            if module.has_relative_attention_bias:\n                module.relative_attention_bias.weight.data.normal_(\n                    mean=0.0, std=factor * ((d_model) ** -0.5)\n                )\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, (T5Attention, T5Stack)):\n            module.gradient_checkpointing = value\n\n    def _shift_right(self, input_ids):\n        decoder_start_token_id = self.config.decoder_start_token_id\n        pad_token_id = self.config.pad_token_id\n\n        assert decoder_start_token_id is not None, (\n            \"self.model.config.decoder_start_token_id has to be defined. In T5 it is usually set to the pad_token_id.\"\n            \" See T5 docs for more information\"\n        )\n\n        # shift inputs to the right\n        if is_torch_fx_proxy(input_ids):\n            # Item assignment is not supported natively for proxies.\n            shifted_input_ids = torch.full(\n                input_ids.shape[:-1] + (1,), decoder_start_token_id\n            )\n            shifted_input_ids = torch.cat(\n                [shifted_input_ids, input_ids[..., :-1]], dim=-1\n            )\n        else:\n            shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n            shifted_input_ids[..., 1:] = input_ids[..., :-1].clone()\n            shifted_input_ids[..., 0] = decoder_start_token_id\n\n        assert (\n            pad_token_id is not None\n        ), \"self.model.config.pad_token_id has to be defined.\"\n        # replace possible -100 values in labels by `pad_token_id`\n        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n        return shifted_input_ids\n\n\nclass T5Stack(T5PreTrainedModel):\n    def __init__(self, config, embed_tokens=None):\n        super().__init__(config)\n\n        self.embed_tokens = embed_tokens\n        self.is_decoder = config.is_decoder\n\n        self.block = nn.ModuleList(\n            [\n                T5Block(config, has_relative_attention_bias=bool(i == 0))\n                for i in range(config.num_layers)\n            ]\n        )\n        self.final_layer_norm = T5LayerNorm(\n            config.d_model, eps=config.layer_norm_epsilon\n        )\n        self.dropout = nn.Dropout(config.dropout_rate)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n        self.gradient_checkpointing = False\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        # Check validity of device_map\n        self.device_map = (\n            get_device_map(len(self.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.block))\n        self.model_parallel = True\n        self.first_device = (\n            \"cpu\"\n            if \"cpu\" in self.device_map.keys()\n            else \"cuda:\" + str(min(self.device_map.keys()))\n        )\n        self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n        # Load onto devices\n        for k, v in self.device_map.items():\n            for layer in v:\n                cuda_device = \"cuda:\" + str(k)\n                self.block[layer] = self.block[layer].to(cuda_device)\n\n        # Set embed_tokens to first layer\n        self.embed_tokens = self.embed_tokens.to(self.first_device)\n        # Set final layer norm to last device\n        self.final_layer_norm = self.final_layer_norm.to(self.last_device)\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.model_parallel = False\n        self.device_map = None\n        self.first_device = \"cpu\"\n        self.last_device = \"cpu\"\n        for i in range(len(self.block)):\n            self.block[i] = self.block[i].to(\"cpu\")\n        self.embed_tokens = self.embed_tokens.to(\"cpu\")\n        self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, new_embeddings):\n        self.embed_tokens = new_embeddings\n\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        encoder_hidden_states=None,\n        encoder_attention_mask=None,\n        inputs_embeds=None,\n        head_mask=None,\n        cross_attn_head_mask=None,\n        past_key_values=None,\n        use_cache=None,\n        output_attentions=None,\n        output_hidden_states=None,\n        return_dict=None,\n    ):\n        # Model parallel\n        if self.model_parallel:\n            torch.cuda.set_device(self.first_device)\n            self.embed_tokens = self.embed_tokens.to(self.first_device)\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        output_attentions = (\n            output_attentions\n            if output_attentions is not None\n            else self.config.output_attentions\n        )\n        output_hidden_states = (\n            output_hidden_states\n            if output_hidden_states is not None\n            else self.config.output_hidden_states\n        )\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        if input_ids is not None and inputs_embeds is not None:\n            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n            raise ValueError(\n                f\"You cannot specify both {err_msg_prefix}input_ids and {err_msg_prefix}inputs_embeds at the same time\"\n            )\n        elif input_ids is not None:\n            input_shape = input_ids.size()\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n        else:\n            err_msg_prefix = \"decoder_\" if self.is_decoder else \"\"\n            raise ValueError(\n                f\"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds\"\n            )\n\n        if inputs_embeds is None:\n            assert (\n                self.embed_tokens is not None\n            ), \"You have to initialize the model with valid token embeddings\"\n            inputs_embeds = self.embed_tokens(input_ids)\n\n        batch_size, seq_length = input_shape\n\n        # required mask seq length can be calculated via length of past\n        mask_seq_length = (\n            past_key_values[0][0].shape[2] + seq_length\n            if past_key_values is not None\n            else seq_length\n        )\n\n        if use_cache is True:\n            assert (\n                self.is_decoder\n            ), f\"`use_cache` can only be set to `True` if {self} is used as a decoder\"\n\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                batch_size, mask_seq_length, device=inputs_embeds.device\n            )\n        if (\n            self.is_decoder\n            and encoder_attention_mask is None\n            and encoder_hidden_states is not None\n        ):\n            encoder_seq_length = encoder_hidden_states.shape[1]\n            encoder_attention_mask = torch.ones(\n                batch_size,\n                encoder_seq_length,\n                device=inputs_embeds.device,\n                dtype=torch.long,\n            )\n\n        # initialize past_key_values with `None` if past does not exist\n        if past_key_values is None:\n            past_key_values = [None] * len(self.block)\n\n        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n        # ourselves in which case we just need to make it broadcastable to all heads.\n        extended_attention_mask = self.get_extended_attention_mask(\n            attention_mask, input_shape\n        )\n\n        # If a 2D or 3D attention mask is provided for the cross-attention\n        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n        if self.is_decoder and encoder_hidden_states is not None:\n            (\n                encoder_batch_size,\n                encoder_sequence_length,\n                _,\n            ) = encoder_hidden_states.size()\n            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n            if encoder_attention_mask is None:\n                encoder_attention_mask = torch.ones(\n                    encoder_hidden_shape, device=inputs_embeds.device\n                )\n            encoder_extended_attention_mask = self.invert_attention_mask(\n                encoder_attention_mask\n            )\n        else:\n            encoder_extended_attention_mask = None\n\n        # Prepare head mask if needed\n        head_mask = self.get_head_mask(head_mask, self.config.num_layers)\n        cross_attn_head_mask = self.get_head_mask(\n            cross_attn_head_mask, self.config.num_layers\n        )\n        present_key_value_states = () if use_cache else None\n        all_hidden_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and self.is_decoder) else None\n        position_bias = None\n        encoder_decoder_position_bias = None\n\n        hidden_states = self.dropout(inputs_embeds)\n\n        for i, (layer_module, past_key_value) in enumerate(\n            zip(self.block, past_key_values)\n        ):\n            layer_head_mask = head_mask[i]\n            cross_attn_layer_head_mask = cross_attn_head_mask[i]\n            # Model parallel\n            if self.model_parallel:\n                torch.cuda.set_device(hidden_states.device)\n                # Ensure that attention_mask is always on the same device as hidden_states\n                if attention_mask is not None:\n                    attention_mask = attention_mask.to(hidden_states.device)\n                if position_bias is not None:\n                    position_bias = position_bias.to(hidden_states.device)\n                if encoder_hidden_states is not None:\n                    encoder_hidden_states = encoder_hidden_states.to(\n                        hidden_states.device\n                    )\n                if encoder_extended_attention_mask is not None:\n                    encoder_extended_attention_mask = (\n                        encoder_extended_attention_mask.to(hidden_states.device)\n                    )\n                if encoder_decoder_position_bias is not None:\n                    encoder_decoder_position_bias = encoder_decoder_position_bias.to(\n                        hidden_states.device\n                    )\n                if layer_head_mask is not None:\n                    layer_head_mask = layer_head_mask.to(hidden_states.device)\n                if cross_attn_layer_head_mask is not None:\n                    cross_attn_layer_head_mask = cross_attn_layer_head_mask.to(\n                        hidden_states.device\n                    )\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            if self.gradient_checkpointing and self.training:\n                if use_cache:\n                    logger.warning(\n                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                    )\n                    use_cache = False\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        return tuple(module(*inputs, use_cache, output_attentions))\n\n                    return custom_forward\n\n                layer_outputs = checkpoint(\n                    create_custom_forward(layer_module),\n                    hidden_states,\n                    extended_attention_mask,\n                    position_bias,\n                    encoder_hidden_states,\n                    encoder_extended_attention_mask,\n                    encoder_decoder_position_bias,\n                    layer_head_mask,\n                    cross_attn_layer_head_mask,\n                    None,  # past_key_value is always None with gradient checkpointing\n                )\n            else:\n                layer_outputs = layer_module(\n                    hidden_states,\n                    attention_mask=extended_attention_mask,\n                    position_bias=position_bias,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_extended_attention_mask,\n                    encoder_decoder_position_bias=encoder_decoder_position_bias,\n                    layer_head_mask=layer_head_mask,\n                    cross_attn_layer_head_mask=cross_attn_layer_head_mask,\n                    past_key_value=past_key_value,\n                    use_cache=use_cache,\n                    output_attentions=output_attentions,\n                )\n\n            # layer_outputs is a tuple with:\n            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n            if use_cache is False:\n                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n\n            hidden_states, present_key_value_state = layer_outputs[:2]\n\n            # We share the position biases between the layers - the first layer store them\n            # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n            # (cross-attention position bias), (cross-attention weights)\n            position_bias = layer_outputs[2]\n            if self.is_decoder and encoder_hidden_states is not None:\n                encoder_decoder_position_bias = layer_outputs[\n                    4 if output_attentions else 3\n                ]\n            # append next layer key value states\n            if use_cache:\n                present_key_value_states = present_key_value_states + (\n                    present_key_value_state,\n                )\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[3],)\n                if self.is_decoder:\n                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n\n            # Model Parallel: If it's the last layer for that device, put things on the next device\n            if self.model_parallel:\n                for k, v in self.device_map.items():\n                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n\n        hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n\n        # Add last layer\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(\n                v\n                for v in [\n                    hidden_states,\n                    present_key_value_states,\n                    all_hidden_states,\n                    all_attentions,\n                    all_cross_attentions,\n                ]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=present_key_value_states,\n            hidden_states=all_hidden_states,\n            attentions=all_attentions,\n            cross_attentions=all_cross_attentions,\n        )\n\n\nT5_START_DOCSTRING = r\"\"\"\n\n    The T5 model was proposed in [Exploring the Limits of Transfer Learning with a Unified Text-to-Text\n    Transformer](https://arxiv.org/abs/1910.10683) by Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\n    Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu. It's an encoder decoder transformer pre-trained in a\n    text-to-text denoising generative setting.\n\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`T5Config`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nT5_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n\n            Indices can be obtained using [`T5Tokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n\n            [What are input IDs?](../glossary#input-ids)\n\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`T5Tokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            T5 uses the `pad_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n            To know more on how to prepare `decoder_input_ids` for pretraining take a look at [T5\n            Training](./t5#training).\n        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules in the encoder. Mask values selected in `[0,\n            1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        decoder_head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules in the decoder. Mask values selected in `[0,\n            1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        cross_attn_head_mask (`torch.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n                Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in\n                `[0, 1]`:\n\n                - 1 indicates the head is **not masked**,\n                - 0 indicates the head is **masked**.\n\n        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n            Tuple consists of (`last_hidden_state`, `optional`: *hidden_states*, `optional`: *attentions*)\n            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` is a sequence of hidden states at\n            the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n            input (see `past_key_values`). This is useful if you want more control over how to convert\n            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n\n            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n            of `inputs_embeds`.\n\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\nT5_ENCODER_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. T5 is a model with relative position embeddings so you\n            should be able to pad the inputs on both the right and the left.\n\n            Indices can be obtained using [`T5Tokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for detail.\n\n            To know more on how to prepare `input_ids` for pretraining take a look a [T5 Training](./t5#training).\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n# Warning message for FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n__HEAD_MASK_WARNING_MSG = \"\"\"\nThe input argument `head_mask` was split into two arguments `head_mask` and `decoder_head_mask`. Currently,\n`decoder_head_mask` is set to copy `head_mask`, but this feature is deprecated and will be removed in future versions.\nIf you do not want to use any `decoder_head_mask` now, please set `decoder_head_mask = torch.ones(num_layers,\nnum_heads)`.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting raw hidden-states without any specific head on top.\",\n    T5_START_DOCSTRING,\n)\nclass T5Model(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.is_decoder = False\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n\n        decoder_config = copy.deepcopy(config)\n        decoder_config.is_decoder = True\n        decoder_config.is_encoder_decoder = False\n        decoder_config.num_layers = config.num_decoder_layers\n        self.decoder = T5Stack(decoder_config, self.shared)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.decoder.parallelize(self.device_map)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.decoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.decoder = self.decoder.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n        self.decoder.set_input_embeddings(new_embeddings)\n\n    def get_encoder(self):\n        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n    @replace_return_docstrings(\n        output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        decoder_head_mask: Optional[torch.FloatTensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        decoder_inputs_embeds: Optional[torch.Tensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n        r\"\"\"\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import T5Tokenizer, T5Model\n\n        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n        >>> model = T5Model.from_pretrained(\"t5-small\")\n\n        >>> input_ids = tokenizer(\n        ...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n        ... ).input_ids  # Batch size 1\n        >>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n\n        >>> # preprocess: Prepend decoder_input_ids with start token which is pad token for T5Model.\n        >>> # This is not needed for torch's T5ForConditionalGeneration as it does this internally using labels arg.\n        >>> decoder_input_ids = model._shift_right(decoder_input_ids)\n\n        >>> # forward pass\n        >>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n        >>> last_hidden_states = outputs.last_hidden_state\n        ```\"\"\"\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n        if head_mask is not None and decoder_head_mask is None:\n            if self.config.num_layers == self.config.num_decoder_layers:\n                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n                decoder_head_mask = head_mask\n\n        # Encode if needed (training, first prediction pass)\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        hidden_states = encoder_outputs[0]\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n            hidden_states = hidden_states.to(self.decoder.first_device)\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(self.decoder.first_device)\n            if decoder_attention_mask is not None:\n                decoder_attention_mask = decoder_attention_mask.to(\n                    self.decoder.first_device\n                )\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            inputs_embeds=decoder_inputs_embeds,\n            past_key_values=past_key_values,\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"\"\"T5 Model with a `language modeling` head on top.\"\"\", T5_START_DOCSTRING\n)\nclass T5ForConditionalGeneration(T5PreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        r\"encoder.embed_tokens.weight\",\n        r\"decoder.embed_tokens.weight\",\n        r\"lm_head.weight\",\n    ]\n    _keys_to_ignore_on_load_unexpected = [\n        r\"decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight\",\n    ]\n\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.model_dim = config.d_model\n\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.is_decoder = False\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n\n        decoder_config = copy.deepcopy(config)\n        decoder_config.is_decoder = True\n        decoder_config.is_encoder_decoder = False\n        decoder_config.num_layers = config.num_decoder_layers\n        self.decoder = T5Stack(decoder_config, self.shared)\n\n        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.decoder.parallelize(self.device_map)\n        self.lm_head = self.lm_head.to(self.decoder.first_device)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.decoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.decoder = self.decoder.to(\"cpu\")\n        self.lm_head = self.lm_head.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n        self.decoder.set_input_embeddings(new_embeddings)\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def get_encoder(self):\n        return self.encoder\n\n    def get_decoder(self):\n        return self.decoder\n\n    @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n    @replace_return_docstrings(\n        output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.BoolTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        decoder_head_mask: Optional[torch.FloatTensor] = None,\n        cross_attn_head_mask: Optional[torch.Tensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        reduction: Optional[str] = \"mean\",\n    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for\n            labels in `[0, ..., config.vocab_size]`\n\n        Returns:\n\n        Examples:\n\n        ```python\n        >>> from transformers import T5Tokenizer, T5ForConditionalGeneration\n\n        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n        >>> model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n\n        >>> # training\n        >>> input_ids = tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n        >>> labels = tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n        >>> outputs = model(input_ids=input_ids, labels=labels)\n        >>> loss = outputs.loss\n        >>> logits = outputs.logits\n\n        >>> # inference\n        >>> input_ids = tokenizer(\n        ...     \"summarize: studies have shown that owning a dog is good for you\", return_tensors=\"pt\"\n        ... ).input_ids  # Batch size 1\n        >>> outputs = model.generate(input_ids)\n        >>> print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n        >>> # studies have shown that owning a dog is good for you.\n        ```\"\"\"\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        # FutureWarning: head_mask was separated into two input args - head_mask, decoder_head_mask\n        if head_mask is not None and decoder_head_mask is None:\n            if self.config.num_layers == self.config.num_decoder_layers:\n                warnings.warn(__HEAD_MASK_WARNING_MSG, FutureWarning)\n                decoder_head_mask = head_mask\n\n        # Encode if needed (training, first prediction pass)\n        if encoder_outputs is None:\n            # Convert encoder inputs in embeddings if needed\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        hidden_states = encoder_outputs[0]\n\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n\n        if (\n            labels is not None\n            and decoder_input_ids is None\n            and decoder_inputs_embeds is None\n        ):\n            # get decoder inputs from shifting lm labels to the right\n            decoder_input_ids = self._shift_right(labels)\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n            hidden_states = hidden_states.to(self.decoder.first_device)\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(self.decoder.first_device)\n            if decoder_attention_mask is not None:\n                decoder_attention_mask = decoder_attention_mask.to(\n                    self.decoder.first_device\n                )\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            inputs_embeds=decoder_inputs_embeds,\n            past_key_values=past_key_values,\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = decoder_outputs[0]\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.encoder.first_device)\n            self.lm_head = self.lm_head.to(self.encoder.first_device)\n            sequence_output = sequence_output.to(self.lm_head.weight.device)\n\n        if self.config.tie_word_embeddings:\n            # Rescale output before projecting on vocab\n            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n            sequence_output = sequence_output * (self.model_dim**-0.5)\n\n        lm_logits = self.lm_head(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-100, reduction=reduction)\n            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n            if reduction == \"none\":\n                loss = loss.view(lm_logits.size(0), -1).sum(1)\n\n        if not return_dict:\n            output = (lm_logits,) + decoder_outputs[1:] + encoder_outputs\n            return ((loss,) + output) if loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        input_ids,\n        past=None,\n        attention_mask=None,\n        head_mask=None,\n        decoder_head_mask=None,\n        cross_attn_head_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n\n        # cut decoder_input_ids if past is used\n        if past is not None:\n            input_ids = input_ids[:, -1:]\n\n        return {\n            \"decoder_input_ids\": input_ids,\n            \"past_key_values\": past,\n            \"encoder_outputs\": encoder_outputs,\n            \"attention_mask\": attention_mask,\n            \"head_mask\": head_mask,\n            \"decoder_head_mask\": decoder_head_mask,\n            \"cross_attn_head_mask\": cross_attn_head_mask,\n            \"use_cache\": use_cache,\n        }\n\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return self._shift_right(labels)\n\n    def _reorder_cache(self, past, beam_idx):\n        # if decoder past is not included in output\n        # speedy decoding is disabled and no need to reorder\n        if past is None:\n            logger.warning(\n                \"You might want to consider setting `use_cache=True` to speed up decoding\"\n            )\n            return past\n\n        reordered_decoder_past = ()\n        for layer_past_states in past:\n            # get the correct batch idx from layer past batch dim\n            # batch dim of `past` is at 2nd position\n            reordered_layer_past_states = ()\n            for layer_past_state in layer_past_states:\n                # need to set correct `past` for each of the four key / value states\n                reordered_layer_past_states = reordered_layer_past_states + (\n                    layer_past_state.index_select(\n                        0, beam_idx.to(layer_past_state.device)\n                    ),\n                )\n\n            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n            assert len(reordered_layer_past_states) == len(layer_past_states)\n\n            reordered_decoder_past = reordered_decoder_past + (\n                reordered_layer_past_states,\n            )\n        return reordered_decoder_past\n\n\n@add_start_docstrings(\n    \"The bare T5 Model transformer outputting encoder's raw hidden-states without any specific head on top.\",\n    T5_START_DOCSTRING,\n)\nclass T5EncoderModel(T5PreTrainedModel):\n    authorized_missing_keys = [\n        r\"encoder.embed_tokens.weight\",\n    ]\n\n    def __init__(self, config: T5Config):\n        super().__init__(config)\n        self.shared = nn.Embedding(config.vocab_size, config.d_model)\n\n        encoder_config = copy.deepcopy(config)\n        encoder_config.use_cache = False\n        encoder_config.is_encoder_decoder = False\n        self.encoder = T5Stack(encoder_config, self.shared)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        # Model parallel\n        self.model_parallel = False\n        self.device_map = None\n\n    @add_start_docstrings(PARALLELIZE_DOCSTRING)\n    def parallelize(self, device_map=None):\n        self.device_map = (\n            get_device_map(len(self.encoder.block), range(torch.cuda.device_count()))\n            if device_map is None\n            else device_map\n        )\n        assert_device_map(self.device_map, len(self.encoder.block))\n        self.encoder.parallelize(self.device_map)\n        self.model_parallel = True\n\n    @add_start_docstrings(DEPARALLELIZE_DOCSTRING)\n    def deparallelize(self):\n        self.encoder.deparallelize()\n        self.encoder = self.encoder.to(\"cpu\")\n        self.model_parallel = False\n        self.device_map = None\n        torch.cuda.empty_cache()\n\n    def get_input_embeddings(self):\n        return self.shared\n\n    def set_input_embeddings(self, new_embeddings):\n        self.shared = new_embeddings\n        self.encoder.set_input_embeddings(new_embeddings)\n\n    def get_encoder(self):\n        return self.encoder\n\n    def _prune_heads(self, heads_to_prune):\n        \"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"\n        for layer, heads in heads_to_prune.items():\n            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n\n    @add_start_docstrings_to_model_forward(T5_ENCODER_INPUTS_DOCSTRING)\n    @replace_return_docstrings(\n        output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC\n    )\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.FloatTensor], BaseModelOutput]:\n        r\"\"\"\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import T5Tokenizer, T5EncoderModel\n\n        >>> tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n        >>> model = T5EncoderModel.from_pretrained(\"t5-small\")\n        >>> input_ids = tokenizer(\n        ...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n        ... ).input_ids  # Batch size 1\n        >>> outputs = model(input_ids=input_ids)\n        >>> last_hidden_states = outputs.last_hidden_state\n        ```\"\"\"\n        return_dict = (\n            return_dict if return_dict is not None else self.config.use_return_dict\n        )\n\n        encoder_outputs = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            head_mask=head_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        return encoder_outputs\n"}
{"type": "source_file", "path": "src/caption/lavis/common/utils.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport io\nimport json\nimport logging\nimport os\nimport pickle\nimport re\nimport shutil\nimport urllib\nimport urllib.error\nimport urllib.request\nfrom typing import Optional\nfrom urllib.parse import urlparse\n\nimport numpy as np\nimport pandas as pd\nimport yaml\nfrom iopath.common.download import download\nfrom iopath.common.file_io import file_lock, g_pathmgr\n#from lavis.common.registry import registry\nfrom torch.utils.model_zoo import tqdm\nfrom torchvision.datasets.utils import (\n    check_integrity,\n    download_file_from_google_drive,\n    extract_archive,\n)\n\n\ndef now():\n    from datetime import datetime\n\n    return datetime.now().strftime(\"%Y%m%d%H%M\")[:-1]\n\n\ndef is_url(url_or_filename):\n    parsed = urlparse(url_or_filename)\n    return parsed.scheme in (\"http\", \"https\")\n\n\n# def get_cache_path(rel_path):\n#     return os.path.expanduser(os.path.join(registry.get_path(\"cache_root\"), rel_path))\n\n\ndef get_abs_path(rel_path):\n    return os.path.join(\".\", rel_path)\n\n\ndef load_json(filename):\n    with open(filename, \"r\") as f:\n        return json.load(f)\n\n\n# The following are adapted from torchvision and vissl\n# torchvision: https://github.com/pytorch/vision\n# vissl: https://github.com/facebookresearch/vissl/blob/main/vissl/utils/download.py\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        print(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef get_redirected_url(url: str):\n    \"\"\"\n    Given a URL, returns the URL it redirects to or the\n    original URL in case of no indirection\n    \"\"\"\n    import requests\n\n    with requests.Session() as session:\n        with session.get(url, stream=True, allow_redirects=True) as response:\n            if response.history:\n                return response.url\n            else:\n                return url\n\n\ndef to_google_drive_download_url(view_url: str) -> str:\n    \"\"\"\n    Utility function to transform a view URL of google drive\n    to a download URL for google drive\n    Example input:\n        https://drive.google.com/file/d/137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp/view\n    Example output:\n        https://drive.google.com/uc?export=download&id=137RyRjvTBkBiIfeYBNZBtViDHQ6_Ewsp\n    \"\"\"\n    splits = view_url.split(\"/\")\n    assert splits[-1] == \"view\"\n    file_id = splits[-2]\n    return f\"https://drive.google.com/uc?export=download&id={file_id}\"\n\n\ndef download_google_drive_url(url: str, output_path: str, output_file_name: str):\n    \"\"\"\n    Download a file from google drive\n    Downloading an URL from google drive requires confirmation when\n    the file of the size is too big (google drive notifies that\n    anti-viral checks cannot be performed on such files)\n    \"\"\"\n    import requests\n\n    with requests.Session() as session:\n\n        # First get the confirmation token and append it to the URL\n        with session.get(url, stream=True, allow_redirects=True) as response:\n            for k, v in response.cookies.items():\n                if k.startswith(\"download_warning\"):\n                    url = url + \"&confirm=\" + v\n\n        # Then download the content of the file\n        with session.get(url, stream=True, verify=True) as response:\n            makedir(output_path)\n            path = os.path.join(output_path, output_file_name)\n            total_size = int(response.headers.get(\"Content-length\", 0))\n            with open(path, \"wb\") as file:\n                from tqdm import tqdm\n\n                with tqdm(total=total_size) as progress_bar:\n                    for block in response.iter_content(\n                        chunk_size=io.DEFAULT_BUFFER_SIZE\n                    ):\n                        file.write(block)\n                        progress_bar.update(len(block))\n\n\ndef _get_google_drive_file_id(url: str) -> Optional[str]:\n    parts = urlparse(url)\n\n    if re.match(r\"(drive|docs)[.]google[.]com\", parts.netloc) is None:\n        return None\n\n    match = re.match(r\"/file/d/(?P<id>[^/]*)\", parts.path)\n    if match is None:\n        return None\n\n    return match.group(\"id\")\n\n\ndef _urlretrieve(url: str, filename: str, chunk_size: int = 1024) -> None:\n    with open(filename, \"wb\") as fh:\n        with urllib.request.urlopen(\n            urllib.request.Request(url, headers={\"User-Agent\": \"vissl\"})\n        ) as response:\n            with tqdm(total=response.length) as pbar:\n                for chunk in iter(lambda: response.read(chunk_size), \"\"):\n                    if not chunk:\n                        break\n                    pbar.update(chunk_size)\n                    fh.write(chunk)\n\n\ndef download_url(\n    url: str,\n    root: str,\n    filename: Optional[str] = None,\n    md5: Optional[str] = None,\n) -> None:\n    \"\"\"Download a file from a url and place it in root.\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under.\n                                  If None, use the basename of the URL.\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    \"\"\"\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    makedir(root)\n\n    # check if file is already present locally\n    if check_integrity(fpath, md5):\n        print(\"Using downloaded and verified file: \" + fpath)\n        return\n\n    # expand redirect chain if needed\n    url = get_redirected_url(url)\n\n    # check if file is located on Google Drive\n    file_id = _get_google_drive_file_id(url)\n    if file_id is not None:\n        return download_file_from_google_drive(file_id, root, filename, md5)\n\n    # download the file\n    try:\n        print(\"Downloading \" + url + \" to \" + fpath)\n        _urlretrieve(url, fpath)\n    except (urllib.error.URLError, IOError) as e:  # type: ignore[attr-defined]\n        if url[:5] == \"https\":\n            url = url.replace(\"https:\", \"http:\")\n            print(\n                \"Failed download. Trying https -> http instead.\"\n                \" Downloading \" + url + \" to \" + fpath\n            )\n            _urlretrieve(url, fpath)\n        else:\n            raise e\n\n    # check integrity of downloaded file\n    if not check_integrity(fpath, md5):\n        raise RuntimeError(\"File not found or corrupted.\")\n\n\ndef download_and_extract_archive(\n    url: str,\n    download_root: str,\n    extract_root: Optional[str] = None,\n    filename: Optional[str] = None,\n    md5: Optional[str] = None,\n    remove_finished: bool = False,\n) -> None:\n    download_root = os.path.expanduser(download_root)\n    if extract_root is None:\n        extract_root = download_root\n    if not filename:\n        filename = os.path.basename(url)\n\n    download_url(url, download_root, filename, md5)\n\n    archive = os.path.join(download_root, filename)\n    print(\"Extracting {} to {}\".format(archive, extract_root))\n    extract_archive(archive, extract_root, remove_finished)\n\n\ndef cache_url(url: str, cache_dir: str) -> str:\n    \"\"\"\n    This implementation downloads the remote resource and caches it locally.\n    The resource will only be downloaded if not previously requested.\n    \"\"\"\n    parsed_url = urlparse(url)\n    dirname = os.path.join(cache_dir, os.path.dirname(parsed_url.path.lstrip(\"/\")))\n    makedir(dirname)\n    filename = url.split(\"/\")[-1]\n    cached = os.path.join(dirname, filename)\n    with file_lock(cached):\n        if not os.path.isfile(cached):\n            logging.info(f\"Downloading {url} to {cached} ...\")\n            cached = download(url, dirname, filename=filename)\n    logging.info(f\"URL {url} cached in {cached}\")\n    return cached\n\n\n# TODO (prigoyal): convert this into RAII-style API\ndef create_file_symlink(file1, file2):\n    \"\"\"\n    Simply create the symlinks for a given file1 to file2.\n    Useful during model checkpointing to symlinks to the\n    latest successful checkpoint.\n    \"\"\"\n    try:\n        if g_pathmgr.exists(file2):\n            g_pathmgr.rm(file2)\n        g_pathmgr.symlink(file1, file2)\n    except Exception as e:\n        logging.info(f\"Could NOT create symlink. Error: {e}\")\n\n\ndef save_file(data, filename, append_to_json=True, verbose=True):\n    \"\"\"\n    Common i/o utility to handle saving data to various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    Specifically for .json, users have the option to either append (default)\n    or rewrite by passing in Boolean value to append_to_json.\n    \"\"\"\n    if verbose:\n        logging.info(f\"Saving data to file: {filename}\")\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"wb\") as fopen:\n            pickle.dump(data, fopen, pickle.HIGHEST_PROTOCOL)\n    elif file_ext == \".npy\":\n        with g_pathmgr.open(filename, \"wb\") as fopen:\n            np.save(fopen, data)\n    elif file_ext == \".json\":\n        if append_to_json:\n            with g_pathmgr.open(filename, \"a\") as fopen:\n                fopen.write(json.dumps(data, sort_keys=True) + \"\\n\")\n                fopen.flush()\n        else:\n            with g_pathmgr.open(filename, \"w\") as fopen:\n                fopen.write(json.dumps(data, sort_keys=True) + \"\\n\")\n                fopen.flush()\n    elif file_ext == \".yaml\":\n        with g_pathmgr.open(filename, \"w\") as fopen:\n            dump = yaml.dump(data)\n            fopen.write(dump)\n            fopen.flush()\n    else:\n        raise Exception(f\"Saving {file_ext} is not supported yet\")\n\n    if verbose:\n        logging.info(f\"Saved data to file: {filename}\")\n\n\ndef load_file(filename, mmap_mode=None, verbose=True, allow_pickle=False):\n    \"\"\"\n    Common i/o utility to handle loading data from various file formats.\n    Supported:\n        .pkl, .pickle, .npy, .json\n    For the npy files, we support reading the files in mmap_mode.\n    If the mmap_mode of reading is not successful, we load data without the\n    mmap_mode.\n    \"\"\"\n    if verbose:\n        logging.info(f\"Loading data from file: {filename}\")\n\n    file_ext = os.path.splitext(filename)[1]\n    if file_ext == \".txt\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = fopen.readlines()\n    elif file_ext in [\".pkl\", \".pickle\"]:\n        with g_pathmgr.open(filename, \"rb\") as fopen:\n            data = pickle.load(fopen, encoding=\"latin1\")\n    elif file_ext == \".npy\":\n        if mmap_mode:\n            try:\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(\n                        fopen,\n                        allow_pickle=allow_pickle,\n                        encoding=\"latin1\",\n                        mmap_mode=mmap_mode,\n                    )\n            except ValueError as e:\n                logging.info(\n                    f\"Could not mmap {filename}: {e}. Trying without g_pathmgr\"\n                )\n                data = np.load(\n                    filename,\n                    allow_pickle=allow_pickle,\n                    encoding=\"latin1\",\n                    mmap_mode=mmap_mode,\n                )\n                logging.info(\"Successfully loaded without g_pathmgr\")\n            except Exception:\n                logging.info(\"Could not mmap without g_pathmgr. Trying without mmap\")\n                with g_pathmgr.open(filename, \"rb\") as fopen:\n                    data = np.load(fopen, allow_pickle=allow_pickle, encoding=\"latin1\")\n        else:\n            with g_pathmgr.open(filename, \"rb\") as fopen:\n                data = np.load(fopen, allow_pickle=allow_pickle, encoding=\"latin1\")\n    elif file_ext == \".json\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = json.load(fopen)\n    elif file_ext == \".yaml\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = yaml.load(fopen, Loader=yaml.FullLoader)\n    elif file_ext == \".csv\":\n        with g_pathmgr.open(filename, \"r\") as fopen:\n            data = pd.read_csv(fopen)\n    else:\n        raise Exception(f\"Reading from {file_ext} is not supported yet\")\n    return data\n\n\ndef abspath(resource_path: str):\n    \"\"\"\n    Make a path absolute, but take into account prefixes like\n    \"http://\" or \"manifold://\"\n    \"\"\"\n    regex = re.compile(r\"^\\w+://\")\n    if regex.match(resource_path) is None:\n        return os.path.abspath(resource_path)\n    else:\n        return resource_path\n\n\ndef makedir(dir_path):\n    \"\"\"\n    Create the directory if it does not exist.\n    \"\"\"\n    is_success = False\n    try:\n        if not g_pathmgr.exists(dir_path):\n            g_pathmgr.mkdirs(dir_path)\n        is_success = True\n    except BaseException:\n        logging.info(f\"Error creating directory: {dir_path}\")\n    return is_success\n\n\ndef is_url(input_url):\n    \"\"\"\n    Check if an input string is a url. look for http(s):// and ignoring the case\n    \"\"\"\n    is_url = re.match(r\"^(?:http)s?://\", input_url, re.IGNORECASE) is not None\n    return is_url\n\n\ndef cleanup_dir(dir):\n    \"\"\"\n    Utility for deleting a directory. Useful for cleaning the storage space\n    that contains various training artifacts like checkpoints, data etc.\n    \"\"\"\n    if os.path.exists(dir):\n        logging.info(f\"Deleting directory: {dir}\")\n        shutil.rmtree(dir)\n    logging.info(f\"Deleted contents of directory: {dir}\")\n\n\ndef get_file_size(filename):\n    \"\"\"\n    Given a file, get the size of file in MB\n    \"\"\"\n    size_in_mb = os.path.getsize(filename) / float(1024**2)\n    return size_in_mb\n"}
{"type": "source_file", "path": "src/caption/lavis/models/eva_vit.py", "content": "# Based on EVA, BEIT, timm and DeiT code bases\n# https://github.com/baaivision/EVA\n# https://github.com/rwightman/pytorch-image-models/tree/master/timm\n# https://github.com/microsoft/unilm/tree/master/beit\n# https://github.com/facebookresearch/deit/\n# https://github.com/facebookresearch/dino\n# --------------------------------------------------------'\nimport math\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as checkpoint\nfrom timm.models.layers import drop_path, to_2tuple, trunc_normal_\nfrom timm.models.registry import register_model\n\nfrom src.caption.lavis.common.dist_utils import download_cached_file\n\ndef _cfg(url='', **kwargs):\n    return {\n        'url': url,\n        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n        'crop_pct': .9, 'interpolation': 'bicubic',\n        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n        **kwargs\n    }\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    \"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n    \n    def extra_repr(self) -> str:\n        return 'p={}'.format(self.drop_prob)\n\n\nclass Mlp(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        # x = self.drop(x)\n        # commit this for the orignal BERT implement \n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass Attention(nn.Module):\n    def __init__(\n            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n            proj_drop=0., window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        if attn_head_dim is not None:\n            head_dim = attn_head_dim\n        all_head_dim = head_dim * self.num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n        if qkv_bias:\n            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n        else:\n            self.q_bias = None\n            self.v_bias = None\n\n        if window_size:\n            self.window_size = window_size\n            self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n            self.relative_position_bias_table = nn.Parameter(\n                torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n            # cls to token & token 2 cls & cls to cls\n\n            # get pair-wise relative position index for each token inside the window\n            coords_h = torch.arange(window_size[0])\n            coords_w = torch.arange(window_size[1])\n            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n            relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n            relative_coords[:, :, 1] += window_size[1] - 1\n            relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n            relative_position_index = \\\n                torch.zeros(size=(window_size[0] * window_size[1] + 1, ) * 2, dtype=relative_coords.dtype)\n            relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n            relative_position_index[0, 0:] = self.num_relative_distance - 3\n            relative_position_index[0:, 0] = self.num_relative_distance - 2\n            relative_position_index[0, 0] = self.num_relative_distance - 1\n\n            self.register_buffer(\"relative_position_index\", relative_position_index)\n        else:\n            self.window_size = None\n            self.relative_position_bias_table = None\n            self.relative_position_index = None\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(all_head_dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x, rel_pos_bias=None):\n        B, N, C = x.shape\n        qkv_bias = None\n        if self.q_bias is not None:\n            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n\n        q = q * self.scale\n        attn = (q @ k.transpose(-2, -1))\n\n        if self.relative_position_bias_table is not None:\n            relative_position_bias = \\\n                self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                    self.window_size[0] * self.window_size[1] + 1,\n                    self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n            attn = attn + relative_position_bias.unsqueeze(0)\n\n        if rel_pos_bias is not None:\n            attn = attn + rel_pos_bias\n        \n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Block(nn.Module):\n\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n                 window_size=None, attn_head_dim=None):\n        super().__init__()\n        self.norm1 = norm_layer(dim)\n        self.attn = Attention(\n            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n            attn_drop=attn_drop, proj_drop=drop, window_size=window_size, attn_head_dim=attn_head_dim)\n        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n        self.norm2 = norm_layer(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\n        if init_values is not None and init_values > 0:\n            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n        else:\n            self.gamma_1, self.gamma_2 = None, None\n\n    def forward(self, x, rel_pos_bias=None):\n        if self.gamma_1 is None:\n            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.mlp(self.norm2(x)))\n        else:\n            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))\n            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n        return x\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x, **kwargs):\n        B, C, H, W = x.shape\n        # FIXME look at relaxing size constraints\n        assert H == self.img_size[0] and W == self.img_size[1], \\\n            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n\n\nclass RelativePositionBias(nn.Module):\n\n    def __init__(self, window_size, num_heads):\n        super().__init__()\n        self.window_size = window_size\n        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3\n        self.relative_position_bias_table = nn.Parameter(\n            torch.zeros(self.num_relative_distance, num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n        # cls to token & token 2 cls & cls to cls\n\n        # get pair-wise relative position index for each token inside the window\n        coords_h = torch.arange(window_size[0])\n        coords_w = torch.arange(window_size[1])\n        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n        relative_coords[:, :, 0] += window_size[0] - 1  # shift to start from 0\n        relative_coords[:, :, 1] += window_size[1] - 1\n        relative_coords[:, :, 0] *= 2 * window_size[1] - 1\n        relative_position_index = \\\n            torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)\n        relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n        relative_position_index[0, 0:] = self.num_relative_distance - 3\n        relative_position_index[0:, 0] = self.num_relative_distance - 2\n        relative_position_index[0, 0] = self.num_relative_distance - 1\n\n        self.register_buffer(\"relative_position_index\", relative_position_index)\n\n        # trunc_normal_(self.relative_position_bias_table, std=.02)\n\n    def forward(self):\n        relative_position_bias = \\\n            self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n                self.window_size[0] * self.window_size[1] + 1,\n                self.window_size[0] * self.window_size[1] + 1, -1)  # Wh*Ww,Wh*Ww,nH\n        return relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\n\nclass VisionTransformer(nn.Module):\n    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None,\n                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False,\n                 use_mean_pooling=True, init_scale=0.001, use_checkpoint=False):\n        super().__init__()\n        self.image_size = img_size\n        self.num_classes = num_classes\n        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n\n        self.patch_embed = PatchEmbed(\n            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        if use_abs_pos_emb:\n            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        else:\n            self.pos_embed = None\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        if use_shared_rel_pos_bias:\n            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n        else:\n            self.rel_pos_bias = None\n        self.use_checkpoint = use_checkpoint\n        \n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n        self.use_rel_pos_bias = use_rel_pos_bias\n        self.blocks = nn.ModuleList([\n            Block(\n                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None)\n            for i in range(depth)])\n#         self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n#         self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n#         self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n        if self.pos_embed is not None:\n            trunc_normal_(self.pos_embed, std=.02)\n        trunc_normal_(self.cls_token, std=.02)\n        # trunc_normal_(self.mask_token, std=.02)\n#         if isinstance(self.head, nn.Linear):\n#             trunc_normal_(self.head.weight, std=.02)\n        self.apply(self._init_weights)\n        self.fix_init_weight()\n#         if isinstance(self.head, nn.Linear):\n#             self.head.weight.data.mul_(init_scale)\n#             self.head.bias.data.mul_(init_scale)\n\n    def fix_init_weight(self):\n        def rescale(param, layer_id):\n            param.div_(math.sqrt(2.0 * layer_id))\n\n        for layer_id, layer in enumerate(self.blocks):\n            rescale(layer.attn.proj.weight.data, layer_id + 1)\n            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            trunc_normal_(m.weight, std=.02)\n            if isinstance(m, nn.Linear) and m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def get_classifier(self):\n        return self.head\n\n    def reset_classifier(self, num_classes, global_pool=''):\n        self.num_classes = num_classes\n        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n\n    def forward_features(self, x):\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            if self.use_checkpoint:\n                x = checkpoint.checkpoint(blk, x, rel_pos_bias)\n            else:\n                x = blk(x, rel_pos_bias)\n        return x\n#         x = self.norm(x)\n\n#         if self.fc_norm is not None:\n#             t = x[:, 1:, :]\n#             return self.fc_norm(t.mean(1))\n#         else:\n#             return x[:, 0]\n\n    def forward(self, x):\n        x = self.forward_features(x)\n#         x = self.head(x)\n        return x\n\n    def get_intermediate_layers(self, x):\n        x = self.patch_embed(x)\n        batch_size, seq_len, _ = x.size()\n\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n        x = torch.cat((cls_tokens, x), dim=1)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        features = []\n        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n        for blk in self.blocks:\n            x = blk(x, rel_pos_bias)\n            features.append(x)\n\n        return features\n    \n    def get_num_layer(self, var_name=\"\"):\n        if var_name in (\"cls_token\", \"mask_token\", \"pos_embed\"):\n            return 0\n        elif var_name.startswith(\"patch_embed\"):\n            return 0\n        elif var_name.startswith(\"rel_pos_bias\"):\n            return len(self.blocks) - 1\n        elif var_name.startswith(\"blocks\"):\n            layer_id = int(var_name.split('.')[1])\n            return layer_id + 1\n        else:\n            return len(self.blocks)\n        \n            \ndef interpolate_pos_embed(model, checkpoint_model):\n    if 'pos_embed' in checkpoint_model:\n        pos_embed_checkpoint = checkpoint_model['pos_embed'].float()\n        embedding_size = pos_embed_checkpoint.shape[-1]\n        num_patches = model.patch_embed.num_patches\n        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n        # height (== width) for the checkpoint position embedding\n        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n        # height (== width) for the new position embedding\n        new_size = int(num_patches ** 0.5)\n        # class_token and dist_token are kept unchanged\n        if orig_size != new_size:\n            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n            # only the position tokens are interpolated\n            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n            pos_tokens = torch.nn.functional.interpolate(\n                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n            checkpoint_model['pos_embed'] = new_pos_embed\n            \n            \ndef convert_weights_to_fp16(model: nn.Module):\n    \"\"\"Convert applicable model parameters to fp16\"\"\"\n\n    def _convert_weights_to_fp16(l):\n        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):\n            l.weight.data = l.weight.data.half()\n            if l.bias is not None:\n                l.bias.data = l.bias.data.half()\n\n#         if isinstance(l, (nn.MultiheadAttention, Attention)):\n#             for attr in [*[f\"{s}_proj_weight\" for s in [\"in\", \"q\", \"k\", \"v\"]], \"in_proj_bias\", \"bias_k\", \"bias_v\"]:\n#                 tensor = getattr(l, attr)\n#                 if tensor is not None:\n#                     tensor.data = tensor.data.half()\n\n    model.apply(_convert_weights_to_fp16)\n    \n    \ndef create_eva_vit_g(img_size=224,drop_path_rate=0.4,use_checkpoint=False,precision=\"fp16\"):\n    model = VisionTransformer(\n        img_size=img_size,\n        patch_size=14,\n        use_mean_pooling=False,\n        embed_dim=1408,\n        depth=39,\n        num_heads=1408//88,\n        mlp_ratio=4.3637,\n        qkv_bias=True,\n        drop_path_rate=drop_path_rate,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        use_checkpoint=use_checkpoint,\n    )  \n    url = \"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/eva_vit_g.pth\"\n    cached_file = download_cached_file(\n        url, check_hash=False, progress=True\n    )\n    state_dict = torch.load(cached_file, map_location=\"cpu\")    \n    interpolate_pos_embed(model,state_dict)\n    \n    incompatible_keys = model.load_state_dict(state_dict, strict=False)\n#     print(incompatible_keys)\n    \n    if precision == \"fp16\":\n#         model.to(\"cuda\") \n        convert_weights_to_fp16(model)\n    return model\n"}
{"type": "source_file", "path": "src/caption/lavis/common/logger.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport datetime\nimport logging\nimport time\nfrom collections import defaultdict, deque\n\nimport torch\nimport torch.distributed as dist\n\nfrom src.caption.lavis.common import dist_utils\n\n\nclass SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0\n        self.fmt = fmt\n\n    def update(self, value, n=1):\n        self.deque.append(value)\n        self.count += n\n        self.total += value * n\n\n    def synchronize_between_processes(self):\n        \"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"\n        if not dist_utils.is_dist_avail_and_initialized():\n            return\n        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n        dist.barrier()\n        dist.all_reduce(t)\n        t = t.tolist()\n        self.count = int(t[0])\n        self.total = t[1]\n\n    @property\n    def median(self):\n        d = torch.tensor(list(self.deque))\n        return d.median().item()\n\n    @property\n    def avg(self):\n        d = torch.tensor(list(self.deque), dtype=torch.float32)\n        return d.mean().item()\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n    @property\n    def max(self):\n        return max(self.deque)\n\n    @property\n    def value(self):\n        return self.deque[-1]\n\n    def __str__(self):\n        return self.fmt.format(\n            median=self.median,\n            avg=self.avg,\n            global_avg=self.global_avg,\n            max=self.max,\n            value=self.value,\n        )\n\n\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(\n            \"'{}' object has no attribute '{}'\".format(type(self).__name__, attr)\n        )\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\"{}: {}\".format(name, str(meter)))\n        return self.delimiter.join(loss_str)\n\n    def global_avg(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\"{}: {:.4f}\".format(name, meter.global_avg))\n        return self.delimiter.join(loss_str)\n\n    def synchronize_between_processes(self):\n        for meter in self.meters.values():\n            meter.synchronize_between_processes()\n\n    def add_meter(self, name, meter):\n        self.meters[name] = meter\n\n    def log_every(self, iterable, print_freq, header=None):\n        i = 0\n        if not header:\n            header = \"\"\n        start_time = time.time()\n        end = time.time()\n        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n        log_msg = [\n            header,\n            \"[{0\" + space_fmt + \"}/{1}]\",\n            \"eta: {eta}\",\n            \"{meters}\",\n            \"time: {time}\",\n            \"data: {data}\",\n        ]\n        if torch.cuda.is_available():\n            log_msg.append(\"max mem: {memory:.0f}\")\n        log_msg = self.delimiter.join(log_msg)\n        MB = 1024.0 * 1024.0\n        for obj in iterable:\n            data_time.update(time.time() - end)\n            yield obj\n            iter_time.update(time.time() - end)\n            if i % print_freq == 0 or i == len(iterable) - 1:\n                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n                if torch.cuda.is_available():\n                    print(\n                        log_msg.format(\n                            i,\n                            len(iterable),\n                            eta=eta_string,\n                            meters=str(self),\n                            time=str(iter_time),\n                            data=str(data_time),\n                            memory=torch.cuda.max_memory_allocated() / MB,\n                        )\n                    )\n                else:\n                    print(\n                        log_msg.format(\n                            i,\n                            len(iterable),\n                            eta=eta_string,\n                            meters=str(self),\n                            time=str(iter_time),\n                            data=str(data_time),\n                        )\n                    )\n            i += 1\n            end = time.time()\n        total_time = time.time() - start_time\n        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n        print(\n            \"{} Total time: {} ({:.4f} s / it)\".format(\n                header, total_time_str, total_time / len(iterable)\n            )\n        )\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\ndef setup_logger():\n    logging.basicConfig(\n        level=logging.INFO if dist_utils.is_main_process() else logging.WARN,\n        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n        handlers=[logging.StreamHandler()],\n    )\n"}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/blip2_vicuna_instruct.py", "content": "\"\"\"\nRequires Transformer 4.28 and above, implementation may change according the Llama implementation\n\"\"\"\nimport logging\nimport string\nfrom packaging import version\n\nimport torch\nfrom torch.cuda.amp import autocast as autocast\nimport torch.nn as nn\n\nimport transformers\n\n# from src.caption.lavis.common.registry import registry\nfrom src.caption.lavis.models.blip2_models.blip2 import Blip2Base, disabled_train\n\n\n# @registry.register_model(\"blip2_vicuna_instruct\")\nclass Blip2VicunaInstruct(Blip2Base):\n    \"\"\"\n    BLIP2 Vicuna model.\n    Supported model types:\n        - vicuna7b\n        - vicuna13b\n    Usage:\n        >>> from lavis.models import load_model\n        >>> model = load_model(\"blip2_vicuna_instruct\", \"vicuna7b\")\n    \"\"\"\n\n    PRETRAINED_MODEL_CONFIG_DICT = {\n        \"vicuna7b\": \"src/caption/lavis/configs/blip2_instruct_vicuna7b.yaml\",\n        \"vicuna13b\": \"src/caption/lavis/configs/blip2_instruct_vicuna13b.yaml\",\n    }\n\n    def __init__(\n        self,\n        vit_model=\"eva_clip_g\",\n        img_size=224,\n        drop_path_rate=0,\n        use_grad_checkpoint=False,\n        vit_precision=\"fp16\",\n        freeze_vit=True,\n        num_query_token=32,\n        llm_model=\"\",\n        prompt=\"\",\n        max_txt_len=128,\n        max_output_txt_len=256,\n        apply_lemmatizer=False,\n        qformer_text_input=True,\n        vision_only=False,\n    ):\n        super().__init__()\n        transformers_version = version.parse(transformers.__version__)\n        assert transformers_version >= version.parse(\n            \"4.28\"\n        ), \"BLIP-2 Vicuna requires transformers>=4.28\"\n        from transformers import LlamaTokenizer\n        from src.caption.lavis.models.blip2_models.modeling_llama import (\n            LlamaForCausalLM,\n        )\n\n        self.tokenizer = self.init_tokenizer(truncation_side=\"left\")\n\n        self.visual_encoder, self.ln_vision = self.init_vision_encoder(\n            vit_model, img_size, drop_path_rate, use_grad_checkpoint, vit_precision\n        )\n        if freeze_vit:\n            for name, param in self.visual_encoder.named_parameters():\n                param.requires_grad = False\n            self.visual_encoder = self.visual_encoder.eval()\n            self.visual_encoder.train = disabled_train\n            logging.info(\"freeze vision encoder\")\n\n        if vision_only:\n            # Don't initialize Qformer or LLM\n            return\n\n        self.Qformer, self.query_tokens = self.init_Qformer(\n            num_query_token, self.visual_encoder.num_features\n        )\n\n        if not qformer_text_input:\n            self.Qformer.bert.embeddings.word_embeddings = None\n            self.Qformer.bert.embeddings.position_embeddings = None\n            for layer in self.Qformer.bert.encoder.layer:\n                layer.output = None\n                layer.intermediate = None\n        else:\n            self.Qformer.resize_token_embeddings(len(self.tokenizer))\n        self.Qformer.cls = None\n\n        self.llm_tokenizer = LlamaTokenizer.from_pretrained(\n            llm_model, use_fast=False, truncation_side=\"left\"\n        )\n        self.llm_model = LlamaForCausalLM.from_pretrained(\n            llm_model, torch_dtype=torch.float16\n        )\n\n        self.llm_tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n        self.llm_tokenizer.add_special_tokens({\"bos_token\": \"</s>\"})\n        self.llm_tokenizer.add_special_tokens({\"eos_token\": \"</s>\"})\n        self.llm_tokenizer.add_special_tokens({\"unk_token\": \"</s>\"})\n        # self.llm_tokenizer.pad_token = self.llm_tokenizer.unk_token\n\n        self.llm_model.resize_token_embeddings(len(self.llm_tokenizer))\n\n        # self.eos_token_id = self.llm_tokenizer(\n        #     self.llm_tokenizer.eos_token, add_special_tokens=False\n        # ).input_ids[0]\n\n        for name, param in self.llm_model.named_parameters():\n            param.requires_grad = False\n\n        self.llm_proj = nn.Linear(\n            self.Qformer.config.hidden_size, self.llm_model.config.hidden_size\n        )\n\n        self.max_txt_len = max_txt_len\n        self.max_output_txt_len = max_output_txt_len\n        self.prompt = prompt\n        prompt_tokens = self.llm_tokenizer(self.prompt, return_tensors=\"pt\")\n        self.prompt_length = prompt_tokens.attention_mask.sum(1)\n\n        self._lemmatizer = None\n\n        self.qformer_text_input = qformer_text_input\n\n    def concat_text_input_output(self, input_ids, input_atts, output_ids, output_atts):\n        input_part_targets_len = []\n        llm_tokens = {\"input_ids\": [], \"attention_mask\": []}\n        for i in range(input_ids.size(0)):\n            this_input_ones = input_atts[i].sum()\n            input_part_targets_len.append(this_input_ones)\n            llm_tokens[\"input_ids\"].append(\n                torch.cat(\n                    [\n                        input_ids[i][:this_input_ones],\n                        output_ids[i][1:],\n                        input_ids[i][this_input_ones:],\n                    ]\n                )\n            )\n            llm_tokens[\"attention_mask\"].append(\n                torch.cat(\n                    [\n                        input_atts[i][:this_input_ones],\n                        output_atts[i][1:],\n                        input_atts[i][this_input_ones:],\n                    ]\n                )\n            )\n        llm_tokens[\"input_ids\"] = torch.stack(llm_tokens[\"input_ids\"])\n        llm_tokens[\"attention_mask\"] = torch.stack(llm_tokens[\"attention_mask\"])\n        return llm_tokens, input_part_targets_len\n\n    def forward(self, samples):\n        # print('-----------------')\n        # print(samples[\"text_input\"])\n        # print(samples[\"text_output\"])\n        # print('-----------------')\n\n        image = samples[\"image\"]\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n            image.device\n        )\n\n        bs = image.size(0)\n\n        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n        if self.qformer_text_input:\n            text_Qformer = self.tokenizer(\n                samples[\"text_input\"],\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n            query_output = self.Qformer.bert(\n                text_Qformer.input_ids,\n                attention_mask=Qformer_atts,\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        else:\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n\n        inputs_llm = self.llm_proj(\n            query_output.last_hidden_state[:, : query_tokens.size(1), :]\n        )\n        atts_llm = torch.ones(inputs_llm.size()[:-1], dtype=torch.long).to(image.device)\n\n        self.llm_tokenizer.padding_side = \"right\"\n        self.llm_tokenizer.truncation_side = \"left\"\n        text_input_tokens = self.llm_tokenizer(\n            samples[\"text_input\"],\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            truncation=True,\n            max_length=self.max_txt_len,\n        ).to(image.device)\n\n        self.llm_tokenizer.truncation_side = \"right\"\n        text_output_tokens = self.llm_tokenizer(\n            [t + self.llm_tokenizer.eos_token for t in samples[\"text_output\"]],\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            truncation=True,\n            max_length=self.max_output_txt_len,\n        ).to(image.device)\n\n        llm_tokens, input_part_targets_len = self.concat_text_input_output(\n            text_input_tokens.input_ids,\n            text_input_tokens.attention_mask,\n            text_output_tokens.input_ids,\n            text_output_tokens.attention_mask,\n        )\n\n        # do not apply loss to the padding\n        targets = llm_tokens[\"input_ids\"].masked_fill(\n            llm_tokens[\"input_ids\"] == self.llm_tokenizer.pad_token_id, -100\n        )\n\n        # do not apply loss to the text input (i.e., instruction)\n        for i, l in enumerate(input_part_targets_len):\n            targets[i][:l] = -100\n\n        # do not apply loss to the query tokens\n        empty_targets = (\n            torch.ones(atts_llm.size(), dtype=torch.long).to(image.device).fill_(-100)\n        )\n        targets = torch.cat([empty_targets, targets], dim=1)\n\n        inputs_embeds = self.llm_model.get_input_embeddings()(llm_tokens[\"input_ids\"])\n        inputs_embeds = torch.cat([inputs_llm, inputs_embeds], dim=1)\n        attention_mask = torch.cat([atts_llm, llm_tokens[\"attention_mask\"]], dim=1)\n\n        with self.maybe_autocast():\n            outputs = self.llm_model(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                return_dict=True,\n                labels=targets,\n            )\n\n        loss = outputs.loss\n\n        return {\"loss\": loss}\n\n    def get_query_embeddings(self, inputs):\n        bs = 1\n        prompt = inputs[\"prompt\"]\n        image = inputs[\"image\"]\n        query_tokens = self.query_tokens.expand(bs, -1, -1)\n        if self.qformer_text_input:\n            text_Qformer = self.tokenizer(\n                prompt,\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n            image.device\n        )\n\n        if self.qformer_text_input:\n            query_output = self.Qformer.bert(\n                text_Qformer.input_ids,\n                attention_mask=Qformer_atts,\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        else:\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        inputs_llm = self.llm_proj(\n            query_output.last_hidden_state[:, : query_tokens.size(1), :]\n        )\n        return inputs_llm\n\n    def _compute_vision_outputs(self, image, input_ids, Qformer_atts, query_tokens):\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n            image.device\n        )\n\n        if self.qformer_text_input:\n            query_output = self.Qformer.bert(\n                input_ids,\n                attention_mask=Qformer_atts,\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n        else:\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n\n        inputs_llm = self.llm_proj(\n            query_output.last_hidden_state[:, : query_tokens.size(1), :]\n        )\n        atts_llm = torch.ones(inputs_llm.size()[:-1], dtype=torch.long).to(image.device)\n        return inputs_llm, atts_llm\n\n    @torch.no_grad()\n    def generate(\n        self,\n        samples,\n        use_nucleus_sampling=False,\n        num_beams=5,\n        max_length=256,\n        min_length=1,\n        top_p=0.9,\n        repetition_penalty=1.5,\n        length_penalty=1,\n        num_captions=1,\n        temperature=1,\n        return_dict=False,\n        prefix_allowed_tokens_fn=None,\n        pure_llm=False,\n        return_embeds=False,\n        inputs_vision=None,\n        atts_vision=None,\n    ):\n        self.llm_tokenizer.padding_side = \"left\"\n\n        if \"prompt\" in samples.keys():\n            prompt = samples[\"prompt\"]\n        else:\n            prompt = self.prompt\n\n        image = samples[\"image\"]\n\n        bs = image.size(0)\n\n        if isinstance(prompt, str):\n            prompt = [prompt] * bs\n        else:\n            assert (\n                len(prompt) == bs\n            ), \"The number of prompts must be equal to the batch size.\"\n\n        # For TextCaps\n        if \"ocr_tokens\" in samples.keys() and \"{}\" in prompt[0]:\n            prompt = [\n                p.format(\", \".join(samples[\"ocr_tokens\"][i][:30]))\n                for i, p in enumerate(prompt)\n            ]\n\n        query_tokens = self.query_tokens.expand(bs, -1, -1)\n        if self.qformer_text_input:\n            # remove ocr tokens in q_former (for eval textvqa)\n            # qformer_prompt = prompt\n            # qformer_prompt = ['Question: ' + qp.split(' Question: ')[1] for qp in qformer_prompt]\n\n            text_Qformer = self.tokenizer(\n                prompt,\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n        if inputs_vision is None and atts_vision is None:\n            inputs_llm, atts_llm = self._compute_vision_outputs(\n                image, text_Qformer.input_ids, Qformer_atts, query_tokens\n            )\n        else:\n            inputs_llm, atts_llm = inputs_vision, atts_vision\n\n        llm_tokens = self.llm_tokenizer(\n            prompt, padding=\"longest\", return_tensors=\"pt\"\n        ).to(image.device)\n\n        with self.maybe_autocast():\n            inputs_embeds = self.llm_model.get_input_embeddings()(llm_tokens.input_ids)\n\n            if pure_llm:\n                attention_mask = llm_tokens.attention_mask\n            else:\n                inputs_embeds = torch.cat([inputs_llm, inputs_embeds], dim=1)\n                attention_mask = torch.cat([atts_llm, llm_tokens.attention_mask], dim=1)\n\n            outputs = self.llm_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                do_sample=use_nucleus_sampling,\n                top_p=top_p,\n                temperature=temperature,\n                num_beams=num_beams,\n                max_length=max_length,\n                min_length=min_length,\n                # generation_config={ \"output_hidden_states\": True },\n                # eos_token_id=self.eos_token_id,\n                repetition_penalty=repetition_penalty,\n                length_penalty=length_penalty,\n                num_return_sequences=num_captions,\n                return_dict_in_generate=return_dict,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                output_scores=True,\n                output_attentions=True,\n                output_hidden_states=True,\n            )\n\n        tokens = outputs[0] if return_dict else outputs\n\n        tokens[tokens == 0] = 2  # convert output id 0 to 2 (eos_token_id)\n        output_text = self.llm_tokenizer.batch_decode(tokens, skip_special_tokens=True)\n        output_text = [text.strip() for text in output_text]\n\n        if return_embeds:\n            return output_text, inputs_embeds, inputs_llm, atts_llm, outputs\n        if return_dict:\n            return output_text, outputs\n\n        return output_text\n\n    def predict_answers(\n        self,\n        samples,\n        num_beams=5,\n        inference_method=\"generate\",\n        max_len=10,\n        min_len=1,\n        num_ans_candidates=128,\n        answer_list=None,\n        prompt=\"\",\n        length_penalty=0,\n        **kwargs,\n    ):\n        if isinstance(samples[\"text_input\"], str):\n            samples[\"text_input\"] = [samples[\"text_input\"]]\n\n        if prompt:\n            if prompt.count(\"{}\") == 2:\n                if \"ocr_tokens\" in samples:\n                    text_input = [\n                        prompt.format(\n                            \", \".join(samples[\"ocr_tokens\"][i][:30]),\n                            samples[\"text_input\"][i],\n                        )\n                        for i in range(len(samples[\"text_input\"]))\n                    ]\n                elif \"choices\" in samples:\n                    text_input = []\n                    for i in range(len(samples[\"text_input\"])):\n                        this_choices = [\n                            f\"({string.ascii_lowercase[j]}) {ch}\"\n                            for j, ch in enumerate(samples[\"choices\"][i])\n                        ]\n                        this_choices = \" \".join(this_choices)\n                        text_input.append(\n                            prompt.format(samples[\"text_input\"][i], this_choices)\n                        )\n            else:\n                text_input = [\n                    prompt.format(question) for question in samples[\"text_input\"]\n                ]\n        else:\n            text_input = samples[\"text_input\"]\n\n        samples[\"prompt\"] = text_input\n\n        output_text = self.generate(\n            samples,\n            num_beams=num_beams,\n            max_length=max_len,\n            min_length=min_len,\n            length_penalty=length_penalty,\n        )\n\n        if \"apply_lemmatizer\" in samples.keys() and samples[\"apply_lemmatizer\"]:\n            output_text = self._lemmatize(output_text)\n\n        return output_text\n\n    def predict_class(\n        self,\n        samples,\n        candidates,\n        n_segments=1,\n    ):\n        self.llm_tokenizer.padding_side = \"left\"\n\n        # If candidates is a list of lists, each sample has its candidates, then we need to iterate one by one\n        if type(candidates[0]) == list:\n            results = []\n\n            for i in range(samples[\"image\"].size(0)):\n                this_sample = {\n                    \"image\": samples[\"image\"][i].unsqueeze(0),\n                    \"prompt\": samples[\"prompt\"],\n                }\n\n                if \"text_input\" in samples.keys():\n                    this_sample[\"text_input\"] = [samples[\"text_input\"][i]]\n\n                if \"context\" in samples.keys():\n                    this_sample[\"context\"] = [samples[\"context\"][i]]\n\n                if \"history\" in samples.keys():\n                    this_sample[\"history\"] = [samples[\"history\"][i]]\n\n                if \"caption\" in samples.keys():\n                    this_sample[\"caption\"] = [samples[\"caption\"][i]]\n\n                this_result = self._predict_class(\n                    this_sample, candidates[i], n_segments\n                )\n                results.append(this_result)\n\n            try:\n                results = torch.cat(results, dim=0)\n            except:\n                results = [res.tolist()[0] for res in results]\n\n            return results\n\n        return self._predict_class(samples, candidates, n_segments)\n\n    def _predict_class(\n        self,\n        samples,\n        candidates,\n        n_segments=1,\n    ):\n        image = samples[\"image\"]\n        prompt = samples[\"prompt\"]\n\n        bs = image.size(0)\n\n        if isinstance(prompt, str):\n            prompt = [prompt] * bs\n        else:\n            assert (\n                len(prompt) == bs\n            ), \"The number of prompts must be equal to the batch size.\"\n\n        if \"text_input\" in samples.keys():\n            if type(samples[\"text_input\"][0]) == list:\n                prompt = [\n                    prompt[i].format(*samples[\"text_input\"][i])\n                    for i in range(len(prompt))\n                ]\n            else:\n                prompt = [\n                    prompt[i].format(samples[\"text_input\"][i])\n                    for i in range(len(prompt))\n                ]\n\n        # scienceqa\n        if \"context\" in samples.keys() and samples[\"context\"] != \"\":\n            prompt = [\n                f'context: {samples[\"context\"][i]}. {prompt[i]}'\n                for i in range(len(prompt))\n            ]\n\n        # visual dialog\n        if \"history\" in samples.keys() and samples[\"history\"][0] != \"\":\n            prompt = [\n                f'dialog history: {samples[\"history\"][i]}\\n{prompt[i]}'\n                for i in range(len(prompt))\n            ]\n\n        if \"caption\" in samples.keys() and samples[\"caption\"][0] != \"\":\n            prompt = [\n                f'This image has the caption \"{samples[\"caption\"][i]}\". {prompt[i]}'\n                for i in range(len(prompt))\n            ]\n\n        query_tokens = self.query_tokens.expand(bs, -1, -1)\n        if self.qformer_text_input:\n            text_Qformer = self.tokenizer(\n                prompt,\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n                return_tensors=\"pt\",\n            ).to(image.device)\n            query_atts = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n            Qformer_atts = torch.cat([query_atts, text_Qformer.attention_mask], dim=1)\n\n        if image.dim() == 5:\n            inputs_llm, atts_llm = [], []\n            for j in range(image.size(2)):\n                this_frame = image[:, :, j, :, :]\n                with self.maybe_autocast():\n                    frame_embeds = self.ln_vision(self.visual_encoder(this_frame))\n                    frame_atts = torch.ones(\n                        frame_embeds.size()[:-1], dtype=torch.long\n                    ).to(image.device)\n\n                if self.qformer_text_input:\n                    frame_query_output = self.Qformer.bert(\n                        text_Qformer.input_ids,\n                        attention_mask=Qformer_atts,\n                        query_embeds=query_tokens,\n                        encoder_hidden_states=frame_embeds,\n                        encoder_attention_mask=frame_atts,\n                        return_dict=True,\n                    )\n                else:\n                    frame_query_output = self.Qformer.bert(\n                        query_embeds=query_tokens,\n                        encoder_hidden_states=frame_embeds,\n                        encoder_attention_mask=frame_atts,\n                        return_dict=True,\n                    )\n\n                frame_inputs_llm = self.llm_proj(\n                    frame_query_output.last_hidden_state[:, : query_tokens.size(1), :]\n                )\n                frame_atts_llm = torch.ones(\n                    frame_inputs_llm.size()[:-1], dtype=torch.long\n                ).to(image.device)\n                inputs_llm.append(frame_inputs_llm)\n                atts_llm.append(frame_atts_llm)\n            inputs_llm = torch.cat(inputs_llm, dim=1)\n            atts_llm = torch.cat(atts_llm, dim=1)\n        else:\n            with self.maybe_autocast():\n                image_embeds = self.ln_vision(self.visual_encoder(image))\n            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n            if self.qformer_text_input:\n                query_output = self.Qformer.bert(\n                    text_Qformer.input_ids,\n                    attention_mask=Qformer_atts,\n                    query_embeds=query_tokens,\n                    encoder_hidden_states=image_embeds,\n                    encoder_attention_mask=image_atts,\n                    return_dict=True,\n                )\n            else:\n                query_output = self.Qformer.bert(\n                    query_embeds=query_tokens,\n                    encoder_hidden_states=image_embeds,\n                    encoder_attention_mask=image_atts,\n                    return_dict=True,\n                )\n\n            inputs_llm = self.llm_proj(\n                query_output.last_hidden_state[:, : query_tokens.size(1), :]\n            )\n            atts_llm = torch.ones(inputs_llm.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n        self.llm_tokenizer.padding_side = \"right\"\n        self.llm_tokenizer.truncation_side = \"left\"\n        text_input_tokens = self.llm_tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            # truncation=True,\n            # max_length=self.max_txt_len,\n        ).to(image.device)\n\n        empty_targets = (\n            torch.ones(atts_llm.size(), dtype=torch.long).to(image.device).fill_(-100)\n        )\n\n        # self.llm_tokenizer.padding_side = \"right\"\n        self.llm_tokenizer.truncation_side = \"right\"\n        n_cands = len(candidates)\n        with self.maybe_autocast(dtype=torch.bfloat16):\n            all_losses = []\n            for n in range(n_segments):\n                seg_len = n_cands // n_segments\n                if n == (n_segments - 1):\n                    seg_len = n_cands - seg_len * (n_segments - 1)\n\n                start_i = n * (n_cands // n_segments)\n                end_i = start_i + seg_len\n\n                this_output_tokens = self.llm_tokenizer(\n                    candidates[start_i:end_i],\n                    return_tensors=\"pt\",\n                    padding=\"longest\",\n                    # truncation=True,\n                    # max_length=self.max_output_txt_len,\n                ).to(image.device)\n\n                this_input_tokens_ids = text_input_tokens.input_ids.repeat_interleave(\n                    seg_len, dim=0\n                )\n                this_input_tokens_atts = (\n                    text_input_tokens.attention_mask.repeat_interleave(seg_len, dim=0)\n                )\n\n                this_output_tokens_ids = this_output_tokens.input_ids.repeat(bs, 1)\n                this_output_tokens_atts = this_output_tokens.attention_mask.repeat(\n                    bs, 1\n                )\n\n                this_llm_tokens, this_input_targets_len = self.concat_text_input_output(\n                    this_input_tokens_ids,\n                    this_input_tokens_atts,\n                    this_output_tokens_ids,\n                    this_output_tokens_atts,\n                )\n\n                this_llm_input_ids = this_llm_tokens[\"input_ids\"]\n                this_llm_atts = this_llm_tokens[\"attention_mask\"]\n                # this_llm_input_ids = torch.cat([this_input_tokens_ids, this_output_tokens_ids], dim=1)\n                # this_llm_atts = torch.cat([this_input_tokens_atts, this_output_tokens_atts], dim=1)\n\n                inputs_embeds = self.llm_model.get_input_embeddings()(\n                    this_llm_input_ids\n                )\n                inputs_embeds = torch.cat(\n                    [inputs_llm.repeat_interleave(seg_len, dim=0), inputs_embeds], dim=1\n                )\n                attention_mask = torch.cat(\n                    [atts_llm.repeat_interleave(seg_len, dim=0), this_llm_atts], dim=1\n                )\n\n                this_targets = this_llm_input_ids.masked_fill(\n                    this_llm_input_ids == self.llm_tokenizer.pad_token_id, -100\n                )\n                # this_targets[:, :this_input_tokens_ids.size(1)] = -100\n                for i, l in enumerate(this_input_targets_len):\n                    this_targets[i][:l] = -100\n\n                this_targets = torch.cat(\n                    [empty_targets.repeat_interleave(seg_len, dim=0), this_targets],\n                    dim=1,\n                )\n\n                outputs = self.llm_model(\n                    inputs_embeds=inputs_embeds,\n                    attention_mask=attention_mask,\n                    return_dict=True,\n                    labels=this_targets,\n                    reduction=\"none\",\n                )\n\n                loss = outputs.loss\n\n                loss = loss.reshape(bs, seg_len)\n                # output_class_ranks = torch.argsort(loss, dim=-1)\n                all_losses.append(loss)\n\n            all_losses = torch.cat(all_losses, dim=-1)\n            output_class_ranks = torch.argsort(all_losses, dim=-1)\n\n        return output_class_ranks\n\n    def _lemmatize(self, answers):\n        def apply(answer):\n            doc = self.lemmatizer(answer)\n\n            words = []\n            for token in doc:\n                if token.pos_ in [\"NOUN\", \"VERB\"]:\n                    words.append(token.lemma_)\n                else:\n                    words.append(token.text)\n            answer = \" \".join(words)\n\n            return answer\n\n        return [apply(answer) for answer in answers]\n\n    @property\n    def lemmatizer(self):\n        if self._lemmatizer is None:\n            try:\n                import spacy\n\n                self._lemmatizer = spacy.load(\"en_core_web_sm\")\n            except ImportError:\n                logging.error(\n                    \"\"\"\n                    Please install spacy and en_core_web_sm model to apply lemmatization.\n                    python -m spacy download en_core_web_sm\n                    OR\n                    import spacy.cli\n                    spacy.cli.download(\"en_core_web_sm\")\n                    \"\"\"\n                )\n                exit(1)\n\n        return self._lemmatizer\n\n    @classmethod\n    def from_config(cls, cfg):\n        vit_model = cfg.get(\"vit_model\", \"eva_clip_g\")\n        img_size = cfg.get(\"image_size\")\n        num_query_token = cfg.get(\"num_query_token\")\n        llm_model = cfg.get(\"llm_model\")\n\n        drop_path_rate = cfg.get(\"drop_path_rate\", 0)\n        use_grad_checkpoint = cfg.get(\"use_grad_checkpoint\", False)\n        vit_precision = cfg.get(\"vit_precision\", \"fp16\")\n        freeze_vit = cfg.get(\"freeze_vit\", True)\n\n        prompt = cfg.get(\"prompt\", \"\")\n        max_txt_len = cfg.get(\"max_txt_len\", 128)\n        max_output_txt_len = cfg.get(\"max_output_txt_len\", 256)\n\n        apply_lemmatizer = cfg.get(\"apply_lemmatizer\", False)\n\n        qformer_text_input = cfg.get(\"qformer_text_input\", True)\n\n        vision_only = cfg.get(\"vision_only\", False)\n\n        model = cls(\n            vit_model=vit_model,\n            img_size=img_size,\n            drop_path_rate=drop_path_rate,\n            use_grad_checkpoint=use_grad_checkpoint,\n            vit_precision=vit_precision,\n            freeze_vit=freeze_vit,\n            num_query_token=num_query_token,\n            llm_model=llm_model,\n            prompt=prompt,\n            max_txt_len=max_txt_len,\n            max_output_txt_len=max_output_txt_len,\n            apply_lemmatizer=apply_lemmatizer,\n            qformer_text_input=qformer_text_input,\n            vision_only=vision_only,\n        )\n\n        # if qformer_text_input:\n        #     # Hard-coded to load from BLIP-2 stage-1 pre-trained model (not ideal)\n        #     model.load_from_pretrained(\n        #         url_or_filename=\"https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/blip2_pretrained.pth\"\n        #     )\n\n        model.load_checkpoint_from_config(cfg)\n\n        return model\n"}
{"type": "source_file", "path": "src/caption/instruct_blip_engine.py", "content": "from typing import Dict, List, Literal, Optional, Union, Any\nimport logging\n\nimport torch\nimport transformers\nfrom transformers import AutoProcessor, Blip2ForConditionalGeneration\nfrom transformers.image_processing_utils import BatchFeature\n\nfrom packaging import version\nfrom PIL import Image\nimport string\n\nfrom src.caption.base import CaptionEngine\nfrom src.caption.utils import postprocess_caption\nfrom src.utils.pytorch import select_device\n\nfrom src.caption.lavis.models import load_model_and_preprocess\n\n_INSTRUCT_BLIP_DEFAULT_PROMPT = \"Write a detailed description.\"\n# _INSTRUCT_BLIP_DEFAULT_PROMPT = \"Write a short description for the image.\"\n\n\nclass InstructBLIP(CaptionEngine):\n    def __init__(\n        self,\n        model: str = \"instruct-blip/vicuna7b\",\n        device: Optional[str] = None,\n        prompt: str = _INSTRUCT_BLIP_DEFAULT_PROMPT,\n        **kwargs,\n    ):\n        super().__init__()\n        logging.info(f\"Using InstructBLIP model {model}\")\n\n        self.vision_only = kwargs.get(\"vision_only\", False)\n\n        if model == \"instruct-blip/vicuna7b\":\n            model_type = \"vicuna7b\"\n            model_name = \"blip2_vicuna_instruct\"\n            tokenizer = \"self.model.llm_tokenizer\"\n            start_token = 1\n        elif model == \"instruct-blip/vicuna13b\":\n            model_type = \"vicuna13b\"\n            model_name = \"blip2_vicuna_instruct\"\n            tokenizer = \"self.model.llm_tokenizer\"\n            start_token = 1\n        elif model == \"instruct-blip/flant5xl\":\n            model_type = \"flant5xl\"\n            model_name = \"blip2_t5_instruct\"\n            tokenizer = \"self.model.t5_tokenizer\"\n            start_token = 0\n        elif model == \"instruct-blip/flant5xxl\":\n            model_type = \"flant5xxl\"\n            model_name = \"blip2_t5_instruct\"\n            tokenizer = \"self.model.t5_tokenizer\"\n            start_token = 0\n        else:\n            raise ValueError(f\"Unknown InstructBLIP model {model}\")\n\n        model, vis_processors, _ = load_model_and_preprocess(\n            name=model_name,\n            model_type=model_type,\n            is_eval=True,\n            device=device,\n            vision_only=self.vision_only,\n        )\n        self.model = model\n        self.vis_processors = vis_processors\n\n        # if not torch.distributed.is_initialized():\n        #     self.model.to(device or \"cpu\").eval()\n        self.prompt = prompt\n        self.device = device or \"cpu\"\n        self.pure_llm = kwargs.get(\"pure_llm\", False)\n        self.return_embeds = kwargs.get(\"return_embeds\", False)\n        self.stop_token = 29889  # Token for period after sentence\n        self.start_token = start_token  # First token predicted by model, after prompt. Used for getting confidences in self.get_caption_distributions\n\n        if self.vision_only:\n            return\n\n        self.tokenizer = eval(tokenizer)\n\n        self._init_cross_attention()\n\n    def _init_cross_attention(self):\n        \"\"\"Save original cross-attention settings, in case of turning off cross-attention.\"\"\"\n        self.layer_idx2original_cross_attention = {}\n        for idx, layer in enumerate(self.model.Qformer.bert.encoder.layer):\n            self.layer_idx2original_cross_attention[idx] = layer.has_cross_attention\n\n    def processor(self, image, prompt=None):\n        if prompt is None:\n            prompt = self.prompt\n        inputs = {\"image\": self._preprocess_image(image), \"prompt\": prompt}\n        return inputs\n\n    def _preprocess_image(\n        self, raw_image: Image.Image, prompt: Optional[str] = None\n    ) -> torch.Tensor:\n        return self.vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n\n    def get_vision_features(self, inputs) -> torch.Tensor:\n        raise NotImplementedError\n        # breakpoint()\n        # with self.maybe_autocast():\n        #     image_embeds = self.ln_vision(self.visual_encoder(image))\n\n    def get_baseline_caption(\n        self,\n        inputs,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        topp=-1,\n        return_embeds=False,\n        return_tokens=False,\n        output_hidden_states=True,\n    ) -> List[str]:\n\n        out = self.model.generate(  # type: ignore\n            inputs,\n            num_beams=num_beams,\n            temperature=temperature,\n            max_length=max_length,\n            return_embeds=return_embeds,\n            top_p=topp,\n            use_nucleus_sampling=topp > 0,\n        )\n        if return_embeds:\n            baseline_caption, inputs_embeds, inputs_query, atts_query, outputs = out\n        else:\n            baseline_caption = out\n\n        baseline_caption = [postprocess_caption(b.strip()) for b in baseline_caption]\n        print(baseline_caption)\n\n        if return_embeds:\n            return baseline_caption, inputs_embeds, inputs_query, outputs\n        if return_tokens:\n            return baseline_caption, out[1]\n        return baseline_caption\n\n    def get_forced_output_distributions(\n        self,\n        raw_image: Image,\n        encoded_caption: torch.Tensor,\n        vocab_size: int,\n        prompt: Optional[str] = None,\n        language_only: bool = False,\n        pure_llm: bool = False,\n    ) -> torch.Tensor:\n        distributions = (\n            []\n        )  # Will be list of len(encoded_caption shape - 1), each entry vocab_size\n        inputs = self.processor(raw_image, prompt)\n\n        if language_only:\n            self._disable_cross_attention()\n\n        from tqdm import tqdm\n\n        # Initialize vision outputs to None; they will be computed on\n        # first iteration and saved.\n        inputs_vision = None\n        atts_vision = None\n\n        for i in tqdm(range(1, encoded_caption.shape[1])):\n\n            def prefix_allowed_tokens_fn(batch_id, sent):\n                if sent.shape[0] < i:\n                    tokens = encoded_caption[batch_id][sent.shape[0]].tolist()\n                else:\n                    tokens = None\n                return tokens\n\n            gen = self.model.generate(  # type: ignore\n                inputs,\n                num_beams=1,\n                max_length=i + 1,\n                return_dict=True,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                pure_llm=False if not language_only else self.pure_llm,\n                return_embeds=True,\n                inputs_vision=inputs_vision,\n                atts_vision=atts_vision\n            )\n\n            inputs_vision, atts_vision = gen[2], gen[3]\n\n            distributions.append(gen[-1].scores[i - 1][0].detach().cpu())\n\n        if language_only:\n            self._enable_cross_attention()\n\n        return torch.stack(distributions)\n\n    def __call__(\n        self,\n        raw_image: Image.Image,\n        n_captions: int = 1,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        topp=-1,\n        prompt=None,\n        return_embeds=False,\n        generation_type=\"normal\",\n    ) -> List[str]:\n        inputs = self.processor(raw_image, prompt)\n\n        if generation_type == \"normal\":\n            return self.get_baseline_caption(\n                inputs,\n                do_sample=do_sample,\n                num_beams=num_beams,\n                max_length=max_length,\n                temperature=temperature,\n                return_embeds=return_embeds,\n                topp=topp,\n            )\n        elif generation_type == \"iterative\":\n            return self.get_caption_iterative_filtering(\n                inputs,\n                do_sample=do_sample,\n                num_beams=num_beams,\n                max_length=max_length,\n                temperature=temperature,\n                raw_image=raw_image,\n            )\n\n    def _disable_cross_attention(self):\n        \"\"\"\n        Turn off cross-attention in model QFormer layers. Used to obtain a caption conditioned only on language, not the image.\n        Modifies self.model in-place.\n        \"\"\"\n        for layer in self.model.Qformer.bert.encoder.layer:\n            layer.has_cross_attention = False\n\n    def _enable_cross_attention(self):\n        \"\"\"\n        Retores cross-attention in model QFormer layers to the original settings.\n        Modifies self.model in-place.\n        \"\"\"\n        for idx, layer in enumerate(self.model.Qformer.bert.encoder.layer):\n            layer.has_cross_attention = self.layer_idx2original_cross_attention[idx]\n\n    def take_grads(\n        self,\n        caption: str,\n        inputs_query,\n        tokens,\n    ):\n\n        prompt_tokens = self.tokenizer(\n            self.prompt, padding=\"longest\", return_tensors=\"pt\"\n        ).to(\n            inputs_query.device\n        )  # has keys input_ids and attention_mask of 1s\n\n        gradients = []\n        final_inputs_embeds = None\n        for i, token in enumerate(tokens[:-1]):\n            llm_tokens = torch.cat(\n                (prompt_tokens[\"input_ids\"], tokens[: i + 1].unsqueeze(0)), dim=1\n            )\n\n            # input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n\n            inputs_embeds = self.model.llm_model.get_input_embeddings()(llm_tokens)\n            inputs_embeds = torch.cat([inputs_query, inputs_embeds], dim=1)\n            attention_mask = torch.ones(inputs_embeds.size()[:-1], dtype=torch.long).to(\n                inputs_embeds.device\n            )\n            # attention_mask = torch.cat([atts_llm, llm_tokens.attention_mask], dim=1)\n\n            input_ids = None\n\n            model_inputs = self.model.llm_model.prepare_inputs_for_generation(\n                input_ids, inputs_embeds=inputs_embeds, attention_mask=attention_mask\n            )\n            model_inputs[\"inputs_embeds\"].requires_grad = True\n            # model_inputs['inputs_embeds']: shape input_sequence_length x input_token_dimension (e.g., 42 x 4096)\n\n            out = self.model.llm_model(\n                **model_inputs,\n                return_dict=True,\n                output_attentions=True,\n                output_hidden_states=True,\n            )\n\n            next_token_logits = out.logits[0, -1, :]\n            # print(tokens[i+1], torch.topk(next_token_logits, 10)[1])\n\n            N_query = self.model.query_tokens.shape[1]\n            # queries = model_inputs['inputs_embeds'][0, :N_query, :]\n\n            selected_index = tokens[i + 1]\n            grads = torch.autograd.grad(\n                next_token_logits[selected_index], model_inputs[\"inputs_embeds\"]\n            )[0][\n                0\n            ]  # same shape as inputs_embeds\n\n            gradients.append(grads)\n            final_inputs_embeds = model_inputs[\"inputs_embeds\"]\n\n        return gradients, final_inputs_embeds\n\n\nclass InstructBLIPVicuna7B(InstructBLIP):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"instruct-blip/vicuna7b\", device=device, **kwargs)\n\n\nclass InstructBLIPVicuna13B(InstructBLIP):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"instruct-blip/vicuna13b\", device=device, **kwargs)\n\n\nclass InstructBLIPFlanT5XL(InstructBLIP):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"instruct-blip/flant5xl\", device=device, **kwargs)\n\n\nclass InstructBLIPFlanT5XXL(InstructBLIP):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"instruct-blip/flant5xxl\", device=device, **kwargs)\n"}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/__init__.py", "content": ""}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/blip2_opt.py", "content": "\"\"\"\n Copyright (c) 2023, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\nimport logging\nfrom packaging import version\n\nimport torch\nfrom torch.cuda.amp import autocast as autocast\nimport torch.nn as nn\n\n\nfrom src.caption.lavis.models.blip2_models.blip2 import Blip2Base, disabled_train\n\n# from lavis.models.blip2_models.modeling_opt import OPTForCausalLM, OPTConfig\nfrom transformers import AutoTokenizer, OPTForCausalLM, OPTConfig\nimport transformers\n\n\n# @registry.register_model(\"blip2_opt\")\nclass Blip2OPT(Blip2Base):\n    \"\"\"\n    BLIP2 OPT model.\n    Supported model types:\n        - pretrained_opt2.7b: pretrained model with OPT2.7b\n        - pretrained_opt6.7b: pretrained model with OPT6.7b\n        - caption_coco_opt2.7b: fintuned image captioning model with OPT2.7b\n        - caption_coco_opt6.7b: fintuned image captioning model with OPT6.7b\n    Usage:\n        >>> from lavis.models import load_model\n        >>> model = load_model(\"blip2_opt\", \"caption_coco_opt2.7b\")\n    \"\"\"\n\n    PRETRAINED_MODEL_CONFIG_DICT = {\n        \"pretrain_opt2.7b\": \"src/caption/lavis/configs/blip2_pretrain_opt2.7b.yaml\",\n        \"pretrain_opt6.7b\": \"src/caption/lavis/configs/blip2_pretrain_opt6.7b.yaml\",\n        \"caption_coco_opt2.7b\": \"src/caption/lavis/configs/blip2_caption_opt2.7b.yaml\",\n        \"caption_coco_opt6.7b\": \"src/caption/lavis/configs/blip2_caption_opt6.7b.yaml\",\n    }\n\n    def __init__(\n        self,\n        vit_model=\"eva_clip_g\",\n        img_size=224,\n        drop_path_rate=0,\n        use_grad_checkpoint=False,\n        vit_precision=\"fp16\",\n        freeze_vit=True,\n        num_query_token=32,\n        opt_model=\"facebook/opt-2.7b\",\n        prompt=\"\",\n        max_txt_len=32,\n        apply_lemmatizer=False,\n    ):\n        \"\"\"\n        apply_lemmatizer: when set to True, postprocess predict_answers() result with lemmas.\n        \"\"\"\n        super().__init__()\n        transformers_version = version.parse(transformers.__version__)\n        assert transformers_version >= version.parse(\n            \"4.27\"\n        ), \"BLIP-2 OPT requires transformers>=4.27\"\n\n        self.tokenizer = self.init_tokenizer()\n\n        self.visual_encoder, self.ln_vision = self.init_vision_encoder(\n            vit_model, img_size, drop_path_rate, use_grad_checkpoint, vit_precision\n        )\n        if freeze_vit:\n            for name, param in self.visual_encoder.named_parameters():\n                param.requires_grad = False\n            self.visual_encoder = self.visual_encoder.eval()\n            self.visual_encoder.train = disabled_train\n            logging.info(\"freeze vision encoder\")\n\n        self.Qformer, self.query_tokens = self.init_Qformer(\n            num_query_token, self.visual_encoder.num_features\n        )\n        self.Qformer.cls = None\n        self.Qformer.bert.embeddings.word_embeddings = None\n        self.Qformer.bert.embeddings.position_embeddings = None\n        for layer in self.Qformer.bert.encoder.layer:\n            layer.output = None\n            layer.intermediate = None\n\n        self.opt_tokenizer = AutoTokenizer.from_pretrained(opt_model, use_fast=False)\n        self.opt_model = OPTForCausalLM.from_pretrained(\n            opt_model, torch_dtype=torch.float16\n        )\n        for name, param in self.opt_model.named_parameters():\n            param.requires_grad = False\n        self.eos_token_id = self.opt_tokenizer(\n            \"\\n\", add_special_tokens=False\n        ).input_ids[0]\n\n        self.opt_proj = nn.Linear(\n            self.Qformer.config.hidden_size, self.opt_model.config.hidden_size\n        )\n\n        self.max_txt_len = max_txt_len\n        self.prompt = prompt\n        prompt_tokens = self.opt_tokenizer(self.prompt, return_tensors=\"pt\")\n        self.prompt_length = prompt_tokens.attention_mask.sum(1)\n\n        self._apply_lemmatizer = apply_lemmatizer\n        self._lemmatizer = None\n\n    def forward(self, samples):\n        image = samples[\"image\"]\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n            image.device\n        )\n\n        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n        query_output = self.Qformer.bert(\n            query_embeds=query_tokens,\n            encoder_hidden_states=image_embeds,\n            encoder_attention_mask=image_atts,\n            return_dict=True,\n        )\n\n        inputs_opt = self.opt_proj(query_output.last_hidden_state)\n        atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long).to(image.device)\n\n        self.opt_tokenizer.padding_side = \"right\"\n\n        text = [t + \"\\n\" for t in samples[\"text_input\"]]\n\n        opt_tokens = self.opt_tokenizer(\n            text,\n            return_tensors=\"pt\",\n            padding=\"longest\",\n            truncation=True,\n            max_length=self.max_txt_len,\n        ).to(image.device)\n\n        targets = opt_tokens.input_ids.masked_fill(\n            opt_tokens.input_ids == self.opt_tokenizer.pad_token_id, -100\n        )\n        if self.prompt:\n            targets[:, : self.prompt_length] = -100  # do not apply loss to the prompt\n\n        empty_targets = (\n            torch.ones(atts_opt.size(), dtype=torch.long).to(image.device).fill_(-100)\n        )\n        targets = torch.cat([empty_targets, targets], dim=1)\n\n        inputs_embeds = self.opt_model.model.decoder.embed_tokens(opt_tokens.input_ids)\n        inputs_embeds = torch.cat([inputs_opt, inputs_embeds], dim=1)\n        attention_mask = torch.cat([atts_opt, opt_tokens.attention_mask], dim=1)\n\n        with self.maybe_autocast():\n            outputs = self.opt_model(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                return_dict=True,\n                labels=targets,\n            )\n        loss = outputs.loss\n\n        return {\"loss\": loss}\n\n    @torch.no_grad()\n    def generate(\n        self,\n        samples,\n        use_nucleus_sampling=False,\n        num_beams=5,\n        max_length=30,\n        min_length=1,\n        top_p=1.0,\n        repetition_penalty=1.0,\n        length_penalty=1.0,\n        num_captions=1,\n        temperature=1,\n        prefix_allowed_tokens_fn=None,\n        pure_llm=False,\n        return_embeds=False,\n        return_dict=False,\n    ):\n        \"\"\"\n        Args:\n            samples (dict): A dictionary containing the following keys:\n                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n            use_nucleus_sampling (bool): Whether to use nucleus sampling. If False, use top-k sampling.\n            num_beams (int): Number of beams for beam search. 1 means no beam search.\n            max_lbength (int): The maximum length of the sequence to be generated.\n            min_length (int): The minimum length of the sequence to be generated.\n            top_p (float): The cumulative probability for nucleus sampling.\n            repetition_penalty (float): The parameter for repetition penalty. 1.0 means no penalty.\n            num_captions (int): Number of captions to be generated for each image.\n        Returns:\n            captions (list): A list of strings of length batch_size * num_captions.\n        \"\"\"\n\n        image = samples[\"image\"]\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n\n            inputs_opt = self.opt_proj(query_output.last_hidden_state)\n            atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n            if \"prompt\" in samples.keys():\n                prompt = samples[\"prompt\"]\n            else:\n                prompt = self.prompt\n\n            prompt = [prompt] * image.size(0)\n\n            opt_tokens = self.opt_tokenizer(\n                prompt,\n                return_tensors=\"pt\",\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n            ).to(image.device)\n\n            inputs_embeds = self.opt_model.get_input_embeddings()(opt_tokens.input_ids)\n\n            if pure_llm:\n                attention_mask = opt_tokens.attention_mask\n            else:\n                attention_mask = torch.cat([atts_opt, opt_tokens.attention_mask], dim=1)\n                inputs_embeds = torch.cat([inputs_opt, inputs_embeds], dim=1)\n\n            # new version for transformers>=4.27\n            outputs = self.opt_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                do_sample=use_nucleus_sampling,\n                top_p=top_p,\n                temperature=temperature,\n                num_beams=num_beams,\n                max_length=max_length,\n                min_length=min_length,\n                eos_token_id=self.eos_token_id,\n                repetition_penalty=repetition_penalty,\n                length_penalty=length_penalty,\n                num_return_sequences=num_captions,\n                return_dict_in_generate=return_dict,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n                output_scores=True,\n                output_attentions=True,\n            )\n\n            tokens = outputs[0] if return_dict else outputs\n\n            output_text = self.opt_tokenizer.batch_decode(\n                tokens, skip_special_tokens=True\n            )\n\n            # previous version for transformers<4.27\n            # if use_nucleus_sampling:\n            #     query_embeds = inputs_opt.repeat_interleave(num_captions, dim=0)\n            #     num_beams = 1\n            # else:\n            #     query_embeds = inputs_opt.repeat_interleave(num_beams, dim=0)\n\n            # outputs = self.opt_model.generate(\n            #     input_ids=input_ids,\n            #     query_embeds=query_embeds,\n            #     attention_mask=attention_mask,\n            #     do_sample=use_nucleus_sampling,\n            #     top_p=top_p,\n            #     temperature=temperature,\n            #     num_beams=num_beams,\n            #     max_new_tokens=max_length,\n            #     min_length=min_length,\n            #     eos_token_id=self.eos_token_id,\n            #     repetition_penalty=repetition_penalty,\n            #     length_penalty=length_penalty,\n            #     num_return_sequences=num_captions,\n            # )\n\n            # prompt_length = opt_tokens.input_ids.shape[1]\n            # output_text = self.opt_tokenizer.batch_decode(\n            #     outputs[:, prompt_length:], skip_special_tokens=True\n            # )\n\n            output_text = [text.strip() for text in output_text]\n\n            if return_embeds:\n                return output_text, inputs_embeds, inputs_opt, outputs\n            if return_dict:\n                return output_text, outputs\n            return output_text\n\n    def predict_answers(\n        self,\n        samples,\n        num_beams=5,\n        inference_method=\"generate\",\n        max_len=10,\n        min_len=1,\n        num_ans_candidates=128,\n        answer_list=None,\n        prompt=\"\",\n        length_penalty=0,\n        **kwargs\n    ):\n        image = samples[\"image\"]\n        with self.maybe_autocast():\n            image_embeds = self.ln_vision(self.visual_encoder(image))\n            image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n            query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n            query_output = self.Qformer.bert(\n                query_embeds=query_tokens,\n                encoder_hidden_states=image_embeds,\n                encoder_attention_mask=image_atts,\n                return_dict=True,\n            )\n\n            inputs_opt = self.opt_proj(query_output.last_hidden_state)\n            atts_opt = torch.ones(inputs_opt.size()[:-1], dtype=torch.long).to(\n                image.device\n            )\n\n            if isinstance(samples[\"text_input\"], str):\n                samples[\"text_input\"] = [samples[\"text_input\"]]\n            if prompt:\n                text_input = [\n                    prompt.format(question) for question in samples[\"text_input\"]\n                ]\n            else:\n                text_input = samples[\"text_input\"]\n\n            self.opt_tokenizer.padding_side = \"left\"\n            opt_tokens = self.opt_tokenizer(\n                text_input,\n                return_tensors=\"pt\",\n                padding=\"longest\",\n                truncation=True,\n                max_length=self.max_txt_len,\n            ).to(image.device)\n\n            attention_mask = torch.cat([atts_opt, opt_tokens.attention_mask], dim=1)\n\n            # require transformers>=4.27\n            inputs_embeds = self.opt_model.get_input_embeddings()(opt_tokens.input_ids)\n            inputs_embeds = torch.cat([inputs_opt, inputs_embeds], dim=1)\n\n            outputs = self.opt_model.generate(\n                inputs_embeds=inputs_embeds,\n                attention_mask=attention_mask,\n                do_sample=False,\n                num_beams=num_beams,\n                max_new_tokens=max_len,\n                min_length=min_len,\n                eos_token_id=self.eos_token_id,\n                length_penalty=length_penalty,\n            )\n            output_text = self.opt_tokenizer.batch_decode(\n                outputs, skip_special_tokens=True\n            )\n            output_text = [text.strip() for text in output_text]\n        if self._apply_lemmatizer or (\n            \"apply_lemmatizer\" in samples.keys() and samples[\"apply_lemmatizer\"]\n        ):\n            output_text = self._lemmatize(output_text)\n\n        return output_text\n\n    def _lemmatize(self, answers):\n        def apply(answer):\n            doc = self.lemmatizer(answer)\n\n            words = []\n            for token in doc:\n                if token.pos_ in [\"NOUN\", \"VERB\"]:\n                    words.append(token.lemma_)\n                else:\n                    words.append(token.text)\n            answer = \" \".join(words)\n\n            return answer\n\n        return [apply(answer) for answer in answers]\n\n    @property\n    def lemmatizer(self):\n        if self._lemmatizer is None:\n            try:\n                import spacy\n\n                self._lemmatizer = spacy.load(\"en_core_web_sm\")\n            except ImportError:\n                logging.error(\n                    \"\"\"\n                    Please install spacy and en_core_web_sm model to apply lemmatization.\n                    python -m spacy download en_core_web_sm\n                    OR\n                    import spacy.cli\n                    spacy.cli.download(\"en_core_web_sm\")\n                    \"\"\"\n                )\n                exit(1)\n\n        return self._lemmatizer\n\n    @classmethod\n    def from_config(cls, cfg):\n        vit_model = cfg.get(\"vit_model\", \"eva_clip_g\")\n        img_size = cfg.get(\"image_size\")\n        num_query_token = cfg.get(\"num_query_token\")\n        opt_model = cfg.get(\"opt_model\")\n\n        drop_path_rate = cfg.get(\"drop_path_rate\", 0)\n        use_grad_checkpoint = cfg.get(\"use_grad_checkpoint\", False)\n        vit_precision = cfg.get(\"vit_precision\", \"fp16\")\n        freeze_vit = cfg.get(\"freeze_vit\", True)\n\n        prompt = cfg.get(\"prompt\", \"\")\n        max_txt_len = cfg.get(\"max_txt_len\", 32)\n\n        apply_lemmatizer = cfg.get(\"apply_lemmatizer\", False)\n\n        model = cls(\n            vit_model=vit_model,\n            img_size=img_size,\n            drop_path_rate=drop_path_rate,\n            use_grad_checkpoint=use_grad_checkpoint,\n            vit_precision=vit_precision,\n            freeze_vit=freeze_vit,\n            num_query_token=num_query_token,\n            opt_model=opt_model,\n            prompt=prompt,\n            max_txt_len=max_txt_len,\n            apply_lemmatizer=apply_lemmatizer,\n        )\n        model.load_checkpoint_from_config(cfg)\n\n        return model\n"}
{"type": "source_file", "path": "src/caption/lavis/processors/randaugment.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport cv2\nimport numpy as np\n\nimport torch\n\n\n## aug functions\ndef identity_func(img):\n    return img\n\n\ndef autocontrast_func(img, cutoff=0):\n    \"\"\"\n    same output as PIL.ImageOps.autocontrast\n    \"\"\"\n    n_bins = 256\n\n    def tune_channel(ch):\n        n = ch.size\n        cut = cutoff * n // 100\n        if cut == 0:\n            high, low = ch.max(), ch.min()\n        else:\n            hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n            low = np.argwhere(np.cumsum(hist) > cut)\n            low = 0 if low.shape[0] == 0 else low[0]\n            high = np.argwhere(np.cumsum(hist[::-1]) > cut)\n            high = n_bins - 1 if high.shape[0] == 0 else n_bins - 1 - high[0]\n        if high <= low:\n            table = np.arange(n_bins)\n        else:\n            scale = (n_bins - 1) / (high - low)\n            offset = -low * scale\n            table = np.arange(n_bins) * scale + offset\n            table[table < 0] = 0\n            table[table > n_bins - 1] = n_bins - 1\n        table = table.clip(0, 255).astype(np.uint8)\n        return table[ch]\n\n    channels = [tune_channel(ch) for ch in cv2.split(img)]\n    out = cv2.merge(channels)\n    return out\n\n\ndef equalize_func(img):\n    \"\"\"\n    same output as PIL.ImageOps.equalize\n    PIL's implementation is different from cv2.equalize\n    \"\"\"\n    n_bins = 256\n\n    def tune_channel(ch):\n        hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n        non_zero_hist = hist[hist != 0].reshape(-1)\n        step = np.sum(non_zero_hist[:-1]) // (n_bins - 1)\n        if step == 0:\n            return ch\n        n = np.empty_like(hist)\n        n[0] = step // 2\n        n[1:] = hist[:-1]\n        table = (np.cumsum(n) // step).clip(0, 255).astype(np.uint8)\n        return table[ch]\n\n    channels = [tune_channel(ch) for ch in cv2.split(img)]\n    out = cv2.merge(channels)\n    return out\n\n\ndef rotate_func(img, degree, fill=(0, 0, 0)):\n    \"\"\"\n    like PIL, rotate by degree, not radians\n    \"\"\"\n    H, W = img.shape[0], img.shape[1]\n    center = W / 2, H / 2\n    M = cv2.getRotationMatrix2D(center, degree, 1)\n    out = cv2.warpAffine(img, M, (W, H), borderValue=fill)\n    return out\n\n\ndef solarize_func(img, thresh=128):\n    \"\"\"\n    same output as PIL.ImageOps.posterize\n    \"\"\"\n    table = np.array([el if el < thresh else 255 - el for el in range(256)])\n    table = table.clip(0, 255).astype(np.uint8)\n    out = table[img]\n    return out\n\n\ndef color_func(img, factor):\n    \"\"\"\n    same output as PIL.ImageEnhance.Color\n    \"\"\"\n    ## implementation according to PIL definition, quite slow\n    #  degenerate = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[:, :, np.newaxis]\n    #  out = blend(degenerate, img, factor)\n    #  M = (\n    #      np.eye(3) * factor\n    #      + np.float32([0.114, 0.587, 0.299]).reshape(3, 1) * (1. - factor)\n    #  )[np.newaxis, np.newaxis, :]\n    M = np.float32(\n        [[0.886, -0.114, -0.114], [-0.587, 0.413, -0.587], [-0.299, -0.299, 0.701]]\n    ) * factor + np.float32([[0.114], [0.587], [0.299]])\n    out = np.matmul(img, M).clip(0, 255).astype(np.uint8)\n    return out\n\n\ndef contrast_func(img, factor):\n    \"\"\"\n    same output as PIL.ImageEnhance.Contrast\n    \"\"\"\n    mean = np.sum(np.mean(img, axis=(0, 1)) * np.array([0.114, 0.587, 0.299]))\n    table = (\n        np.array([(el - mean) * factor + mean for el in range(256)])\n        .clip(0, 255)\n        .astype(np.uint8)\n    )\n    out = table[img]\n    return out\n\n\ndef brightness_func(img, factor):\n    \"\"\"\n    same output as PIL.ImageEnhance.Contrast\n    \"\"\"\n    table = (np.arange(256, dtype=np.float32) * factor).clip(0, 255).astype(np.uint8)\n    out = table[img]\n    return out\n\n\ndef sharpness_func(img, factor):\n    \"\"\"\n    The differences the this result and PIL are all on the 4 boundaries, the center\n    areas are same\n    \"\"\"\n    kernel = np.ones((3, 3), dtype=np.float32)\n    kernel[1][1] = 5\n    kernel /= 13\n    degenerate = cv2.filter2D(img, -1, kernel)\n    if factor == 0.0:\n        out = degenerate\n    elif factor == 1.0:\n        out = img\n    else:\n        out = img.astype(np.float32)\n        degenerate = degenerate.astype(np.float32)[1:-1, 1:-1, :]\n        out[1:-1, 1:-1, :] = degenerate + factor * (out[1:-1, 1:-1, :] - degenerate)\n        out = out.astype(np.uint8)\n    return out\n\n\ndef shear_x_func(img, factor, fill=(0, 0, 0)):\n    H, W = img.shape[0], img.shape[1]\n    M = np.float32([[1, factor, 0], [0, 1, 0]])\n    out = cv2.warpAffine(\n        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n    ).astype(np.uint8)\n    return out\n\n\ndef translate_x_func(img, offset, fill=(0, 0, 0)):\n    \"\"\"\n    same output as PIL.Image.transform\n    \"\"\"\n    H, W = img.shape[0], img.shape[1]\n    M = np.float32([[1, 0, -offset], [0, 1, 0]])\n    out = cv2.warpAffine(\n        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n    ).astype(np.uint8)\n    return out\n\n\ndef translate_y_func(img, offset, fill=(0, 0, 0)):\n    \"\"\"\n    same output as PIL.Image.transform\n    \"\"\"\n    H, W = img.shape[0], img.shape[1]\n    M = np.float32([[1, 0, 0], [0, 1, -offset]])\n    out = cv2.warpAffine(\n        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n    ).astype(np.uint8)\n    return out\n\n\ndef posterize_func(img, bits):\n    \"\"\"\n    same output as PIL.ImageOps.posterize\n    \"\"\"\n    out = np.bitwise_and(img, np.uint8(255 << (8 - bits)))\n    return out\n\n\ndef shear_y_func(img, factor, fill=(0, 0, 0)):\n    H, W = img.shape[0], img.shape[1]\n    M = np.float32([[1, 0, 0], [factor, 1, 0]])\n    out = cv2.warpAffine(\n        img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR\n    ).astype(np.uint8)\n    return out\n\n\ndef cutout_func(img, pad_size, replace=(0, 0, 0)):\n    replace = np.array(replace, dtype=np.uint8)\n    H, W = img.shape[0], img.shape[1]\n    rh, rw = np.random.random(2)\n    pad_size = pad_size // 2\n    ch, cw = int(rh * H), int(rw * W)\n    x1, x2 = max(ch - pad_size, 0), min(ch + pad_size, H)\n    y1, y2 = max(cw - pad_size, 0), min(cw + pad_size, W)\n    out = img.copy()\n    out[x1:x2, y1:y2, :] = replace\n    return out\n\n\n### level to args\ndef enhance_level_to_args(MAX_LEVEL):\n    def level_to_args(level):\n        return ((level / MAX_LEVEL) * 1.8 + 0.1,)\n\n    return level_to_args\n\n\ndef shear_level_to_args(MAX_LEVEL, replace_value):\n    def level_to_args(level):\n        level = (level / MAX_LEVEL) * 0.3\n        if np.random.random() > 0.5:\n            level = -level\n        return (level, replace_value)\n\n    return level_to_args\n\n\ndef translate_level_to_args(translate_const, MAX_LEVEL, replace_value):\n    def level_to_args(level):\n        level = (level / MAX_LEVEL) * float(translate_const)\n        if np.random.random() > 0.5:\n            level = -level\n        return (level, replace_value)\n\n    return level_to_args\n\n\ndef cutout_level_to_args(cutout_const, MAX_LEVEL, replace_value):\n    def level_to_args(level):\n        level = int((level / MAX_LEVEL) * cutout_const)\n        return (level, replace_value)\n\n    return level_to_args\n\n\ndef solarize_level_to_args(MAX_LEVEL):\n    def level_to_args(level):\n        level = int((level / MAX_LEVEL) * 256)\n        return (level,)\n\n    return level_to_args\n\n\ndef none_level_to_args(level):\n    return ()\n\n\ndef posterize_level_to_args(MAX_LEVEL):\n    def level_to_args(level):\n        level = int((level / MAX_LEVEL) * 4)\n        return (level,)\n\n    return level_to_args\n\n\ndef rotate_level_to_args(MAX_LEVEL, replace_value):\n    def level_to_args(level):\n        level = (level / MAX_LEVEL) * 30\n        if np.random.random() < 0.5:\n            level = -level\n        return (level, replace_value)\n\n    return level_to_args\n\n\nfunc_dict = {\n    \"Identity\": identity_func,\n    \"AutoContrast\": autocontrast_func,\n    \"Equalize\": equalize_func,\n    \"Rotate\": rotate_func,\n    \"Solarize\": solarize_func,\n    \"Color\": color_func,\n    \"Contrast\": contrast_func,\n    \"Brightness\": brightness_func,\n    \"Sharpness\": sharpness_func,\n    \"ShearX\": shear_x_func,\n    \"TranslateX\": translate_x_func,\n    \"TranslateY\": translate_y_func,\n    \"Posterize\": posterize_func,\n    \"ShearY\": shear_y_func,\n}\n\ntranslate_const = 10\nMAX_LEVEL = 10\nreplace_value = (128, 128, 128)\narg_dict = {\n    \"Identity\": none_level_to_args,\n    \"AutoContrast\": none_level_to_args,\n    \"Equalize\": none_level_to_args,\n    \"Rotate\": rotate_level_to_args(MAX_LEVEL, replace_value),\n    \"Solarize\": solarize_level_to_args(MAX_LEVEL),\n    \"Color\": enhance_level_to_args(MAX_LEVEL),\n    \"Contrast\": enhance_level_to_args(MAX_LEVEL),\n    \"Brightness\": enhance_level_to_args(MAX_LEVEL),\n    \"Sharpness\": enhance_level_to_args(MAX_LEVEL),\n    \"ShearX\": shear_level_to_args(MAX_LEVEL, replace_value),\n    \"TranslateX\": translate_level_to_args(translate_const, MAX_LEVEL, replace_value),\n    \"TranslateY\": translate_level_to_args(translate_const, MAX_LEVEL, replace_value),\n    \"Posterize\": posterize_level_to_args(MAX_LEVEL),\n    \"ShearY\": shear_level_to_args(MAX_LEVEL, replace_value),\n}\n\n\nclass RandomAugment(object):\n    def __init__(self, N=2, M=10, isPIL=False, augs=[]):\n        self.N = N\n        self.M = M\n        self.isPIL = isPIL\n        if augs:\n            self.augs = augs\n        else:\n            self.augs = list(arg_dict.keys())\n\n    def get_random_ops(self):\n        sampled_ops = np.random.choice(self.augs, self.N)\n        return [(op, 0.5, self.M) for op in sampled_ops]\n\n    def __call__(self, img):\n        if self.isPIL:\n            img = np.array(img)\n        ops = self.get_random_ops()\n        for name, prob, level in ops:\n            if np.random.random() > prob:\n                continue\n            args = arg_dict[name](level)\n            img = func_dict[name](img, *args)\n        return img\n\n\nclass VideoRandomAugment(object):\n    def __init__(self, N=2, M=10, p=0.0, tensor_in_tensor_out=True, augs=[]):\n        self.N = N\n        self.M = M\n        self.p = p\n        self.tensor_in_tensor_out = tensor_in_tensor_out\n        if augs:\n            self.augs = augs\n        else:\n            self.augs = list(arg_dict.keys())\n\n    def get_random_ops(self):\n        sampled_ops = np.random.choice(self.augs, self.N, replace=False)\n        return [(op, self.M) for op in sampled_ops]\n\n    def __call__(self, frames):\n        assert (\n            frames.shape[-1] == 3\n        ), \"Expecting last dimension for 3-channels RGB (b, h, w, c).\"\n\n        if self.tensor_in_tensor_out:\n            frames = frames.numpy().astype(np.uint8)\n\n        num_frames = frames.shape[0]\n\n        ops = num_frames * [self.get_random_ops()]\n        apply_or_not = num_frames * [np.random.random(size=self.N) > self.p]\n\n        frames = torch.stack(\n            list(map(self._aug, frames, ops, apply_or_not)), dim=0\n        ).float()\n\n        return frames\n\n    def _aug(self, img, ops, apply_or_not):\n        for i, (name, level) in enumerate(ops):\n            if not apply_or_not[i]:\n                continue\n            args = arg_dict[name](level)\n            img = func_dict[name](img, *args)\n        return torch.from_numpy(img)\n\n\nif __name__ == \"__main__\":\n    a = RandomAugment()\n    img = np.random.randn(32, 32, 3)\n    a(img)\n"}
{"type": "source_file", "path": "src/caption/llava_engine.py", "content": "from typing import Dict, List, Literal, Optional, Union, Any\nimport logging\nfrom PIL import Image\nimport numpy as np\nimport torch\n\nfrom src.caption.base import CaptionEngine\n\nfrom src.caption.llava.LLaVA.llava.constants import (\n    IMAGE_TOKEN_INDEX,\n    DEFAULT_IMAGE_TOKEN,\n    DEFAULT_IM_START_TOKEN,\n    DEFAULT_IM_END_TOKEN,\n)\nfrom src.caption.llava.LLaVA.llava.mm_utils import (\n    get_model_name_from_path,\n    process_images,\n    tokenizer_image_token,\n    KeywordsStoppingCriteria,\n)\nfrom src.caption.llava.LLaVA.llava.conversation import conv_templates, SeparatorStyle\nfrom src.caption.llava.LLaVA.llava.model.builder import load_pretrained_model\n\n\n_LLAVA_DEFAULT_PROMPT = \"Provide a detailed description of the given image.\"\n\n\nclass NameSpace:\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n\nclass LLaVA(CaptionEngine):\n    def __init__(\n        self,\n        model: str = \"llava-7b\",\n        device: Optional[str] = None,\n        prompt: str = _LLAVA_DEFAULT_PROMPT,\n        **kwargs,\n    ):\n        super().__init__()\n        logging.info(f\"Using LLaVA model {model}\")\n\n        # Hard-coded model dirs for now\n        if \"7b\" in model:\n            # model_dir = \"/checkpoint/spetryk/llm/llava/llava-v1.5-7b\"\n            model_dir = \"/shared/spetryk/large_model_checkpoints/lmm/llava-v1.5-7b/\"\n        elif \"13b\" in model:\n            # model_dir = \"/checkpoint/spetryk/llm/llava/llava-v1.5-13b\"\n            model_dir = \"/shared/spetryk/large_model_checkpoints/lmm/llava-v1.5-13b/\"\n        else:\n            raise ValueError(f\"Unknown model {model}\")\n\n        self.model_name = get_model_name_from_path(model_dir)\n        tokenizer, model, image_processor, context_len = load_pretrained_model(\n            model_path=model_dir, model_base=None, model_name=self.model_name\n        )\n        self.model = model\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.context_len = context_len\n        image_aspect_ratio = \"pad\"  # Default from llava\n        self.image_processor_config = NameSpace(image_aspect_ratio=image_aspect_ratio)\n        self.device = device\n\n        if \"llama-2\" in self.model_name.lower():\n            conv_mode = \"llava_llama_2\"\n        elif \"v1\" in self.model_name.lower():\n            conv_mode = \"llava_v1\"\n        # elif \"mpt\" in self.model_name.lower():\n        #     conv_mode = \"mpt\"\n        else:\n            conv_mode = \"llava_v0\"\n        self.conv_mode = conv_mode\n\n    def generate(\n        self,\n        inputs,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        topp=-1,\n        prefix_allowed_tokens_fn=None,\n    ):\n        \"\"\"\n        inputs: dictionary with keys \"input_ids\", \"stopping_criteria\", and optionally \"image_tensor\" if using image input\n        \"\"\"\n        input_ids = inputs[\"input_ids\"]\n\n        with torch.inference_mode():\n            outputs = self.model.generate(\n                input_ids,\n                images=inputs.get(\"image_tensor\", None),\n                do_sample=do_sample,\n                num_beams=num_beams,\n                temperature=temperature,\n                max_new_tokens=max_length,\n                use_cache=False,\n                stopping_criteria=[inputs[\"stopping_criteria\"]],\n                return_dict_in_generate=True,\n                output_scores=True,\n                top_k=None,\n                top_p=topp,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            )\n        output_ids = outputs[0]\n        # output_ids = outputs.sequences\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (\n            (input_ids != output_ids[:, :input_token_len]).sum().item()\n        )\n        if n_diff_input_output > 0:\n            print(\n                f\"[Warning] {n_diff_input_output} output_ids are not the same as the input_ids\"\n            )\n        out_ids = output_ids[:, input_token_len:]\n        return outputs, out_ids\n\n    def get_baseline_caption(\n        self,\n        inputs,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        topp=-1,\n        return_embeds=False,\n    ) -> List[str]:\n        \"\"\"\n        inputs: dictionary with keys \"input_ids\", \"stopping_criteria\", and optionally \"image_tensor\" if using image input\n        \"\"\"\n\n        outputs = self.generate(\n            inputs,\n            do_sample=do_sample,\n            num_beams=num_beams,\n            max_length=max_length,\n            temperature=temperature,\n            topp=topp,\n        )\n        output_ids = outputs[1]\n        # output_ids = outputs.sequences\n\n        caption = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n        token_ids = output_ids[0][:-1]  # remove EOS\n        token_ids = [token_ids]\n        logging.info(caption)\n\n        if return_embeds:\n            return caption, token_ids\n        return caption\n\n    def _preprocess_image(\n        self, raw_image: Image.Image, prompt: Optional[str] = None\n    ) -> torch.Tensor:\n        image_tensor = process_images(\n            [raw_image], self.image_processor, self.image_processor_config\n        )\n        if type(image_tensor) is list:\n            image_tensor = [\n                image.to(self.model.device, dtype=torch.float16)\n                for image in image_tensor\n            ]\n        else:\n            image_tensor = image_tensor.to(self.model.device, dtype=torch.float16)\n        return image_tensor\n\n    def processor(self, image: Image.Image, prompt=None, add_image=True):\n        if add_image:\n            image_tensor = self._preprocess_image(image)\n        else:\n            image_tensor = None\n        conv = conv_templates[self.conv_mode].copy()\n        qs = prompt if prompt is not None else _LLAVA_DEFAULT_PROMPT\n        if add_image:\n            if self.model.config.mm_use_im_start_end:\n                qs = (\n                    DEFAULT_IM_START_TOKEN\n                    + DEFAULT_IMAGE_TOKEN\n                    + DEFAULT_IM_END_TOKEN\n                    + \"\\n\"\n                    + qs\n                )\n            else:\n                qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n\n        conv = conv_templates[self.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        conv.append_message(conv.roles[1], None)\n        new_prompt = conv.get_prompt()\n\n        input_ids = (\n            tokenizer_image_token(\n                new_prompt, self.tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\"\n            )\n            .unsqueeze(0)\n            .cuda()\n        )\n\n        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2\n        keywords = [stop_str]\n        stopping_criteria = KeywordsStoppingCriteria(\n            keywords, self.tokenizer, input_ids\n        )\n\n        inputs = {\n            \"input_ids\": input_ids,\n            \"image_tensor\": image_tensor,\n            \"stopping_criteria\": stopping_criteria,\n        }\n\n        return inputs\n\n    def __call__(\n        self,\n        raw_image: Image.Image,\n        n_captions: int = 1,\n        do_sample=False,\n        num_beams=16,\n        max_length=256,\n        temperature=1.0,\n        topp=-1,\n        prompt=None,\n        return_embeds=False,\n        generation_type=\"normal\",\n    ) -> List[str]:\n\n        inputs = self.processor(raw_image, prompt)\n\n        return self.get_baseline_caption(\n            inputs,\n            do_sample=do_sample,\n            num_beams=num_beams,\n            max_length=max_length,\n            temperature=temperature,\n            return_embeds=return_embeds,\n            topp=topp,\n        )\n\n    def get_forced_output_distributions(\n        self,\n        raw_image: Image,\n        encoded_caption: torch.Tensor,\n        vocab_size: int,\n        prompt: Optional[str] = None,\n        language_only: bool = False,\n        pure_llm: bool = False,\n    ) -> torch.Tensor:\n\n        distributions = (\n            []\n        )  # Will be list of len(encoded_caption shape - 1), each entry vocab_size\n\n        inputs = self.processor(raw_image, prompt=prompt, add_image=not language_only)\n\n        if language_only:\n            inputs.pop(\"image_tensor\")\n\n        from tqdm import tqdm\n\n        N_input_ids = inputs[\"input_ids\"].shape[1]\n\n        for i in tqdm(range(0, encoded_caption.shape[1])):\n\n            def prefix_allowed_tokens_fn(batch_id, sent):\n                # diff_idx = (sent.shape[0] - N_input_ids) + 1\n                diff_idx = sent.shape[0] - N_input_ids\n                if diff_idx < i:\n                    tokens = encoded_caption[batch_id][diff_idx].tolist()\n                else:\n                    tokens = None\n                return tokens\n\n            gen, gen_ids = self.generate(  # type: ignore\n                inputs,\n                num_beams=1,\n                max_length=i + 1,\n                prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n            )\n\n            distributions.append(gen[1][i][0].detach().cpu())\n\n        return torch.stack(distributions)\n\n\nclass LLaVA7B(LLaVA):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"llava-7b\", device=device, **kwargs)\n\n\nclass LLaVA13B(LLaVA):\n    def __init__(self, device: Optional[str] = None, **kwargs):\n        super().__init__(model=\"llava-13b\", device=device, **kwargs)\n"}
{"type": "source_file", "path": "src/caption/lavis/models/base_model.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport logging\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom omegaconf import OmegaConf\n\nfrom src.caption.lavis.common.dist_utils import (\n    download_cached_file,\n    is_dist_avail_and_initialized,\n)\nfrom src.caption.lavis.common.utils import get_abs_path, is_url\n\n\nclass BaseModel(nn.Module):\n    \"\"\"Base class for models.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    @property\n    def device(self):\n        return list(self.parameters())[0].device\n\n    def load_checkpoint(self, url_or_filename):\n        \"\"\"\n        Load from a finetuned checkpoint.\n\n        This should expect no mismatch in the model keys and the checkpoint keys.\n        \"\"\"\n\n        if is_url(url_or_filename):\n            cached_file = download_cached_file(\n                url_or_filename, check_hash=False, progress=True\n            )\n            checkpoint = torch.load(cached_file, map_location=\"cpu\")\n        elif os.path.isfile(url_or_filename):\n            checkpoint = torch.load(url_or_filename, map_location=\"cpu\")\n        else:\n            raise RuntimeError(\"checkpoint url or path is invalid\")\n\n        if \"model\" in checkpoint.keys():\n            state_dict = checkpoint[\"model\"]\n        else:\n            state_dict = checkpoint\n\n        msg = self.load_state_dict(state_dict, strict=False)\n\n        # Gives false alarm for LLM keys loaded in earlier step.\n        # logging.info(\"Missing keys {}\".format(msg.missing_keys))\n        logging.info(\"load checkpoint from %s\" % url_or_filename)\n\n        return msg\n\n    @classmethod\n    def from_pretrained(cls, model_type, vision_only=False):\n        \"\"\"\n        Build a pretrained model from default configuration file, specified by model_type.\n\n        Args:\n            - model_type (str): model type, specifying architecture and checkpoints.\n\n        Returns:\n            - model (nn.Module): pretrained or finetuned model, depending on the configuration.\n        \"\"\"\n        model_cfg = OmegaConf.load(cls.default_config_path(model_type)).model\n\n        if vision_only and \"blip2\" not in model_type:\n            model_cfg[\"vision_only\"] = True\n\n        model = cls.from_config(model_cfg)\n\n        return model\n\n    @classmethod\n    def default_config_path(cls, model_type):\n        assert (\n            model_type in cls.PRETRAINED_MODEL_CONFIG_DICT\n        ), \"Unknown model type {}\".format(model_type)\n        return get_abs_path(cls.PRETRAINED_MODEL_CONFIG_DICT[model_type])\n\n    def load_checkpoint_from_config(self, cfg, **kwargs):\n        \"\"\"\n        Load checkpoint as specified in the config file.\n\n        If load_finetuned is True, load the finetuned model; otherwise, load the pretrained model.\n        When loading the pretrained model, each task-specific architecture may define their\n        own load_from_pretrained() method.\n        \"\"\"\n        load_finetuned = cfg.get(\"load_finetuned\", True)\n        if load_finetuned:\n            finetune_path = cfg.get(\"finetuned\", None)\n            assert (\n                finetune_path is not None\n            ), \"Found load_finetuned is True, but finetune_path is None.\"\n            self.load_checkpoint(url_or_filename=finetune_path)\n        else:\n            load_pretrained = cfg.get(\"load_pretrained\", True)\n            if load_pretrained:\n                # load pre-trained weights\n                pretrain_path = cfg.get(\"pretrained\", None)\n                assert \"Found load_finetuned is False, but pretrain_path is None.\"\n                self.load_from_pretrained(url_or_filename=pretrain_path, **kwargs)\n\n    def get_optimizer_params(self, weight_decay, lr_scale=1):\n        p_wd, p_non_wd = [], []\n        for n, p in self.named_parameters():\n            if not p.requires_grad:\n                continue  # frozen weights\n            if p.ndim < 2 or \"bias\" in n or \"ln\" in n or \"bn\" in n:\n                p_non_wd.append(p)\n            else:\n                p_wd.append(p)\n        optim_params = [\n            {\"params\": p_wd, \"weight_decay\": weight_decay, \"lr_scale\": lr_scale},\n            {\"params\": p_non_wd, \"weight_decay\": 0, \"lr_scale\": lr_scale},\n        ]\n        return optim_params\n\n    def before_evaluation(self, **kwargs):\n        pass\n\n    def show_n_params(self, return_str=True):\n        tot = 0\n        for p in self.parameters():\n            w = 1\n            for x in p.shape:\n                w *= x\n            tot += w\n        if return_str:\n            if tot >= 1e6:\n                return \"{:.1f}M\".format(tot / 1e6)\n            else:\n                return \"{:.1f}K\".format(tot / 1e3)\n        else:\n            return tot\n\n\nclass BaseEncoder(nn.Module):\n    \"\"\"\n    Base class for primitive encoders, such as ViT, TimeSformer, etc.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n\n    def forward_features(self, samples, **kwargs):\n        raise NotImplementedError\n\n    @property\n    def device(self):\n        return list(self.parameters())[0].device\n\n\nclass SharedQueueMixin:\n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, image_feat, text_feat, idxs=None):\n        # gather keys before updating queue\n        image_feats = concat_all_gather(image_feat)\n        text_feats = concat_all_gather(text_feat)\n\n        batch_size = image_feats.shape[0]\n\n        ptr = int(self.queue_ptr)\n        assert self.queue_size % batch_size == 0  # for simplicity\n\n        # replace the keys at ptr (dequeue and enqueue)\n        self.image_queue[:, ptr : ptr + batch_size] = image_feats.T\n        self.text_queue[:, ptr : ptr + batch_size] = text_feats.T\n\n        if idxs is not None:\n            idxs = concat_all_gather(idxs)\n            self.idx_queue[:, ptr : ptr + batch_size] = idxs.T\n\n        ptr = (ptr + batch_size) % self.queue_size  # move pointer\n        self.queue_ptr[0] = ptr\n\n\nclass MomentumDistilationMixin:\n    @torch.no_grad()\n    def copy_params(self):\n        for model_pair in self.model_pairs:\n            for param, param_m in zip(\n                model_pair[0].parameters(), model_pair[1].parameters()\n            ):\n                param_m.data.copy_(param.data)  # initialize\n                param_m.requires_grad = False  # not update by gradient\n\n    @torch.no_grad()\n    def _momentum_update(self):\n        for model_pair in self.model_pairs:\n            for param, param_m in zip(\n                model_pair[0].parameters(), model_pair[1].parameters()\n            ):\n                param_m.data = param_m.data * self.momentum + param.data * (\n                    1.0 - self.momentum\n                )\n\n\nclass GatherLayer(torch.autograd.Function):\n    \"\"\"\n    Gather tensors from all workers with support for backward propagation:\n    This implementation does not cut the gradients as torch.distributed.all_gather does.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, x):\n        output = [\n            torch.zeros_like(x) for _ in range(torch.distributed.get_world_size())\n        ]\n        torch.distributed.all_gather(output, x)\n        return tuple(output)\n\n    @staticmethod\n    def backward(ctx, *grads):\n        all_gradients = torch.stack(grads)\n        torch.distributed.all_reduce(all_gradients)\n        return all_gradients[torch.distributed.get_rank()]\n\n\ndef all_gather_with_grad(tensors):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    Graph remains connected for backward grad computation.\n    \"\"\"\n    # Queue the gathered tensors\n    world_size = torch.distributed.get_world_size()\n    # There is no need for reduction in the single-proc case\n    if world_size == 1:\n        return tensors\n\n    # tensor_all = GatherLayer.apply(tensors)\n    tensor_all = GatherLayer.apply(tensors)\n\n    return torch.cat(tensor_all, dim=0)\n\n\n@torch.no_grad()\ndef concat_all_gather(tensor):\n    \"\"\"\n    Performs all_gather operation on the provided tensors.\n    *** Warning ***: torch.distributed.all_gather has no gradient.\n    \"\"\"\n    # if use distributed training\n    if not is_dist_avail_and_initialized():\n        return tensor\n\n    tensors_gather = [\n        torch.ones_like(tensor) for _ in range(torch.distributed.get_world_size())\n    ]\n    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n\n    output = torch.cat(tensors_gather, dim=0)\n    return output\n\n\ndef tile(x, dim, n_tile):\n    init_dim = x.size(dim)\n    repeat_idx = [1] * x.dim()\n    repeat_idx[dim] = n_tile\n    x = x.repeat(*(repeat_idx))\n    order_index = torch.LongTensor(\n        np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])\n    )\n    return torch.index_select(x, dim, order_index.to(x.device))\n"}
{"type": "source_file", "path": "src/caption/ofa/__init__.py", "content": "# flake8: noqa\n# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n# module, but to preserve other warnings. So, don't check this module at all.\n\n# Copyright 2020 The HuggingFace Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom typing import TYPE_CHECKING\n\n# rely on isort to merge the imports\nfrom transformers.file_utils import _LazyModule, is_tokenizers_available, is_torch_available\n\n\n_import_structure = {\n    \"configuration_ofa\": [\"OFA_PRETRAINED_CONFIG_ARCHIVE_MAP\", \"OFAConfig\"],\n    \"tokenization_ofa\": [\"OFATokenizer\"],\n}\n\nif is_tokenizers_available():\n    _import_structure[\"tokenization_ofa_fast\"] = [\"OFATokenizerFast\"]\n\nif is_torch_available():\n    _import_structure[\"modeling_ofa\"] = [\n        \"OFA_PRETRAINED_MODEL_ARCHIVE_LIST\",\n        \"OFAModel\",\n        \"OFAPreTrainedModel\",\n    ]\n\n\nif TYPE_CHECKING:\n    from .configuration_ofa import OFA_PRETRAINED_CONFIG_ARCHIVE_MAP, OFAConfig\n    from .tokenization_ofa import OFATokenizer\n\n    if is_tokenizers_available():\n        from .tokenization_ofa_fast import OFATokenizerFast\n\n    if is_torch_available():\n        from .modeling_ofa import (\n            OFA_PRETRAINED_MODEL_ARCHIVE_LIST,\n            OFAModel,\n            OFAPreTrainedModel,\n        )\n\n\nelse:\n    import sys\n\n    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)\n"}
{"type": "source_file", "path": "src/caption/ofa/configuration_ofa.py", "content": "# coding=utf-8\n# Copyright 2022 The OFA-Sys Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" OFA model configuration\"\"\"\nimport warnings\nfrom transformers.configuration_utils import PretrainedConfig\nfrom transformers.utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nOFA_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"OFA-Sys/OFA-tiny\": \"https://huggingface.co/OFA-Sys/OFA-tiny/blob/main/config.json\",\n    \"OFA-Sys/OFA-medium\": \"https://huggingface.co/OFA-Sys/OFA-medium/blob/main/config.json\",\n    \"OFA-Sys/OFA-base\": \"https://huggingface.co/OFA-Sys/OFA-base/blob/main/config.json\",\n    \"OFA-Sys/OFA-large\": \"https://huggingface.co/OFA-Sys/OFA-large/blob/main/config.json\",\n    # See all OFA models at https://huggingface.co/models?filter=ofa\n}\n\n\nclass OFAConfig(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`~OFAModel`]. It is used to instantiate an OFA\n    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n    defaults will yield a similar configuration to that of the OFA [ofa-base](https://huggingface.co/ofa-base)\n    architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 50265):\n            Vocabulary size of the OFA model. Defines the number of different tokens that can be represented by the\n            `inputs_ids` passed when calling [`~OFAModel`] or [`~TFOFAModel`].\n        d_model (`int`, *optional*, defaults to 1024):\n            Dimension of the layers and the pooler layer.\n        encoder_layers (`int`, *optional*, defaults to 12):\n            Number of encoder layers.\n        decoder_layers (`int`, *optional*, defaults to 12):\n            Number of decoder layers.\n        encoder_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer encoder.\n        decoder_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer decoder.\n        decoder_ffn_dim (`int`, *optional*, defaults to 4096):\n            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n        encoder_ffn_dim (`int`, *optional*, defaults to 4096):\n            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n        activation_function (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n        dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for the attention probabilities.\n        activation_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for activations inside the fully connected layer.\n        classifier_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout ratio for classifier.\n        max_position_embeddings (`int`, *optional*, defaults to 1024):\n            The maximum sequence length that this model might ever be used with. Typically set this to something large\n            just in case (e.g., 512 or 1024 or 2048).\n        init_std (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        encoder_layerdrop: (`float`, *optional*, defaults to 0.0):\n            The LayerDrop probability for the encoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n            for more details.\n        decoder_layerdrop: (`float`, *optional*, defaults to 0.0):\n            The LayerDrop probability for the decoder. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n            for more details.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models).\n    \"\"\"\n\n    model_type = \"ofa\"\n    keys_to_ignore_at_inference = [\"past_key_values\"]\n\n    attribute_map = {\"num_attention_heads\": \"encoder_attention_heads\", \"hidden_size\": \"d_model\"}\n\n    def __init__(\n        self,\n        vocab_size=59457,\n        max_position_embeddings=1024,\n        encoder_layers=4,\n        encoder_ffn_dim=512 * 4,\n        encoder_attention_heads=8,\n        decoder_layers=4,\n        decoder_ffn_dim=512 * 4,\n        decoder_attention_heads=8,\n        encoder_layerdrop=0.0,\n        decoder_layerdrop=0.0,\n        use_cache=True,\n        is_encoder_decoder=True,\n        activation_function=\"gelu\",\n        d_model=512,\n        dropout=0.1,\n        attention_dropout=0.0,\n        activation_dropout=0.0,\n        init_std=0.02,\n        classifier_dropout=0.0,\n        scale_embedding=False,\n        pad_token_id=1,\n        bos_token_id=0,\n        decoder_start_token_id=0,\n        eos_token_id=2,\n        forced_eos_token_id=2,\n        encoder_normalize_before=True,\n        decoder_normalize_before=True,\n        normformer=True,\n        encoder_drop_path_rate=0.0,\n        decoder_drop_path_rate=0.0,\n        layernorm_embedding=True,\n        patch_layernorm_embedding=True,\n        resnet_type=\"resnet101\",\n        resnet_model_path=None,\n        resnet_drop_path_rate=0.0,\n        token_bucket_size=256,\n        image_bucket_size=42,\n        add_type_embedding=True,\n        share_decoder_input_output_embed=True,\n        attn_scale_factor=2.0,\n        code_layernorm_embedding=True,\n        code_image_size=128,\n        entangle_position_embedding=False,\n        **kwargs,\n    ):\n        self.vocab_size = vocab_size\n        self.max_position_embeddings = max_position_embeddings\n        self.d_model = d_model\n        self.encoder_ffn_dim = encoder_ffn_dim\n        self.encoder_layers = encoder_layers\n        self.encoder_attention_heads = encoder_attention_heads\n        self.decoder_ffn_dim = decoder_ffn_dim\n        self.decoder_layers = decoder_layers\n        self.decoder_attention_heads = decoder_attention_heads\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.activation_dropout = activation_dropout\n        self.activation_function = activation_function\n        self.init_std = init_std\n        self.encoder_layerdrop = encoder_layerdrop\n        self.decoder_layerdrop = decoder_layerdrop\n        self.classifier_dropout = classifier_dropout\n        self.use_cache = use_cache\n        self.num_hidden_layers = encoder_layers\n        self.scale_embedding = scale_embedding  # scale factor will be sqrt(d_model) if True\n        self.encoder_normalize_before = encoder_normalize_before\n        self.decoder_normalize_before = decoder_normalize_before\n        self.normformer = normformer\n        self.encoder_drop_path_rate = encoder_drop_path_rate\n        self.decoder_drop_path_rate = decoder_drop_path_rate\n        self.layernorm_embedding = layernorm_embedding\n        self.patch_layernorm_embedding = patch_layernorm_embedding\n        self.resnet_type = resnet_type\n        self.resnet_model_path = resnet_model_path\n        self.resnet_drop_path_rate = resnet_drop_path_rate\n        self.token_bucket_size = token_bucket_size\n        self.image_bucket_size = image_bucket_size\n        self.add_type_embedding = add_type_embedding\n        self.share_decoder_input_output_embed = share_decoder_input_output_embed\n        self.attn_scale_factor = attn_scale_factor\n        self.code_layernorm_embedding = code_layernorm_embedding\n        self.code_image_size = code_image_size\n        self.entangle_position_embedding = entangle_position_embedding\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            is_encoder_decoder=is_encoder_decoder,\n            decoder_start_token_id=bos_token_id,\n            forced_eos_token_id=forced_eos_token_id,\n            **kwargs,\n        )\n\n        # ensure backward compatibility for BART CNN models\n        if self.forced_bos_token_id is None and kwargs.get(\"force_bos_token_to_be_generated\", False):\n            self.forced_bos_token_id = self.bos_token_id\n            warnings.warn(\n                f\"Please make sure the config includes `forced_bos_token_id={self.bos_token_id}` in future versions. \"\n                \"The config can simply be saved and uploaded again to be fixed.\"\n            )\n"}
{"type": "source_file", "path": "src/caption/ofa/generate/__init__.py", "content": "\n"}
{"type": "source_file", "path": "src/caption/lavis/processors/blip_processors.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport re\n\n# from lavis.common.registry import registry\nfrom src.caption.lavis.processors.base_processor import BaseProcessor\nfrom src.caption.lavis.processors.randaugment import RandomAugment\nfrom omegaconf import OmegaConf\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\n\nclass BlipImageBaseProcessor(BaseProcessor):\n    def __init__(self, mean=None, std=None):\n        if mean is None:\n            mean = (0.48145466, 0.4578275, 0.40821073)\n        if std is None:\n            std = (0.26862954, 0.26130258, 0.27577711)\n\n        self.normalize = transforms.Normalize(mean, std)\n\n\n# @registry.register_processor(\"blip_caption\")\nclass BlipCaptionProcessor(BaseProcessor):\n    def __init__(self, prompt=\"\", max_words=50):\n        self.prompt = prompt\n        self.max_words = max_words\n\n    def __call__(self, caption):\n        caption = self.prompt + self.pre_caption(caption)\n\n        return caption\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        if cfg is None:\n            cfg = OmegaConf.create()\n\n        prompt = cfg.get(\"prompt\", \"\")\n        max_words = cfg.get(\"max_words\", 50)\n\n        return cls(prompt=prompt, max_words=max_words)\n\n    def pre_caption(self, caption):\n        caption = re.sub(\n            r\"([.!\\\"()*#:;~])\",\n            \" \",\n            caption.lower(),\n        )\n        caption = re.sub(\n            r\"\\s{2,}\",\n            \" \",\n            caption,\n        )\n        caption = caption.rstrip(\"\\n\")\n        caption = caption.strip(\" \")\n\n        # truncate caption\n        caption_words = caption.split(\" \")\n        if len(caption_words) > self.max_words:\n            caption = \" \".join(caption_words[: self.max_words])\n\n        return caption\n\n\n# @registry.register_processor(\"blip_question\")\nclass BlipQuestionProcessor(BaseProcessor):\n    def __init__(self, max_words=50):\n        self.max_words = max_words\n\n    def __call__(self, question):\n        return self.pre_question(question)\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        if cfg is None:\n            cfg = OmegaConf.create()\n\n        max_words = cfg.get(\"max_words\", 50)\n\n        return cls(max_words=max_words)\n\n    def pre_question(self, question):\n        question = re.sub(\n            r\"([.!\\\"()*#:;~])\",\n            \"\",\n            question.lower(),\n        )\n        question = question.rstrip(\" \")\n\n        # truncate question\n        question_words = question.split(\" \")\n        if len(question_words) > self.max_words:\n            question = \" \".join(question_words[: self.max_words])\n\n        return question\n\n\n# @registry.register_processor(\"blip_image_train\")\nclass BlipImageTrainProcessor(BlipImageBaseProcessor):\n    def __init__(\n        self, image_size=384, mean=None, std=None, min_scale=0.5, max_scale=1.0\n    ):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    image_size,\n                    scale=(min_scale, max_scale),\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                transforms.RandomHorizontalFlip(),\n                RandomAugment(\n                    2,\n                    5,\n                    isPIL=True,\n                    augs=[\n                        \"Identity\",\n                        \"AutoContrast\",\n                        \"Brightness\",\n                        \"Sharpness\",\n                        \"Equalize\",\n                        \"ShearX\",\n                        \"ShearY\",\n                        \"TranslateX\",\n                        \"TranslateY\",\n                        \"Rotate\",\n                    ],\n                ),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        if cfg is None:\n            cfg = OmegaConf.create()\n\n        image_size = cfg.get(\"image_size\", 384)\n\n        mean = cfg.get(\"mean\", None)\n        std = cfg.get(\"std\", None)\n\n        min_scale = cfg.get(\"min_scale\", 0.5)\n        max_scale = cfg.get(\"max_scale\", 1.0)\n\n        return cls(\n            image_size=image_size,\n            mean=mean,\n            std=std,\n            min_scale=min_scale,\n            max_scale=max_scale,\n        )\n\n\n# @registry.register_processor(\"blip_image_eval\")\nclass BlipImageEvalProcessor(BlipImageBaseProcessor):\n    def __init__(self, image_size=384, mean=None, std=None):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                transforms.Resize(\n                    (image_size, image_size), interpolation=InterpolationMode.BICUBIC\n                ),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        if cfg is None:\n            cfg = OmegaConf.create()\n\n        image_size = cfg.get(\"image_size\", 384)\n\n        mean = cfg.get(\"mean\", None)\n        std = cfg.get(\"std\", None)\n\n        return cls(image_size=image_size, mean=mean, std=std)\n\n\n# @registry.register_processor(\"blip2_image_train\")\nclass Blip2ImageTrainProcessor(BlipImageBaseProcessor):\n    def __init__(\n        self, image_size=364, mean=None, std=None, min_scale=0.5, max_scale=1.0\n    ):\n        super().__init__(mean=mean, std=std)\n\n        self.transform = transforms.Compose(\n            [\n                transforms.RandomResizedCrop(\n                    image_size,\n                    scale=(min_scale, max_scale),\n                    interpolation=InterpolationMode.BICUBIC,\n                ),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                self.normalize,\n            ]\n        )\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        if cfg is None:\n            cfg = OmegaConf.create()\n\n        image_size = cfg.get(\"image_size\", 364)\n\n        mean = cfg.get(\"mean\", None)\n        std = cfg.get(\"std\", None)\n\n        min_scale = cfg.get(\"min_scale\", 0.5)\n        max_scale = cfg.get(\"max_scale\", 1.0)\n\n        return cls(\n            image_size=image_size,\n            mean=mean,\n            std=std,\n            min_scale=min_scale,\n            max_scale=max_scale,\n        )\n"}
{"type": "source_file", "path": "src/caption/lavis/processors/base_processor.py", "content": "\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nfrom omegaconf import OmegaConf\n\n\nclass BaseProcessor:\n    def __init__(self):\n        self.transform = lambda x: x\n        return\n\n    def __call__(self, item):\n        return self.transform(item)\n\n    @classmethod\n    def from_config(cls, cfg=None):\n        return cls()\n\n    def build(self, **kwargs):\n        cfg = OmegaConf.create(kwargs)\n\n        return self.from_config(cfg)\n"}
{"type": "source_file", "path": "src/caption/lavis/models/blip2_models/modeling_llama.py", "content": "# coding=utf-8\n# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\nfrom transformers.models.llama.configuration_llama import LlamaConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\n# Copied from transformers.models.bart.modeling_bart._make_causal_mask\ndef _make_causal_mask(\n    input_ids_shape: torch.Size, dtype: torch.dtype, device: torch.device, past_key_values_length: int = 0\n):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min, device=device), device=device)\n    mask_cond = torch.arange(mask.size(-1), device=device)\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype, device=device), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\n# Copied from transformers.models.bart.modeling_bart._expand_mask\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\nclass LlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n    gather_indices = position_ids[:, None, :, None]  # [bs, 1, seq_len, 1]\n    gather_indices = gather_indices.repeat(1, cos.shape[1], 1, cos.shape[3])\n    cos = torch.gather(cos.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    sin = torch.gather(sin.repeat(gather_indices.shape[0], 1, 1, 1), 2, gather_indices)\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.config = config\n        self.hidden_size = config.hidden_size\n        self.num_heads = config.num_attention_heads\n        self.head_dim = self.hidden_size // self.num_heads\n        self.max_position_embeddings = config.max_position_embeddings\n\n        if (self.head_dim * self.num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {self.num_heads}).\"\n            )\n        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        if past_key_value is not None:\n            kv_seq_len += past_key_value[0].shape[-2]\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n        # [bsz, nh, t, hd]\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(config=config)\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_value=past_key_value,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlamaModel):\n            module.gradient_checkpointing = value\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n            config.n_positions - 1]`.\n\n            [What are position IDs?](../glossary#position-ids)\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape,\n                inputs_embeds.dtype,\n                device=inputs_embeds.device,\n                past_key_values_length=past_key_values_length,\n            )\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n\n        if position_ids is None:\n            device = input_ids.device if input_ids is not None else inputs_embeds.device\n            position_ids = torch.arange(\n                past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n            )\n            position_ids = position_ids.unsqueeze(0).view(-1, seq_length)\n        else:\n            position_ids = position_ids.view(-1, seq_length).long()\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs, output_attentions, None)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    position_ids,\n                    None,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    position_ids=position_ids,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        reduction: Optional[str] = \"mean\",\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(reduction=reduction)\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n            if reduction == \"none\":\n                # loss = loss.view(logits.size(0), -1).sum(1)\n                loss = loss.view(logits.size(0), -1).mean(1)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        position_ids = kwargs.get(\"position_ids\", None)\n        if attention_mask is not None and position_ids is None:\n            # create position_ids on the fly for batch generation\n            position_ids = attention_mask.long().cumsum(-1) - 1\n            position_ids.masked_fill_(attention_mask == 0, 1)\n            if past_key_values:\n                position_ids = position_ids[:, -1].unsqueeze(-1)\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"position_ids\": position_ids,\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            position_ids=position_ids,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            labels = labels.to(logits.device)\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )"}
{"type": "source_file", "path": "src/caption/ofa/generate/utils.py", "content": "# # Copyright (c) Facebook, Inc. and its affiliates.\n# #\n# # This source code is licensed under the MIT license found in the\n# # LICENSE file in the root directory of this source tree.\n#\n\nfrom itertools import accumulate\n\nimport torch\nimport torch.nn.functional as F\nimport collections\n\n\ntry:\n    from amp_C import multi_tensor_l2norm\n\n    multi_tensor_l2norm_available = True\nexcept ImportError:\n    multi_tensor_l2norm_available = False\n\ntry:\n    import torch_xla.core.xla_model as xm\nexcept ImportError:\n    xm = None\n\n\n\n\nMANIFOLD_PATH_SEP = \"|\"\n\ndef apply_to_sample(f, sample):\n    if hasattr(sample, \"__len__\") and len(sample) == 0:\n        return {}\n\n    def _apply(x):\n        if torch.is_tensor(x):\n            return f(x)\n        elif isinstance(x, collections.OrderedDict):\n            # OrderedDict has attributes that needs to be preserved\n            od = collections.OrderedDict((key, _apply(value)) for key, value in x.items())\n            od.__dict__ = x.__dict__\n            return od\n        elif isinstance(x, dict):\n            return {key: _apply(value) for key, value in x.items()}\n        elif isinstance(x, list):\n            return [_apply(x) for x in x]\n        elif isinstance(x, tuple):\n            return tuple(_apply(x) for x in x)\n        elif isinstance(x, set):\n            return {_apply(x) for x in x}\n        else:\n            return x\n\n    return _apply(sample)\n\ndef move_to_cuda(sample, device=None):\n    device = device or torch.cuda.current_device()\n\n    def _move_to_cuda(tensor):\n        # non_blocking is ignored if tensor is not pinned, so we can always set\n        # to True (see github.com/PyTorchLightning/pytorch-lightning/issues/620)\n        return tensor.to(device=device, non_blocking=True)\n\n    return apply_to_sample(_move_to_cuda, sample)\n\ndef strip_pad(tensor, pad):\n    return tensor[tensor.ne(pad)]\n\ndef get_token_to_word_mapping(tokens, exclude_list):\n    n = len(tokens)\n    word_start = [int(token not in exclude_list) for token in tokens]\n    word_idx = list(accumulate(word_start))\n    token_to_word = {i: word_idx[i] for i in range(n)}\n    return token_to_word\n\n\ndef extract_hard_alignment(attn, src_sent, tgt_sent, pad, eos):\n    tgt_valid = (\n        ((tgt_sent != pad) & (tgt_sent != eos)).nonzero(as_tuple=False).squeeze(dim=-1)\n    )\n    src_invalid = (\n        ((src_sent == pad) | (src_sent == eos)).nonzero(as_tuple=False).squeeze(dim=-1)\n    )\n    src_token_to_word = get_token_to_word_mapping(src_sent, [eos, pad])\n    tgt_token_to_word = get_token_to_word_mapping(tgt_sent, [eos, pad])\n    alignment = []\n    if len(tgt_valid) != 0 and len(src_invalid) < len(src_sent):\n        attn_valid = attn[tgt_valid]\n        attn_valid[:, src_invalid] = float(\"-inf\")\n        _, src_indices = attn_valid.max(dim=1)\n        for tgt_idx, src_idx in zip(tgt_valid, src_indices):\n            alignment.append(\n                (\n                    src_token_to_word[src_idx.item()] - 1,\n                    tgt_token_to_word[tgt_idx.item()] - 1,\n                )\n            )\n    return alignment\n\ndef softmax(x, dim: int, onnx_trace: bool = False):\n    if onnx_trace:\n        return F.softmax(x.float(), dim=dim)\n    else:\n        return F.softmax(x, dim=dim, dtype=torch.float32)\n\n\ndef log_softmax(x, dim: int, onnx_trace: bool = False):\n    if onnx_trace:\n        return F.log_softmax(x.float(), dim=dim)\n    else:\n        return F.log_softmax(x, dim=dim, dtype=torch.float32)\n\ndef extract_soft_alignment(attn, src_sent, tgt_sent, pad, eos):\n    tgt_valid = ((tgt_sent != pad)).nonzero(as_tuple=False)\n    src_valid = ((src_sent != pad)).nonzero(as_tuple=False).squeeze(dim=-1)\n    alignment = []\n    if len(tgt_valid) != 0 and len(src_valid) != 0:\n        attn_valid = attn[tgt_valid, src_valid]\n        alignment = [\n            [\"{:.6f}\".format(p) for p in src_probs.tolist()] for src_probs in attn_valid\n        ]\n    return alignment\n"}
{"type": "source_file", "path": "src/caption/ofa/generate/ngram_repeat_block.py", "content": "# Originally from Microsoft Corporation.\n# Licensed under the MIT License.\n\n\"\"\" Wrapper for ngram_repeat_block cuda extension \"\"\"\nimport torch\nfrom torch import nn\n\nimport math\nfrom typing import Dict, List, Optional\nimport warnings\n\ntry:\n    from fairseq import ngram_repeat_block_cuda\n\n    EXTENSION_BUILT = True\nexcept ImportError:\n    EXTENSION_BUILT = False\n\n\ndef is_cuda_extension_usable() -> bool:\n    \"\"\"Check whether ngram_repeat_block_cuda is built properly\"\"\"\n    if not EXTENSION_BUILT or not torch.cuda.is_available():\n        return False\n    bsz = 2\n    tokens = torch.tensor([[4, 4, 3, 2], [1, 2, 3, 4]], dtype=torch.long, device=\"cuda\")\n    lprobs = torch.rand((8, 12), device=\"cuda\")\n    try:\n        outputs = ngram_repeat_block_cuda.forward(tokens, lprobs, bsz, 3, 4, 3)\n        outputs = outputs + 4  # This line breaks if the extension is built incorrectly.\n        return True\n    except RuntimeError:\n        warnings.warn(\n            \"NGramRepeatBlock extension must be rebuilt.\"\n            'Run TORCH_CUDA_ARCH_LIST=\"6.0;6.1;7.0\" python setup.py build_ext --inplace'\n        )\n        return False\n\n\nclass NGramRepeatBlock(nn.Module):\n    \"\"\" Wrapper class for calling ngram_repeat_block cuda extension \"\"\"\n\n    def __init__(self, no_repeat_ngram_size: int, use_extension: bool = True):\n        super().__init__()\n        self.use_extension = is_cuda_extension_usable() if use_extension else False\n        self.no_repeat_ngram_size = no_repeat_ngram_size\n\n    def reset_parameters(self):\n        pass\n\n    @torch.jit.unused\n    def call_cuda_extension(\n        self,\n        tokens,\n        lprobs,\n        bsz: int,\n        beam_size: int,\n        step: int,\n    ):\n        return ngram_repeat_block_cuda.forward(\n            tokens, lprobs, bsz, step, beam_size, self.no_repeat_ngram_size\n        )\n\n    def forward(\n        self,\n        tokens,\n        lprobs,\n        bsz: int,\n        beam_size: int,\n        step: int,\n    ):\n        \"\"\"\n        Args:\n            tokens(Tensor): Input tokens(Bsz*beam, seq_len)\n            lprobs(Tensor): likelihood probability,\n            Expected to be updated in place.(Bsz*beam, vocab_size)\n            bsz(int): batch size\n            step(int): current step\n            beam_size(int): beam size\n            no_repeat_ngram_size(int): Ngram size\n        \"\"\"\n        msg = f\"expected {bsz *beam_size} got\"\n        assert tokens.size(0) == bsz * beam_size, f\"{msg} {tokens.size(0)}\"\n        assert lprobs.size(0) == bsz * beam_size, f\"{msg} {lprobs.size(0)}\"\n        if self.use_extension:\n            return self.call_cuda_extension(tokens, lprobs, bsz, beam_size, step)\n\n        else:\n            return self._no_repeat_ngram(\n                tokens,\n                lprobs,\n                bsz,\n                beam_size,\n                step,\n            )\n\n    def _no_repeat_ngram(self, tokens, lprobs, bsz: int, beam_size: int, step: int):\n        \"\"\"For each hypothesis generate a list of previous ngrams and set associated lprobs to -inf\"\"\"\n        gen_ngrams: List[Dict[str, List[int]]] = [\n            torch.jit.annotate(Dict[str, List[int]], {})\n            for bbsz_idx in range(bsz * beam_size)\n        ]\n        cpu_tokens = tokens.cpu()\n        for bbsz_idx in range(bsz * beam_size):\n            gen_tokens: List[int] = cpu_tokens[bbsz_idx].tolist()\n            for ngram in self.transpose_list(\n                [gen_tokens[i:] for i in range(self.no_repeat_ngram_size)]\n            ):\n                key = \",\".join([str(x) for x in ngram[:-1]])\n                gen_ngrams[bbsz_idx][key] = gen_ngrams[bbsz_idx].get(\n                    key, torch.jit.annotate(List[int], [])\n                ) + [ngram[-1]]\n        if step + 2 - self.no_repeat_ngram_size >= 0:\n            # no banned tokens if we haven't generated no_repeat_ngram_size tokens yet\n            banned_tokens = [\n                self.calculate_banned_tokens(\n                    tokens, step, gen_ngrams, self.no_repeat_ngram_size, bbsz_idx\n                )\n                for bbsz_idx in range(bsz * beam_size)\n            ]\n        else:\n            banned_tokens = [\n                torch.jit.annotate(List[int], []) for bbsz_idx in range(bsz * beam_size)\n            ]\n        for bbsz_idx in range(bsz * beam_size):\n            lprobs[bbsz_idx][\n                torch.tensor(banned_tokens[bbsz_idx], dtype=torch.int64)\n            ] = torch.tensor(-math.inf).to(lprobs)\n        return lprobs\n\n    @staticmethod\n    def calculate_banned_tokens(\n        tokens,\n        step: int,\n        gen_ngrams: List[Dict[str, List[int]]],\n        no_repeat_ngram_size: int,\n        bbsz_idx: int,\n    ):\n        tokens_list: List[int] = tokens[\n            bbsz_idx, step + 2 - no_repeat_ngram_size : step + 1\n        ].tolist()\n        # before decoding the next token, prevent decoding of ngrams that have already appeared\n        ngram_index = \",\".join([str(x) for x in tokens_list])\n        return gen_ngrams[bbsz_idx].get(ngram_index, torch.jit.annotate(List[int], []))\n\n    @staticmethod\n    def transpose_list(l: List[List[int]]):\n        # GeneratorExp aren't supported in TS so ignoring the lint\n        min_len = min([len(x) for x in l])  # noqa\n        l2 = [[row[i] for row in l] for i in range(min_len)]\n        return l2\n"}
{"type": "source_file", "path": "src/caption/ofa/generate/sequence_generator.py", "content": "# Copyright 2022 The OFA-Sys Team.\n# All rights reserved.\n# This source code is licensed under the Apache 2.0 license\n# found in the LICENSE file in the root directory.\n\nimport math\nfrom typing import Dict, List, Optional, Tuple\nimport sys\n\nimport torch\nimport torch.nn as nn\nfrom .search import BeamSearch\nfrom torch import Tensor\nfrom .ngram_repeat_block import NGramRepeatBlock\n\n\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    r\"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)\n\n\nclass SequenceGenerator(nn.Module):\n    def __init__(\n        self,\n        tokenizer,\n        beam_size=1,\n        max_len_a=0,\n        max_len_b=200,\n        max_len=0,\n        min_len=1,\n        normalize_scores=True,\n        len_penalty=1.0,\n        unk_penalty=0.0,\n        temperature=1.0,\n        match_source_len=False,\n        no_repeat_ngram_size=0,\n        search_strategy=None,\n        eos=None,\n        symbols_to_strip_from_output=None,\n        lm_model=None,\n        lm_weight=1.0,\n        constraint_trie=None,\n        constraint_range=None,\n        gen_code=False,\n        gen_box=False,\n        ignore_eos=False,\n        zero_shot=False\n    ):\n        \"\"\"Generates translations of a given source sentence.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models,\n                currently support fairseq.models.TransformerModel for scripting\n            beam_size (int, optional): beam width (default: 1)\n            max_len_a/b (int, optional): generate sequences of maximum length\n                ax + b, where x is the source length\n            max_len (int, optional): the maximum length of the generated output\n                (not including end-of-sentence)\n            min_len (int, optional): the minimum length of the generated output\n                (not including end-of-sentence)\n            normalize_scores (bool, optional): normalize scores by the length\n                of the output (default: True)\n            len_penalty (float, optional): length penalty, where <1.0 favors\n                shorter, >1.0 favors longer sentences (default: 1.0)\n            unk_penalty (float, optional): unknown word penalty, where <0\n                produces more unks, >0 produces fewer (default: 0.0)\n            temperature (float, optional): temperature, where values\n                >1.0 produce more uniform samples and values <1.0 produce\n                sharper samples (default: 1.0)\n            match_source_len (bool, optional): outputs should match the source\n                length (default: False)\n        \"\"\"\n        super().__init__()\n        self.gen_code = gen_code\n        self.gen_box = gen_box\n        self.ignore_eos = ignore_eos\n        self.tokenizer = tokenizer\n        self.tgt_dict = {value: key for key,value in tokenizer.get_vocab().items()}\n        added = {value: key for key,value in tokenizer.get_added_vocab().items()}\n        self.tgt_dict.update(added)\n        self.pad = tokenizer.pad_token_id\n        self.unk = tokenizer.unk_token_id\n        self.bos = tokenizer.bos_token_id\n        self.eos = tokenizer.eos_token_id\n        self.symbols_to_strip_from_output = (\n            symbols_to_strip_from_output.union({self.eos})\n            if symbols_to_strip_from_output is not None\n            else {self.bos, self.eos}\n        )\n        self.vocab_size = len(self.tgt_dict)\n        self.beam_size = beam_size\n        # the max beam size is the dictionary size - 1, since we never select pad\n        self.beam_size = min(beam_size, self.vocab_size - 1)\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.min_len = min_len\n        self.max_len = max_len\n\n        self.normalize_scores = normalize_scores\n        self.len_penalty = len_penalty\n        self.unk_penalty = unk_penalty\n        self.temperature = temperature\n        self.match_source_len = match_source_len\n        self.zero_shot = zero_shot\n\n        if no_repeat_ngram_size > 0:\n            self.repeat_ngram_blocker = NGramRepeatBlock(no_repeat_ngram_size)\n        else:\n            self.repeat_ngram_blocker = None\n\n        assert temperature > 0, \"--temperature must be greater than 0\"\n\n        self.search = (\n            BeamSearch(self.tokenizer) if search_strategy is None else search_strategy\n        )\n        # We only need to set src_lengths in LengthConstrainedBeamSearch.\n        # As a module attribute, setting it would break in multithread\n        # settings when the model is shared.\n        self.should_set_src_lengths = (\n            hasattr(self.search, \"needs_src_lengths\") and self.search.needs_src_lengths\n        )\n\n        # self.model.eval()\n\n        self.lm_model = lm_model\n        self.lm_weight = lm_weight\n        if self.lm_model is not None:\n            self.lm_model.eval()\n\n        self.constraint_trie = constraint_trie\n\n        self.constraint_start = None\n        self.constraint_end = None\n        if constraint_range is not None:\n            constraint_start, constraint_end = constraint_range.split(',')\n            self.constraint_start = int(constraint_start)\n            self.constraint_end = int(constraint_end)\n\n\n    @torch.no_grad()\n    def forward(\n        self,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        \"\"\"Generate a batch of translations.\n\n        Args:\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n            bos_token (int, optional): beginning of sentence token\n                (default: self.eos)\n        \"\"\"\n        return self._generate(sample, prefix_tokens, bos_token=bos_token)\n\n    @torch.no_grad()\n    def generate(self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs) -> List[List[Dict[str, Tensor]]]:\n        \"\"\"Generate translations. Match the api of other fairseq generators.\n\n        Args:\n            models (List[~fairseq.models.FairseqModel]): ensemble of models\n            sample (dict): batch\n            prefix_tokens (torch.LongTensor, optional): force decoder to begin\n                with these tokens\n            constraints (torch.LongTensor, optional): force decoder to include\n                the list of constraints\n            bos_token (int, optional): beginning of sentence token\n                (default: self.eos)\n        \"\"\"\n        return self._generate(models, sample, **kwargs)\n\n    def _generate(\n        self,\n        models,\n        sample: Dict[str, Dict[str, Tensor]],\n        prefix_tokens: Optional[Tensor] = None,\n        constraints: Optional[Tensor] = None,\n        bos_token: Optional[int] = None,\n    ):\n        model = EnsembleModel(models)\n        incremental_states = torch.jit.annotate(\n            List[Tuple[Tuple[torch.Tensor]]],\n            [\n                torch.jit.annotate(Tuple[Tuple[torch.Tensor]], {})\n                for i in range(model.models_size)\n            ],\n        )\n        net_input = sample[\"net_input\"]\n\n        if \"src_tokens\" in net_input:\n            src_tokens = net_input[\"src_tokens\"]\n            # length of the source text being the character length except EndOfSentence and pad\n            src_lengths = (\n                (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n            )\n        elif \"input_ids\" in net_input:\n            src_tokens = net_input[\"input_ids\"]\n            # length of the source text being the character length except EndOfSentence and pad\n            src_lengths = (\n                (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)\n            )\n        elif \"source\" in net_input:\n            src_tokens = net_input[\"source\"]\n            src_lengths = (\n                net_input[\"padding_mask\"].size(-1) - net_input[\"padding_mask\"].sum(-1)\n                if net_input[\"padding_mask\"] is not None\n                else torch.tensor(src_tokens.size(-1)).to(src_tokens)\n            )\n        elif \"features\" in net_input:\n            src_tokens = net_input[\"features\"]\n            src_lengths = (\n                net_input[\"padding_mask\"].size(-1) - net_input[\"padding_mask\"].sum(-1)\n                if net_input[\"padding_mask\"] is not None\n                else torch.tensor(src_tokens.size(-1)).to(src_tokens)\n            )\n        else:\n            raise Exception(\"expected src_tokens or source in net input. input keys: \" + str(net_input.keys()))\n\n        # bsz: total number of sentences in beam\n        # Note that src_tokens may have more than 2 dimensions (i.e. audio features)\n        bsz, src_len = src_tokens.size()[:2]\n        beam_size = self.beam_size\n\n        if constraints is not None and not self.search.supports_constraints:\n            raise NotImplementedError(\n                \"Target-side constraints were provided, but search method doesn't support them\"\n            )\n\n        # Initialize constraints, when active\n        self.search.init_constraints(constraints, beam_size)\n\n        max_len: int = -1\n        if self.match_source_len:\n            max_len = src_lengths.max().item()\n        else:\n            max_len = int(self.max_len_a * src_len + self.max_len_b)\n        assert (\n            self.min_len <= max_len\n        ), \"min_len cannot be larger than max_len, please adjust these!\"\n        # compute the encoder output for each beam\n        with torch.autograd.profiler.record_function(\"EnsembleModel: forward_encoder\"):\n            encoder_outs = model.forward_encoder(net_input)\n\n        # placeholder of indices for bsz * beam_size to hold tokens and accumulative scores\n        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)\n        new_order = new_order.to(src_tokens.device).long()\n        encoder_outs = model.reorder_encoder_out(encoder_outs, new_order)\n        # ensure encoder_outs is a List.\n        assert encoder_outs is not None\n\n        # initialize buffers\n        scores = (\n            torch.zeros(bsz * beam_size, max_len + 1).to(src_tokens).float()\n        )  # +1 for eos; pad is never chosen for scoring\n        tokens = (\n            torch.zeros(bsz * beam_size, max_len + 2)\n            .to(src_tokens)\n            .long()\n            .fill_(self.pad)\n        )  # +2 for eos and pad\n        # tokens[:, 0] = self.eos if bos_token is None else bos_token\n        tokens[:, 0] = self.bos\n        attn: Optional[Tensor] = None\n\n        # A list that indicates candidates that should be ignored.\n        # For example, suppose we're sampling and have already finalized 2/5\n        # samples. Then cands_to_ignore would mark 2 positions as being ignored,\n        # so that we only finalize the remaining 3 samples.\n        cands_to_ignore = (\n            torch.zeros(bsz, beam_size).to(src_tokens).eq(-1)\n        )  # forward and backward-compatible False mask\n\n        # list of completed sentences\n        finalized = torch.jit.annotate(\n            List[List[Dict[str, Tensor]]],\n            [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)],\n        )  # contains lists of dictionaries of infomation about the hypothesis being finalized at each step\n\n        # a boolean array indicating if the sentence at the index is finished or not\n        finished = [False for i in range(bsz)]\n        num_remaining_sent = bsz  # number of sentences remaining\n\n        # number of candidate hypos per step\n        cand_size = 2 * beam_size  # 2 x beam size in case half are EOS\n\n        # offset arrays for converting between different indexing schemes\n        bbsz_offsets = (\n            (torch.arange(0, bsz) * beam_size)\n            .unsqueeze(1)\n            .type_as(tokens)\n            .to(src_tokens.device)\n        )\n        cand_offsets = torch.arange(0, cand_size).type_as(tokens).to(src_tokens.device)\n\n        reorder_state: Optional[Tensor] = None\n        batch_idxs: Optional[Tensor] = None\n\n        original_batch_idxs: Optional[Tensor] = None\n        if \"id\" in sample and isinstance(sample[\"id\"], Tensor):\n            original_batch_idxs = sample[\"id\"]\n        else:\n            original_batch_idxs = torch.arange(0, bsz).type_as(tokens)\n\n        for step in range(max_len + 1):  # one extra step for EOS marker\n            # reorder decoder internal states based on the prev choice of beams\n            if reorder_state is not None:\n                if batch_idxs is not None:\n                    # update beam indices to take into account removed sentences\n                    corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(\n                        batch_idxs\n                    )\n                    reorder_state.view(-1, beam_size).add_(\n                        corr.unsqueeze(-1) * beam_size\n                    )\n                    original_batch_idxs = original_batch_idxs[batch_idxs]\n                model.reorder_incremental_state(incremental_states, reorder_state)\n                encoder_outs = model.reorder_encoder_out(\n                    encoder_outs, reorder_state\n                )\n\n            with torch.autograd.profiler.record_function(\"EnsembleModel: forward_decoder\"):\n                lprobs, avg_attn_scores = model.forward_decoder(\n                    tokens[:, : step + 1],\n                    encoder_outs,\n                    incremental_states,\n                    self.temperature,\n                    constraint_trie=self.constraint_trie,\n                    constraint_start=self.constraint_start,\n                    constraint_end=self.constraint_end,\n                    gen_code=self.gen_code,\n                    zero_shot=self.zero_shot,\n                    prefix_tokens=prefix_tokens\n                )\n\n            if self.lm_model is not None:\n                lm_out = self.lm_model(tokens[:, : step + 1])\n                probs = self.lm_model.get_normalized_probs(\n                    lm_out, log_probs=True, sample=None\n                )\n                probs = probs[:, -1, :] * self.lm_weight\n                lprobs += probs\n            # handle prefix tokens (possibly with different lengths)\n            if (\n                prefix_tokens is not None\n                and step < prefix_tokens.size(1)\n                and step < max_len\n            ):\n                lprobs, tokens, scores = self._prefix_tokens(\n                    step, lprobs, scores, tokens, prefix_tokens, beam_size\n                )\n            elif step < self.min_len:\n                # minimum length constraint (does not apply if using prefix_tokens)\n                lprobs[:, self.eos] = -math.inf\n\n            lprobs[lprobs != lprobs] = torch.tensor(-math.inf).to(lprobs)\n\n            lprobs[:, self.pad] = -math.inf  # never select pad\n            lprobs[:, self.unk] -= self.unk_penalty  # apply unk penalty\n\n            if (self.gen_code or self.gen_box) and step < max_len:\n                lprobs[:, :4] = -math.inf\n            if self.gen_box:\n                lprobs[:, -1] = -math.inf\n                if (step + 1) % 5 == 0:\n                    lprobs[:, self.constraint_start:59457] = -math.inf\n                else:\n                    lprobs[:, 59457:] = -math.inf\n\n            # handle max length constraint\n            if step >= max_len:\n                lprobs[:, : self.eos] = -math.inf\n                lprobs[:, self.eos + 1 :] = -math.inf\n                if self.ignore_eos:\n                    lprobs[:, self.eos] = 1\n\n            # Record attention scores, only support avg_attn_scores is a Tensor\n            if avg_attn_scores is not None:\n                if attn is None:\n                    attn = torch.empty(\n                        bsz * beam_size, avg_attn_scores.size(1), max_len + 2\n                    ).to(scores)\n                attn[:, :, step + 1].copy_(avg_attn_scores)\n\n            scores = scores.type_as(lprobs)\n            eos_bbsz_idx = torch.empty(0).to(\n                tokens\n            )  # indices of hypothesis ending with eos (finished sentences)\n            eos_scores = torch.empty(0).to(\n                scores\n            )  # scores of hypothesis ending with eos (finished sentences)\n\n            if self.should_set_src_lengths:\n                self.search.set_src_lengths(src_lengths)\n\n            if self.repeat_ngram_blocker is not None:\n                lprobs = self.repeat_ngram_blocker(tokens, lprobs, bsz, beam_size, step)\n\n            # Shape: (batch, cand_size)\n            cand_scores, cand_indices, cand_beams = self.search.step(\n                step,\n                lprobs.view(bsz, -1, self.vocab_size),\n                scores.view(bsz, beam_size, -1)[:, :, :step],\n                tokens[:, : step + 1],\n                original_batch_idxs,\n            )\n\n            # cand_bbsz_idx contains beam indices for the top candidate\n            # hypotheses, with a range of values: [0, bsz*beam_size),\n            # and dimensions: [bsz, cand_size]\n            cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n\n            # finalize hypotheses that end in eos\n            # Shape of eos_mask: (batch size, beam size)\n            eos_mask = cand_indices.eq(self.eos) & cand_scores.ne(-math.inf)\n            eos_mask[:, :beam_size][cands_to_ignore] = torch.tensor(0).to(eos_mask)\n\n            # only consider eos when it's among the top beam_size indices\n            # Now we know what beam item(s) to finish\n            # Shape: 1d list of absolute-numbered\n            eos_bbsz_idx = torch.masked_select(\n                cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size]\n            )\n\n            finalized_sents: List[int] = []\n            if eos_bbsz_idx.numel() > 0:\n                eos_scores = torch.masked_select(\n                    cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size]\n                )\n\n                finalized_sents = self.finalize_hypos(\n                    step,\n                    eos_bbsz_idx,\n                    eos_scores,\n                    tokens,\n                    scores,\n                    finalized,\n                    finished,\n                    beam_size,\n                    attn,\n                    src_lengths,\n                    max_len,\n                )\n                num_remaining_sent -= len(finalized_sents)\n\n            assert num_remaining_sent >= 0\n            if num_remaining_sent == 0:\n                break\n            if self.search.stop_on_max_len and step >= max_len:\n                break\n            assert step < max_len, f\"{step} < {max_len}\"\n\n            # Remove finalized sentences (ones for which {beam_size}\n            # finished hypotheses have been generated) from the batch.\n            if len(finalized_sents) > 0:\n                new_bsz = bsz - len(finalized_sents)\n\n                # construct batch_idxs which holds indices of batches to keep for the next pass\n                batch_mask = torch.ones(\n                    bsz, dtype=torch.bool, device=cand_indices.device\n                )\n                batch_mask[finalized_sents] = False\n\n                batch_idxs = torch.arange(\n                    bsz, device=cand_indices.device\n                ).masked_select(batch_mask)\n\n                # Choose the subset of the hypothesized constraints that will continue\n                self.search.prune_sentences(batch_idxs)\n\n                eos_mask = eos_mask[batch_idxs]\n                cand_beams = cand_beams[batch_idxs]\n                bbsz_offsets.resize_(new_bsz, 1)\n                cand_bbsz_idx = cand_beams.add(bbsz_offsets)\n                cand_scores = cand_scores[batch_idxs]\n                cand_indices = cand_indices[batch_idxs]\n\n                if prefix_tokens is not None:\n                    prefix_tokens = prefix_tokens[batch_idxs]\n                src_lengths = src_lengths[batch_idxs]\n                cands_to_ignore = cands_to_ignore[batch_idxs]\n\n                scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n                tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)\n                if attn is not None:\n                    attn = attn.view(bsz, -1)[batch_idxs].view(\n                        new_bsz * beam_size, attn.size(1), -1\n                    )\n                bsz = new_bsz\n            else:\n                batch_idxs = None\n\n            # Set active_mask so that values > cand_size indicate eos hypos\n            # and values < cand_size indicate candidate active hypos.\n            # After, the min values per row are the top candidate active hypos\n\n            # Rewrite the operator since the element wise or is not supported in torchscript.\n\n            eos_mask[:, :beam_size] = ~((~cands_to_ignore) & (~eos_mask[:, :beam_size]))\n            active_mask = torch.add(\n                eos_mask.type_as(cand_offsets) * cand_size,\n                cand_offsets[: eos_mask.size(1)],\n            )\n\n            # get the top beam_size active hypotheses, which are just\n            # the hypos with the smallest values in active_mask.\n            # {active_hypos} indicates which {beam_size} hypotheses\n            # from the list of {2 * beam_size} candidates were\n            # selected. Shapes: (batch size, beam size)\n            new_cands_to_ignore, active_hypos = torch.topk(\n                active_mask, k=beam_size, dim=1, largest=False\n            )\n\n            # update cands_to_ignore to ignore any finalized hypos.\n            cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]\n            # Make sure there is at least one active item for each sentence in the batch.\n            assert (~cands_to_ignore).any(dim=1).all()\n\n            # update cands_to_ignore to ignore any finalized hypos\n\n            # {active_bbsz_idx} denotes which beam number is continued for each new hypothesis (a beam\n            # can be selected more than once).\n            active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)\n            active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)\n\n            active_bbsz_idx = active_bbsz_idx.view(-1)\n            active_scores = active_scores.view(-1)\n\n            # copy tokens and scores for active hypotheses\n\n            # Set the tokens for each beam (can select the same row more than once)\n            tokens[:, : step + 1] = torch.index_select(\n                tokens[:, : step + 1], dim=0, index=active_bbsz_idx\n            )\n            # Select the next token for each of them\n            tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(\n                cand_indices, dim=1, index=active_hypos\n            )\n            if step > 0:\n                scores[:, :step] = torch.index_select(\n                    scores[:, :step], dim=0, index=active_bbsz_idx\n                )\n            scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(\n                cand_scores, dim=1, index=active_hypos\n            )\n\n            # Update constraints based on which candidates were selected for the next beam\n            self.search.update_constraints(active_hypos)\n\n            # copy attention for active hypotheses\n            if attn is not None:\n                attn[:, :, : step + 2] = torch.index_select(\n                    attn[:, :, : step + 2], dim=0, index=active_bbsz_idx\n                )\n\n            # reorder incremental state in decoder\n            reorder_state = active_bbsz_idx\n\n        # sort by score descending\n        for sent in range(len(finalized)):\n            scores = torch.tensor(\n                [float(elem[\"score\"].item()) for elem in finalized[sent]]\n            )\n            _, sorted_scores_indices = torch.sort(scores, descending=True)\n            finalized[sent] = [finalized[sent][ssi] for ssi in sorted_scores_indices]\n            finalized[sent] = torch.jit.annotate(\n                List[Dict[str, Tensor]], finalized[sent]\n            )\n        return finalized\n\n    def _prefix_tokens(\n        self, step: int, lprobs, scores, tokens, prefix_tokens, beam_size: int\n    ):\n        \"\"\"Handle prefix tokens\"\"\"\n        prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)\n        prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))\n        prefix_mask = prefix_toks.ne(self.pad)\n        if self.constraint_trie is None:\n            lprobs[prefix_mask] = torch.min(prefix_lprobs) - 1\n        else:\n            lprobs[prefix_mask] = -math.inf\n        lprobs[prefix_mask] = lprobs[prefix_mask].scatter(\n            -1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lprobs[prefix_mask]\n        )\n        # if prefix includes eos, then we should make sure tokens and\n        # scores are the same across all beams\n        eos_mask = prefix_toks.eq(self.eos)\n        if eos_mask.any():\n            # validate that the first beam matches the prefix\n            first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[\n                :, 0, 1 : step + 1\n            ]\n            eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]\n            target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]\n            assert (first_beam == target_prefix).all()\n\n            # copy tokens, scores and lprobs from the first beam to all beams\n            tokens = self.replicate_first_beam(tokens, eos_mask_batch_dim, beam_size)\n            scores = self.replicate_first_beam(scores, eos_mask_batch_dim, beam_size)\n            lprobs = self.replicate_first_beam(lprobs, eos_mask_batch_dim, beam_size)\n        return lprobs, tokens, scores\n\n    def replicate_first_beam(self, tensor, mask, beam_size: int):\n        tensor = tensor.view(-1, beam_size, tensor.size(-1))\n        tensor[mask] = tensor[mask][:, :1, :]\n        return tensor.view(-1, tensor.size(-1))\n\n    def finalize_hypos(\n        self,\n        step: int,\n        bbsz_idx,\n        eos_scores,\n        tokens,\n        scores,\n        finalized: List[List[Dict[str, Tensor]]],\n        finished: List[bool],\n        beam_size: int,\n        attn: Optional[Tensor],\n        src_lengths,\n        max_len: int,\n    ):\n        \"\"\"Finalize hypothesis, store finalized information in `finalized`, and change `finished` accordingly.\n        A sentence is finalized when {beam_size} finished items have been collected for it.\n\n        Returns number of sentences (not beam items) being finalized.\n        These will be removed from the batch and not processed further.\n        Args:\n            bbsz_idx (Tensor):\n        \"\"\"\n        assert bbsz_idx.numel() == eos_scores.numel()\n\n        # clone relevant token and attention tensors.\n        # tokens is (batch * beam, max_len). So the index_select\n        # gets the newly EOS rows, then selects cols 1..{step + 2}\n        tokens_clone = tokens.index_select(0, bbsz_idx)[\n            :, 1 : step + 2\n        ]  # skip the first index, which is EOS\n\n        tokens_clone[:, step] = self.eos\n        attn_clone = (\n            attn.index_select(0, bbsz_idx)[:, :, 1 : step + 2]\n            if attn is not None\n            else None\n        )\n\n        # compute scores per token position\n        pos_scores = scores.index_select(0, bbsz_idx)[:, : step + 1]\n        pos_scores[:, step] = eos_scores\n        # convert from cumulative to per-position scores\n        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]\n\n        # normalize sentence-level scores\n        if self.normalize_scores:\n            eos_scores /= (step + 1) ** self.len_penalty\n\n        # cum_unfin records which sentences in the batch are finished.\n        # It helps match indexing between (a) the original sentences\n        # in the batch and (b) the current, possibly-reduced set of\n        # sentences.\n        cum_unfin: List[int] = []\n        prev = 0\n        for f in finished:\n            if f:\n                prev += 1\n            else:\n                cum_unfin.append(prev)\n        cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int).to(bbsz_idx)\n\n        unfin_idx = bbsz_idx // beam_size\n        sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)\n\n        # Create a set of \"{sent}{unfin_idx}\", where\n        # \"unfin_idx\" is the index in the current (possibly reduced)\n        # list of sentences, and \"sent\" is the index in the original,\n        # unreduced batch\n        # For every finished beam item\n        # sentence index in the current (possibly reduced) batch\n        seen = (sent << 32) + unfin_idx\n        unique_seen: List[int] = torch.unique(seen).tolist()\n\n        if self.match_source_len:\n            condition = step > torch.index_select(src_lengths, 0, unfin_idx)\n            eos_scores = torch.where(condition, torch.tensor(-math.inf), eos_scores)\n        sent_list: List[int] = sent.tolist()\n        for i in range(bbsz_idx.size()[0]):\n            # An input sentence (among those in a batch) is finished when\n            # beam_size hypotheses have been collected for it\n            if len(finalized[sent_list[i]]) < beam_size:\n                if attn_clone is not None:\n                    # remove padding tokens from attn scores\n                    hypo_attn = attn_clone[i]\n                else:\n                    hypo_attn = torch.empty(0)\n\n                finalized[sent_list[i]].append(\n                    {\n                        \"tokens\": tokens_clone[i],\n                        \"score\": eos_scores[i],\n                        \"attention\": hypo_attn,  # src_len x tgt_len\n                        \"alignment\": torch.empty(0),\n                        \"positional_scores\": pos_scores[i],\n                    }\n                )\n\n        newly_finished: List[int] = []\n        for unique_s in unique_seen:\n            # check termination conditions for this sentence\n            unique_sent: int = unique_s >> 32\n            unique_unfin_idx: int = unique_s - (unique_sent << 32)\n\n            if not finished[unique_sent] and self.is_finished(\n                step, unique_unfin_idx, max_len, len(finalized[unique_sent]), beam_size\n            ):\n                finished[unique_sent] = True\n                newly_finished.append(unique_unfin_idx)\n\n        return newly_finished\n\n    def is_finished(\n        self,\n        step: int,\n        unfin_idx: int,\n        max_len: int,\n        finalized_sent_len: int,\n        beam_size: int,\n    ):\n        \"\"\"\n        Check whether decoding for a sentence is finished, which\n        occurs when the list of finalized sentences has reached the\n        beam size, or when we reach the maximum length.\n        \"\"\"\n        assert finalized_sent_len <= beam_size\n        if finalized_sent_len == beam_size or step == max_len:\n            return True\n        return False\n\n\nclass EnsembleModel(nn.Module):\n    \"\"\"A wrapper around an ensemble of models.\"\"\"\n\n    def __init__(self, models):\n        super().__init__()\n        self.models_size = len(models)\n        # method '__len__' is not supported in ModuleList for torch script\n        self.single_model = models[0]\n        self.models = nn.ModuleList(models)\n\n\n        self.has_incremental = True\n\n    def forward(self):\n        pass\n\n    def has_encoder(self):\n        return hasattr(self.single_model, \"encoder\")\n\n    def has_incremental_states(self):\n        return self.has_incremental\n\n    def max_decoder_positions(self):\n        return min([m.max_decoder_positions() for m in self.models if hasattr(m, \"max_decoder_positions\")] + [sys.maxsize])  #\n\n    @torch.jit.export\n    def forward_encoder(self, net_input: Dict[str, Tensor]):\n        if not self.has_encoder():\n            return None\n        encoder_input = {\n            k: v for k, v in net_input.items() if k != \"decoder_input_ids\"\n        }\n        encoder_input[\"output_hidden_states\"] = True\n        return [model.encoder.forward(**encoder_input) for model in self.models]\n\n    @torch.jit.export\n    def forward_decoder(\n        self,\n        tokens,\n        encoder_outs: List[Dict[str, List[Tensor]]],\n        incremental_states: List[Optional[torch.Tensor]],\n        temperature: float = 1.0,\n        constraint_trie=None,\n        constraint_start=None,\n        constraint_end=None,\n        gen_code=False,\n        zero_shot=False,\n        prefix_tokens=None\n    ):\n        log_probs = []\n        avg_attn: Optional[Tensor] = None\n        encoder_out: Optional[Dict[str, List[Tensor]]] = None\n        code_mask = (tokens.new_ones(tokens.size(0))*gen_code).bool()\n\n        for i, model in enumerate(self.models):\n            if self.has_encoder():\n                encoder_out = encoder_outs[i]\n                encoder_hidden_states = encoder_out.last_hidden_state\n                encoder_attention_mask = _expand_mask(~encoder_out.padding_mask, encoder_hidden_states.dtype,\n                                                      tokens.shape[-1])\n                src_pos_embed = encoder_out.position_embedding\n\n                # if tokens.eq(self.single_model.config.pad_token_id).any():\n                attention_mask = ~tokens.eq(self.single_model.padding_idx)\n\n\n            # decode each model\n            if self.has_incremental_states():\n                decoder_out = model.decoder.forward(\n                    input_ids=tokens,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    code_masks=code_mask,\n                    src_pos_embed=src_pos_embed,\n                    past_key_values=incremental_states[i],\n                    use_cache=True,\n                    output_attentions = True\n                )\n            else:\n                if hasattr(model, \"decoder\"):\n                    decoder_out = model.decoder.forward(\n                    input_ids=tokens,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    code_masks=code_mask,\n                    src_pos_embed=src_pos_embed\n                )\n                else:\n                    decoder_out = model.forward(tokens)\n\n\n            attn: Optional[Tensor] = None\n            decoder_len = len(decoder_out)\n\n            if \"cross_attentions\" in decoder_out:\n                attn = decoder_out[\"cross_attentions\"][-1].transpose(1, 0)\n                attn = attn.mean(dim=0)  # (B, tgt_len, src_len)\n                if attn is not None:\n                    attn = attn[:, -1, :]\n\n\n            decoder_out_tuple = (\n                decoder_out[0][:, -1:, :].div_(temperature),\n                None if decoder_len <= 1 else attn,\n            )\n\n            beam_size = decoder_out_tuple[0].size(0) // prefix_tokens.size(0) if prefix_tokens is not None else 0\n            if constraint_trie is not None and not zero_shot:\n                assert constraint_start is None and constraint_end is None\n                constraint_masks = decoder_out_tuple[0].new_zeros(decoder_out_tuple[0].size()).bool()\n                constraint_prefix_tokens = tokens.tolist()\n                for token_index, constraint_prefix_token in enumerate(constraint_prefix_tokens):\n                    prefix_len = prefix_tokens[token_index // beam_size].ne(1).sum().item() if prefix_tokens is not None else 0\n                    if len(constraint_prefix_token) > prefix_len:\n                        constraint_prefix_token = [0] + constraint_prefix_token[prefix_len+1:]\n                        constraint_nodes = constraint_trie.get_next_layer(constraint_prefix_token)\n                        constraint_masks[token_index][:, constraint_nodes] = True\n                    else:\n                        constraint_masks[token_index] = True\n                decoder_out_tuple[0].masked_fill_(~constraint_masks, -math.inf)\n            if constraint_start is not None and constraint_end is not None and not zero_shot:\n                assert constraint_trie is None\n                decoder_out_tuple[0][:, :, 4:constraint_start] = -math.inf\n                decoder_out_tuple[0][:, :, constraint_end:] = -math.inf\n\n            probs = model.get_normalized_probs(\n                decoder_out_tuple, log_probs=True, sample=None\n            )\n            if constraint_trie is not None and zero_shot:\n                assert constraint_start is None and constraint_end is None\n                constraint_masks = decoder_out_tuple[0].new_zeros(decoder_out_tuple[0].size()).bool()\n                constraint_prefix_tokens = tokens.tolist()\n                for token_index, constraint_prefix_token in enumerate(constraint_prefix_tokens):\n                    constraint_nodes = constraint_trie.get_next_layer(constraint_prefix_token)\n                    constraint_masks[token_index][:, constraint_nodes] = True\n                probs.masked_fill_(~constraint_masks, -math.inf)\n            if constraint_start is not None and constraint_end is not None and zero_shot:\n                assert constraint_trie is None\n                probs[:, :, 4:constraint_start] = -math.inf\n                probs[:, :, constraint_end:] = -math.inf\n            probs = probs[:, -1, :]\n            if self.models_size == 1:\n                return probs, attn\n\n            log_probs.append(probs)\n            if attn is not None:\n                if avg_attn is None:\n                    avg_attn = attn\n                else:\n                    avg_attn.add_(attn)\n\n        avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(\n            self.models_size\n        )\n\n        if avg_attn is not None:\n            avg_attn.div_(self.models_size)\n        return avg_probs, avg_attn\n\n    @torch.jit.export\n    def reorder_encoder_out(\n        self, encoder_outs: Optional[List[Dict[str, List[Tensor]]]], new_order\n    ):\n        \"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n        new_outs: List[Dict[str, List[Tensor]]] = []\n        if not self.has_encoder():\n            return new_outs\n        for i, model in enumerate(self.models):\n            assert encoder_outs is not None\n            new_outs.append(\n                model.encoder.reorder_encoder_out(encoder_outs[i], new_order)\n            )\n        return new_outs\n\n    @torch.jit.export\n    def reorder_incremental_state(\n        self,\n        incremental_states: List[Optional[torch.Tensor]],\n        new_order,\n    ):\n        if not self.has_incremental_states():\n            return\n        for i, model in enumerate(self.models):\n            model.decoder.reorder_incremental_state_scripting(\n                incremental_states[i], new_order\n            )\n\n"}
{"type": "source_file", "path": "src/caption/ofa/generate/token_generation_constraints.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\"\"\"Implements tracking of constraints for a beam item.\n\nA list of constraints is given as a list of one or more token\nsequences, each of length at least one token. For example, for an input sentence\n\n> Die maschinelle bersetzung ist schwer zu kontrollieren.\n\nWe could have the constraints:\n* to influence\n* hard\n\nThere are two implementations:\n* OrderedConstraintState: Tracks progress through an ordered list of multitoken constraints.\n* UnorderedConstraintState: Tracks progress through an unordered list of multitoken constraints.\n\nThe difference is that in the first, the constraints are assumed to be\nin order; the algorithm will permit zero or more tokens between them.\nIn the second, the constraints are not ordered, so many orderings will\nbe explored.\n\nThe same sequence can be present any number of times, and will appear\nthat many times in the output.\n\"\"\"\n\nfrom collections import Counter\nfrom typing import List, Optional, Set, Tuple\n\nimport torch\n\n\nclass ConstraintState:\n    def __init__(self):\n        pass\n\n\ndef pack_constraints(batch_constraints: List[List[torch.Tensor]]) -> torch.Tensor:\n    \"\"\"Takes a list of list of constraints in tensor form (a list of\n    tensor constraints for each sentence) and transforms it into a\n    packed Tensor. For example, here is a batch of size 3 with 3, 0,\n    and 1 constraints:\n\n        [ [ [3 1 2], [3], [4 5 6 7], ]\n          [],\n          [ [1 8 9 10 1 4 11 12], ]\n        ]\n\n    Its corresponding packed structure is:\n\n        [ [ 3  3  1  2  0  3  0  4  5  6  7  0],\n          [ 0  0  0  0  0  0  0  0  0  0  0  0],\n          [ 1  1  8  9 10  1  4 11 12  0  0  0] ]\n\n    The packed tensor has shape (batch size, maxlen), where\n    maxlen is defined below. Each row contains concatenated\n    constraint tokens for that sentence, with 0 appended after\n    each constraint. The first item in each row is the number\n    of constraints for that sentence. So maxlen is the maximum\n    of\n\n    (number of constraints) + (sum length of constraints) + 1.\n\n    across all sentences in the batch.\n    \"\"\"\n    # The maximum word length of concatenated constraints for any sentence\n    max_constraints_len = 1\n    for sentence_constraints in batch_constraints:\n        if len(sentence_constraints):\n            # number of constraints, plus sum of constrain lens, plus a zero after each\n            constraints_len = (\n                1\n                + sum([c.size(0) for c in sentence_constraints])\n                + len(sentence_constraints)\n            )\n            max_constraints_len = max(max_constraints_len, constraints_len)\n\n    batch_size = len(batch_constraints)\n    constraints_tensor = torch.zeros((batch_size, max_constraints_len)).long()\n    for i, sentence_constraints in enumerate(batch_constraints):\n        constraints_tensor[i, 0] = len(sentence_constraints)\n        offset = 1\n        for j, constraint in enumerate(sentence_constraints):\n            this_len = constraint.size(0)\n            constraints_tensor[i, offset : offset + this_len] = constraint\n            offset += this_len + 1\n\n    return constraints_tensor.long()\n\n\ndef unpack_constraints(constraint_tensor: torch.Tensor) -> List[torch.Tensor]:\n    \"\"\"\n    Transforms *one row* of a packed constraint tensor (e.g., for one\n    sentence in the batch) into a list of constraint tensors.\n    \"\"\"\n    constraint_list = []\n    num_constraints = constraint_tensor[0]\n    constraints = constraint_tensor.tolist()\n    offset = 1\n    for i in range(num_constraints):\n        where = constraints.index(0, offset)\n        constraint_list.append(constraint_tensor[offset:where])\n        offset = where + 1\n\n    return constraint_list\n\n\nclass ConstraintNode:\n    \"\"\"\n    Represents a node in a trie managing unordered constraints.\n    \"\"\"\n\n    def __init__(self, token: int = None, parent=None):\n        # The token associate with this node (None for the root)\n        self.token = int(token) if token is not None else None\n        # The parent (None at the root)\n        self.parent = parent\n        # Whether this node is a completed constraint\n        self.terminal = 0\n        # List of child nodes\n        self.children = {}\n\n        # The cumulative number of constraints from this point in the\n        # trie forward\n        self.num_constraints = 0\n\n    @property\n    def id(self):\n        return self.token\n\n    def __str__(self):\n        term = self.terminal != 0\n        return f\"[{self.token}].{term}#{self.num_constraints}\"\n\n    def __getitem__(self, key: int):\n        return self.children.get(key, None)\n\n    def next_tokens(self) -> Set[int]:\n        \"\"\"The set of child labels.\"\"\"\n        return set(self.children.keys())\n\n    @staticmethod\n    def create(constraints: List[List[int]]):\n        root = ConstraintNode()\n        for sequence in constraints:\n            root.add_sequence(sequence)\n\n        return root\n\n    @staticmethod\n    def print_graph(node: \"ConstraintNode\"):\n        if len(node.children) == 0:\n            return str(node)\n        else:\n            s = f\"({node}\"\n            for child in node.children.values():\n                s += \" \" + ConstraintNode.print_graph(child)\n            s += \")\"\n            return s\n\n    def token_counts(self) -> Counter:\n        \"\"\"Returns a counter of the number of times each token is used\n        in a constraint.\n        \"\"\"\n        token_counts = Counter()\n        kids = list(self.children.values())\n        while len(kids) > 0:\n            kid = kids.pop()\n            token_counts[kid.id] += kid.num_constraints\n            kids += list(kid.children.values())\n\n        return token_counts\n\n    def tokens(self) -> Set[int]:\n        \"\"\"Returns the set of tokens in constraints.\"\"\"\n        return set(self.token_counts().keys())\n\n    def add_sequence(self, sequence: List[int]):\n        \"\"\"Adds a constraint, represented as a list of integers, to\n        the trie.\"\"\"\n        assert len(sequence) > 0\n\n        token = int(sequence[0])\n        if token not in self.children:\n            self.children[token] = ConstraintNode(token, parent=self)\n\n        node = self.children[token]\n        if len(sequence) == 1:\n            node.terminal += 1\n            node.num_constraints += 1\n            parent = node.parent\n            while parent is not None:\n                parent.num_constraints += 1\n                parent = parent.parent\n        else:\n            node.add_sequence(sequence[1:])\n\n\nclass UnorderedConstraintState(ConstraintState):\n    \"\"\"\n    Records progress through the set of constraints for each item in the beam\n    using a trie.\n    \"\"\"\n\n    def __init__(self, node: ConstraintNode, copy_from: \"ConstraintState\" = None):\n        self.node = node\n\n        if copy_from is None:\n            # The root node\n            self.root = node\n            # The set of states in the graph that have been completed\n            self.completed = Counter()\n            # The...\n            self.generated = Counter()\n            # The list of tokens we need to generate\n            self.needed_tokens = self.root.tokens()\n        else:\n            self.completed = Counter(copy_from.completed)\n            self.generated = Counter(copy_from.generated)\n            self.root = copy_from.root\n\n        # Mark the node as generated\n        if self.node != self.root:\n            self.generated[node] += 1\n\n    @staticmethod\n    def create(constraint_tensor: torch.Tensor):\n        constraint_list = unpack_constraints(constraint_tensor)\n        constraint_trie_root = ConstraintNode.create(constraint_list)\n        return UnorderedConstraintState(constraint_trie_root)\n\n    def __str__(self):\n        gen_str = \",\".join([str(node) for node in self.generated])\n        return f\"{self.name}/{self.bank}({gen_str})x{self.num_completed}\"\n\n    def __copy__(self):\n        copied_state = UnorderedConstraintState(self.node, copy_from=self)\n        return copied_state\n\n    def copy(self):\n        return self.__copy__()\n\n    @property\n    def name(self):\n        if self.node.id is None:\n            return \"ROOT\"\n        else:\n            return str(self.node.id)\n\n    @property\n    def is_root(self):\n        return self.node == self.root\n\n    @property\n    def bank(self):\n        return sum(self.generated.values())\n\n    @property\n    def num_completed(self):\n        \"\"\"The number of constraints (not constraint tokens) that are completed.\n        In addition to the already-completed states, we need to account for the\n        current state, which might get marked as completed when another token\n        is generated.\n        \"\"\"\n        in_final = self.node.terminal and self.completed[self.node] < self.node.terminal\n        return sum(self.completed.values()) + in_final\n\n    @property\n    def finished(self):\n        return self.root.num_constraints - self.num_completed == 0\n\n    @property\n    def token_counts(self):\n        return self.root.token_counts()\n\n    @property\n    def tokens(self):\n        return self.root.tokens()\n\n    @property\n    def num_constraint_tokens(self):\n        return sum(self.token_counts.values())\n\n    def next_tokens(self) -> Set[int]:\n        \"\"\"Returns the list of tokens that could come next.\n        These are (a) all tokens extending the root state and, for\n        non-root states, additionally all tokens extending the current\n        state.\"\"\"\n\n        if self.node != self.root:\n            return self.root.next_tokens().union(self.node.next_tokens())\n        else:\n            return self.root.next_tokens()\n\n    def advance(self, token: int):\n        \"\"\"Reads in a token and advances the state. Here's how it works.\n\n        We can advance to the next state if:\n        - there is a matching child\n        - its path isn't blocked\n\n        A path is blocked when all constraints that are descendants of\n        that node have already been generated, in the current state.\n\n        If we are not able to advance from the current state, we \"fall\n        off the graph\" and return to the root state. There, we again\n        try to advance, checking the same criteria.\n\n        In any case, when falling off the graph, we need to do some\n        bookkeeping. We:\n        - check whether any constraints were met (all prefixes of\n          current state)\n        - if one is found, mark it as completed\n        - adjust visited nodes accordingly\n        \"\"\"\n        token = int(token)\n\n        next_state = None\n        child = self.node[token]\n        if child is not None and self.generated[child] < child.num_constraints:\n            next_state = UnorderedConstraintState(child, copy_from=self)\n\n        def rewind():\n            \"\"\"If we're mid-trie and an \"illegal\" token is chosen next, we need\n            to reset our state to the root state. However, along the way, we need\n            to check whether a prefix of the current trie state represents a state\n            we could mark as completed.\n            \"\"\"\n            node = self.node\n            while node != self.root:\n                if node.terminal and self.completed[node] < node.terminal:\n                    next_state.completed[node] += 1\n                    return\n\n                next_state.generated[node] -= 1\n                node = node.parent\n\n        # Fall off the graph, check the root\n        if next_state is None and token in self.root.next_tokens():\n            child = self.root[token]\n            # We can only traverse this edge if it's not saturated\n            if self.generated[child] < child.num_constraints:\n                next_state = UnorderedConstraintState(child, copy_from=self)\n            else:\n                next_state = UnorderedConstraintState(self.root, copy_from=self)\n\n            # Rewind\n            rewind()\n\n        elif next_state is None:\n            next_state = UnorderedConstraintState(self.root, copy_from=self)\n            # Rewind\n            rewind()\n\n        return next_state\n\n\nclass ConstraintSequence:\n    def __init__(self, sequences: List[List[int]]):\n        \"\"\"Represents a set of possibly multitoken constraints by\n        concatenating them and internally recording the end points.\n        \"\"\"\n        self.sequences = []\n        self.endpoints = []\n        self.num_tokens = 0\n        self.tokens = set()\n        for sequence in sequences:\n            for token in sequence:\n                self.tokens.add(token)\n            self.num_tokens += len(sequence)\n            self.endpoints += [False for x in range(len(sequence) - 1)] + [True]\n            self.sequences += sequence\n\n    def __getitem__(self, key: int):\n        return self.sequences[key]\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __str__(self):\n        return str(self.sequences)\n\n\nclass OrderedConstraintState(ConstraintState):\n    \"\"\"\n    Records progress through the set of linear nonbranching constraints with gaps.\n    \"\"\"\n\n    def __init__(self, sequence: ConstraintSequence, state: int = -1):\n        self.sequence = sequence\n        self.state = state\n\n    @staticmethod\n    def create(constraint_tensor: torch.Tensor):\n        constraint_list = unpack_constraints(constraint_tensor)\n        return OrderedConstraintState(ConstraintSequence(constraint_list), -1)\n\n    def __str__(self):\n        return f\"{self.state}/{self.bank}x{self.num_completed}\"\n\n    def __copy__(self):\n        return OrderedConstraintState(self.sequence, self.state)\n\n    def copy(self):\n        return self.__copy__()\n\n    @property\n    def num_completed(self):\n        if self.state == -1:\n            return 0\n        count = len(\n            list(filter(lambda x: x, self.sequence.endpoints[0 : self.state + 1]))\n        )\n        return count\n\n    @property\n    def is_root(self):\n        return self.state == -1\n\n    @property\n    def name(self):\n        if self.state == -1:\n            return \"ROOT\"\n        else:\n            return str(self.sequence[self.state])\n\n    @property\n    def bank(self) -> int:\n        return self.state + 1\n\n    @property\n    def finished(self):\n        return self.state + 1 == len(self.sequence)\n\n    @property\n    def token_counts(self):\n        return self.sequence.token_counts()\n\n    @property\n    def tokens(self):\n        return self.sequence.tokens\n\n    @property\n    def num_constraint_tokens(self):\n        return sum(self.token_counts.values())\n\n    def next_tokens(self) -> Set[int]:\n        \"\"\"Returns the list of tokens that could come next.\n        These are (a) all tokens extending the root state and, for\n        non-root states, additionally all tokens extending the current\n        state.\"\"\"\n\n        tokens = set()\n        if self.state > 0:\n            tokens.add(self.sequence[0])\n        if not self.finished:\n            tokens.add(self.sequence[self.state + 1])\n        return tokens\n\n    def advance(self, token: int):\n        \"\"\"Reads in a token and advances the state. Here's how it works.\n\n        We can advance to the next state if:\n        - there is a matching child\n        - its path isn't blocked\n\n        A path is blocked when all constraints that are descendants of\n        that node have already been generated, in the current state.\n\n        If we are not able to advance from the current state, we \"fall\n        off the graph\" and return to the root state. There, we again\n        try to advance, checking the same criteria.\n\n        In any case, when falling off the graph, we need to do some\n        bookkeeping. We:\n        - check whether any constraints were met (all prefixes of\n          current state)\n        - if one is found, mark it as completed\n        - adjust visited nodes accordingly\n        \"\"\"\n        token = int(token)\n        # print(f\"{self} ADVANCE({token}) {self.sequence} -> \", end=\"\")\n\n        if self.finished:\n            # Accept anything\n            next_state = self.copy()\n\n        elif self.sequence[self.state + 1] == token:\n            # Advance to the next token\n            next_state = OrderedConstraintState(self.sequence, self.state + 1)\n\n        elif self.sequence.endpoints[self.state]:\n            # Accept anything between constraints (*)\n            next_state = self.copy()\n\n        elif token == self.sequence[0]:\n            # Start over having generated the first token\n            next_state = OrderedConstraintState(self.sequence, 0)\n        else:\n            # Start over from the root\n            next_state = OrderedConstraintState(self.sequence, -1)\n\n        return next_state\n"}
{"type": "source_file", "path": "src/caption/ofa/resnet.py", "content": "# coding=utf-8\n# Copyright 2022 The OFA-Sys Team. All rights reserved.\n\nimport torch\nimport torch.nn as nn\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as 'Drop Connect' is a.sh different form of dropout in a.sh separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n    changing the layer and argument names to 'drop path' rather than mix DropConnect as a.sh layer name and use\n    'survival rate' as the argument.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(\n        in_planes,\n        out_planes,\n        kernel_size=3,\n        stride=stride,\n        padding=dilation,\n        groups=groups,\n        bias=False,\n        dilation=dilation,\n    )\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(\n        self, inplanes, planes, stride=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=None\n    ):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n        if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        assert False\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n    # This variant is also known as ResNet V1.5 and improves accuracy according to\n    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n\n    expansion = 4\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        downsample=None,\n        groups=1,\n        base_width=64,\n        dilation=1,\n        norm_layer=None,\n        drop_path_rate=0.0,\n    ):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.0)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out = identity + self.drop_path(out)\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(\n        self,\n        layers,\n        zero_init_residual=False,\n        groups=1,\n        width_per_group=64,\n        replace_stride_with_dilation=None,\n        norm_layer=None,\n        drop_path_rate=0.0,\n    ):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(\n                \"replace_stride_with_dilation should be None \"\n                \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation)\n            )\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(Bottleneck, 64, layers[0], drop_path_rate=drop_path_rate)\n        self.layer2 = self._make_layer(\n            Bottleneck, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0], drop_path_rate=drop_path_rate\n        )\n        self.layer3 = self._make_layer(\n            Bottleneck, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1], drop_path_rate=drop_path_rate\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            elif isinstance(m, (nn.SyncBatchNorm, nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False, drop_path_rate=0.0):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(\n            block(\n                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n            )\n        )\n        self.inplanes = planes * block.expansion\n\n        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, blocks)]\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    self.inplanes,\n                    planes,\n                    groups=self.groups,\n                    base_width=self.base_width,\n                    dilation=self.dilation,\n                    norm_layer=norm_layer,\n                    drop_path_rate=dpr[i],\n                )\n            )\n\n        return nn.Sequential(*layers)\n\n    def _forward_impl(self, x):\n        # See note [TorchScript super()]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        return x\n\n    def forward(self, x):\n        return self._forward_impl(x)\n"}
{"type": "source_file", "path": "src/caption/ofa/generate/search.py", "content": "# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport math\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\nfrom .token_generation_constraints import (\n    ConstraintState,\n    OrderedConstraintState,\n    UnorderedConstraintState,\n)\nfrom torch import Tensor\n\n\nclass Search(nn.Module):\n    def __init__(self, tokenizer):\n        super().__init__()\n        self.pad = tokenizer.pad_token_id\n        self.unk = tokenizer.unk_token_id\n        self.eos = tokenizer.eos_token_id\n        tgt_dict = {value: key for key, value in tokenizer.get_vocab().items()}\n        added = {value: key for key, value in tokenizer.get_added_vocab().items()}\n        tgt_dict.update(added)\n        self.vocab_size = len(tgt_dict)\n        self.src_lengths = torch.tensor(-1)\n        self.supports_constraints = False\n        self.stop_on_max_len = False\n\n    def step(\n        self, step, lprobs, scores, prev_output_tokens=None, original_batch_idxs=None\n    ):\n        \"\"\"Take a single search step.\n\n        Args:\n            step: the current search step, starting at 0\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n            scores: (bsz x input_beam_size x step)\n                the historical model scores of each hypothesis up to this point\n            prev_output_tokens: (bsz x step)\n                the previously generated oputput tokens\n            original_batch_idxs: (bsz)\n                the tensor with the batch indices, in the range [0, bsz)\n                this is useful in case there has been applied a re-ordering\n                and we need to know the orignal indices\n\n        Return: A tuple of (scores, indices, beams) where:\n            scores: (bsz x output_beam_size)\n                the scores of the chosen elements; output_beam_size can be\n                larger than input_beam_size, e.g., we may return\n                2*input_beam_size to account for EOS\n            indices: (bsz x output_beam_size)\n                the indices of the chosen elements\n            beams: (bsz x output_beam_size)\n                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)\n        \"\"\"\n        raise NotImplementedError\n\n    @torch.jit.export\n    def set_src_lengths(self, src_lengths):\n        self.src_lengths = src_lengths\n\n    @torch.jit.export\n    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):\n        \"\"\"Initialize constraint states for constrained decoding (if supported).\n\n        Args:\n            batch_constraints: (torch.Tensor, optional)\n                the list of constraints, in packed form\n            beam_size: (int)\n                the beam size\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n        pass\n\n    def prune_sentences(self, batch_idxs: Tensor):\n        \"\"\"\n        Removes constraint states for completed sentences (if supported).\n        This is called from sequence_generator._generate() when sentences are\n        deleted from the batch.\n\n        Args:\n            batch_idxs: Indices of *sentences* whose constraint state should be *kept*.\n        \"\"\"\n        pass\n\n    def update_constraints(self, active_hypos: Tensor):\n        \"\"\"\n        Updates the constraint states by selecting the beam items that are retained.\n        This is called at each time step of sequence_generator._generate() when\n        the set of 2 * {beam_size} candidate hypotheses are reduced to the beam size.\n\n        Args:\n            active_hypos: (batch size, beam size)\n              list of integers denoting, for each sentence, which beam candidate items\n              should be kept.\n        \"\"\"\n        pass\n\n\nclass BeamSearch(Search):\n    def __init__(self, tgt_dict):\n        super().__init__(tgt_dict)\n        self.constraint_states = None\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores: Optional[Tensor],\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            assert scores is not None\n            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n\n        top_prediction = torch.topk(\n            lprobs.view(bsz, -1),\n            k=min(\n                # Take the best 2 x beam_size predictions. We'll choose the first\n                # beam_size of these which don't predict eos to continue with.\n                beam_size * 2,\n                lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n            ),\n        )\n        scores_buf = top_prediction[0]\n        indices_buf = top_prediction[1]\n        # Project back into relative indices and beams\n        beams_buf = indices_buf // vocab_size\n        indices_buf = indices_buf.fmod(vocab_size)\n\n        # At this point, beams_buf and indices_buf are single-dim and contain relative indices\n        return scores_buf, indices_buf, beams_buf\n\n\nclass PrefixConstrainedBeamSearch(Search):\n    def __init__(self, tgt_dict, prefix_allowed_tokens_fn):\n        super().__init__(tgt_dict)\n        self.prefix_allowed_tokens_fn = prefix_allowed_tokens_fn\n        self.stop_on_max_len = True\n\n    @torch.jit.export\n    def apply_mask(self, x, prev_output_tokens, original_batch_idxs):\n        beam_size = x.shape[0] // original_batch_idxs.shape[0]\n        original_batch_idxs = (\n            original_batch_idxs.unsqueeze(-1).repeat((1, beam_size)).flatten().tolist()\n        )\n\n        mask = torch.full_like(x, -math.inf)\n        for sent_i, (sent, batch_i) in enumerate(\n            zip(prev_output_tokens, original_batch_idxs)\n        ):\n            mask[sent_i, :, self.prefix_allowed_tokens_fn(batch_i, sent)] = 0\n\n        return mask\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs: Tensor,\n        scores: Tensor,\n        prev_output_tokens: Tensor,\n        original_batch_idxs: Tensor,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        lprobs += self.apply_mask(\n            lprobs.view(bsz * beam_size, 1, vocab_size),\n            prev_output_tokens,\n            original_batch_idxs,\n        ).view(bsz, beam_size, vocab_size)\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            assert scores is not None\n            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n\n        top_prediction = torch.topk(\n            lprobs.view(bsz, -1),\n            k=min(\n                # Take the best beam_size predictions. We'll choose the first\n                # beam_size of these which don't predict eos to continue with.\n                beam_size,\n                lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n            ),\n        )\n        scores_buf = top_prediction[0]\n        indices_buf = top_prediction[1]\n        beams_buf = indices_buf // vocab_size\n        indices_buf = indices_buf.fmod(vocab_size)\n        return scores_buf, indices_buf, beams_buf\n\n\nclass LexicallyConstrainedBeamSearch(Search):\n    \"\"\"Implements lexically constrained beam search as described in\n\n        Fast Lexically Constrained Decoding with Dynamic Beam\n        Allocation for Neural Machine Translation.  Post & Vilar,\n        NAACL 2018.  https://www.aclweb.org/anthology/N18-1119/\n\n    and\n\n        Improved Lexically Constrained Decoding for Translation and\n        Monolingual Rewriting. Hu et al, NAACL\n        2019. https://www.aclweb.org/anthology/N19-1090/\n\n    This is accomplished by maintaining, for each beam hypothesis, a\n    ConstraintState object (see constraints.py) that tracks which\n    constraints have been generated and using this information to\n    shape the beam for each input sentence.\n    \"\"\"\n\n    def __init__(self, tokenizer, representation):\n        super().__init__(tokenizer)\n        self.representation = representation\n        tgt_dict = {value: key for key, value in tokenizer.get_vocab().items()}\n        added = {value: key for key, value in tokenizer.get_added_vocab().items()}\n        tgt_dict.update(added)\n        self.vocab_size = len(tgt_dict)\n        self.num_cands = 0\n        self.supports_constraints = True\n\n    @torch.jit.export\n    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):\n        self.constraint_states = []\n        for constraint_tensor in batch_constraints:\n            if self.representation == \"ordered\":\n                constraint_state = OrderedConstraintState.create(constraint_tensor)\n            elif self.representation == \"unordered\":\n                constraint_state = UnorderedConstraintState.create(constraint_tensor)\n\n            self.constraint_states.append([constraint_state for i in range(beam_size)])\n\n    @torch.jit.export\n    def prune_sentences(self, batch_idxs: Tensor):\n        self.constraint_states = [\n            self.constraint_states[i] for i in batch_idxs.tolist()\n        ]\n\n    @torch.jit.export\n    def update_constraints(self, active_hypos: Tensor):\n        if self.constraint_states:\n            batch_size = active_hypos.size(0)\n            for sentid in range(batch_size):\n                self.constraint_states[sentid] = [\n                    self.constraint_states[sentid][i] for i in active_hypos[sentid]\n                ]\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs: Tensor,\n        scores: Optional[Tensor],\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        \"\"\"\n        A constrained step builds a large candidates list from the following:\n        - the top 2 * {beam_size} items over the whole beam\n        - for each item in the beam\n          - the top {each_k} (default 1)\n          - all next constraints\n        We then compute the constrained state of each beam item, and assign\n        stripe codes: 0 to the best in each bank, 1 to the 2nd-best, and so\n        on. We then sort by (stripe, score), and truncate the list at\n        2 * beam size.\n\n        Args:\n            step: the decoder step\n            lprobs: (batch size, beam size, target vocab)\n                the target-vocab distributions for each item in the beam.\n        Retrun: A tuple of (scores, indices, beams, constraints) where:\n            scores: (batch, output beam size)\n                the scores of the chosen elements\n            indices: (batch, output beam size)\n                the target vocab indices of the chosen elements\n            beams: (batch, output beam size)\n                the 0-indexed hypothesis ids of the chosen elements\n            constraints: (batch, output beam size)\n                the new constraint states\n        \"\"\"\n        each_k = 1\n        device = lprobs.device\n\n        batch_size, beam_size, vocab_size = lprobs.size()\n\n        self.num_cands = min(\n            # Just take the k-best. We'll get another k from the 1-best from each\n            # row, plus more from the constraints\n            beam_size * 2,\n            lprobs.view(batch_size, -1).size(1) - 1,  # -1 so we never select pad\n        )\n\n        # STEP 0: Preliminary. Prevent EOS for unfinished hyps across all batch items\n        constraint_states = self.constraint_states\n        if constraint_states and step > 0:\n            not_finished_indices = []\n            for sentno, sent_constraints in enumerate(constraint_states):\n                for beamno, state in enumerate(sent_constraints):\n                    index = sentno * beam_size + beamno\n                    if not state.finished:\n                        not_finished_indices.append(index)\n            not_finished_indices = torch.tensor(not_finished_indices)\n            if not_finished_indices.numel() > 0:\n                lprobs.view(batch_size * beam_size, -1)[\n                    not_finished_indices, self.eos\n                ] = -math.inf\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam entry for each batch item\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n        else:\n            # make probs contain cumulative scores for each hypothesis\n            assert scores is not None\n            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)\n\n        top_prediction = torch.topk(\n            lprobs.view(batch_size, -1),\n            self.num_cands,\n        )\n        scores_buf, indices_buf = top_prediction\n        # Project back into relative indices and beams\n        beams_buf = indices_buf // vocab_size\n        indices_buf = indices_buf.fmod(vocab_size)\n\n        # Short circuit if there are no constraints in this batch\n        if not constraint_states:\n            return scores_buf, indices_buf, beams_buf\n\n        # STEP 1: get top-1 from each hypothesis across all sentences in the batch\n        if step > 0:\n            top_scores, top_indices = torch.topk(\n                lprobs.view(batch_size * beam_size, -1),\n                k=each_k,\n                dim=1,\n            )\n            top_scores = top_scores.view(batch_size, -1)\n            top_indices = top_indices.view(batch_size, -1)\n            scores_buf = torch.cat((scores_buf, top_scores), dim=1)\n            indices_buf = torch.cat((indices_buf, top_indices), dim=1)\n            new_beams = torch.arange(0, beam_size, device=device).repeat(batch_size, 1)\n            beams_buf = torch.cat((beams_buf, new_beams), dim=1)\n\n        # Now, process sentences in the batch one by one.\n        new_scores_buf = torch.zeros((batch_size, 2 * beam_size), device=device)\n        new_indices_buf = torch.zeros((batch_size, 2 * beam_size), device=device).long()\n        new_beams_buf = torch.zeros((batch_size, 2 * beam_size), device=device).long()\n        for sentno, states in enumerate(constraint_states):\n            scores, indices, beams, new_states = self.step_sentence(\n                step,\n                sentno,\n                lprobs[sentno],\n                constraint_states[sentno],\n                beams_buf[sentno].clone(),\n                indices_buf[sentno].clone(),\n                scores_buf[sentno].clone(),\n            )\n            new_scores_buf[sentno] = scores\n            new_indices_buf[sentno] = indices\n            new_beams_buf[sentno] = beams\n            self.constraint_states[sentno] = new_states\n\n        return new_scores_buf, new_indices_buf, new_beams_buf\n\n    @torch.jit.export\n    def step_sentence(\n        self,\n        step: int,\n        sentno: int,\n        lprobs: Tensor,\n        constraint_states: List[List[ConstraintState]],\n        beams_buf: Tensor,\n        indices_buf: Tensor,\n        scores_buf: Tensor,\n    ):\n        \"\"\"Does per-sentence processing. Adds all constraints for each\n        hypothesis to the list of candidates; then removes duplicates,\n        sorts, and dynamically stripes across the banks. All tensor inputs\n        are collapsed to those pertaining to a single input sentence.\n        \"\"\"\n        device = lprobs.device\n\n        # STEP 2: Add all constraints for each beam item\n        for beamno, state in enumerate(constraint_states):\n            next_tokens = torch.tensor(list(state.next_tokens()), device=device).long()\n            if next_tokens.numel() != 0:\n                indices_buf = torch.cat((indices_buf, next_tokens))\n                next_beams = (\n                    torch.tensor(beamno, device=device)\n                    .repeat(next_tokens.size(0))\n                    .long()\n                )\n                beams_buf = torch.cat((beams_buf, next_beams))\n                next_values = lprobs[beamno].take(next_tokens.view(-1))\n                scores_buf = torch.cat((scores_buf, next_values))\n\n            # At the 0th time step, there is just one beam item\n            if step == 0:\n                break\n\n        # STEP 3: Compute the \"bank\" for each candidate. This is the\n        # number of constraints it's generated. We need this so that\n        # we can do round-robin allocation of the beam across these\n        # banks. If C is the number of constraints, we select the best\n        # item in bank C, then the best in bank C-1, etc, followed by\n        # the 2nd-best in bank C, the 2nd-best in bank C-1, etc, and so\n        # on, until the maximum beam size. We accomplish this by\n        # creating a sort key and striping across the banks.\n\n        # Compute the new states for all candidates\n        cands_size = indices_buf.size(0)\n        constraint_states = [\n            constraint_states[beams_buf[i]].advance(indices_buf[i])\n            for i in range(cands_size)\n        ]\n\n        banks = torch.tensor([state.bank for state in constraint_states], device=device)\n\n        # STEP 4: Sort\n        num_constraint_tokens = len(state.tokens)\n\n        # Sort by keys (bank, score) (i.e., sort banks together, and scores\n        # within banks). AFAIK pytorch doesn't support either stable sort or\n        # multi-key sorting, so we have to hack this.\n        MAX_SCORE = -100\n        sort_key = (num_constraint_tokens - banks) * MAX_SCORE + scores_buf\n        sort_values, sort_indices = sort_key.sort(dim=0, descending=True)\n        scores_buf = scores_buf[sort_indices]\n        indices_buf = indices_buf[sort_indices]\n        beams_buf = beams_buf[sort_indices]\n        banks = banks[sort_indices]\n\n        # Sort the constraints to follow suit\n        constraint_states = [constraint_states[i] for i in sort_indices]\n\n        # STEP 5: Remove duplicates. The topk calls (overall and\n        # per-row) plus the per-row generation of constraints will\n        # produce duplicates. Here we remove them.\n\n        def roll(t):\n            \"\"\"Rolls a 1d tensor left by 1.\n\n            [0, 1, 2, 3, 4] becomes [4, 0, 1, 2, 3]\n            \"\"\"\n            return torch.cat((t[-1].unsqueeze(0), t[0:-1]), dim=0)\n\n        # We map candidates (beam, token_id) to a single dimension.\n        # This is then shifted by 1. We can then easily identify\n        # duplicates and create a mask that identifies unique\n        # extensions.\n        uniques_mask = beams_buf * (self.vocab_size + 1) + indices_buf\n        uniques_mask = roll(uniques_mask) != uniques_mask\n\n        # Use the mask to pare down the data structures\n        scores_buf = torch.masked_select(scores_buf, uniques_mask)\n        indices_buf = torch.masked_select(indices_buf, uniques_mask)\n        beams_buf = torch.masked_select(beams_buf, uniques_mask)\n        banks = torch.masked_select(banks, uniques_mask)\n        i = 1\n        for mask in uniques_mask[1:]:\n            if not mask:\n                constraint_states.pop(i)\n            i += mask\n\n        # STEP 6: Assign IDs round-robin across banks, sort, and\n        # truncate. Now that the candidates are sorted by (bank,\n        # score) and uniqed, we dynamically allocate the {beam_size}\n        # beam by striping across the candidates. These stripes will\n        # be used as sort keys to do round-robin selection. This is\n        # accomplished in a single pass with offsets. Sorting by\n        # highest-banks (furthest-along hypotheses) first ensures\n        # progress through the constraints.\n        #\n        # e.g., BANKS: 3 3 3 2 2 2 2 1 1 1 0 0\n        # OLD STRIPES: 0 1 2 0 1 2 3 0 1 2 0 1\n        # NEW STRIPES: 0 1+4 2+8 0+1 1+5 2+9 3+11 0+2 1+6 2+10 0+3 1+7\n        #            = 0 5 10 1 6 11 13 2 7 12 3 8\n        #\n        # Sorting by this then gives the following banks:\n        #\n        #             3 2 1 0 3 2 1 0 3 2 1 2\n        #\n        # We'll take the top {beam_size} of these.\n        stripe_offsets = [offset * (len(banks) + 1) for offset in range(len(banks) + 1)]\n        stripes = torch.zeros_like(banks)\n        cur_bank_count = -1\n        cur_bank = banks[0]\n        for i, bank in enumerate(banks):\n            if bank != cur_bank:\n                cur_bank_count = 0\n                cur_bank = bank\n            else:\n                cur_bank_count += 1\n            stripes[i] = num_constraint_tokens - bank + stripe_offsets[cur_bank_count]\n\n        # STEP 7: Sort by the stripes values\n        sort_values, sort_indices = stripes.sort(dim=0)\n        scores_buf = scores_buf[sort_indices]\n        indices_buf = indices_buf[sort_indices]\n        beams_buf = beams_buf[sort_indices]\n        constraint_states = [constraint_states[i] for i in sort_indices]\n\n        # STEP 8: Truncate to the candidates size!\n        scores_buf = scores_buf[: self.num_cands]\n        indices_buf = indices_buf[: self.num_cands]\n        beams_buf = beams_buf[: self.num_cands]\n\n        return scores_buf, indices_buf, beams_buf, constraint_states\n\n\nclass LengthConstrainedBeamSearch(Search):\n    def __init__(self, tgt_dict, min_len_a, min_len_b, max_len_a, max_len_b):\n        super().__init__(tgt_dict)\n        self.min_len_a = min_len_a\n        self.min_len_b = min_len_b\n        self.max_len_a = max_len_a\n        self.max_len_b = max_len_b\n        self.beam = BeamSearch(tgt_dict)\n        self.needs_src_lengths = True\n\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        min_lens = self.min_len_a * self.src_lengths + self.min_len_b\n        max_lens = self.max_len_a * self.src_lengths + self.max_len_b\n        lprobs[step < min_lens, :, self.eos] = -math.inf\n        lprobs[step >= max_lens, :, self.eos] = 0\n        return self.beam.step(step, lprobs, scores)\n\n\nclass DiverseBeamSearch(Search):\n    \"\"\"Diverse Beam Search.\n\n    See \"Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence\n    Models\" for details.\n\n    We only implement the Hamming Diversity penalty here, which performed best\n    in the original paper.\n    \"\"\"\n\n    def __init__(self, tgt_dict, num_groups, diversity_strength):\n        super().__init__(tgt_dict)\n        self.num_groups = num_groups\n        self.diversity_strength = -diversity_strength\n        self.beam = BeamSearch(tgt_dict)\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n        if beam_size % self.num_groups != 0:\n            raise ValueError(\n                \"DiverseBeamSearch requires --beam to be divisible by the number of groups\"\n            )\n\n        # initialize diversity penalty\n        diversity_buf = torch.zeros(lprobs[:, 0, :].size()).to(lprobs)\n\n        scores_G, indices_G, beams_G = [], [], []\n        for g in range(self.num_groups):\n            lprobs_g = lprobs[:, g :: self.num_groups, :]\n            scores_g = scores[:, g :: self.num_groups, :] if step > 0 else None\n\n            # apply diversity penalty\n            if g > 0:\n                lprobs_g = torch.add(\n                    lprobs_g,\n                    other=diversity_buf.unsqueeze(1),\n                    alpha=self.diversity_strength,\n                )\n            else:\n                lprobs_g = lprobs_g.contiguous()\n\n            scores_buf, indices_buf, beams_buf = self.beam.step(\n                step, lprobs_g, scores_g\n            )\n            beams_buf.mul_(self.num_groups).add_(g)\n\n            scores_G.append(scores_buf.clone())\n            indices_G.append(indices_buf.clone())\n            beams_G.append(beams_buf.clone())\n\n            # update diversity penalty\n            diversity_buf.scatter_add_(\n                1, indices_buf, torch.ones(indices_buf.size()).to(diversity_buf)\n            )\n\n        # interleave results from different groups\n        scores_buf = torch.stack(scores_G, dim=2).view(bsz, -1)\n        indices_buf = torch.stack(indices_G, dim=2).view(bsz, -1)\n        beams_buf = torch.stack(beams_G, dim=2).view(bsz, -1)\n        return scores_buf, indices_buf, beams_buf\n\n\nclass Sampling(Search):\n    sampling_topk: int\n    sampling_topp: float\n\n    def __init__(self, tgt_dict, sampling_topk=-1, sampling_topp=-1.0):\n        super().__init__(tgt_dict)\n        self.sampling_topk = sampling_topk\n        self.sampling_topp = sampling_topp\n\n    def _sample_topp(self, lprobs):\n        \"\"\"Sample among the smallest set of elements whose cumulative probability mass exceeds p.\n\n        See `\"The Curious Case of Neural Text Degeneration\"\n        (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.\n\n        Args:\n            lprobs: (bsz x input_beam_size x vocab_size)\n                the model's log-probabilities over the vocabulary at the current step\n\n        Return: A tuple of (trimed_probs, truncated_indices) where:\n            trimed_probs: (bsz x input_beam_size x ?)\n                the model's probabilities over the elements selected to sample from. The\n                width of the third dimension is determined by top-P.\n            truncated_indices: (bsz x input_beam_size x ?)\n                the indices of the chosen elements.\n        \"\"\"\n        probs = lprobs.exp_()\n\n        # sort the last dimension (vocab dimension) in descending order\n        sorted_probs, sorted_indices = probs.sort(descending=True)\n\n        # compute a mask to indicate the words to be included in the top-P set.\n        cumsum_probs = sorted_probs.cumsum(dim=2)\n        mask = cumsum_probs.lt(self.sampling_topp)\n\n        # note that mask was computed by 'lt'. One more word needs to be included\n        # so that the cumulative probability mass can exceed p.\n        cumsum_mask = mask.cumsum(dim=2)\n        last_included = cumsum_mask[:, :, -1:]\n        last_included.clamp_(0, mask.size()[2] - 1)\n        mask = mask.scatter_(2, last_included, 1)\n\n        # truncate unnecessary dims.\n        max_dim = last_included.max()\n        truncated_mask = mask[:, :, : max_dim + 1]\n        truncated_probs = sorted_probs[:, :, : max_dim + 1]\n        truncated_indices = sorted_indices[:, :, : max_dim + 1]\n\n        # trim the words that are not in top-P by setting their probabilities\n        # to 0, so that they would not be sampled later.\n        trim_mask = ~truncated_mask\n        trimed_probs = truncated_probs.masked_fill_(trim_mask, 0)\n        return trimed_probs, truncated_indices\n\n    @torch.jit.export\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n\n        if step == 0:\n            # at the first step all hypotheses are equally likely, so use\n            # only the first beam\n            lprobs = lprobs[:, ::beam_size, :].contiguous()\n\n        if self.sampling_topp > 0:\n            # only sample from the smallest set of words whose cumulative probability mass exceeds p\n            probs, top_indices = self._sample_topp(lprobs)\n        elif self.sampling_topk > 0:\n            # only sample from top-k candidates\n            lprobs, top_indices = lprobs.topk(self.sampling_topk)\n            probs = lprobs.exp_()\n        else:\n            probs = lprobs.exp_()\n\n            # dummy data to be consistent with true branch for type check\n            top_indices = torch.empty(0).to(probs)\n        # sample\n        if step == 0:\n            indices_buf = torch.multinomial(\n                probs.view(bsz, -1),\n                beam_size,\n                replacement=True,\n            ).view(bsz, beam_size)\n        else:\n            indices_buf = torch.multinomial(\n                probs.view(bsz * beam_size, -1),\n                1,\n                replacement=True,\n            ).view(bsz, beam_size)\n\n        if step == 0:\n            # expand to beam size\n            probs = probs.expand(bsz, beam_size, -1)\n\n        # gather scores\n        scores_buf = torch.gather(probs, dim=2, index=indices_buf.unsqueeze(-1))\n        scores_buf = scores_buf.log_().view(bsz, -1)\n\n        # remap indices if using top-k or top-P sampling\n        if self.sampling_topk > 0 or self.sampling_topp > 0:\n            indices_buf = torch.gather(\n                top_indices.expand(bsz, beam_size, -1),\n                dim=2,\n                index=indices_buf.unsqueeze(-1),\n            ).squeeze(2)\n\n        if step == 0:\n            beams_buf = indices_buf.new_zeros(bsz, beam_size)\n        else:\n            beams_buf = torch.arange(0, beam_size).to(indices_buf).repeat(bsz, 1)\n            # make scores cumulative\n            scores_buf.add_(\n                torch.gather(scores[:, :, step - 1], dim=1, index=beams_buf)\n            )\n\n        return scores_buf, indices_buf, beams_buf\n\n\nclass DiverseSiblingsSearch(Search):\n    \"\"\"\n    Beam search with diverse siblings.\n\n    See \"A Simple, Fast Diverse Decoding Algorithm for Neural Generation\" for details.\n    https://arxiv.org/abs/1611.08562\n\n    1/ Calculate hypotheses for each beam\n    2/ Intra-sibling ordering\n    3/ Rewrite scores\n    4/ Choose top K hypotheses\n\n    if diversity_rate == 0 is equivalent to BeamSearch\n    \"\"\"\n\n    def __init__(self, tgt_dict, diversity_rate):\n        super().__init__(tgt_dict)\n        self.diversity_rate = diversity_rate\n        self.beam = BeamSearch(tgt_dict)\n\n    def step(\n        self,\n        step: int,\n        lprobs,\n        scores,\n        prev_output_tokens: Optional[Tensor] = None,\n        original_batch_idxs: Optional[Tensor] = None,\n    ):\n        bsz, beam_size, vocab_size = lprobs.size()\n        k = min(\n            # Take the best 2 x beam_size predictions. We'll choose the first\n            # beam_size of these which don't predict eos to continue with.\n            beam_size * 2,\n            lprobs.view(bsz, -1).size(1) - 1,  # -1 so we never select pad\n        )\n        s_list: List[Tensor]\n        i_list: List[Tensor]\n        s_list = [torch.empty(0).to(lprobs) for i in range(beam_size)]\n        i_list = [torch.LongTensor().to(device=lprobs.device) for i in range(beam_size)]\n        sibling_score = torch.arange(1, k + 1).to(lprobs) * self.diversity_rate\n\n        if step == 0:\n            return self.beam.step(step, lprobs, scores)\n        lprobs.add_(scores[:, :, step - 1].unsqueeze(-1))\n\n        # 1/ Calculate hypotheses for each beam\n        for i in range(beam_size):\n            torch.topk(lprobs[:, i, :].view(bsz, -1), k, out=(s_list[i], i_list[i]))\n            i_list[i].fmod_(vocab_size)\n\n            # 2/ Intra-sibling ordering by default from topk + 3/ Rewrite scores\n            s_list[i].sub_(sibling_score)\n\n        # 4/ Choose top K hypotheses\n        indices = torch.stack(i_list, dim=1).view(bsz, -1)\n\n        final_scores = torch.empty(0).to(lprobs)\n        final_indices = torch.LongTensor().to(device=lprobs.device)\n        final_beams = torch.LongTensor().to(device=lprobs.device)\n        (final_scores, final_indices) = torch.topk(\n            torch.stack(s_list, dim=1).view(bsz, -1),\n            k,\n        )\n\n        final_beams = final_indices // k\n\n        for i in range(bsz):\n            final_indices[i] = indices[i][final_indices[i]]\n\n        return final_scores, final_indices, final_beams\n"}
{"type": "source_file", "path": "src/caption/ofa/modeling_ofa.py", "content": "# coding=utf-8\n# Copyright 2022 The OFA-Sys Team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch OFA model.\"\"\"\n\nimport math\nimport random\nfrom typing import Optional, Tuple\nfrom dataclasses import dataclass\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.activations import ACT2FN\nfrom transformers.file_utils import (\n    add_code_sample_docstrings,\n    add_end_docstrings,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    replace_return_docstrings,\n)\nfrom transformers.file_utils import ModelOutput\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPastAndCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import logging\nfrom .configuration_ofa import OFAConfig\nfrom .resnet import ResNet\nfrom torch import Tensor\nfrom typing import Dict, List, Optional, Tuple\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"OFA-Sys/OFA-base\"\n_CONFIG_FOR_DOC = \"OFAConfig\"\n_TOKENIZER_FOR_DOC = \"OFATokenizer\"\n\nDEFAULT_MAX_SOURCE_POSITIONS = 1024\nDEFAULT_MAX_TARGET_POSITIONS = 1024\n\nDEFAULT_MIN_PARAMS_TO_WRAP = int(1e8)\n\nOFA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"OFA-Sys/OFA-tiny\",\n    \"OFA-Sys/OFA-medium\",\n    \"OFA-Sys/OFA-base\",\n    \"OFA-Sys/OFA-large\",\n]\n\ntry:\n    from apex.normalization import FusedLayerNorm as _FusedLayerNorm\n\n    has_fused_layernorm = True\n\n    class FusedLayerNorm(_FusedLayerNorm):\n        @torch.jit.unused\n        def forward(self, x):\n            if not x.is_cuda:\n                return super().forward(x)\n            else:\n                with torch.cuda.device(x.device):\n                    return super().forward(x)\n\nexcept ImportError:\n    has_fused_layernorm = False\n\n\ndef LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):\n    r\"\"\"\n    Layer normalization.\n    If apex is available, use `FusedLayerNorm` instead.\n    \"\"\"\n    if torch.jit.is_scripting():\n        export = True\n    if not export and torch.cuda.is_available() and has_fused_layernorm:\n        return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n\n\ndef make_token_bucket_position(bucket_size, max_position=DEFAULT_MAX_SOURCE_POSITIONS):\n    r\"\"\"\n    Make relative position indices for the text.\n    \"\"\"\n    context_pos = torch.arange(max_position, dtype=torch.long)[:, None]\n    memory_pos = torch.arange(max_position, dtype=torch.long)[None, :]\n    relative_pos = context_pos - memory_pos\n    sign = torch.sign(relative_pos)\n    mid = bucket_size // 2\n    abs_pos = torch.where((relative_pos < mid) & (relative_pos > -mid), mid - 1, torch.abs(relative_pos))\n    log_pos = torch.ceil(torch.log(abs_pos / mid) / math.log((max_position - 1) / mid) * (mid - 1)) + mid\n    log_pos = log_pos.int()\n    bucket_pos = torch.where(abs_pos.le(mid), relative_pos, log_pos * sign).long()\n    return bucket_pos + bucket_size - 1\n\n\ndef make_image_bucket_position(bucket_size, num_relative_distance):\n    r\"\"\"\n    Make relative position indices for the image.\n    \"\"\"\n    coords_h = torch.arange(bucket_size)\n    coords_w = torch.arange(bucket_size)\n    coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n    relative_coords[:, :, 0] += bucket_size - 1  # shift to start from 0\n    relative_coords[:, :, 1] += bucket_size - 1\n    relative_coords[:, :, 0] *= 2 * bucket_size - 1\n    relative_position_index = torch.zeros(size=(bucket_size * bucket_size + 1,) * 2, dtype=relative_coords.dtype)\n    relative_position_index[1:, 1:] = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n    relative_position_index[0, 0:] = num_relative_distance - 3\n    relative_position_index[0:, 0] = num_relative_distance - 2\n    relative_position_index[0, 0] = num_relative_distance - 1\n    return relative_position_index\n\n\ndef new_arange(x, *size):\n    r\"\"\"\n    Return a Tensor of `size` filled with a range function on the device of x.\n    If size is empty, using the size of the variable x.\n    \"\"\"\n    if len(size) == 0:\n        size = x.size()\n    return torch.arange(size[-1], device=x.device).expand(*size).contiguous()\n\n\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    r\"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\ndef _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n    r\"\"\"\n    Make causal mask used for uni-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.finfo(dtype).min)\n    mask_cond = torch.arange(mask.size(-1))\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.ones(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    r\"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)\n\n\ndef Embedding(num_embeddings, embedding_dim, padding_idx=None, zero_init=False):\n    r\"\"\"\n    Embedding for tokens\n    \"\"\"\n    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n    nn.init.normal_(m.weight, mean=0, std=embedding_dim**-0.5)\n    if padding_idx is not None:\n        nn.init.constant_(m.weight[padding_idx], 0)\n    if zero_init:\n        nn.init.constant_(m.weight, 0)\n    return m\n\n\ndef Linear(in_features, out_features, bias=True):\n    r\"\"\"\n    Implementation of linear projection with xavier initialization\n    \"\"\"\n    m = nn.Linear(in_features, out_features, bias)\n    nn.init.xavier_uniform_(m.weight)\n    if bias:\n        nn.init.constant_(m.bias, 0.0)\n    return m\n\n\nclass LayerDropModuleList(nn.ModuleList):\n    r\"\"\"\n    A LayerDrop implementation based on :class:`torch.nn.ModuleList`.\n\n    Args:\n        p (float): probability of dropping out each layer\n        modules (iterable, optional): an iterable of modules to add\n    \"\"\"\n\n    def __init__(self, p, modules=None):\n        super().__init__(modules)\n        self.p = p\n\n    def __iter__(self):\n        dropout_probs = torch.empty(len(self)).uniform_()\n        for i, m in enumerate(super().__iter__()):\n            if not self.training or (dropout_probs[i] > self.p):\n                yield m\n\n\ndef drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    r\"\"\"\n    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    Args:\n        x (`nn.Modules`): input nn layers.\n        drop_prob (`float`): drop path ratio.\n        training (`bool`): whether is training or inference.\n    \"\"\"\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (1, x.shape[1], 1)\n    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.Module):\n    r\"\"\"\n    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    Args:\n        drop_prob: drop path ratio.\n    \"\"\"\n\n    def __init__(self, drop_prob=None):\n        super().__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n\n    def extra_repr(self) -> str:\n        return \"p={}\".format(self.drop_prob)\n\n\nclass OFAAttention(nn.Module):\n    r\"\"\"\n    Multi-headed attention, with additional implementation for NormFormer.\n\n    Args:\n        embed_dim (`int`): embedding dimension.\n        num_heads (`int`): the number of attention heads.\n        dropout (`float32`): the ratio for dropout.\n        is_decoder (`bool`): whether or not decoder attention.\n        bias (`bool`): whether to add bias.\n        scale_heads (`bool`): whether to learn scaling heads, only for Normformer.\n    \"\"\"\n\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        scale_heads: bool = True,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        assert (\n            self.head_dim * num_heads == self.embed_dim\n        ), f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"\n        scale_factor = 2\n        self.scaling = float(self.head_dim * scale_factor) ** -0.5\n        self.is_decoder = is_decoder\n\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.attn_dropout = nn.Dropout(p=dropout)\n        self.c_attn = nn.Parameter(torch.ones((self.num_heads,)), requires_grad=True) if scale_heads else None\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        r\"\"\"\n        Reshape tensors for multi-head attention.\n        \"\"\"\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        key_value_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n        attn_bias: Optional[torch.Tensor] = None,\n    ):\n        r\"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor` of shape `(bsz, tgt_len, embed_dim)`)`: input states.\n            key_value_states (`torch.FloatTensor` of shape (bsz, tgt_len, embed_dim), *optional*): key value states.\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*):\n                cached past key value states for fast inference.\n            attention_mask (`torch.FloatTensor` of shape `(bsz, 1, tgt_len, seq_len)`, *optional*): attention mask.\n            output_attentions (`bool`, *optional*): whether to output attention weights of all layers.\n            attn_bias (`torch.FloatTensor` of shape `(bsz, 1, tgt_len, src_len)`, *optional*):\n                the attention bias for positional information.\n\n        Returns:\n            attn_output (`torch.FloatTensor` of shape `(bsz, tgt_len, embed_dim)`): attention outputs.\n            attn_weights_reshaped (`torch.FloatTensor`, *optional*): attention weights of all layers.\n            past_key_value (`torch.FloatTensor`, *optional*): cached key value states for fast inference.\n        \"\"\"\n\n        # if key_value_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = key_value_states is not None\n        bsz, tgt_len, embed_dim = hidden_states.size()\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        if is_cross_attention and past_key_value is not None:\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n        key_states = key_states.view(*proj_shape)\n        value_states = value_states.view(*proj_shape)\n\n        src_len = key_states.size(1)\n        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n\n        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n            )\n\n        # Add attention bias for positional information\n        if attn_bias is not None:\n            attn_weights += attn_bias\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        attn_weights = F.softmax(attn_weights, dim=-1)\n\n        if output_attentions:\n            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n        else:\n            attn_weights_reshaped = None\n\n        attn_probs = self.attn_dropout(attn_weights)\n\n        attn_output = torch.bmm(attn_probs, value_states)\n\n        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, tgt_len, embed_dim)\n\n        if self.c_attn is not None:\n            attn_output = attn_output.view(bsz, tgt_len, self.num_heads, self.head_dim)\n            attn_output = torch.einsum(\"bthd,h->bthd\", attn_output, self.c_attn)\n            attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights_reshaped, past_key_value\n\n\nclass OFAEncoderLayer(nn.Module):\n    r\"\"\"\n    OFA encoder layer implementation.\n\n    Args:\n        config: configuration for OFA.\n        drop_path_rate: the ratio for drop path.\n    \"\"\"\n\n    def __init__(self, config: OFAConfig, drop_path_rate=0.0):\n        super().__init__()\n        self.embed_dim = config.d_model\n        self.self_attn = OFAAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.encoder_attention_heads,\n            dropout=config.attention_dropout,\n        )\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.self_attn_mid_layer_norm = LayerNorm(self.embed_dim) if config.normformer else None\n        self.dropout = nn.Dropout(config.dropout)\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = nn.Dropout(config.activation_dropout)\n        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n        self.ffn_layer_norm = LayerNorm(config.encoder_ffn_dim) if config.normformer else None\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n        self.normalize_before = config.encoder_normalize_before\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n\n    def residual_connection(self, x, residual):\n        r\"\"\"\n        Residual connection with drop path.\n        \"\"\"\n        return residual + self.drop_path(x)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n        output_attentions: bool = False,\n        attn_bias: Optional[torch.Tensor] = None,\n    ):\n        r\"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape *(bsz, src_len, embed_dim)*\n            attention_mask (`torch.FloatTensor`): attention mask of size\n                *(bsz, 1, src_len, src_len)* where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                whether to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            attn_bias (`torch.FloatTensor`): bias for positional information.\n\n        Returns:\n            outputs (`tuple(torch.FloatTensor)`):\n                output hidden states of size (bsz, src_len, embed_dim), optionally with attention weights.\n        \"\"\"\n\n        residual = hidden_states\n        if self.normalize_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states, attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            attn_bias=attn_bias,\n        )\n        if self.self_attn_mid_layer_norm:\n            hidden_states = self.self_attn_mid_layer_norm(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.residual_connection(hidden_states, residual)\n        if not self.normalize_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        residual = hidden_states\n\n        if self.normalize_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = self.activation_dropout(hidden_states)\n        if self.ffn_layer_norm:\n            hidden_states = self.ffn_layer_norm(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.residual_connection(hidden_states, residual)\n        if not self.normalize_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n\n        if hidden_states.dtype == torch.float16 and (\n            torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n        ):\n            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n\nclass OFADecoderLayer(nn.Module):\n    r\"\"\"\n    OFA decoder layer implementation.\n\n    Args:\n        config: configuration for OFA.\n        drop_path_rate: the ratio for drop path.\n    \"\"\"\n\n    def __init__(self, config: OFAConfig, drop_path_rate=0.0):\n        super().__init__()\n        self.embed_dim = config.d_model\n\n        self.self_attn = OFAAttention(\n            embed_dim=self.embed_dim,\n            num_heads=config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True,\n        )\n        self.dropout = nn.Dropout(p=config.dropout)\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.activation_dropout = nn.Dropout(p=config.activation_dropout)\n\n        self.self_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.self_attn_mid_layer_norm = LayerNorm(self.embed_dim) if config.normformer else None\n        self.cross_attn = OFAAttention(\n            self.embed_dim,\n            config.decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True,\n        )\n        self.cross_attn_layer_norm = LayerNorm(self.embed_dim)\n        self.cross_attn_mid_layer_norm = LayerNorm(self.embed_dim) if config.normformer else None\n        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n        self.ffn_layer_norm = LayerNorm(config.decoder_ffn_dim) if config.normformer else None\n        self.final_layer_norm = LayerNorm(self.embed_dim)\n        self.normalize_before = config.decoder_normalize_before\n        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n\n    def residual_connection(self, x, residual):\n        r\"\"\"\n        Residual connection with drop path.\n        \"\"\"\n        return residual + self.drop_path(x)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        self_attn_bias: Optional[torch.Tensor] = None,\n        cross_attn_bias: Optional[torch.Tensor] = None,\n    ):\n        r\"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`): input to the layer.\n            attention_mask (`torch.FloatTensor` of shape `(bsz, 1, tgt_len, src_len)`):\n                attention mask where padding elements are indicated by very large negative values.\n            encoder_hidden_states (`torch.FloatTensor` of shape `(batch, seq_len, embed_dim)`):\n                cross attention input to the layer.\n            encoder_attention_mask (`torch.FloatTensor` of shape `(bsz, 1, tgt_len, src_len)`):\n                encoder attention mask where padding elements are indicated by very large negative values.\n            past_key_value (`Tuple(torch.FloatTensor)`): cached past key and value projection states\n            output_attentions (`bool`, *optional*): whether to return the attentions tensors of all attention layers.\n            use_cache (`bool`, *optional*): whether to use cache\n            self_attn_bias (`torch.FloatTensor`): self attention bias for positional information.\n            cross_attn_bias (`torch.FloatTensor`): cross attention bias for positional information.\n        \"\"\"\n\n        # Self attention with intermediate layernorm\n        residual = hidden_states\n        if self.normalize_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        # add present self-attn cache to position 1,2 of present_key_value tuple\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            attn_bias=self_attn_bias,\n        )\n        if self.self_attn_mid_layer_norm:\n            hidden_states = self.self_attn_mid_layer_norm(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.residual_connection(hidden_states, residual)\n        if not self.normalize_before:\n            hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Cross attention with intermediate layernorm\n        cross_attn_present_key_value = None\n        cross_attn_weights = None\n        if encoder_hidden_states is not None:\n            residual = hidden_states\n            if self.normalize_before:\n                hidden_states = self.cross_attn_layer_norm(hidden_states)\n            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.cross_attn(\n                hidden_states=hidden_states,\n                key_value_states=encoder_hidden_states,\n                attention_mask=encoder_attention_mask,\n                past_key_value=cross_attn_past_key_value,\n                output_attentions=output_attentions,\n                attn_bias=cross_attn_bias,\n            )\n            if self.cross_attn_mid_layer_norm:\n                hidden_states = self.cross_attn_mid_layer_norm(hidden_states)\n            hidden_states = self.dropout(hidden_states)\n            hidden_states = self.residual_connection(hidden_states, residual)\n            if not self.normalize_before:\n                hidden_states = self.cross_attn_layer_norm(hidden_states)\n\n            # add cross-attn to positions 3,4 of present_key_value tuple\n            present_key_value = present_key_value + cross_attn_present_key_value\n\n        # FFN with intermediate layernorm\n        residual = hidden_states\n        if self.normalize_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n        hidden_states = self.activation_fn(self.fc1(hidden_states))\n        hidden_states = self.activation_dropout(hidden_states)\n        if self.ffn_layer_norm:\n            hidden_states = self.ffn_layer_norm(hidden_states)\n        hidden_states = self.fc2(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.residual_connection(hidden_states, residual)\n        if not self.normalize_before:\n            hidden_states = self.final_layer_norm(hidden_states)\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights, cross_attn_weights)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nclass OFAPreTrainedModel(PreTrainedModel):\n    r\"\"\"\n    Base class OFA\n    \"\"\"\n\n    config_class = OFAConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n\n    def _init_weights(self, module):\n        r\"\"\"\n        Weight initialization which follows BERT.\n        \"\"\"\n        std = self.config.init_std\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        r\"\"\"\n        Turn on the switch of gradient checkpointing.\n        \"\"\"\n        if isinstance(module, (OFADecoder, OFAEncoder)):\n            module.gradient_checkpointing = value\n\n\n@dataclass\nclass OFAEncoderOutput(ModelOutput):\n    r\"\"\"\n    Base class for OFA's outputs.\n\n    Args:\n        last_hidden_state (`torch.FloatTensor` of shape `(bsz, seq_len, hidden)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed\n            or when `config.output_hidden_states=True`):\n\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n            shape `(bsz, seq_len, hidden)`.\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed\n            or when `config.output_attentions=True`):\n\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(bsz, num_heads, seq_len, seq_len)`.\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n\n        position_embedding (`torch.FloatTensor` of shape `(bsz, seq_len, hidden)`):\n            postional embeddings of the inputs.\n    \"\"\"\n\n    last_hidden_state: torch.FloatTensor = None\n    padding_mask: torch.Tensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None\n    position_embedding: Optional[torch.FloatTensor] = None\n\n\nOFA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`~OFAConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\nOFA_GENERATION_EXAMPLE = r\"\"\"\n    Image captioning example:\n\n    ```python\n    >>> from PIL import Image\n    >>> from torchvision import transforms\n    >>> from transformers import OFATokenizer, OFAForConditionalGeneration\n\n    >>> mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n    >>> resolution = 256\n    >>> patch_resize_transform = transforms.Compose([\n            lambda image: image.convert(\"RGB\"),\n            transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=mean, std=std)\n        ])\n\n    >>> model = OFAForConditionalGeneration.from_pretrained(ckpt_dir)\n    >>> tokenizer = OFATokenizer.from_pretrained(ckpt_dir)\n\n    >>> txt = \" what is the description of the image?\"\n    >>> inputs = tokenizer([txt], max_length=1024, return_tensors=\"pt\")[\"input_ids\"]\n    >>> img = Image.open(path_to_image)\n    >>> patch_img = patch_resize_transform(img).unsqueeze(0)\n\n    >>> gen = model.generate(inputs, patch_img=patch_img, num_beams=4)\n    >>> print(tokenizer.decode(gen, skip_special_tokens=True, clean_up_tokenization_spaces=False))\n    ```\n\"\"\"\n\n\nOFA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(bsz, seq_len)`):\n            indices of input sequence tokens in the vocabular, and padding will be ignored by default;\n\n            indices can be obtained using [`~OFATokenizer`].\n\n        patch_images (`torch.FloatTensor` of shape `(bsz, 3, height, width)`):\n            the resized image, which are transformed by the default operations.\n        patch_images_2 (`torch.FloatTensor` of shape `(bsz, 3, height, width)`):\n            the second (if it exists) image.\n        patch_masks (`torch.BoolTensor`): the patches to be masked.\n        token_embeddings (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`): token embeddings.\n        sample_patch_num (`int`): the number of patches to sample.\n        decoder_input_ids (`torch.LongTensor` of shape `(bsz, seq_len)`): indices of the sequence in the vocabulary.\n        code_masks (`torch.Tensor` of shape `(bsz, seq_len)`): masks only for code generation.\n        attention_mask (`torch.Tensor` of shape `(bsz, seq_len)`): attention mask for decoding.\n        encoder_outputs (`OFAEncoderOutput`):\n            encoder outputs with hidden states, positional embeddings, and padding masks.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n            shape `(bsz, num_heads, tgt_len, head_size)`) and 2 additional tensors of\n            shape `(bsz, num_heads, src_len, head_size)`.\n        use_cache (`bool`): whether to use cache for faster inference.\n        output_attentions (`bool`): whether to output attention weights.\n        output_hidden_states (`bool`): whether to output hidden states.\n        return_dict (`bool`): unused. Keep it for generation only.\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n\"\"\"\n\n\nclass OFAEncoder(OFAPreTrainedModel):\n    r\"\"\"\n    OFA encoder consisting of layers of [`OFAEncoderLayer`].\n\n    Args:\n        config: OFAConfig\n        embed_tokens (`nn.Embedding`, *optional*): output embedding\n    \"\"\"\n\n    def __init__(self, config: OFAConfig, embed_tokens: Optional[nn.Embedding] = None):\n        super().__init__(config)\n\n        self.dropout = nn.Dropout(config.dropout)\n        self.encoder_layerdrop = config.encoder_layerdrop\n\n        embed_dim = config.d_model\n        self.padding_idx = config.pad_token_id\n        self.max_source_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n        self.num_attention_heads = config.encoder_attention_heads\n\n        if getattr(config, \"layernorm_embedding\", False):\n            self.layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n        if embed_tokens is not None:\n            self.embed_tokens = embed_tokens\n        else:\n            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n\n        if config.add_type_embedding:\n            self.type_embedding = Embedding(2, embed_dim, padding_idx=None)\n        else:\n            self.type_embedding = None\n\n        if config.resnet_type == \"resnet18\":\n            self.embed_images = ResNet([2, 2, 2], drop_path_rate=config.resnet_drop_path_rate)\n        elif config.resnet_type == \"resnet34\":\n            self.embed_images = ResNet([3, 4, 6], drop_path_rate=config.resnet_drop_path_rate)\n        elif config.resnet_type == \"resnet50\":\n            self.embed_images = ResNet([3, 4, 6], drop_path_rate=config.resnet_drop_path_rate)\n        elif config.resnet_type == \"resnet101\":\n            self.embed_images = ResNet([3, 4, 23], drop_path_rate=config.resnet_drop_path_rate)\n        elif config.resnet_type == \"resnet152\":\n            self.embed_images = ResNet([3, 8, 36], drop_path_rate=config.resnet_drop_path_rate)\n        else:\n            raise NotImplementedError\n        self.image_proj = Linear(1024, embed_dim)\n\n        if config.resnet_model_path:\n            resnet_state_dict = torch.load(config.resnet_model_path)\n            self.embed_images.load_state_dict(resnet_state_dict)\n        if config.patch_layernorm_embedding:\n            self.patch_layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.patch_layernorm_embedding = None\n\n        self.embed_positions = Embedding(self.max_source_positions + 2, embed_dim)\n        self.embed_image_positions = Embedding(config.image_bucket_size**2 + 1, embed_dim)\n        self.pos_ln = LayerNorm(embed_dim)\n        self.image_pos_ln = LayerNorm(embed_dim)\n        self.pos_scaling = float(embed_dim / self.num_attention_heads * config.attn_scale_factor) ** -0.5\n        self.pos_q_linear = nn.Linear(embed_dim, embed_dim)\n        self.pos_k_linear = nn.Linear(embed_dim, embed_dim)\n\n        if self.encoder_layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.encoder_layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n\n        dpr = [x.item() for x in torch.linspace(0, config.encoder_drop_path_rate, config.encoder_layers)]\n        self.layers.extend([OFAEncoderLayer(config, drop_path_rate=dpr[i]) for i in range(config.encoder_layers)])\n        self.num_layers = len(self.layers)\n\n        if config.encoder_normalize_before:\n            self.layer_norm = LayerNorm(embed_dim)\n        else:\n            self.layer_norm = None\n\n        self.token_bucket_size = config.token_bucket_size\n        token_num_rel_dis = 2 * config.token_bucket_size - 1\n        token_rp_bucket = make_token_bucket_position(config.token_bucket_size)\n        self.token_rel_pos_table_list = nn.ModuleList(\n            [\n                Embedding(token_num_rel_dis, self.num_attention_heads, zero_init=True)\n                for _ in range(config.encoder_layers)\n            ]\n        )\n\n        self.image_bucket_size = config.image_bucket_size\n        image_num_rel_dis = (2 * config.image_bucket_size - 1) * (2 * config.image_bucket_size - 1) + 3\n        image_rp_bucket = make_image_bucket_position(config.image_bucket_size, image_num_rel_dis)\n        self.image_rel_pos_table_list = nn.ModuleList(\n            [\n                Embedding(image_num_rel_dis, self.num_attention_heads, zero_init=True)\n                for _ in range(config.encoder_layers)\n            ]\n        )\n\n        if config.layernorm_embedding:\n            self.layernorm_embedding = LayerNorm(embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n        self.register_buffer(\"token_rp_bucket\", token_rp_bucket)\n        self.register_buffer(\"image_rp_bucket\", image_rp_bucket)\n        self.entangle_position_embedding = config.entangle_position_embedding\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        r\"\"\"\n        Get the embedding weight.\n        \"\"\"\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        r\"\"\"\n        Set the weight of embedding with the given tensor.\n        \"\"\"\n        self.embed_tokens = value\n\n    def get_rel_pos_bias(self, x, idx):\n        r\"\"\"\n        Get the relative positional bias of the text, for attention.\n        \"\"\"\n\n        seq_len = x.size(1)\n        rp_bucket = self.token_rp_bucket[:seq_len, :seq_len]\n        values = F.embedding(rp_bucket, self.token_rel_pos_table_list[idx].weight)\n        values = values.unsqueeze(0).expand(x.size(0), -1, -1, -1)\n        values = values.permute([0, 3, 1, 2])\n        return values.contiguous()\n\n    def get_image_rel_pos_bias(self, image_position_ids, idx):\n        r\"\"\"\n        Get the relative positional bias of the image, for attention.\n        \"\"\"\n\n        bsz, seq_len = image_position_ids.shape\n        rp_bucket_size = self.image_rp_bucket.size(1)\n\n        rp_bucket = (\n            self.image_rp_bucket.unsqueeze(0)\n            .expand(bsz, rp_bucket_size, rp_bucket_size)\n            .gather(1, image_position_ids[:, :, None].expand(bsz, seq_len, rp_bucket_size))\n            .gather(2, image_position_ids[:, None, :].expand(bsz, seq_len, seq_len))\n        )\n        values = F.embedding(rp_bucket, self.image_rel_pos_table_list[idx].weight)\n        values = values.permute(0, 3, 1, 2)\n        return values\n\n    def get_patch_images_info(self, patch_images, sample_patch_num, device):\n        r\"\"\"\n        Get the basic information of the resized image.\n\n        Args:\n            patch_images (`torch.FloatTensor` of shape `(bsz, 3, height, width)`): the resized image.\n            sample_patch_num (`int`):\n                the number of patches to sample. If it is equal to -1, no sampling will be performed.\n            device: GPU device.\n\n        Returns:\n            image_embed (`torch.FloatTensor` of shape `(bsz, h * w, hidden)`): the output of the visual encoder.\n            image_num_patches (`int`, equal to `h * w`): the number of patches.\n            image_padding_mask (`torch.BooleanTensor` of shape `(bsz, h*w)`): image padding mask.\n            image_position_ids (`torch.LongTensor` of shape `(bsz, h*w)`): image position ids.\n            image_pos_embed (`torch.FloatTensor` of shape (bsz, h*w, hidden)): the positional embedding.\n        \"\"\"\n\n        image_embed = self.embed_images(patch_images)\n        h, w = image_embed.shape[-2:]\n        image_num_patches = h * w\n        image_padding_mask = patch_images.new_zeros((patch_images.size(0), image_num_patches)).bool()\n        image_position_idx = (\n            torch.arange(w).unsqueeze(0).expand(h, w) + torch.arange(h).unsqueeze(1) * self.image_bucket_size + 1\n        )\n        image_position_idx = image_position_idx.view(-1).to(device)\n        image_position_ids = image_position_idx[None, :].expand(patch_images.size(0), image_num_patches)\n\n        image_embed = image_embed.flatten(2).transpose(1, 2)\n        if sample_patch_num is not None:\n            patch_orders = [\n                random.sample(range(image_num_patches), k=sample_patch_num) for _ in range(patch_images.size(0))\n            ]\n            patch_orders = torch.LongTensor(patch_orders).to(device)\n            image_embed = image_embed.gather(1, patch_orders.unsqueeze(2).expand(-1, -1, image_embed.size(2)))\n            image_num_patches = sample_patch_num\n            image_padding_mask = image_padding_mask.gather(1, patch_orders)\n            image_position_ids = image_position_ids.gather(1, patch_orders)\n        image_pos_embed = self.embed_image_positions(image_position_ids)\n\n        return image_embed, image_num_patches, image_padding_mask, image_position_ids, image_pos_embed\n\n    def forward_embedding(\n        self,\n        input_ids,\n        image_embed: Optional[torch.Tensor] = None,\n        image_embed_2: Optional[torch.Tensor] = None,\n        token_embedding: Optional[torch.Tensor] = None,\n        pos_embed: Optional[torch.Tensor] = None,\n        image_pos_embed: Optional[torch.Tensor] = None,\n        image_pos_embed_2: Optional[torch.Tensor] = None,\n    ):\n        r\"\"\"\n        Generate embeddings of both the image and the text.\n        Actually since OFA unifies both unimodal and multimodal data,\n        image inputs are optional.\n\n        Args:\n            input_ids (`torch.LongTensor` of shape `(bsz, seq_len)`): indices of the tokens in the vocabulary.\n            image_embed (`torch.FloatTensor` of shape `(bsz, h*w, embed_dim)`, *optional*): image embeddings.\n            image_embed_2 (`torch.FloatTensor` of shape `(bsz, h*w, embed_dim)`, *optional*):\n                image embeddings of the second image (if it exists).\n            token_embedding (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`, *optional*):\n                input token embeddings to replace the embeddings of input ids.\n            image_pos_embed (`torch.FloatTensor` of shape `(bsz, h*w, embed_dim)`, *optional*):\n                positional embeddings of the image.\n            image_pos_embed_2 (`torch.FloatTensor` of shape `(bsz, h*w, embed_dim)`, *optional*):\n                positional embeddings of the second image.\n\n        Returns:\n            x (`torch.FloatTensor` of shape `(bsz, h*w+seq_len, embed_dim)`): embeddings of the input.\n            embed (`torch.FloatTensor` of shape `(bsz, h*w+seq_len, embed_dim)`):\n                embeddings without adding positional and type embeddings.\n        \"\"\"\n\n        # embed tokens and positions\n        if token_embedding is None:\n            token_embedding = self.embed_tokens(input_ids)\n        x = embed = self.embed_scale * token_embedding\n        if self.entangle_position_embedding and pos_embed is not None:\n            x += pos_embed\n        if self.type_embedding is not None:\n            x += self.type_embedding(input_ids.new_zeros(x.size()[:2]))\n        if self.layernorm_embedding is not None:\n            x = self.layernorm_embedding(x)\n        x = self.dropout(x)\n\n        # embed raw images\n        if image_embed is not None:\n            image_embed = self.image_proj(image_embed)\n            image_x = image_embed = self.embed_scale * image_embed\n            if self.entangle_position_embedding and image_pos_embed is not None:\n                image_x += image_pos_embed\n            if self.type_embedding is not None:\n                image_x += self.type_embedding(input_ids.new_ones(image_x.size()[:2]))\n            if self.patch_layernorm_embedding is not None:\n                image_x = self.patch_layernorm_embedding(image_x)\n            image_x = self.dropout(image_x)\n            x = torch.cat([image_x, x], dim=1)\n            embed = torch.cat([image_embed, embed], dim=1)\n\n        if image_embed_2 is not None:\n            assert self.type_embedding is not None\n            image_embed_2 = self.image_proj(image_embed_2)\n            image_x_2 = image_embed_2 = self.embed_scale * image_embed_2\n            if self.entangle_position_embedding and image_pos_embed_2 is not None:\n                image_x_2 += image_pos_embed_2\n            if self.type_embedding is not None:\n                image_x_2 += self.type_embedding(input_ids.new_full(image_x_2.size()[:2], fill_value=2))\n            if self.patch_layernorm_embedding is not None:\n                image_x_2 = self.patch_layernorm_embedding(image_x_2)\n            image_x_2 = self.dropout(image_x_2)\n            if self.quant_noise is not None:\n                image_x_2 = self.quant_noise(image_x_2)\n            x = torch.cat([image_x_2, x], dim=1)\n            embed = torch.cat([image_embed_2, embed], dim=1)\n\n        return x, embed\n\n    def reorder_encoder_out(self, encoder_out, new_order):\n        \"\"\"\n        Reorder encoder output according to *new_order*.\n\n        Args:\n            encoder_out: output from the ``forward()`` method\n            new_order (LongTensor): desired order\n\n        Returns:\n            *encoder_out* rearranged according to *new_order*\n        \"\"\"\n\n        if \"last_hidden_state\" not in encoder_out:\n            new_encoder_out = None\n        else:\n            new_encoder_out = encoder_out[\"last_hidden_state\"].index_select(0, new_order)\n\n        if \"padding_mask\" not in encoder_out:\n            new_encoder_padding_mask = None\n        else:\n            new_encoder_padding_mask = encoder_out[\"padding_mask\"].index_select(0, new_order)\n\n        if \"position_embedding\" not in encoder_out:\n            new_position_embeddings = None\n        else:\n            new_position_embeddings = encoder_out[\"position_embedding\"].index_select(0, new_order)\n\n        if \"hidden_states\" not in encoder_out:\n            new_encoer_states = None\n        else:\n            encoder_states = encoder_out[\"hidden_states\"]\n            new_encoer_states = ()\n            if len(encoder_states) > 0:\n                for idx, state in enumerate(encoder_states):\n                    new_encoer_states += (state.index_select(0, new_order),)\n\n        if \"attentions\" not in encoder_out:\n            attentions = None\n        else:\n            attentions = encoder_out[\"attentions\"]\n\n        return OFAEncoderOutput(\n            last_hidden_state=new_encoder_out,\n            padding_mask=new_encoder_padding_mask,\n            hidden_states=new_encoer_states,\n            attentions=attentions,\n            position_embedding=new_position_embeddings,\n        )\n\n    def forward(\n        self,\n        input_ids=None,\n        patch_images: Optional[torch.Tensor] = None,\n        patch_images_2: Optional[torch.Tensor] = None,\n        patch_masks: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n        token_embeddings: Optional[torch.Tensor] = None,\n        sample_patch_num: Optional[int] = None,\n    ):\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(bsz, seq_len)`):\n                indices of input sequence tokens in the vocabular, and padding will be ignored by default;\n\n                indices can be obtained using [`~OFATokenizer`].\n\n            patch_images (`torch.FloatTensor` of shape `(bsz, 3, height, width)`):\n                the resized image, which are transformed by the default operations.\n            patch_images_2 (`torch.FloatTensor` of shape `(bsz, 3, height, width)`):\n                the second (if it exists) image.\n            patch_masks (`torch.BoolTensor`): the patches to be masked.\n            output_attentions (`bool`): whether to return all attention weights,\n            output_hidden_states (`bool`): whether to return all hidden states.\n            token_embeddings (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`): token embeddings.\n            sample_patch_num (`int`): the number of patches to sample.\n\n        Returns:\n            [`OFAEncoderOutput`]:\n                last_hidden_state (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`):\n                    the states of the last layer.\n                padding_mask (`torch.BoolTensor` of shape `(bsz, seq_len)`):\n                    the padding mask of the source context.\n                hidden_states (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`):\n                    the states of all layers including the embeddings.\n                attentions (`torch.FloatTensor` of shape `(bsz, num_heads, seq_len, seq_len)`):\n                    the attention weights of all layers.\n                position_embedding (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`):\n                    positional embeddings of the input image and tokens.\n        \"\"\"\n\n        image_embed = None\n        image_embed_2 = None\n        image_pos_embed = None\n        image_pos_embed_2 = None\n        if patch_images is not None:\n            (\n                image_embed,\n                image_num_patches,\n                image_padding_mask,\n                image_position_ids,\n                image_pos_embed,\n            ) = self.get_patch_images_info(patch_images, sample_patch_num, input_ids.device)\n            # image_padding_mask[~patch_masks] = True # comment the line to temporarily fix the bug of mismatch\n        if patch_images_2 is not None:\n            (\n                image_embed_2,\n                image_num_patches_2,\n                image_padding_mask_2,\n                image_position_ids_2,\n                image_pos_embed_2,\n            ) = self.get_patch_images_info(patch_images_2, sample_patch_num, input_ids.device)\n            image_padding_mask_2[~patch_masks] = True\n\n        encoder_padding_mask = input_ids.eq(self.padding_idx)\n        if patch_images is not None:\n            encoder_padding_mask = torch.cat([image_padding_mask, encoder_padding_mask], dim=1)\n        if patch_images_2 is not None:\n            encoder_padding_mask = torch.cat([image_padding_mask_2, encoder_padding_mask], dim=1)\n        has_pads = encoder_padding_mask.any()\n\n        pos_embed = self.embed_positions(new_arange(input_ids))\n        x, encoder_embedding = self.forward_embedding(\n            input_ids, image_embed, image_embed_2, token_embeddings, pos_embed, image_pos_embed, image_pos_embed_2\n        )\n\n        # account for padding while computing the representation\n        if has_pads:\n            x = x * (1 - encoder_padding_mask.unsqueeze(-1).type_as(x))\n\n        pos_embed = self.pos_ln(pos_embed)\n        if patch_images is not None:\n            image_pos_embed = self.image_pos_ln(image_pos_embed)\n            pos_embed = torch.cat([image_pos_embed, pos_embed], dim=1)\n        if patch_images_2 is not None:\n            image_pos_embed_2 = self.image_pos_ln(image_pos_embed_2)\n            pos_embed = torch.cat([image_pos_embed_2, pos_embed], dim=1)\n\n        pos_q = (\n            self.pos_q_linear(pos_embed).view(x.size(0), x.size(1), self.num_attention_heads, -1).transpose(1, 2)\n            * self.pos_scaling\n        )\n        pos_k = self.pos_k_linear(pos_embed).view(x.size(0), x.size(1), self.num_attention_heads, -1).transpose(1, 2)\n        abs_pos_bias = torch.matmul(pos_q, pos_k.transpose(2, 3))\n\n        # expand attention_mask\n        if has_pads:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            attention_mask = _expand_mask(~encoder_padding_mask, dtype=x.dtype)\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        # encoder layers\n        for idx, layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states += (x,)\n            self_attn_bias = abs_pos_bias.clone()\n            self_attn_bias[:, :, -input_ids.size(1) :, -input_ids.size(1) :] += self.get_rel_pos_bias(input_ids, idx)\n            if patch_images_2 is not None:\n                self_attn_bias[:, :, :image_num_patches_2, :image_num_patches_2] += self.get_image_rel_pos_bias(\n                    image_position_ids_2, idx\n                )\n                self_attn_bias[\n                    :,\n                    :,\n                    image_num_patches_2 : image_num_patches_2 + image_num_patches,\n                    image_num_patches_2 : image_num_patches_2 + image_num_patches,\n                ] += self.get_image_rel_pos_bias(image_position_ids, idx)\n            elif patch_images is not None:\n                self_attn_bias[\n                    :, :, : x.size(1) - input_ids.size(1), : x.size(1) - input_ids.size(1)\n                ] += self.get_image_rel_pos_bias(image_position_ids, idx)\n            self_attn_bias = self_attn_bias.reshape(-1, x.size(1), x.size(1))\n\n            hidden_outputs = layer(\n                x, attention_mask if has_pads else None, attn_bias=self_attn_bias, output_attentions=output_attentions\n            )\n            x = hidden_outputs[0]\n\n            if output_attentions:\n                attention = hidden_outputs[1]\n                all_attentions = all_attentions + (attention,)\n\n        if output_hidden_states:\n            encoder_states += (x,)\n\n        if self.layer_norm is not None:\n            x = self.layer_norm(x)\n\n        return OFAEncoderOutput(\n            last_hidden_state=x,\n            padding_mask=encoder_padding_mask,\n            hidden_states=encoder_states,\n            attentions=all_attentions,\n            position_embedding=pos_embed,\n        )\n\n\nclass OFADecoder(OFAPreTrainedModel):\n    r\"\"\"\n    OFA decoder consisting of layers of [`OFADecoderLayer`]\n\n    Args:\n        config: OFAConfig\n        embed_tokens (`nn.Embedding`, *optional*): output embedding\n    \"\"\"\n\n    def __init__(self, config: OFAConfig, embed_tokens: Optional[nn.Embedding] = None, output_projection=None):\n        super().__init__(config)\n        self.dropout = nn.Dropout(config.dropout)\n        self.decoder_layerdrop = config.decoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.max_target_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n\n        self._future_mask = torch.empty(0)\n        self.share_input_output_embed = config.share_decoder_input_output_embed\n        self.num_attention_heads = config.decoder_attention_heads\n\n        if embed_tokens is not None:\n            self.embed_tokens = embed_tokens\n        else:\n            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n\n        self.embed_dim = config.d_model\n        self.output_embed_dim = config.d_model\n\n        self.layers = nn.ModuleList([OFADecoderLayer(config) for _ in range(config.decoder_layers)])\n        if config.layernorm_embedding:\n            self.layernorm_embedding = LayerNorm(self.embed_dim)\n        else:\n            self.layernorm_embedding = None\n\n        self.window_size = config.code_image_size // 8\n\n        self.embed_positions = Embedding(self.max_target_positions + 2, self.embed_dim)\n        self.embed_image_positions = Embedding(config.image_bucket_size**2 + 1, self.embed_dim)\n        self.pos_ln = LayerNorm(self.embed_dim)\n        self.image_pos_ln = LayerNorm(self.embed_dim)\n        self.pos_scaling = float(self.embed_dim / self.num_attention_heads * config.attn_scale_factor) ** -0.5\n        self.self_pos_q_linear = nn.Linear(self.embed_dim, self.embed_dim)\n        self.self_pos_k_linear = nn.Linear(self.embed_dim, self.embed_dim)\n        self.cross_pos_q_linear = nn.Linear(self.embed_dim, self.embed_dim)\n        self.cross_pos_k_linear = nn.Linear(self.embed_dim, self.embed_dim)\n\n        if config.code_layernorm_embedding:\n            self.code_layernorm_embedding = LayerNorm(self.embed_dim)\n        else:\n            self.code_layernorm_embedding = None\n\n        if self.decoder_layerdrop > 0.0:\n            self.layers = LayerDropModuleList(p=self.decoder_layerdrop)\n        else:\n            self.layers = nn.ModuleList([])\n\n        dpr = [x.item() for x in torch.linspace(0, config.decoder_drop_path_rate, config.decoder_layers)]\n        self.layers.extend([OFADecoderLayer(config, drop_path_rate=dpr[i]) for i in range(config.decoder_layers)])\n        self.num_layers = len(self.layers)\n\n        if config.decoder_normalize_before:\n            self.layer_norm = LayerNorm(self.embed_dim)\n        else:\n            self.layer_norm = None\n\n        self.adaptive_softmax = None\n        self.output_projection = output_projection\n        if self.output_projection is None:\n            self.build_output_projection(config)\n\n        self.token_bucket_size = config.token_bucket_size\n        token_num_rel_dis = 2 * config.token_bucket_size - 1\n        token_rp_bucket = make_token_bucket_position(config.token_bucket_size)\n        self.token_rel_pos_table_list = nn.ModuleList(\n            [\n                Embedding(token_num_rel_dis, self.num_attention_heads, zero_init=True)\n                for _ in range(config.decoder_layers)\n            ]\n        )\n\n        self.image_bucket_size = config.image_bucket_size\n        image_num_rel_dis = (2 * config.image_bucket_size - 1) * (2 * config.image_bucket_size - 1) + 3\n        image_rp_bucket = make_image_bucket_position(config.image_bucket_size, image_num_rel_dis)\n        image_position_idx = (\n            torch.arange(self.window_size).unsqueeze(0).expand(self.window_size, self.window_size)\n            + torch.arange(self.window_size).unsqueeze(1) * config.image_bucket_size\n            + 1\n        )\n        image_position_idx = torch.cat([torch.tensor([0]), image_position_idx.view(-1)])\n        image_position_idx = torch.cat([image_position_idx, torch.tensor([1024] * 768)])\n        self.image_rel_pos_table_list = nn.ModuleList(\n            [\n                Embedding(image_num_rel_dis, self.num_attention_heads, zero_init=True)\n                for _ in range(config.decoder_layers)\n            ]\n        )\n\n        self.register_buffer(\"token_rp_bucket\", token_rp_bucket)\n        self.register_buffer(\"image_rp_bucket\", image_rp_bucket)\n        self.register_buffer(\"image_position_idx\", image_position_idx)\n        self.entangle_position_embedding = config.entangle_position_embedding\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def build_output_projection(self, config):\n        if self.share_input_output_embed:\n            self.output_projection = nn.Linear(\n                self.embed_tokens.weight.shape[1],\n                self.embed_tokens.weight.shape[0],\n                bias=False,\n            )\n            self.output_projection.weight = self.embed_tokens.weight\n        else:\n            self.output_projection = nn.Linear(self.output_embed_dim, config.vocab_size, bias=False)\n            nn.init.normal_(self.output_projection.weight, mean=0, std=self.output_embed_dim**-0.5)\n\n    def get_rel_pos_bias(self, x, idx):\n        r\"\"\"\n        Get the relative positional bias of the text, for attention.\n        \"\"\"\n\n        seq_len = x.size(1)\n        rp_bucket = self.token_rp_bucket[:seq_len, :seq_len]\n        values = F.embedding(rp_bucket, self.token_rel_pos_table_list[idx].weight)\n        values = values.permute([2, 0, 1])\n        return values.contiguous()\n\n    def get_image_rel_pos_bias(self, x, idx):\n        r\"\"\"\n        Get the relative positional bias of the image, for attention.\n        \"\"\"\n\n        seq_len = x.size(1)\n        image_position_idx = self.image_position_idx[:seq_len]\n        rp_bucket = self.image_rp_bucket[image_position_idx][:, image_position_idx]\n        values = F.embedding(rp_bucket, self.image_rel_pos_table_list[idx].weight)\n        values = values.permute(2, 0, 1)\n        return values\n\n    def get_pos_info(self, tgt_pos_embed, src_pos_embed=None, use_image=False):\n        r\"\"\"\n        Get the positional information.\n\n        Args:\n            tgt_pos_embed (`torch.FloatTensor` of shape `(bsz, tgt_len, embed_dim)`):\n                the target-side positional embeddings.\n            src_pos_embed (`torch.FloatTensor` of shape `(bsz, src_len, embed_dim)`, *optional*):\n                the source-side positional embeddings.\n            use_image (`bool`): whether to use image.\n\n        Returns:\n            abs_pos_bias (`torch.FloatTensor` of shape `(bsz, src_len, tgt_len, src_len)`):\n                absolute positional bias for attention.\n        \"\"\"\n\n        batch_size = tgt_pos_embed.size(0)\n        tgt_len = tgt_pos_embed.size(1)\n        tgt_pos_embed = self.image_pos_ln(tgt_pos_embed) if use_image else self.pos_ln(tgt_pos_embed)\n\n        if src_pos_embed is not None:\n            src_len = src_pos_embed.size(1)\n            pos_q = (\n                self.cross_pos_q_linear(tgt_pos_embed)\n                .view(batch_size, tgt_len, self.num_attention_heads, -1)\n                .transpose(1, 2)\n                * self.pos_scaling\n            )\n            pos_k = (\n                self.cross_pos_k_linear(src_pos_embed)\n                .view(batch_size, src_len, self.num_attention_heads, -1)\n                .transpose(1, 2)\n            )\n        else:\n            src_len = tgt_pos_embed.size(1)\n            pos_q = (\n                self.self_pos_q_linear(tgt_pos_embed)\n                .view(batch_size, tgt_len, self.num_attention_heads, -1)\n                .transpose(1, 2)\n                * self.pos_scaling\n            )\n            pos_k = (\n                self.self_pos_k_linear(tgt_pos_embed)\n                .view(batch_size, src_len, self.num_attention_heads, -1)\n                .transpose(1, 2)\n            )\n        abs_pos_bias = torch.matmul(pos_q, pos_k.transpose(2, 3))\n\n        return abs_pos_bias\n\n    def get_input_embeddings(self):\n        r\"\"\"\n        Get the input embeddings\n        \"\"\"\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        r\"\"\"\n        Set the weights of the embeddings with the given tensor.\n        \"\"\"\n        self.embed_tokens = value\n\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, dtype, past_key_values_length):\n        r\"\"\"\n        Create causal mask for unidirectional decoding.\n        [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        \"\"\"\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape, dtype, past_key_values_length=past_key_values_length\n            ).to(self.device)\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            expanded_attn_mask = _expand_mask(attention_mask, dtype, tgt_len=input_shape[-1])\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    def max_positions(self):\n        \"\"\"Maximum output length supported by the decoder.\"\"\"\n        if self.embed_positions is None:\n            return self.max_target_positions\n        return self.max_target_positions\n\n    def get_normalized_probs(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)\n\n    def get_normalized_probs_scriptable(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n\n        if hasattr(self, \"adaptive_softmax\") and self.adaptive_softmax is not None:\n            if sample is not None:\n                assert \"target\" in sample\n                target = sample[\"target\"]\n            else:\n                target = None\n            out = self.adaptive_softmax.get_log_prob(net_output[0], target=target)\n            return out.exp_() if not log_probs else out\n\n        logits = net_output[0]\n        if log_probs:\n            return F.log_softmax(logits, dim=-1)\n        else:\n            return F.softmax(logits, dim=-1)\n\n    def reorder_incremental_state_scripting(\n        self,\n        # incremental_state: Dict[str, Dict[str, Optional[Tensor]]],\n        past_key_values: Optional[torch.Tensor],\n        new_order: Tensor,\n    ):\n        \"\"\"Main entry point for reordering the incremental state.\n\n        Due to limitations in TorchScript, we call this function in\n        :class:`fairseq.sequence_generator.SequenceGenerator` instead of\n        calling :func:`reorder_incremental_state` directly.\n        \"\"\"\n        input_buffer = past_key_values\n        new_past_key_values = []\n        if input_buffer is not None:\n            for input_buffer_k in input_buffer:\n                new_input_buffer_k = []\n                for input in input_buffer_k:\n                    if input is None:\n                        input = None\n                    else:\n                        input = input.index_select(0, new_order)\n                    new_input_buffer_k.append(input)\n                new_past_key_values.append(new_input_buffer_k)\n        return new_past_key_values\n\n    def forward(\n        self,\n        input_ids: torch.Tensor = None,\n        attention_mask: torch.Tensor = None,\n        encoder_hidden_states: torch.Tensor = None,\n        encoder_attention_mask: torch.Tensor = None,\n        code_masks: Optional[torch.Tensor] = None,\n        src_pos_embed: torch.Tensor = None,\n        past_key_values: Optional[torch.Tensor] = None,\n        use_cache: bool = False,\n        output_attentions: bool = False,\n        output_hidden_states: bool = False,\n    ):\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(bsz, seq_len)`): indices of the sequence in the vocabulary.\n            attention_mask (`torch.Tensor` of shape `(bsz, seq_len)`): mask to avoid attention on padding tokens.\n            encoder_hidden_states (`torch.FloatTensor` of shape `(bsz, seq_len, hidden)`): the last hidden state of the encoder.\n            encoder_attention_mask (`torch.Tensor` of shape `(bsz, seq_len)`): the padding mask of the source side.\n            code_masks (`torch.Tensor` of shape `(bsz, seq_len)`): masks only for code generation.\n            src_pos_embed (`torch.FloatTensor` of shape `(bsz, seq_len, hidden)`): the positional embeddings of the source side.\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(bsz, num_heads, tgt_len, head_size)`) and 2 additional tensors of\n                shape `(bsz, num_heads, src_len, head_size)`.\n            use_cache (`bool`): whether to use cache for faster inference.\n            output_attentions (`bool`): whether to output attention weights.\n            output_hidden_states (`bool`): whether to output hidden states.\n\n        Returns:\n            BaseModelOutputWithPastAndCrossAttentions or a plain tuple:\n                last_hidden_state (`torch.FloatTensor` of shape `(bsz, seq_len, hidden)`): the last hidden states.\n                past_key_values (`tuple(tuple(torch.FloatTensor)): past keys and values for faster inference.\n                hidden_states (`tuple(torch.FloatTensor)`): hidden states of all layers.\n                attentions (`tuple(torch.FloatTensor)): self attention weights of all layers.\n                cross_attentions (`tuple(torch.FloatTensor)): cross attention weights of all layers.\n        \"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        if past_key_values is not None and len(past_key_values) > 0:\n            size = past_key_values[0][0].size()\n            bsz, tgt_len = size[0], size[-2] + 1\n            token_position_idx = torch.arange(tgt_len, device=input_ids.device).expand([bsz, tgt_len]).contiguous()\n        else:\n            bsz, tgt_len = input_ids.shape\n            token_position_idx = new_arange(input_ids)\n        tgt_pos_embed = self.embed_positions(token_position_idx)\n        if code_masks is not None and torch.any(code_masks):\n            image_position_idx = self.image_position_idx[: input_ids.size(1)].unsqueeze(0).expand(bsz, tgt_len)\n            tgt_pos_embed[code_masks] = self.embed_image_positions(image_position_idx)[code_masks]\n\n        # self attn position bias\n        self_abs_pos_bias = self.get_pos_info(tgt_pos_embed, use_image=False)\n        if code_masks is not None and torch.any(code_masks):\n            self_image_abs_pos_bias = self.get_pos_info(tgt_pos_embed, use_image=True)\n            self_abs_pos_bias[code_masks] = self_image_abs_pos_bias[code_masks]\n        # cross attn position bias\n        cross_abs_pos_bias = self.get_pos_info(tgt_pos_embed, src_pos_embed=src_pos_embed)\n        if code_masks is not None and torch.any(code_masks):\n            cross_image_abs_pos_bias = self.get_pos_info(tgt_pos_embed, src_pos_embed=src_pos_embed, use_image=True)\n            cross_abs_pos_bias[code_masks] = cross_image_abs_pos_bias[code_masks]\n        cross_abs_pos_bias = cross_abs_pos_bias.reshape(-1, *cross_abs_pos_bias.size()[-2:])\n\n        all_prev_output_tokens = input_ids.clone()\n        if past_key_values is not None and len(past_key_values) > 0:\n            input_ids = input_ids[:, -1:]\n            cross_abs_pos_bias = cross_abs_pos_bias[:, -1:, :]\n            tgt_pos_embed = tgt_pos_embed[:, -1:, :]\n\n        # embed tokens and positions\n        x = self.embed_scale * self.embed_tokens(input_ids)\n\n        if self.entangle_position_embedding and not self.disable_entangle:\n            x += tgt_pos_embed\n\n        if self.layernorm_embedding is not None:\n            if code_masks is None or not code_masks.any() or not self.code_layernorm_embedding:\n                x = self.layernorm_embedding(x)\n            elif code_masks is not None and code_masks.all():\n                x = self.code_layernorm_embedding(x)\n            else:\n                x[~code_masks] = self.layernorm_embedding(x[~code_masks])\n                x[code_masks] = self.code_layernorm_embedding(x[code_masks])\n\n        hidden_states = self.dropout(x)\n\n        # past_key_values_length\n        past_key_values_length = (\n            past_key_values[0][0].shape[2] if past_key_values is not None and len(past_key_values) > 0 else 0\n        )\n\n        shape, dtype = input_ids.shape, hidden_states.dtype\n        attention_mask = self._prepare_decoder_attention_mask(attention_mask, shape, dtype, past_key_values_length)\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n        next_decoder_cache = () if use_cache else None\n\n        # decoder layers\n        for idx, layer in enumerate(self.layers):\n            # add hidden states from the last decoder layer\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None and len(past_key_values) > 0 else None\n\n            self_attn_bias = self_abs_pos_bias.clone()\n            if code_masks is None or not code_masks.any():\n                self_attn_bias += self.get_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n            elif code_masks is not None and code_masks.all():\n                self_attn_bias += self.get_image_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n            else:\n                self_attn_bias[~code_masks] += self.get_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n                self_attn_bias[code_masks] += self.get_image_rel_pos_bias(all_prev_output_tokens, idx).unsqueeze(0)\n            self_attn_bias = self_attn_bias.reshape(-1, *self_attn_bias.size()[-2:])\n            if past_key_value is not None and len(past_key_values) > 0:\n                self_attn_bias = self_attn_bias[:, -1:, :]\n\n            layer_outputs = layer(\n                hidden_states,\n                attention_mask=attention_mask,\n                encoder_hidden_states=encoder_hidden_states,\n                encoder_attention_mask=encoder_attention_mask,\n                past_key_value=past_key_value,\n                output_attentions=output_attentions,\n                use_cache=use_cache,\n                self_attn_bias=self_attn_bias,\n                cross_attn_bias=cross_abs_pos_bias,\n            )\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[2],)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n\n        if self.layer_norm is not None:\n            hidden_states = self.layer_norm(hidden_states)\n\n        if self.output_projection is not None:\n            hidden_states = self.output_projection(hidden_states)\n\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            cross_attentions=all_cross_attentions,\n        )\n\n\n@add_start_docstrings(\n    \"The bare OFA Model outputting raw hidden-states without any specific head on top.\",\n    OFA_START_DOCSTRING,\n)\nclass OFAModel(OFAPreTrainedModel):\n    r\"\"\"\n    The OFA model built with an encoder and a decoder only, without any classification head.\n\n    Args:\n        config (OFAConfig): OFA configuration.\n    \"\"\"\n\n    def __init__(self, config: OFAConfig, **kwargs):\n        super().__init__(config)\n        self.disable_entangle = getattr(kwargs, \"disable_entangle\", False)\n\n        self.padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n        shared = nn.Embedding(vocab_size, config.d_model, self.padding_idx)\n\n        self.encoder = OFAEncoder(config, shared)\n        self.decoder = OFADecoder(config, shared)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        r\"\"\"\n        Retrieve input embeddings.\n        \"\"\"\n        return self.encoder.get_input_embeddings()\n\n    def set_input_embeddings(self, value):\n        r\"\"\"\n        Set values for input embeddings\n        \"\"\"\n        shared = value\n        self.encoder.embed_tokens = shared\n        self.decoder.embed_tokens = shared\n\n    def get_encoder(self):\n        r\"\"\"\n        Retrieve the encoder\n        \"\"\"\n        return self.encoder\n\n    def get_decoder(self):\n        r\"\"\"\n        Retrieve the decoder\n        \"\"\"\n        return self.decoder\n\n    @add_start_docstrings_to_model_forward(OFA_INPUTS_DOCSTRING)\n    @add_code_sample_docstrings(\n        processor_class=_TOKENIZER_FOR_DOC,\n        checkpoint=_CHECKPOINT_FOR_DOC,\n        output_type=Seq2SeqModelOutput,\n        config_class=_CONFIG_FOR_DOC,\n    )\n    def max_decoder_positions(self):\n        \"\"\"Maximum length supported by the decoder.\"\"\"\n        return self.decoder.max_positions()\n\n    def get_normalized_probs(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)\n\n    def get_normalized_probs_scriptable(\n        self,\n        net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]],\n        log_probs: bool,\n        sample: Optional[Dict[str, Tensor]] = None,\n    ):\n        \"\"\"Scriptable helper function for get_normalized_probs in ~BaseFairseqModel\"\"\"\n        if hasattr(self, \"decoder\"):\n            return self.decoder.get_normalized_probs(net_output, log_probs, sample)\n        elif torch.is_tensor(net_output):\n            # syntactic sugar for simple models which don't have a decoder\n            # (e.g., the classification tutorial)\n            logits = net_output.float()\n            if log_probs:\n                return F.log_softmax(logits, dim=-1)\n            else:\n                return F.softmax(logits, dim=-1)\n        raise NotImplementedError\n\n    def forward(\n        self,\n        input_ids=None,\n        patch_images=None,\n        patch_images_2=None,\n        patch_masks=None,\n        token_embeddings=None,\n        sample_patch_num=None,\n        decoder_input_ids=None,\n        code_masks=None,\n        attention_mask=None,\n        encoder_outputs=None,\n        past_key_values=None,\n        use_cache=False,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=False,\n    ):\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(bsz, seq_len)`):\n                indices of input sequence tokens in the vocabular, and padding will be ignored by default;\n\n                indices can be obtained using [`~OFATokenizer`].\n\n            patch_images (`torch.FloatTensor` of shape `(bsz, 3, height, width)`):\n                the resized image, which are transformed by the default operations.\n            patch_images_2 (`torch.FloatTensor` of shape `(bsz, 3, height, width)`):\n                the second (if it exists) image.\n            patch_masks (`torch.BoolTensor`): the patches to be masked.\n            token_embeddings (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`): token embeddings.\n            sample_patch_num (`int`): the number of patches to sample.\n            decoder_input_ids (`torch.LongTensor` of shape `(bsz, seq_len)`): indices of the sequence in the vocabulary.\n            code_masks (`torch.Tensor` of shape `(bsz, seq_len)`): masks only for code generation.\n            attention_mask (`torch.Tensor` of shape `(bsz, seq_len)`): attention mask for decoding.\n            encoder_outputs (`OFAEncoderOutput`):\n                encoder outputs with hidden states, positional embeddings, and padding masks.\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(bsz, num_heads, tgt_len, head_size)`) and 2 additional tensors of\n                shape `(bsz, num_heads, src_len, head_size)`.\n            use_cache (`bool`): whether to use cache for faster inference.\n            output_attentions (`bool`): whether to output attention weights.\n            output_hidden_states (`bool`): whether to output hidden states.\n            return_dict (`bool`): unused. Keep it for generation only.\n\n        Returns:\n            Seq2SeqLMOutput:\n                logits (`torch.FloatTensor` of shape `(bsz, seq_len, hidden)`): the last decoder hidden states.\n                past_key_values (`tuple(tuple(torch.FloatTensor)): past keys and values for faster inference.\n                decoder_hidden_states (`tuple(torch.FloatTensor)`): the decoder hidden states of all layers.\n                decoder_attentions (`tuple(torch.FloatTensor)): the decoder self attention weights of all layers.\n                cross_attentions (`tuple(torch.FloatTensor)): cross attention weights of all layers.\n                encoder_last_hidden_state (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`):\n                    the encoder last hidden state.\n                encoder_hidden_states (`torch.FloatTensor` of shape `(bsz, seq_len, embed_dim)`):\n                    the encoder states of all layers including the embeddings.\n                encoder_attentions (`torch.FloatTensor` of shape `(bsz, num_heads, seq_len, seq_len)`):\n                    the encoder attention weights of all layers.\n        \"\"\"\n\n        output_attentions = output_attentions if output_attentions else self.config.output_attentions\n        output_hidden_states = output_hidden_states if output_hidden_states else self.config.output_hidden_states\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                patch_images=patch_images,\n                patch_images_2=patch_images_2,\n                patch_masks=patch_masks,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                token_embeddings=token_embeddings,\n                sample_patch_num=sample_patch_num,\n            )\n\n        # if decoder_input_ids.eq(self.config.pad_token_id).any():\n        #     attention_mask = decoder_input_ids.eq(self.padding_idx)\n\n        encoder_hidden_states = encoder_outputs.last_hidden_state\n        if past_key_values is not None and len(past_key_values) > 0:\n            encoder_attention_mask = _expand_mask(\n                ~encoder_outputs.padding_mask, encoder_hidden_states.dtype, decoder_input_ids[:, -1:].shape[-1]\n            )\n        else:\n            encoder_attention_mask = _expand_mask(\n                ~encoder_outputs.padding_mask, encoder_hidden_states.dtype, decoder_input_ids.shape[-1]\n            )\n        src_pos_embed = encoder_outputs.position_embedding\n\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=attention_mask,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            code_masks=code_masks,\n            src_pos_embed=src_pos_embed,\n            past_key_values=past_key_values,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n        )\n\n        return Seq2SeqLMOutput(\n            logits=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids=None,\n        past=None,\n        attention_mask=None,\n        code_masks=None,\n        use_cache=False,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # if attention_mask is None:\n        attention_mask = decoder_input_ids.new_ones(decoder_input_ids.shape)\n\n        # cut decoder_input_ids if past is used\n        # if past is not None:\n        #     decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,\n            \"patch_images\": None,\n            \"patch_images_2\": None,\n            \"patch_masks\": None,\n            \"token_embeddings\": None,\n            \"sample_patch_num\": None,\n            \"attention_mask\": attention_mask,\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"code_masks\": code_masks,\n            \"use_cache\": use_cache,\n        }\n\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n\n    def _prepare_encoder_decoder_kwargs_for_generation(\n        self, inputs_tensor: torch.Tensor, model_kwargs, model_input_name: Optional[str] = None\n    ):\n        # 1. get encoder\n        encoder = self.get_encoder()\n\n        # 2. prepare encoder args and encoder kwargs from model kwargs\n        irrelevant_prefix = [\"decoder_\", \"cross_attn\", \"use_cache\", \"attention_mask\"]\n        encoder_kwargs = {\n            argument: value\n            for argument, value in model_kwargs.items()\n            if not any(argument.startswith(p) for p in irrelevant_prefix)\n        }\n\n        if encoder_kwargs.get(\"patch_masks\") is None:\n            encoder_kwargs[\"patch_masks\"] = torch.ones(\n                (len(inputs_tensor), 1), dtype=torch.bool, device=inputs_tensor.device\n            )\n\n        # 3. make sure that encoder returns `ModelOutput`\n        model_input_name = model_input_name if model_input_name is not None else self.main_input_name\n        encoder_kwargs[model_input_name] = inputs_tensor\n        model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n        model_kwargs[\"attention_mask\"] = None\n\n        return model_kwargs\n\n    @staticmethod\n    def _reorder_cache(past, beam_idx):\n        reordered_past = ()\n        for layer_past in past:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past\n\n    @staticmethod\n    def _expand_inputs_for_generation(\n        input_ids: torch.LongTensor,\n        expand_size: int = 1,\n        is_encoder_decoder: bool = False,\n        attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[ModelOutput] = None,\n        **model_kwargs,\n    ):\n        expanded_return_idx = (\n            torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)\n        )\n        input_ids = input_ids.index_select(0, expanded_return_idx)\n\n        if \"token_type_ids\" in model_kwargs:\n            token_type_ids = model_kwargs[\"token_type_ids\"]\n            model_kwargs[\"token_type_ids\"] = token_type_ids.index_select(0, expanded_return_idx)\n\n        if attention_mask is not None:\n            model_kwargs[\"attention_mask\"] = attention_mask.index_select(0, expanded_return_idx)\n\n        if is_encoder_decoder:\n            if encoder_outputs is None:\n                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n            encoder_outputs[\"last_hidden_state\"] = encoder_outputs.last_hidden_state.index_select(\n                0, expanded_return_idx.to(encoder_outputs.last_hidden_state.device)\n            )\n            encoder_outputs[\"position_embedding\"] = encoder_outputs.position_embedding.index_select(\n                0, expanded_return_idx.to(encoder_outputs.position_embedding.device)\n            )\n            encoder_outputs[\"padding_mask\"] = encoder_outputs.padding_mask.index_select(\n                0, expanded_return_idx.to(encoder_outputs.padding_mask.device)\n            )\n            model_kwargs[\"encoder_outputs\"] = encoder_outputs\n        return input_ids, model_kwargs\n"}
