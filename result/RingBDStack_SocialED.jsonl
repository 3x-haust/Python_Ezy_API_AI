{"repo_info": {"repo_name": "SocialED", "repo_owner": "RingBDStack", "repo_url": "https://github.com/RingBDStack/SocialED"}}
{"type": "test_file", "path": "SocialED/tests/__init__.py", "content": "from .test_hisevent import *\nfrom .test_hypersed import *\nfrom .test_wmd import *\nfrom .test_adpsemevent import *\nfrom .test_finevent import *\nfrom .test_hcrc import *\nfrom .test_utility import *\nfrom .test_word2vec import *\nfrom .test_sbert import *\nfrom .test_eventx import *\nfrom .test_clkd import *\nfrom .test_kpgnn import *\nfrom .test_qsgnn import *\nfrom .test_uclsed import *\nfrom .test_rplmsed import *\nfrom .test_lda import *\nfrom .test_bilstm import *\nfrom .test_glove import *\nfrom .test_bert import *\n\n__all__ = [\n    'test_hisevent',\n    'test_hypersed',\n    'test_wmd',\n    'test_adpsemevent',\n    'test_finevent',\n    'test_hcrc',\n    'test_utility',\n    'test_word2vec',\n    'test_sbert',\n    'test_eventx',\n    'test_clkd',\n    'test_kpgnn',\n    'test_qsgnn',\n    'test_uclsed',\n    'test_rplmsed',\n    'test_lda',\n    'test_bilstm',\n    'test_glove',\n    'test_bert'\n]\n\n"}
{"type": "test_file", "path": "SocialED/tests/test_bilstm.py", "content": "import sys\nsys.path.append('/home/zhangkun/py_projects/socialEDv3/SocialED/src/')\n\nimport pytest\nimport numpy as np\nimport pandas as pd\nimport torch\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom detector.bilstm import *\nfrom torch.utils.data import DataLoader\n\n# 创建一个示例数据集\n@pytest.fixture\ndef sample_dataset():\n    data = {\n        'event_id': [1, 2, 1, 2, 3, 1, 2, 3, 1, 2],  # 增加样本数量\n        'words': [['hello', 'world'], ['goodbye', 'world'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'],\n                  ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world']],\n        'filtered_words': [['hello', 'world'], ['goodbye', 'world'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'],\n                           ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world']]\n    }\n    return pd.DataFrame(data)\n\n# 测试数据预处理\ndef test_preprocess(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    df = bilstm.preprocess()\n    assert 'event_id' in df.columns\n    assert 'words' in df.columns\n    assert 'filtered_words' in df.columns\n    assert 'wordsidx' in df.columns\n\n# 测试数据分割\ndef test_split(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.split()\n    assert bilstm.train_df is not None\n    assert bilstm.test_df is not None\n    assert bilstm.val_df is not None\n\n# 测试加载嵌入\ndef test_load_embeddings(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.preprocess()\n    bilstm.load_embeddings()\n    assert bilstm.weight is not None\n    assert bilstm.weight.shape[0] == len(bilstm.word2idx)\n\n# 测试LSTM模型\ndef test_lstm_model(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.preprocess()\n    bilstm.load_embeddings()\n    lstm_model = LSTM(bilstm.embedding_size, bilstm.weight, bilstm.num_hidden_nodes, bilstm.hidden_dim2,\n                      bilstm.num_layers, bilstm.bi_directional, bilstm.dropout_keep_prob, bilstm.pad_index,\n                      bilstm.batch_size)\n    assert lstm_model is not None\n\n# 测试VectorizeData类\ndef test_vectorize_data(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.preprocess()\n    vectorized_data = VectorizeData(bilstm.df, bilstm.max_len)\n    assert vectorized_data is not None\n    assert len(vectorized_data) == len(bilstm.df)\n\n# 测试OnlineTripletLoss类\ndef test_online_triplet_loss(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.preprocess()\n    bilstm.load_embeddings()\n    lstm_model = LSTM(bilstm.embedding_size, bilstm.weight, bilstm.num_hidden_nodes, bilstm.hidden_dim2,\n                      bilstm.num_layers, bilstm.bi_directional, bilstm.dropout_keep_prob, bilstm.pad_index,\n                      bilstm.batch_size)\n    vectorized_data = VectorizeData(bilstm.df, bilstm.max_len)\n    train_iterator = DataLoader(vectorized_data, batch_size=bilstm.batch_size, shuffle=True)\n    loss_func = OnlineTripletLoss(bilstm.margin, RandomNegativeTripletSelector(bilstm.margin))\n    for batch in train_iterator:\n        text, text_lengths = batch['text']\n        predictions = lstm_model(text, text_lengths)\n        loss, num_triplets = loss_func(predictions, batch['label'])\n        assert loss is not None\n        assert num_triplets > 0\n\n# 测试DatasetLoader类\ndef test_dataset_loader():\n    loader = DatasetLoader(dataset='arabic_twitter')\n    df = loader.load_data()\n    assert df is not None\n    assert isinstance(df, pd.DataFrame)\n\n# 测试BiLSTM类的fit方法\ndef test_fit(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.preprocess()\n    bilstm.fit()\n    assert bilstm.best_model is not None\n    assert bilstm.best_epoch is not None\n\n# 测试BiLSTM类的detection方法\ndef test_detection(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.preprocess()\n    bilstm.fit()\n    ground_truths, predictions = bilstm.detection()\n    assert ground_truths is not None\n    assert predictions is not None\n    assert len(ground_truths) == len(predictions)\n\n# 测试BiLSTM类的evaluate方法\ndef test_evaluate(sample_dataset):\n    bilstm = BiLSTM(sample_dataset)\n    bilstm.preprocess()\n    bilstm.fit()\n    ground_truths, predictions = bilstm.detection()\n    bilstm.evaluate(ground_truths, predictions)\n\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "test_file", "path": "SocialED/tests/test_wmd.py", "content": "import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport pytest\nimport numpy as np\nimport pandas as pd\nfrom gensim.models import Word2Vec\nfrom gensim.similarities import WmdSimilarity\nfrom sklearn import metrics\nfrom dataset.dataloader import DatasetLoader\nfrom detector.wmd import *\n\n# 创建一个示例数据集\n@pytest.fixture\ndef sample_dataset():\n    data = {\n        'event_id': [1, 2, 1, 2, 3, 1, 2, 3, 1, 2],\n        'filtered_words': [\n            ['hello', 'world'], ['goodbye', 'world'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'],\n            ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world']\n        ]\n    }\n    return pd.DataFrame(data)\n\n# 测试数据预处理\ndef test_preprocess(sample_dataset):\n    wmd = WMD(sample_dataset)\n    df = wmd.preprocess()\n    assert 'processed_text' in df.columns\n    assert df['processed_text'].apply(lambda x: isinstance(x, list)).all()\n\n# 测试训练 Word2Vec 模型\ndef test_fit(sample_dataset):\n    wmd = WMD(sample_dataset)\n    wmd.preprocess()\n    wmd.fit()\n    assert os.path.exists(wmd.model_path)\n\n# 测试检测\ndef test_detection(sample_dataset):\n    wmd = WMD(sample_dataset)\n    wmd.preprocess()\n    wmd.fit()\n    ground_truths, predictions = wmd.detection()\n    assert len(ground_truths) == len(predictions)\n\n# 测试评估\ndef test_evaluate(sample_dataset):\n    wmd = WMD(sample_dataset)\n    wmd.preprocess()\n    wmd.fit()\n    ground_truths, predictions = wmd.detection()\n    wmd.evaluate(ground_truths, predictions)\n\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "test_file", "path": "SocialED/tests/test_clkd.py", "content": "import pytest\nimport torch\nimport numpy as np\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom detector.clkd import *\n\n# Create args object\n\nargs = args_define().args\n# Test the CLKD class initialization\ndef test_clkd_initialization():\n    clkd = CLKD(args)\n    assert clkd.args == args\n\n# Test the preprocess method\ndef test_preprocess():\n    print(args)\n    clkd = CLKD(args)\n    clkd.preprocess()\n    # Add assertions to check if the preprocessing steps were successful\n    assert clkd.embedding_save_path is not None\n    assert clkd.data_split is not None\n\n# Test the fit method\ndef test_fit():\n    clkd = CLKD(args)\n    clkd.preprocess()\n    clkd.fit()\n    # Add assertions to check if the model was trained successfully\n    assert clkd.model is not None\n\n# Test the detection method\ndef test_detection():\n    clkd = CLKD(args)\n    clkd.preprocess()\n    clkd.fit()\n    predictions, ground_truths = clkd.detection()\n    # Add assertions to check if the detection was successful\n    assert isinstance(predictions, np.ndarray)\n    assert isinstance(ground_truths, np.ndarray)\n\n# Test the evaluate method\ndef test_evaluate():\n    clkd = CLKD(args)\n    clkd.preprocess()\n    clkd.fit()\n    predictions, ground_truths = clkd.detection()\n    results = clkd.evaluate(predictions, ground_truths)\n    # Add assertions to check if the evaluation was successful\n    assert isinstance(results, tuple)\n    assert len(results) == 3\n    assert all(isinstance(val, float) for val in results)\n\n# Test the evaluate2 method\ndef test_evaluate2():\n    clkd = CLKD(args)\n    clkd.preprocess()\n    clkd.fit()\n    predictions, ground_truths = clkd.detection()\n    ars, ami, nmi = clkd.evaluate2(predictions, ground_truths)\n    # Add assertions to check if the evaluation2 was successful\n    assert isinstance(ars, float)\n    assert isinstance(ami, float)\n    assert isinstance(nmi, float)\n\n# Test the generate_initial_features method\ndef test_generate_initial_features():\n    preprocessor = CLKD(args).preprocess()\n    preprocessor.generate_initial_features()\n    # Add assertions to check if the initial features were generated successfully\n    assert os.path.exists(args.file_path + '/features/features_69612_0709_spacy_lg_zero_multiclasses_filtered_English.npy')\n\n# Test the construct_graph method\ndef test_construct_graph():\n    preprocessor = CLKD(args).preprocess()\n    preprocessor.construct_graph()\n    # Add assertions to check if the graph was constructed successfully\n    assert os.path.exists(args.file_path + '/English/0/s_bool_A_tid_tid.npz')\n\n# Test the extract_time_feature method\ndef test_extract_time_feature():\n    preprocessor = CLKD(args).preprocess()\n    time_feature = preprocessor.extract_time_feature('2012-10-11 07:19:34')\n    # Add assertions to check if the time feature was extracted successfully\n    assert isinstance(time_feature, list)\n    assert len(time_feature) == 2\n\n# Test the documents_to_features method\ndef test_documents_to_features():\n    preprocessor = CLKD(args).preprocess()\n    df = DatasetLoader('event2012').load_data()\n    features = preprocessor.documents_to_features(df, 'English')\n    # Add assertions to check if the document features were generated successfully\n    assert isinstance(features, np.ndarray)\n    assert features.shape[1] == 300\n\n# Test the get_word2id_emb method\ndef test_get_word2id_emb():\n    preprocessor = CLKD(args).preprocess()\n    word2id, embeddings = preprocessor.get_word2id_emb(args.wordpath, args.embpath)\n    # Add assertions to check if the word2id and embeddings were loaded successfully\n    assert isinstance(word2id, dict)\n    assert isinstance(embeddings, np.ndarray)\n\n# Test the nonlinear_transform_features method\ndef test_nonlinear_transform_features():\n    preprocessor = CLKD(args).preprocess()\n    df = DatasetLoader('event2012').load_data()\n    features = preprocessor.nonlinear_transform_features(args.wordpath, args.embpath, df)\n    # Add assertions to check if the nonlinear transformed features were generated successfully\n    assert isinstance(features, np.ndarray)\n    assert features.shape[1] == 300\n\n# Test the getlinear_transform_features method\ndef test_getlinear_transform_features():\n    preprocessor = CLKD(args).preprocess()\n    df = DatasetLoader('event2012').load_data()\n    features = preprocessor.documents_to_features(df, 'English')\n    transformed_features = preprocessor.getlinear_transform_features(features, 'English', 'French')\n    # Add assertions to check if the linear transformed features were generated successfully\n    assert isinstance(transformed_features, np.ndarray)\n    assert transformed_features.shape[1] == 300\n\n# Test the construct_graph_from_df method\ndef test_construct_graph_from_df():\n    preprocessor = CLKD(args).preprocess()\n    df = DatasetLoader('event2012').load_data()\n    G = preprocessor.construct_graph_from_df(df)\n    # Add assertions to check if the graph was constructed successfully\n    assert isinstance(G, nx.Graph)\n    assert G.number_of_nodes() > 0\n\n# Test the networkx_to_dgl_graph method\ndef test_networkx_to_dgl_graph():\n    preprocessor = CLKD(args).preprocess()\n    df = DatasetLoader('event2012').load_data()\n    G = preprocessor.construct_graph_from_df(df)\n    all_mins, message = preprocessor.networkx_to_dgl_graph(G)\n    # Add assertions to check if the graph was converted successfully\n    assert isinstance(all_mins, float)\n    assert isinstance(message, str)\n\n# Test the construct_incremental_dataset method\ndef test_construct_incremental_dataset():\n    preprocessor = CLKD(args).preprocess()\n    df = DatasetLoader('event2012').load_data()\n    features = np.random.rand(df.shape[0], 300)\n    nfeatures = np.random.rand(df.shape[0], 300)\n    message, data_split, all_graph_mins = preprocessor.construct_incremental_dataset(args, df, args.file_path + '/English', features, nfeatures)\n    # Add assertions to check if the incremental dataset was constructed successfully\n    assert isinstance(message, str)\n    assert isinstance(data_split, list)\n    assert isinstance(all_graph_mins, list)\n\n# Test the SocialDataset class\ndef test_social_dataset():\n    dataset = SocialDataset(args.data_path, 0)\n    # Add assertions to check if the dataset was loaded successfully\n    assert isinstance(dataset.features, np.ndarray)\n    assert isinstance(dataset.labels, np.ndarray)\n    assert isinstance(dataset.matrix, sparse.csr_matrix)\n\n# Test the graph_statistics function\ndef test_graph_statistics():\n    preprocessor = CLKD(args).preprocess()\n    df = DatasetLoader('event2012').load_data()\n    G = preprocessor.construct_graph_from_df(df)\n    num_isolated_nodes = graph_statistics(G, args.file_path + '/English/0')\n    # Add assertions to check if the graph statistics were calculated successfully\n    assert isinstance(num_isolated_nodes, int)\n\n# Run the tests\nif __name__ == '__main__':\n    pytest.main()"}
{"type": "test_file", "path": "SocialED/tests/test_rplmsed.py", "content": "import pytest\nimport torch\nimport numpy as np\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom detector.rplmsed import *\n# 创建一个示例数据集\n@pytest.fixture\ndef sample_dataset():\n    data = [\n        DataItem(event_id=1, tweet_id=1, text=\"Hello world\", user_id=1, created_at=\"2023-10-01\", user_loc=\"USA\", place_type=\"\", place_full_name=\"\", place_country_code=\"\", hashtags=[], user_mentions=[], urls=[], entities=[], words=[\"Hello\", \"world\"], filtered_words=[], sampled_words=[]),\n        DataItem(event_id=1, tweet_id=2, text=\"Goodbye world\", user_id=2, created_at=\"2023-10-02\", user_loc=\"USA\", place_type=\"\", place_full_name=\"\", place_country_code=\"\", hashtags=[], user_mentions=[], urls=[], entities=[], words=[\"Goodbye\", \"world\"], filtered_words=[], sampled_words=[]),\n        DataItem(event_id=2, tweet_id=3, text=\"Hello universe\", user_id=3, created_at=\"2023-10-03\", user_loc=\"USA\", place_type=\"\", place_full_name=\"\", place_country_code=\"\", hashtags=[], user_mentions=[], urls=[], entities=[], words=[\"Hello\", \"universe\"], filtered_words=[], sampled_words=[])\n    ]\n    return data\n\n# 测试 Preprocessor 类的功能\ndef test_preprocessor_split_into_blocks(sample_dataset):\n    preprocessor = Preprocessor()\n    blocks = preprocessor.split_into_blocks(sample_dataset)\n    assert len(blocks) > 0\n    assert isinstance(blocks[0], list)\n\ndef test_preprocessor_process_block(sample_dataset):\n    preprocessor = Preprocessor()\n    block = {\"train\": sample_dataset, \"test\": sample_dataset, \"valid\": sample_dataset}\n    processed_block = preprocessor.process_block(block)\n    assert \"train\" in processed_block\n    assert \"test\" in processed_block\n    assert \"valid\" in processed_block\n\n# 测试 RPLM_SED 类的功能\ndef test_rplmsed_preprocess(sample_dataset):\n    args = args_define().args\n    rplmsed = RPLM_SED(args, sample_dataset)\n    rplmsed.preprocess()\n    # 这里可以添加更多的断言来验证预处理的结果\n\ndef test_rplmsed_fit(sample_dataset):\n    args = args_define().args\n    rplmsed = RPLM_SED(args, sample_dataset)\n    rplmsed.fit()\n    # 这里可以添加更多的断言来验证模型的训练结果\n\ndef test_rplmsed_detection(sample_dataset):\n    args = args_define().args\n    rplmsed = RPLM_SED(args, sample_dataset)\n    predictions, ground_truths = rplmsed.detection()\n    assert isinstance(predictions, np.ndarray)\n    assert isinstance(ground_truths, np.ndarray)\n\ndef test_rplmsed_evaluate(sample_dataset):\n    args = args_define().args\n    rplmsed = RPLM_SED(args, sample_dataset)\n    predictions = np.array([0, 1, 1])\n    ground_truths = np.array([0, 1, 0])\n    ars, ami, nmi = rplmsed.evaluate(predictions, ground_truths)\n    assert isinstance(ars, float)\n    assert isinstance(ami, float)\n    assert isinstance(nmi, float)\n\n# 测试其他辅助函数\ndef test_batch_to_tensor():\n    from rplmsed import batch_to_tensor\n    args = args_define().args\n    batch = [\n        (1, 1, 0, 1, [0, 1], [101, 102], [0, 1]),\n        (0, 2, 1, 2, [1, 0], [103, 104], [1, 0])\n    ]\n    toks, typs, prefix, tags, events = batch_to_tensor(batch, args)\n    assert isinstance(toks, torch.Tensor)\n    assert isinstance(typs, torch.Tensor)\n    assert isinstance(prefix, torch.Tensor)\n    assert isinstance(tags, torch.Tensor)\n    assert isinstance(events, torch.Tensor)\n\ndef test_create_trainer():\n    from rplmsed import create_trainer, get_model\n    args = args_define().args\n    model = get_model(args)\n    optimizer, lr_scheduler = initialize(model, args, 100)\n    trainer = create_trainer(model, optimizer, lr_scheduler, args)\n    assert isinstance(trainer, torch.nn.Module)\n\ndef test_create_evaluator():\n    from rplmsed import create_evaluator, get_model\n    args = args_define().args\n    model = get_model(args)\n    evaluator = create_evaluator(model, args)\n    assert isinstance(evaluator, torch.nn.Module)\n\ndef test_create_tester():\n    from rplmsed import create_tester, get_model\n    args = args_define().args\n    model = get_model(args)\n    msg_feats = torch.zeros((10, model.feat_size()), device='cpu')\n    ref_num = torch.zeros((10,), dtype=torch.long, device='cpu')\n    tester = create_tester(model, args, msg_feats, ref_num)\n    assert isinstance(tester, torch.nn.Module)\n\n# 添加更多的测试用例来覆盖其他功能和方法"}
{"type": "test_file", "path": "SocialED/tests/test_hisevent.py", "content": "import pytest\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_mutual_info_score, adjusted_rand_score\nfrom sentence_transformers import SentenceTransformer\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\nfrom detector.hisevent import *\n# Mock data for testing\n@pytest.fixture\ndef mock_dataset():\n    data = {\n        'filtered_words': [['word1', 'word2'], ['word3', 'word4'], ['word5', 'word6']],\n        'event_id': [1, 2, 1]\n    }\n    return pd.DataFrame(data)\n\n@pytest.fixture\ndef hisevent_instance():\n    return HISEvent()\n\n@pytest.fixture\ndef preprocessor_instance():\n    return Preprocessor()\n\ndef test_preprocess(hisevent_instance):\n    hisevent_instance.preprocess()\n    # Add assertions to check if preprocessing was successful\n    assert True\n\ndef test_detection(hisevent_instance):\n    hisevent_instance.detection()\n    # Add assertions to check if detection was successful\n    assert True\n'''\ndef test_get_stable_point(tmpdir):\n    path = tmpdir.mkdir(\"test_dir\")\n    embeddings = np.random.rand(10, 100)\n    embeddings_path = os.path.join(path, 'SBERT_embeddings.pkl')\n    with open(embeddings_path, 'wb') as f:\n        pickle.dump(embeddings, f)\n    \n    # 确保文件存在\n    assert os.path.exists(embeddings_path)\n    \n    stable_points = get_stable_point(str(path))\n    assert isinstance(stable_points, dict)\n    assert 'first' in stable_points\n    assert 'global' in stable_points\n'''\ndef test_run_hier_2D_SE_mini_Event2012_open_set(tmpdir):\n    save_path = str(tmpdir.mkdir(\"test_dir\"))\n    embeddings = np.random.rand(10, 100)\n    df_np = np.array([[i, i % 3, f'tweet_{i}', f'user_{i}', '2023-01-01'] for i in range(10)])\n    df = pd.DataFrame(data=df_np, columns=[\"original_index\", \"event_id\", \"text\", \"user_id\", \"created_at\"])\n    block_path = os.path.join(save_path, '1')\n    os.makedirs(block_path, exist_ok=True)\n    np.save(os.path.join(block_path, '1.npy'), df_np)\n    embeddings_path = os.path.join(block_path, 'SBERT_embeddings.pkl')\n    with open(embeddings_path, 'wb') as f:\n        pickle.dump(embeddings, f)\n    \n    # 确保文件存在\n    assert os.path.exists(embeddings_path)\n    \n    run_hier_2D_SE_mini_Event2012_open_set(n=10, e_a=True, e_s=True, test_with_one_block=True)\n    # Add assertions to check if the function ran successfully\n    assert True\n\ndef test_evaluate():\n    labels_true = [1, 2, 1]\n    labels_pred = [0, 1, 0]\n    nmi, ami, ari = evaluate(labels_true, labels_pred)\n    assert isinstance(nmi, float)\n    assert isinstance(ami, float)\n    assert isinstance(ari, float)\n\ndef test_decode():\n    division = [[1, 2], [3, 4]]\n    prediction = decode(division)\n    assert isinstance(prediction, list)\n    assert len(prediction) == 4\n\n# Additional tests can be added as needed"}
{"type": "test_file", "path": "SocialED/tests/test_adpsemevent.py", "content": "import unittest\nimport numpy as np\nimport pandas as pd\nimport os\nimport torch\nfrom ..detector.ADPSEMEvent import ADPSEMEvent, Preprocessor\nfrom unittest.mock import MagicMock, patch\n\nclass TestADPSEMEvent(unittest.TestCase):\n    def setUp(self):\n        # Create mock dataset\n        self.mock_dataset = MagicMock()\n        self.mock_dataset.get_dataset_language.return_value = \"English\"\n        self.mock_dataset.get_dataset_name.return_value = \"test_dataset\"\n        \n        # Initialize model\n        self.model = ADPSEMEvent(self.mock_dataset)\n        \n        # Create sample data\n        self.sample_data = pd.DataFrame({\n            'tweet_id': [1, 2, 3],\n            'text': ['tweet1', 'tweet2', 'tweet3'],\n            'event_id': [0, 0, 1],\n            'created_at': pd.date_range('2023-01-01', periods=3),\n            'user_id': [101, 102, 103],\n            'user_mentions': [[], [123], [456]],\n            'entities': [['entity1'], ['entity2'], ['entity3']],\n            'hashtags': [['tag1'], ['tag2'], ['tag3']],\n            'urls': [[], [], []],\n        })\n\n    def test_initialization(self):\n        self.assertEqual(self.model.language, \"English\")\n        self.assertEqual(self.model.dataset_name, \"test_dataset\")\n        self.assertTrue(self.model.save_path.endswith(\"test_dataset/\"))\n\n    @patch('os.path.exists')\n    def test_preprocess(self, mock_exists):\n        mock_exists.return_value = False\n        \n        # Mock dataset load_data method\n        self.mock_dataset.load_data.return_value = self.sample_data.to_numpy()\n        \n        try:\n            self.model.preprocess()\n        except Exception as e:\n            self.fail(f\"Preprocessing failed with error: {str(e)}\")\n\n    def test_detection(self):\n        # Test with sample data\n        ground_truths = [0, 0, 1, 1]\n        predictions = [0, 0, 1, 1]\n        \n        with patch.object(self.model, 'detection', return_value=(ground_truths, predictions)):\n            gt, pred = self.model.detection()\n            self.assertEqual(len(gt), len(pred))\n            self.assertEqual(gt, ground_truths)\n            self.assertEqual(pred, predictions)\n\n    def test_evaluate(self):\n        # Test perfect predictions\n        ground_truths = [0, 0, 1, 1]\n        predictions = [0, 0, 1, 1]\n        \n        with patch('sys.stdout') as mock_stdout:\n            self.model.evaluate(ground_truths, predictions)\n            # All metrics should be 1.0 for perfect predictions\n            self.assertTrue(\"1.0\" in str(mock_stdout.getvalue()))\n\n    def test_split_open_set(self):\n        preprocessor = Preprocessor(self.mock_dataset)\n        test_path = \"../test_data/open_set/\"\n        \n        # Create test directory if it doesn't exist\n        os.makedirs(test_path, exist_ok=True)\n        \n        try:\n            preprocessor.split_open_set(self.sample_data, test_path)\n            # Check if files were created\n            self.assertTrue(os.path.exists(os.path.join(test_path, \"0\")))\n        finally:\n            # Cleanup\n            import shutil\n            if os.path.exists(test_path):\n                shutil.rmtree(test_path)\n\n    def test_run_hier_2D_SE_mini_closed_set(self):\n        # Test with minimal sample data\n        with patch('numpy.load') as mock_load:\n            mock_load.return_value = self.sample_data.to_numpy()\n            \n            try:\n                ground_truths, predictions = self.model.detection()\n                self.assertIsInstance(ground_truths, list)\n                self.assertIsInstance(predictions, list)\n            except Exception as e:\n                self.skipTest(f\"Detection test skipped due to: {str(e)}\")\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_eventx.py", "content": "import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport pytest\nimport numpy as np\nimport pandas as pd\nfrom detector.eventx import *\n\n# 创建一个示例数据集\n@pytest.fixture\ndef sample_dataset():\n    data = {\n        'event_id': [1, 2, 1, 2, 3, 1, 2, 3, 1, 2],\n        'filtered_words': [\n            ['hello', 'world'], ['goodbye', 'world'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'],\n            ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world']\n        ],\n        'message_ids': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n        'entities': [\n            [('hello', 'world')], [('goodbye', 'world')], [('hello', 'world')], [('goodbye', 'world')], [('new', 'event')],\n            [('hello', 'world')], [('goodbye', 'world')], [('new', 'event')], [('hello', 'world')], [('goodbye', 'world')]\n        ]\n    }\n    return pd.DataFrame(data)\n\n# 测试数据预处理\ndef test_preprocess(sample_dataset):\n    eventx = EventX(sample_dataset)\n    eventx.preprocess()\n    assert 'filtered_words' in eventx.df.columns\n    assert 'message_ids' in eventx.df.columns\n    assert 'entities' in eventx.df.columns\n\n# 测试数据分割\ndef test_split(sample_dataset):\n    eventx = EventX(sample_dataset)\n    eventx.split()\n    assert eventx.train_df is not None\n    assert eventx.test_df is not None\n    assert eventx.val_df is not None\n\n# 测试事件检测\ndef test_detection(sample_dataset):\n    eventx = EventX(sample_dataset)\n    eventx.preprocess()\n    ground_truths, predictions = eventx.detection()\n    assert len(ground_truths) == len(predictions)\n\n# 测试评估\ndef test_evaluate(sample_dataset):\n    eventx = EventX(sample_dataset)\n    eventx.preprocess()\n    ground_truths, predictions = eventx.detection()\n    eventx.evaluate(ground_truths, predictions)\n\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "test_file", "path": "SocialED/tests/test_metric.py", "content": "import unittest\nimport numpy as np\nfrom ..metrics.metric import (\n    eval_nmi,\n    eval_ami,\n    eval_ari,\n    eval_f1,\n    eval_acc\n)\n\nclass TestMetrics(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        # Create sample ground truth and prediction arrays\n        self.ground_truths = np.array([0, 0, 1, 1, 2, 2, 2])\n        self.predictions = np.array([0, 0, 1, 1, 1, 2, 2])\n        \n        # Perfect predictions for testing upper bounds\n        self.perfect_predictions = np.array([0, 0, 1, 1, 2, 2, 2])\n        \n        # Completely wrong predictions for testing lower bounds\n        self.wrong_predictions = np.array([2, 2, 0, 0, 1, 1, 1])\n\n    def test_eval_nmi(self):\n        \"\"\"Test Normalized Mutual Information score\"\"\"\n        # Test with sample predictions\n        nmi = eval_nmi(self.ground_truths, self.predictions)\n        self.assertTrue(0 <= nmi <= 1)\n        \n        # Test perfect predictions\n        perfect_nmi = eval_nmi(self.ground_truths, self.perfect_predictions)\n        self.assertAlmostEqual(perfect_nmi, 1.0)\n        \n        # Test symmetry\n        nmi_1 = eval_nmi(self.ground_truths, self.predictions)\n        nmi_2 = eval_nmi(self.predictions, self.ground_truths)\n        self.assertAlmostEqual(nmi_1, nmi_2)\n\n    def test_eval_ami(self):\n        \"\"\"Test Adjusted Mutual Information score\"\"\"\n        # Test with sample predictions\n        ami = eval_ami(self.ground_truths, self.predictions)\n        self.assertTrue(-1 <= ami <= 1)\n        \n        # Test perfect predictions\n        perfect_ami = eval_ami(self.ground_truths, self.perfect_predictions)\n        self.assertAlmostEqual(perfect_ami, 1.0)\n        \n        # Test symmetry\n        ami_1 = eval_ami(self.ground_truths, self.predictions)\n        ami_2 = eval_ami(self.predictions, self.ground_truths)\n        self.assertAlmostEqual(ami_1, ami_2)\n\n    def test_eval_ari(self):\n        \"\"\"Test Adjusted Rand Index score\"\"\"\n        # Test with sample predictions\n        ari = eval_ari(self.ground_truths, self.predictions)\n        self.assertTrue(-1 <= ari <= 1)\n        \n        # Test perfect predictions\n        perfect_ari = eval_ari(self.ground_truths, self.perfect_predictions)\n        self.assertAlmostEqual(perfect_ari, 1.0)\n        \n        # Test symmetry\n        ari_1 = eval_ari(self.ground_truths, self.predictions)\n        ari_2 = eval_ari(self.predictions, self.ground_truths)\n        self.assertAlmostEqual(ari_1, ari_2)\n\n    def test_eval_f1(self):\n        \"\"\"Test F1 score\"\"\"\n        # Test with sample predictions\n        f1 = eval_f1(self.ground_truths, self.predictions)\n        self.assertTrue(0 <= f1 <= 1)\n        \n        # Test perfect predictions\n        perfect_f1 = eval_f1(self.ground_truths, self.perfect_predictions)\n        self.assertAlmostEqual(perfect_f1, 1.0)\n        \n        # Test completely wrong predictions\n        wrong_f1 = eval_f1(self.ground_truths, self.wrong_predictions)\n        self.assertLess(wrong_f1, perfect_f1)\n\n    def test_eval_acc(self):\n        \"\"\"Test Accuracy score\"\"\"\n        # Test with sample predictions\n        acc = eval_acc(self.ground_truths, self.predictions)\n        self.assertTrue(0 <= acc <= 1)\n        \n        # Test perfect predictions\n        perfect_acc = eval_acc(self.ground_truths, self.perfect_predictions)\n        self.assertAlmostEqual(perfect_acc, 1.0)\n        \n        # Test completely wrong predictions\n        wrong_acc = eval_acc(self.ground_truths, self.wrong_predictions)\n        self.assertLess(wrong_acc, perfect_acc)\n\n    def test_input_validation(self):\n        \"\"\"Test input validation and error handling\"\"\"\n        # Test with arrays of different lengths\n        with self.assertRaises(ValueError):\n            eval_nmi(self.ground_truths, self.predictions[:-1])\n        \n        # Test with empty arrays\n        with self.assertRaises(ValueError):\n            eval_ami([], [])\n        \n        # Test with non-numeric data\n        with self.assertRaises(ValueError):\n            eval_ari(['a', 'b'], ['c', 'd'])\n\n    def test_edge_cases(self):\n        \"\"\"Test edge cases\"\"\"\n        # Single class\n        single_class = np.zeros(5)\n        score = eval_nmi(single_class, single_class)\n        self.assertTrue(0 <= score <= 1)\n        \n        # Binary classification\n        binary_gt = np.array([0, 0, 1, 1])\n        binary_pred = np.array([0, 0, 1, 1])\n        score = eval_f1(binary_gt, binary_pred)\n        self.assertAlmostEqual(score, 1.0)\n        \n        # All wrong predictions\n        all_wrong = np.ones(len(self.ground_truths))\n        score = eval_acc(self.ground_truths, all_wrong)\n        self.assertLess(score, 0.5)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_lda.py", "content": "import os\nimport pytest\nimport pandas as pd\nfrom unittest.mock import patch\n\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom detector.lda import LDA\nfrom dataset.dataloader import DatasetLoader\n\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim import corpora\n\n# 设置测试数据路径\nTEST_DATA_PATH = os.path.join(os.path.dirname(__file__), 'test_data')\n\n\n# 创建一个测试数据集\n@pytest.fixture\ndef test_dataset():\n    data = {\n        'filtered_words': [['word1', 'word2'], ['word3', 'word4'], ['word5', 'word6']],\n        'event_id': [1, 2, 3]\n    }\n    return pd.DataFrame(data)\n\n\n# 测试预处理方法\ndef test_preprocess(test_dataset):\n    lda = LDA(dataset=test_dataset)\n    df = lda.preprocess()\n    assert 'processed_text' in df.columns\n    assert df['processed_text'].tolist() == [['word1', 'word2'], ['word3', 'word4'], ['word5', 'word6']]\n\n\n# 测试创建语料库方法\ndef test_create_corpus(test_dataset):\n    lda = LDA(dataset=test_dataset)\n    lda.preprocess()\n    corpus, dictionary = lda.create_corpus(lda.df, 'processed_text')\n    assert isinstance(corpus, list)\n    assert isinstance(dictionary, corpora.Dictionary)\n\n\n# 测试模型训练方法\ndef test_fit(test_dataset, tmpdir):\n    lda = LDA(dataset=test_dataset, file_path=str(tmpdir))\n    lda.preprocess()\n    lda.fit()\n    assert os.path.exists(os.path.join(tmpdir, 'lda_model'))\n\n\n# 测试模型加载方法\ndef test_load_model(test_dataset, tmpdir):\n    lda = LDA(dataset=test_dataset, file_path=str(tmpdir))\n    lda.preprocess()\n    lda.fit()\n    loaded_model = lda.load_model()\n    assert isinstance(loaded_model, LdaModel)\n\n\n# 测试主题显示方法\ndef test_display_topics(test_dataset, tmpdir, capsys):\n    lda = LDA(dataset=test_dataset, file_path=str(tmpdir))\n    lda.preprocess()\n    lda.fit()\n    lda.load_model()\n    lda.display_topics()\n    captured = capsys.readouterr()\n    assert \"Topic\" in captured.out\n\n\n# 测试检测方法\ndef test_detection(test_dataset, tmpdir):\n    lda = LDA(dataset=test_dataset, file_path=str(tmpdir))\n    lda.preprocess()\n    lda.fit()\n    ground_truths, predictions = lda.detection()\n    assert isinstance(ground_truths, list)\n    assert isinstance(predictions, list)\n    assert len(ground_truths) == len(predictions)\n\n\n# 测试评估方法\ndef test_evaluate(test_dataset, tmpdir):\n    lda = LDA(dataset=test_dataset, file_path=str(tmpdir))\n    lda.preprocess()\n    lda.fit()\n    ground_truths, predictions = lda.detection()\n    lda.evaluate(ground_truths, predictions)\n    assert os.path.exists(os.path.join(tmpdir, 'lda_model_evaluation.txt'))\n\n\n# 测试主函数\n@patch('dataset.dataloader.DatasetLoader.load_data')\ndef test_main(mock_load_data, tmpdir):\n    # 模拟数据集加载\n    mock_load_data.return_value = pd.DataFrame({\n        'filtered_words': [['word1', 'word2'], ['word3', 'word4'], ['word5', 'word6']],\n        'event_id': [1, 2, 3]\n    })\n\n    lda = LDA(dataset=DatasetLoader(\"event2012\").load_data(), file_path=str(tmpdir))\n    lda.preprocess()\n    lda.fit()\n    ground_truths, predictions = lda.detection()\n    lda.evaluate(ground_truths, predictions)\n    assert os.path.exists(os.path.join(tmpdir, 'lda_model'))\n    assert os.path.exists(os.path.join(tmpdir, 'unique_ground_truths_predictions.csv'))\n    assert os.path.exists(os.path.join(tmpdir, 'lda_model_evaluation.txt'))\n"}
{"type": "test_file", "path": "SocialED/tests/test_glove.py", "content": "import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport pytest\nimport numpy as np\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom dataset.dataloader import DatasetLoader\nfrom detector.glove import *\n\n# 创建一个示例数据集\n@pytest.fixture\ndef sample_dataset():\n    data = {\n        'event_id': [1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2],  # 增加样本数量\n        'filtered_words': [\n            ['hello', 'world'], ['goodbye', 'world'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'],\n            ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world'],\n            ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world'],\n            ['new', 'event'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'],\n            ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'],\n            ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world']\n        ]\n    }\n    return pd.DataFrame(data)\n\n# 测试数据预处理\ndef test_preprocess(sample_dataset):\n    glove = GloVe(sample_dataset)\n    df = glove.preprocess()\n    assert 'processed_text' in df.columns\n    assert df['processed_text'].apply(lambda x: isinstance(x, list)).all()\n\n# 测试加载 GloVe 向量\ndef test_load_glove_vectors():\n    glove = GloVe(None)\n    embeddings_index = glove.load_glove_vectors()\n    assert isinstance(embeddings_index, dict)\n    assert len(embeddings_index) > 0\n\n# 测试文本转换为 GloVe 向量\ndef test_text_to_glove_vector(sample_dataset):\n    glove = GloVe(sample_dataset)\n    glove.load_glove_vectors()\n    text = ['hello', 'world']\n    vector = glove.text_to_glove_vector(text)\n    assert isinstance(vector, np.ndarray)\n    assert vector.shape == (100,)\n\n# 测试创建向量\ndef test_create_vectors(sample_dataset):\n    glove = GloVe(sample_dataset)\n    glove.load_glove_vectors()\n    glove.preprocess()\n    vectors = glove.create_vectors(glove.df, 'processed_text')\n    assert isinstance(vectors, np.ndarray)\n    assert vectors.shape[0] == len(glove.df)\n    assert vectors.shape[1] == 100\n\n# 测试训练 KMeans 模型\ndef test_fit(sample_dataset):\n    glove = GloVe(sample_dataset, num_clusters=5)  # 减少聚类数量\n    glove.preprocess()\n    glove.fit()\n    assert os.path.exists(glove.model_path)\n\n# 测试加载 KMeans 模型\ndef test_load_model(sample_dataset):\n    glove = GloVe(sample_dataset, num_clusters=5)  # 减少聚类数量\n    glove.preprocess()\n    glove.fit()\n    kmeans_model = glove.load_model()\n    assert isinstance(kmeans_model, KMeans)\n\n# 测试检测\ndef test_detection(sample_dataset):\n    glove = GloVe(sample_dataset, num_clusters=5)  # 减少聚类数量\n    glove.preprocess()\n    glove.fit()\n    ground_truths, predicted_labels = glove.detection()\n    assert len(ground_truths) == len(predicted_labels)\n\n# 测试评估\ndef test_evaluate(sample_dataset):\n    glove = GloVe(sample_dataset, num_clusters=5)  # 减少聚类数量\n    glove.preprocess()\n    glove.fit()\n    ground_truths, predicted_labels = glove.detection()\n    glove.evaluate(ground_truths, predicted_labels)\n\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "test_file", "path": "SocialED/tests/test_finevent.py", "content": "import unittest\nimport torch\nimport numpy as np\nfrom ..detector.finevent import (\n    FinEvent, \n    args_define,\n    Preprocessor,\n    MarGNN,\n    OnlineTripletLoss,\n    RandomNegativeTripletSelector\n)\nfrom ..dataset.dataloader import DatasetLoader\n\nclass TestFinEvent(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.args = args_define()\n        self.dataset = DatasetLoader(\"maven\").load_data()\n        self.finevent = FinEvent(self.args, self.dataset)\n\n    def test_init(self):\n        \"\"\"Test initialization of FinEvent\"\"\"\n        self.assertIsInstance(self.finevent, FinEvent)\n        self.assertEqual(self.finevent.dataset, self.dataset)\n\n    def test_evaluate(self):\n        \"\"\"Test evaluate method\"\"\"\n        # Create sample predictions and ground truths\n        predictions = np.array([0, 1, 0, 1, 2])\n        ground_truths = np.array([0, 1, 0, 1, 1])\n        \n        # Test evaluate method\n        ars, ami, nmi = self.finevent.evaluate(predictions, ground_truths)\n        \n        # Check if metrics are between 0 and 1\n        self.assertTrue(0 <= ars <= 1)\n        self.assertTrue(0 <= ami <= 1)\n        self.assertTrue(0 <= nmi <= 1)\n\n\nclass TestPreprocessor(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.preprocessor = Preprocessor()\n\n    def test_extract_time_feature(self):\n        \"\"\"Test time feature extraction\"\"\"\n        # Test with a sample datetime\n        time_str = \"2023-01-01T12:00:00\"\n        features = self.preprocessor.extract_time_feature(time_str)\n        \n        self.assertEqual(len(features), 2)\n        self.assertIsInstance(features[0], float)\n        self.assertIsInstance(features[1], float)\n\n\nclass TestMarGNN(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.in_dim = 300\n        self.hidden_dim = 128\n        self.out_dim = 64\n        self.heads = 4\n        self.num_relations = 3\n        self.gnn_args = (self.in_dim, self.hidden_dim, self.out_dim, self.heads)\n        \n        self.model = MarGNN(\n            self.gnn_args,\n            num_relations=self.num_relations,\n            inter_opt='cat_w_avg',\n            is_shared=False\n        )\n\n    def test_init(self):\n        \"\"\"Test initialization of MarGNN\"\"\"\n        self.assertEqual(len(self.model.intra_aggs), self.num_relations)\n        self.assertEqual(self.model.inter_opt, 'cat_w_avg')\n        self.assertFalse(self.model.is_shared)\n\n    def test_forward_shape(self):\n        \"\"\"Test forward pass output shape\"\"\"\n        batch_size = 32\n        x = torch.randn(batch_size, self.in_dim)\n        \n        # Create dummy adjacency matrices\n        adjs = [(torch.tensor([[0, 1], [1, 0]]), None, (batch_size, batch_size)) for _ in range(self.num_relations)]\n        \n        # Create dummy node IDs\n        n_ids = [torch.arange(batch_size) for _ in range(self.num_relations)]\n        \n        # Create dummy thresholds\n        RL_thresholds = torch.ones(self.num_relations, 1)\n        \n        # Move tensors to CPU for testing\n        device = torch.device('cpu')\n        \n        # Forward pass\n        output = self.model(x, adjs, n_ids, device, RL_thresholds)\n        \n        # Check output shape\n        expected_shape = (batch_size, self.out_dim * self.num_relations)\n        self.assertEqual(output.shape, expected_shape)\n\n\nclass TestTripletLoss(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.margin = 1.0\n        self.triplet_selector = RandomNegativeTripletSelector(margin=self.margin)\n        self.loss_fn = OnlineTripletLoss(margin=self.margin, triplet_selector=self.triplet_selector)\n\n    def test_triplet_loss(self):\n        \"\"\"Test triplet loss computation\"\"\"\n        # Create sample embeddings and labels\n        embeddings = torch.randn(10, 64)  # 10 samples, 64 dimensions\n        labels = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])  # 5 classes, 2 samples each\n        \n        # Compute loss\n        loss, num_triplets = self.loss_fn(embeddings, labels)\n        \n        # Check if loss is a scalar\n        self.assertEqual(loss.dim(), 0)\n        # Check if loss is non-negative\n        self.assertGreaterEqual(loss.item(), 0)\n        # Check if number of triplets is non-negative\n        self.assertGreaterEqual(num_triplets, 0)\n\n\nclass TestArgsDefine(unittest.TestCase):\n    def test_default_args(self):\n        \"\"\"Test default arguments\"\"\"\n        args = args_define()\n        \n        # Test some default values\n        self.assertEqual(args.n_epochs, 1)\n        self.assertEqual(args.window_size, 3)\n        self.assertEqual(args.patience, 5)\n        self.assertEqual(args.margin, 3.0)\n        self.assertEqual(args.lr, 1e-3)\n        self.assertEqual(args.batch_size, 50)\n        self.assertEqual(args.hidden_dim, 128)\n        self.assertEqual(args.out_dim, 64)\n        self.assertEqual(args.heads, 4)\n\n    def test_custom_args(self):\n        \"\"\"Test custom arguments\"\"\"\n        custom_args = {\n            'n_epochs': 5,\n            'window_size': 5,\n            'batch_size': 32,\n            'hidden_dim': 256\n        }\n        \n        args = args_define(**custom_args)\n        \n        # Test custom values\n        self.assertEqual(args.n_epochs, 5)\n        self.assertEqual(args.window_size, 5)\n        self.assertEqual(args.batch_size, 32)\n        self.assertEqual(args.hidden_dim, 256)\n        \n        # Test that other defaults remain unchanged\n        self.assertEqual(args.margin, 3.0)\n        self.assertEqual(args.lr, 1e-3)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_qsgnn.py", "content": "import unittest\nimport torch\nimport numpy as np\nfrom ..detector.qsgnn import (\n    QSGNN,\n    args_define,\n    Preprocessor,\n    GAT,\n    OnlineTripletLoss,\n    RandomNegativeTripletSelector,\n    Arabic_preprocessor\n)\nfrom ..dataset.dataloader import DatasetLoader\n\nclass TestQSGNN(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.args = args_define()\n        self.dataset = DatasetLoader(\"maven\").load_data()\n        self.qsgnn = QSGNN(self.args, self.dataset)\n\n    def test_init(self):\n        \"\"\"Test initialization of QSGNN\"\"\"\n        self.assertIsInstance(self.qsgnn, QSGNN)\n        self.assertEqual(self.qsgnn.dataset, self.dataset)\n        self.assertEqual(self.qsgnn.use_cuda, self.args.use_cuda)\n\n    def test_evaluate(self):\n        \"\"\"Test evaluate method\"\"\"\n        # Create sample predictions and ground truths\n        predictions = np.array([0, 1, 0, 1, 2])\n        ground_truths = np.array([0, 1, 0, 1, 1])\n        \n        # Test evaluate method\n        ars, ami, nmi = self.qsgnn.evaluate(predictions, ground_truths)\n        \n        # Check if metrics are between 0 and 1\n        self.assertTrue(0 <= ars <= 1)\n        self.assertTrue(0 <= ami <= 1)\n        self.assertTrue(0 <= nmi <= 1)\n\n\nclass TestPreprocessor(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.args = args_define()\n        self.preprocessor = Preprocessor(self.args)\n\n    def test_extract_time_feature(self):\n        \"\"\"Test time feature extraction\"\"\"\n        # Test with a sample datetime\n        time_str = \"2023-01-01T12:00:00\"\n        features = self.preprocessor.extract_time_feature(time_str)\n        \n        self.assertEqual(len(features), 2)\n        self.assertIsInstance(features[0], float)\n        self.assertIsInstance(features[1], float)\n\n\nclass TestArabicPreprocessor(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.arabic_preprocessor = Arabic_preprocessor(tokenizer=None)\n\n    def test_clean_text(self):\n        \"\"\"Test Arabic text cleaning\"\"\"\n        # Test with sample Arabic text containing common patterns\n        test_text = \"أهلاً وسهلاً...\"\n        cleaned_text = self.arabic_preprocessor.clean_text(test_text)\n        \n        # Check if text is cleaned properly\n        self.assertIsInstance(cleaned_text, str)\n        self.assertNotEqual(cleaned_text, test_text)\n        self.assertNotIn('...', cleaned_text)\n\n\nclass TestGAT(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.in_dim = 300\n        self.hidden_dim = 16\n        self.out_dim = 64\n        self.num_heads = 4\n        self.use_residual = True\n        \n        self.model = GAT(\n            self.in_dim,\n            self.hidden_dim,\n            self.out_dim,\n            self.num_heads,\n            self.use_residual\n        )\n\n    def test_init(self):\n        \"\"\"Test initialization of GAT\"\"\"\n        self.assertIsInstance(self.model.layer1, torch.nn.Module)\n        self.assertIsInstance(self.model.layer2, torch.nn.Module)\n\n    def test_forward_shape(self):\n        \"\"\"Test forward pass output shape\"\"\"\n        batch_size = 32\n        \n        # Create dummy blocks\n        blocks = [\n            {\n                'srcdata': {'features': torch.randn(batch_size, self.in_dim)},\n                'number_of_dst_nodes': lambda: batch_size\n            },\n            {\n                'srcdata': {'features': torch.randn(batch_size, self.hidden_dim * self.num_heads)},\n                'number_of_dst_nodes': lambda: batch_size\n            }\n        ]\n        \n        # Forward pass\n        output = self.model(blocks)\n        \n        # Check output shape\n        expected_shape = (batch_size, self.out_dim)\n        self.assertEqual(output.shape, expected_shape)\n\n\nclass TestTripletLoss(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.margin = 1.0\n        self.triplet_selector = RandomNegativeTripletSelector(margin=self.margin)\n        self.loss_fn = OnlineTripletLoss(margin=self.margin, triplet_selector=self.triplet_selector)\n\n    def test_triplet_loss(self):\n        \"\"\"Test triplet loss computation\"\"\"\n        # Create sample embeddings and labels\n        embeddings = torch.randn(10, 64)  # 10 samples, 64 dimensions\n        labels = torch.tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])  # 5 classes, 2 samples each\n        \n        # Compute loss\n        loss, num_triplets = self.loss_fn(embeddings, labels)\n        \n        # Check if loss is a scalar\n        self.assertEqual(loss.dim(), 0)\n        # Check if loss is non-negative\n        self.assertGreaterEqual(loss.item(), 0)\n        # Check if number of triplets is non-negative\n        self.assertGreaterEqual(num_triplets, 0)\n\n\nclass TestArgsDefine(unittest.TestCase):\n    def test_default_args(self):\n        \"\"\"Test default arguments\"\"\"\n        args = args_define()\n        \n        # Test some default values\n        self.assertEqual(args.finetune_epochs, 1)\n        self.assertEqual(args.n_epochs, 5)\n        self.assertEqual(args.window_size, 3)\n        self.assertEqual(args.patience, 5)\n        self.assertEqual(args.margin, 3.0)\n        self.assertEqual(args.lr, 1e-3)\n        self.assertEqual(args.batch_size, 1000)\n        self.assertEqual(args.hidden_dim, 16)\n        self.assertEqual(args.out_dim, 64)\n        self.assertEqual(args.num_heads, 4)\n\n    def test_custom_args(self):\n        \"\"\"Test custom arguments\"\"\"\n        custom_args = {\n            'finetune_epochs': 3,\n            'n_epochs': 10,\n            'batch_size': 500,\n            'hidden_dim': 32\n        }\n        \n        args = args_define(**custom_args)\n        \n        # Test custom values\n        self.assertEqual(args.finetune_epochs, 3)\n        self.assertEqual(args.n_epochs, 10)\n        self.assertEqual(args.batch_size, 500)\n        self.assertEqual(args.hidden_dim, 32)\n        \n        # Test that other defaults remain unchanged\n        self.assertEqual(args.margin, 3.0)\n        self.assertEqual(args.lr, 1e-3)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_hypersed.py", "content": "import unittest\nimport numpy as np\nimport pandas as pd\nimport os\nimport torch\nfrom ..detector.Hypersed import Hypersed, Preprocessor, SE\nfrom unittest.mock import MagicMock, patch\n\nclass TestHypersed(unittest.TestCase):\n    def setUp(self):\n        # Create mock dataset\n        self.mock_dataset = MagicMock()\n        self.mock_dataset.get_dataset_language.return_value = \"English\"\n        self.mock_dataset.get_dataset_name.return_value = \"test_dataset\"\n        \n        # Initialize model\n        self.model = Hypersed(self.mock_dataset)\n        \n        # Create sample data\n        self.sample_data = pd.DataFrame({\n            'tweet_id': [1, 2, 3],\n            'text': ['tweet1', 'tweet2', 'tweet3'],\n            'event_id': [0, 0, 1],\n            'created_at': pd.date_range('2023-01-01', periods=3),\n            'user_id': [101, 102, 103],\n            'user_mentions': [[], [123], [456]],\n            'entities': [['entity1'], ['entity2'], ['entity3']],\n            'hashtags': [['tag1'], ['tag2'], ['tag3']],\n            'urls': [[], [], []],\n        })\n\n    def test_initialization(self):\n        self.assertEqual(self.model.language, \"English\")\n        self.assertEqual(self.model.dataset_name, \"test_dataset\")\n        self.assertTrue(self.model.save_path.endswith(\"test_dataset/\"))\n\n    @patch('os.path.exists')\n    def test_preprocess(self, mock_exists):\n        mock_exists.return_value = False\n        \n        # Mock dataset load_data method\n        self.mock_dataset.load_data.return_value = self.sample_data.to_numpy()\n        \n        try:\n            self.model.preprocess()\n        except Exception as e:\n            self.fail(f\"Preprocessing failed with error: {str(e)}\")\n\n    def test_detection(self):\n        # Test with sample data\n        ground_truths = [0, 0, 1, 1]\n        predictions = [0, 0, 1, 1]\n        \n        with patch.object(self.model, 'detection', return_value=(ground_truths, predictions)):\n            gt, pred = self.model.detection()\n            self.assertEqual(len(gt), len(pred))\n            self.assertEqual(gt, ground_truths)\n            self.assertEqual(pred, predictions)\n\n    def test_evaluate(self):\n        # Test perfect predictions\n        ground_truths = [0, 0, 1, 1]\n        predictions = [0, 0, 1, 1]\n        \n        with patch('sys.stdout') as mock_stdout:\n            self.model.evaluate(ground_truths, predictions)\n            # All metrics should be 1.0 for perfect predictions\n            self.assertTrue(\"1.0\" in str(mock_stdout.getvalue()))\n\n    def test_get_closed_set_test_df(self):\n        preprocessor = Preprocessor(self.mock_dataset)\n        test_path = \"../test_data/closed_set/\"\n        \n        # Create test directory if it doesn't exist\n        os.makedirs(test_path, exist_ok=True)\n        \n        try:\n            preprocessor.get_closed_set_test_df(self.sample_data)\n            # Check if files were created\n            self.assertTrue(os.path.exists(os.path.join(test_path, \"test_set.npy\")))\n        finally:\n            # Cleanup\n            import shutil\n            if os.path.exists(test_path):\n                shutil.rmtree(test_path)\n\n    def test_SE_class(self):\n        # Create sample graph for testing SE class\n        import networkx as nx\n        G = nx.Graph()\n        G.add_edges_from([(1, 2), (2, 3), (3, 4)])\n        \n        se = SE(G)\n        \n        # Test initialization\n        self.assertIsNotNone(se.graph)\n        self.assertEqual(se.vol, 6)  # 3 edges * 2\n        \n        # Test division initialization\n        se.init_division()\n        self.assertEqual(len(se.division), len(G.nodes))\n        \n        # Test update_struc_data_2d\n        se.update_struc_data_2d()\n        self.assertGreater(len(se.struc_data_2d), 0)\n\n    def test_construct_open_set_graphs(self):\n        preprocessor = Preprocessor(self.mock_dataset)\n        \n        # Mock dataset methods\n        self.mock_dataset.get_num_blocks.return_value = 2\n        \n        with patch('os.path.exists') as mock_exists:\n            mock_exists.return_value = True\n            \n            try:\n                preprocessor.construct_open_set_graphs(self.mock_dataset)\n            except Exception as e:\n                self.skipTest(f\"Graph construction test skipped due to: {str(e)}\")\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_kpgnn.py", "content": "import pytest\nimport torch\nimport numpy as np\nfrom sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, normalized_mutual_info_score\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom detector.kpgnn import KPGNN, args_define, SocialDataset\n\ntmpdir=\"../model/model_saved/kpgnn/kpgnn_incremental_test/\"\n\n# 模拟数据集\nclass MockDataset:\n    def __init__(self):\n        self.features = np.random.rand(100, 300)\n        self.labels = np.random.randint(0, 10, 100)\n        self.matrix = np.random.rand(100, 100)\n\n# 测试 KPGNN 类的初始化\ndef test_kpgnn_init():\n    args = args_define().args\n    dataset = MockDataset()\n    kpgnn = KPGNN(args, dataset)\n    assert kpgnn.args == args\n    assert kpgnn.dataset == dataset\n    assert kpgnn.model is None\n    assert kpgnn.loss_fn is None\n    assert kpgnn.loss_fn_dgi is None\n    assert kpgnn.metrics is None\n    assert kpgnn.train_indices is None\n    assert kpgnn.indices_to_remove is None\n    assert kpgnn.embedding_save_path is None\n    assert kpgnn.data_split is None\n\n# 测试 KPGNN 类的 preprocess 方法\ndef test_kpgnn_preprocess(tmpdir):\n    args = args_define().args\n    args.data_path = str(tmpdir)\n    dataset = MockDataset()\n    kpgnn = KPGNN(args, dataset)\n    kpgnn.preprocess()\n    # 这里可以添加更多的断言来检查预处理后的数据\n\n# 测试 KPGNN 类的 fit 方法\ndef test_kpgnn_fit(tmpdir):\n    args = args_define().args\n    args.data_path = str(tmpdir)\n    dataset = MockDataset()\n    kpgnn = KPGNN(args, dataset)\n    kpgnn.fit()\n    # 这里可以添加更多的断言来检查模型是否正确训练\n\n# 测试 KPGNN 类的 detection 方法\ndef test_kpgnn_detection(tmpdir):\n    args = args_define().args\n    args.data_path = str(tmpdir)\n    dataset = MockDataset()\n    kpgnn = KPGNN(args, dataset)\n    kpgnn.fit()\n    predictions, ground_truths = kpgnn.detection()\n    assert isinstance(predictions, np.ndarray)\n    assert isinstance(ground_truths, np.ndarray)\n    assert len(predictions) == len(ground_truths)\n\n# 测试 KPGNN 类的 evaluate 方法\ndef test_kpgnn_evaluate(tmpdir):\n    args = args_define().args\n    args.data_path = str(tmpdir)\n    dataset = MockDataset()\n    kpgnn = KPGNN(args, dataset)\n    kpgnn.fit()\n    predictions, ground_truths = kpgnn.detection()\n    ars, ami, nmi = kpgnn.evaluate(predictions, ground_truths)\n    assert isinstance(ars, float)\n    assert isinstance(ami, float)\n    assert isinstance(nmi, float)\n    assert 0 <= ars <= 1\n    assert 0 <= ami <= 1\n    assert 0 <= nmi <= 1\n\n# 测试 SocialDataset 类的初始化\ndef test_social_dataset_init(tmpdir):\n    data_path = str(tmpdir.mkdir(\"0\"))\n    np.save(os.path.join(data_path, \"features.npy\"), np.random.rand(100, 300))\n    np.save(os.path.join(data_path, \"labels.npy\"), np.random.randint(0, 10, 100))\n    sparse.save_npz(os.path.join(data_path, \"s_bool_A_tid_tid.npz\"), sparse.csr_matrix(np.random.rand(100, 100)))\n    dataset = SocialDataset(str(tmpdir), 0)\n    assert isinstance(dataset.features, np.ndarray)\n    assert isinstance(dataset.labels, np.ndarray)\n    assert isinstance(dataset.matrix, sparse.csr_matrix)\n\n# 测试 SocialDataset 类的 __len__ 方法\ndef test_social_dataset_len(tmpdir):\n    data_path = str(tmpdir.mkdir(\"0\"))\n    np.save(os.path.join(data_path, \"features.npy\"), np.random.rand(100, 300))\n    np.save(os.path.join(data_path, \"labels.npy\"), np.random.randint(0, 10, 100))\n    sparse.save_npz(os.path.join(data_path, \"s_bool_A_tid_tid.npz\"), sparse.csr_matrix(np.random.rand(100, 100)))\n    dataset = SocialDataset(str(tmpdir), 0)\n    assert len(dataset) == dataset.features.shape[0]\n\n# 测试 SocialDataset 类的 __getitem__ 方法\ndef test_social_dataset_getitem(tmpdir):\n    data_path = str(tmpdir.mkdir(\"0\"))\n    np.save(os.path.join(data_path, \"features.npy\"), np.random.rand(100, 300))\n    np.save(os.path.join(data_path, \"labels.npy\"), np.random.randint(0, 10, 100))\n    sparse.save_npz(os.path.join(data_path, \"s_bool_A_tid_tid.npz\"), sparse.csr_matrix(np.random.rand(100, 100)))\n    dataset = SocialDataset(str(tmpdir), 0)\n    feature, label = dataset[0]\n    assert isinstance(feature, np.ndarray)\n    assert isinstance(label, np.ndarray)\n\n# 测试 SocialDataset 类的 load_adj_matrix 方法\ndef test_social_dataset_load_adj_matrix(tmpdir):\n    data_path = str(tmpdir.mkdir(\"0\"))\n    np.save(os.path.join(data_path, \"features.npy\"), np.random.rand(100, 300))\n    np.save(os.path.join(data_path, \"labels.npy\"), np.random.randint(0, 10, 100))\n    sparse.save_npz(os.path.join(data_path, \"s_bool_A_tid_tid.npz\"), sparse.csr_matrix(np.random.rand(100, 100)))\n    dataset = SocialDataset(str(tmpdir), 0)\n    assert isinstance(dataset.matrix, sparse.csr_matrix)\n\n# 测试 SocialDataset 类的 remove_obsolete_nodes 方法\ndef test_social_dataset_remove_obsolete_nodes(tmpdir):\n    data_path = str(tmpdir.mkdir(\"0\"))\n    np.save(os.path.join(data_path, \"features.npy\"), np.random.rand(100, 300))\n    np.save(os.path.join(data_path, \"labels.npy\"), np.random.randint(0, 10, 100))\n    sparse.save_npz(os.path.join(data_path, \"s_bool_A_tid_tid.npz\"), sparse.csr_matrix(np.random.rand(100, 100)))\n    dataset = SocialDataset(str(tmpdir), 0)\n    original_length = len(dataset)\n    dataset.remove_obsolete_nodes([0, 1, 2])\n    assert len(dataset) == original_length - 3\n\n# 运行测试\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "test_file", "path": "SocialED/tests/test_word2vec.py", "content": "import pytest\nimport sys\nimport os\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\n\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n# Assuming the WORD2VEC class is in a file named word2vec.py\nfrom detector.word2vec import *\n\n# Mock data for testing\n@pytest.fixture\ndef mock_dataset():\n    data = {\n        'filtered_words': [['word1', 'word2'], ['word3', 'word4'], ['word5', 'word6']],\n        'event_id': [1, 2, 1]\n    }\n    return pd.DataFrame(data)\n\n@pytest.fixture\ndef word2vec_instance(mock_dataset):\n    word2vec = WORD2VEC(dataset=mock_dataset)\n    word2vec.preprocess()  # Ensure preprocessing is done before any other method\n    return word2vec\n\ndef test_preprocess(word2vec_instance):\n    df = word2vec_instance.df\n    assert isinstance(df, pd.DataFrame)\n    assert 'processed_text' in df.columns\n    assert df['processed_text'].tolist() == [['word1', 'word2'], ['word3', 'word4'], ['word5', 'word6']]\n\ndef test_fit(word2vec_instance, tmpdir):\n    word2vec_instance.file_path = os.path.join(tmpdir, 'word2vec_model.model')\n    model = word2vec_instance.fit()\n    assert isinstance(model, Word2Vec)\n    assert os.path.exists(word2vec_instance.file_path)\n\ndef test_load_model(word2vec_instance, tmpdir):\n    word2vec_instance.file_path = os.path.join(tmpdir, 'word2vec_model.model')\n    word2vec_instance.fit()  # Ensure the model is saved\n    loaded_model = word2vec_instance.load_model()\n    assert isinstance(loaded_model, Word2Vec)\n\ndef test_document_vector(word2vec_instance):\n    word2vec_instance.fit()  # Ensure the model is trained\n    document = ['word1', 'word2']\n    vector = word2vec_instance.document_vector(document)\n    assert isinstance(vector, np.ndarray)\n    assert vector.shape == (word2vec_instance.vector_size,)\n\ndef test_detection(word2vec_instance):\n    word2vec_instance.fit()  # Ensure the model is trained\n    ground_truths, predictions = word2vec_instance.detection()\n    assert isinstance(ground_truths, list)\n    assert isinstance(predictions, np.ndarray)\n    assert len(ground_truths) == len(predictions)\n\ndef test_evaluate(word2vec_instance):\n    ground_truths = [1, 2, 1]\n    predictions = np.array([0, 1, 0])\n    ari, ami, nmi = word2vec_instance.evaluate(ground_truths, predictions)\n    assert isinstance(ari, float)\n    assert isinstance(ami, float)\n    assert isinstance(nmi, float)"}
{"type": "test_file", "path": "SocialED/tests/test_hcrc.py", "content": "import unittest\nimport torch\nimport numpy as np\nfrom datetime import datetime\nfrom ..detector.hcrc import (\n    HCRC,\n    args_define,\n    SinglePass,\n    extract_time_feature,\n    df_to_t_features,\n    evaluate_fun,\n    random_cluster\n)\nfrom ..dataset.dataloader import DatasetLoader\n\n\nclass TestHCRC(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.args = args_define()\n        self.dataset = DatasetLoader(\"maven\").load_data()\n        self.hcrc = HCRC(self.args, self.dataset)\n\n    def test_init(self):\n        \"\"\"Test initialization of HCRC\"\"\"\n        self.assertIsInstance(self.hcrc, HCRC)\n        self.assertEqual(self.hcrc.dataset, self.dataset)\n        self.assertEqual(self.hcrc.args, self.args)\n\n    def test_evaluate(self):\n        \"\"\"Test evaluate method\"\"\"\n        # Create sample predictions and ground truths\n        predictions = np.array([0, 1, 0, 1, 2])\n        ground_truths = np.array([0, 1, 0, 1, 1])\n        \n        # Test evaluate method\n        ars, ami, nmi = self.hcrc.evaluate(predictions, ground_truths)\n        \n        # Check if metrics are between 0 and 1\n        self.assertTrue(0 <= ars <= 1)\n        self.assertTrue(0 <= ami <= 1)\n        self.assertTrue(0 <= nmi <= 1)\n\n\nclass TestSinglePass(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures before each test method\"\"\"\n        self.threshold = 0.7\n        self.embeddings = np.random.rand(10, 64)  # 10 samples, 64 dimensions\n        self.flag = 0\n        self.pred_label = None\n        self.size = 5\n        self.para = 0.5\n        self.sim_init = 0.8\n        self.sim = True\n        \n        self.single_pass = SinglePass(\n            self.threshold,\n            self.embeddings,\n            self.flag,\n            self.pred_label,\n            self.size,\n            self.para,\n            self.sim_init,\n            self.sim\n        )\n\n    def test_get_center(self):\n        \"\"\"Test get_center method\"\"\"\n        # Create sample data\n        data = np.random.rand(10, 5)  # 10 samples, 5 features\n        labels = np.array([0, 0, 1, 1, 1, 2, 2, 2, 2, 2])  # 3 clusters\n        \n        # Get centers and indices\n        centers, indices_per_cluster = self.single_pass.get_center(labels, data)\n        \n        # Check centers shape\n        self.assertEqual(len(centers), len(np.unique(labels)))\n        self.assertEqual(len(centers[0]), data.shape[1])\n        \n        # Check indices\n        self.assertTrue(all(isinstance(idx_list, list) for idx_list in indices_per_cluster))\n\n\nclass TestTimeFeatures(unittest.TestCase):\n    def test_extract_time_feature(self):\n        \"\"\"Test time feature extraction\"\"\"\n        # Test with a sample datetime\n        time_str = \"2023-01-01T12:00:00\"\n        features = extract_time_feature(time_str)\n        \n        # Check output\n        self.assertEqual(len(features), 2)\n        self.assertIsInstance(features[0], float)\n        self.assertIsInstance(features[1], float)\n        self.assertTrue(0 <= features[1] <= 1)  # seconds should be normalized\n\n    def test_df_to_t_features(self):\n        \"\"\"Test dataframe to time features conversion\"\"\"\n        # Create sample dataframe\n        dates = [\"2023-01-01T12:00:00\", \"2023-01-02T13:30:00\"]\n        df = {'created_at': dates}\n        df = pd.DataFrame(df)\n        \n        # Convert to features\n        features = df_to_t_features(df)\n        \n        # Check output\n        self.assertEqual(features.shape, (2, 2))\n        self.assertTrue(np.all(features >= 0))\n\n\nclass TestClustering(unittest.TestCase):\n    def test_random_cluster(self):\n        \"\"\"Test random clustering\"\"\"\n        # Create sample embeddings\n        embeddings = np.random.rand(20, 64)\n        block_num = 0\n        pred_label = None\n        \n        # Perform clustering\n        cluster_result, threshold = random_cluster(embeddings, block_num, pred_label)\n        \n        # Check outputs\n        self.assertTrue(0.6 <= threshold <= 0.8)\n        self.assertEqual(len(cluster_result), len(embeddings))\n        self.assertTrue(all(isinstance(x, (int, np.integer)) for x in cluster_result))\n\n    def test_evaluate_fun(self):\n        \"\"\"Test evaluate function\"\"\"\n        # Create sample data\n        embeddings = np.random.rand(20, 64)\n        labels = np.array([0, 0, 1, 1, 1] * 4)\n        block_num = 0\n        pred_label = None\n        result_path = \"test_results.txt\"\n        task = \"random\"\n        \n        # Run evaluation\n        y_pred = evaluate_fun(embeddings, labels, block_num, pred_label, result_path, task)\n        \n        # Check output\n        self.assertEqual(len(y_pred), len(labels))\n        self.assertTrue(all(isinstance(x, (int, np.integer)) for x in y_pred))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_data_process.py", "content": "import unittest\nimport numpy as np\nimport pandas as pd\nimport networkx as nx\nimport torch\nfrom ..utils.data_process import (\n    construct_graph,\n    load_data,\n    documents_to_features,\n    extract_time_feature,\n    check_class_sizes\n)\n\nclass TestDataProcess(unittest.TestCase):\n    def setUp(self):\n        # Sample data for testing\n        self.sample_df = pd.DataFrame({\n            'tweet_id': [1, 2],\n            'user_mentions': [[123, 456], [789]],\n            'user_id': [111, 222],\n            'entities': [['entity1', 'entity2'], ['entity3']],\n            'sampled_words': [['word1', 'word2'], ['word3']]\n        })\n        \n    def test_construct_graph(self):\n        G = construct_graph(self.sample_df)\n        self.assertIsInstance(G, nx.Graph)\n        # Check if nodes were added correctly\n        self.assertTrue('t_1' in G.nodes())\n        self.assertTrue('u_111' in G.nodes())\n        self.assertTrue('entity1' in G.nodes())\n        self.assertTrue('w_word1' in G.nodes())\n\n    def test_check_class_sizes(self):\n        ground_truths = [0, 0, 1, 1, 1, 2, 2]\n        predictions = [0, 0, 0, 1, 1, 2, 2]\n        large_classes = check_class_sizes(ground_truths, predictions)\n        self.assertIsInstance(large_classes, list)\n\n    def test_extract_time_feature(self):\n        time_str = \"2023-01-01T12:00:00\"\n        features = extract_time_feature(time_str)\n        self.assertEqual(len(features), 2)\n        self.assertIsInstance(features[0], float)\n        self.assertIsInstance(features[1], float)\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_sbert.py", "content": "import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport pytest\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn import metrics\nfrom dataset.dataloader import DatasetLoader\nfrom detector.sbert import *\n\n# 创建一个示例数据集\n@pytest.fixture\ndef sample_dataset():\n    data = {\n        'event_id': [1, 2, 1, 2, 3, 1, 2, 3, 1, 2],\n        'filtered_words': [\n            ['hello', 'world'], ['goodbye', 'world'], ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'],\n            ['hello', 'world'], ['goodbye', 'world'], ['new', 'event'], ['hello', 'world'], ['goodbye', 'world']\n        ]\n    }\n    return pd.DataFrame(data)\n\n# 测试数据预处理\ndef test_preprocess(sample_dataset):\n    sbert = SBERT(sample_dataset)\n    df = sbert.preprocess()\n    assert 'processed_text' in df.columns\n    assert df['processed_text'].apply(lambda x: isinstance(x, str)).all()\n\n# 测试获取 SBERT 嵌入\ndef test_get_sbert_embeddings(sample_dataset):\n    sbert = SBERT(sample_dataset)\n    sbert.preprocess()\n    text = 'hello world'\n    embedding = sbert.get_sbert_embeddings(text)\n    assert isinstance(embedding, np.ndarray)\n    assert embedding.shape == (384,)  # 根据模型不同，维度可能不同\n\n# 测试检测\ndef test_detection(sample_dataset):\n    sbert = SBERT(sample_dataset)\n    sbert.preprocess()\n    ground_truths, predictions = sbert.detection()\n    assert len(ground_truths) == len(predictions)\n\n# 测试评估\ndef test_evaluate(sample_dataset):\n    sbert = SBERT(sample_dataset)\n    sbert.preprocess()\n    ground_truths, predictions = sbert.detection()\n    sbert.evaluate(ground_truths, predictions)\n\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "test_file", "path": "SocialED/tests/test_utility.py", "content": "import unittest\nimport torch\nimport numpy as np\nfrom ..utils.utility import (\n    tokenize_text,\n    validate_device,\n    check_parameter,\n    preprocess_sentence,\n    evaluate_metrics,\n    DS_Combin\n)\n\nclass TestUtility(unittest.TestCase):\n    def test_tokenize_text(self):\n        text = \"This is a test sentence.\"\n        tokens = tokenize_text(text)\n        self.assertIsInstance(tokens, list)\n        self.assertEqual(len(tokens), 5)\n        \n        # Test max_length parameter\n        long_text = \" \".join([\"word\"] * 1000)\n        tokens = tokenize_text(long_text, max_length=100)\n        self.assertEqual(len(tokens), 100)\n\n    def test_validate_device(self):\n        # Test CPU\n        device = validate_device(-1)\n        self.assertEqual(device, 'cpu')\n        \n        # Test invalid GPU ID\n        with self.assertRaises(ValueError):\n            validate_device(100)\n\n    def test_check_parameter(self):\n        # Test valid parameter\n        self.assertTrue(check_parameter(5, 0, 10, \"test_param\"))\n        \n        # Test invalid parameter\n        with self.assertRaises(ValueError):\n            check_parameter(-1, 0, 10, \"test_param\")\n\n    def test_preprocess_sentence(self):\n        text = \"@user Hello! This is a test :) http://test.com\"\n        processed = preprocess_sentence(text)\n        self.assertNotIn(\"@user\", processed)\n        self.assertNotIn(\"http://\", processed)\n        self.assertNotIn(\":)\", processed)\n\n    def test_evaluate_metrics(self):\n        labels_true = [0, 0, 1, 1]\n        labels_pred = [0, 0, 1, 1]\n        nmi, ami, ari = evaluate_metrics(labels_true, labels_pred)\n        self.assertEqual(nmi, 1.0)\n        self.assertEqual(ami, 1.0)\n        self.assertEqual(ari, 1.0)\n\n    def test_DS_Combin(self):\n        alpha = [\n            torch.tensor([[2., 1.], [1., 2.]]),\n            torch.tensor([[1.5, 1.5], [2., 1.]])\n        ]\n        classes = 2\n        alpha_combined, u_combined = DS_Combin(alpha, classes)\n        self.assertEqual(alpha_combined.shape, (2, 2))\n        self.assertEqual(u_combined.shape, (2, 1))\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_uclsed.py", "content": "import pytest\nimport torch\nimport numpy as np\nimport os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom detector.uclsed import *\n# 创建一个虚拟的DatasetLoader实例\nclass MockDatasetLoader:\n    def load_data(self):\n        return pd.DataFrame({\n            'tweet_id': [1, 2, 3],\n            'user_mentions': [['user1'], ['user2'], ['user3']],\n            'text': ['text1', 'text2', 'text3'],\n            'hashtags': [['hashtag1'], ['hashtag2'], ['hashtag3']],\n            'entities': [['entity1'], ['entity2'], ['entity3']],\n            'urls': [['url1'], ['url2'], ['url3']],\n            'filtered_words': [['word1'], ['word2'], ['word3']],\n            'created_at': ['2023-01-01', '2023-01-02', '2023-01-03'],\n            'event_id': [0, 1, 0]\n        })\n\n# 替换DatasetLoader为MockDatasetLoader\n@pytest.fixture\ndef mock_dataset():\n    return MockDatasetLoader().load_data()\n\n# 测试UCLSED类的初始化\ndef test_uclsed_init(mock_dataset):\n    args = args_define().args\n    uclsed = UCLSED(args, mock_dataset)\n    assert uclsed.save_path is None\n    assert uclsed.test_indices is None\n    assert uclsed.val_indices is None\n    assert uclsed.train_indices is None\n    assert uclsed.mask_path is None\n    assert uclsed.labels is None\n    assert uclsed.times is None\n    assert uclsed.g_dict is None\n    assert uclsed.views is None\n    assert uclsed.features is None\n\n# 测试Preprocessor类的初始化\ndef test_preprocessor_init(mock_dataset):\n    preprocessor = Preprocessor(mock_dataset)\n    assert preprocessor is not None\n\n# 测试Preprocessor类的load_data方法\ndef test_preprocessor_load_data(mock_dataset):\n    preprocessor = Preprocessor(mock_dataset)\n    event_df = preprocessor.load_data(mock_dataset)\n    assert isinstance(event_df, pd.DataFrame)\n    assert event_df.shape == (3, 9)\n\n# 测试Preprocessor类的get_nlp方法\ndef test_preprocessor_get_nlp():\n    preprocessor = Preprocessor(None)\n    nlp = preprocessor.get_nlp(\"English\")\n    assert nlp is not None\n    nlp = preprocessor.get_nlp(\"French\")\n    assert nlp is not None\n\n# 测试Preprocessor类的construct_graph方法\ndef test_preprocessor_construct_graph(mock_dataset, tmpdir):\n    args = args_define().args\n    args.file_path = str(tmpdir) + '/'\n    preprocessor = Preprocessor(mock_dataset)\n    preprocessor.construct_graph(mock_dataset)\n    assert os.path.exists(str(tmpdir) + '/features.npy')\n    assert os.path.exists(str(tmpdir) + '/time.npy')\n    assert os.path.exists(str(tmpdir) + '/label.npy')\n\n# 测试UCLSED类的fit方法\ndef test_uclsed_fit(mock_dataset, tmpdir):\n    args = args_define().args\n    args.file_path = str(tmpdir) + '/'\n    args.save_path = str(tmpdir) + '/'\n    uclsed = UCLSED(args, mock_dataset)\n    uclsed.fit()\n    assert os.path.exists(str(tmpdir) + '/train_indices.pt')\n    assert os.path.exists(str(tmpdir) + '/val_indices.pt')\n    assert os.path.exists(str(tmpdir) + '/test_indices.pt')\n\n# 测试UCLSED类的detection方法\ndef test_uclsed_detection(mock_dataset, tmpdir):\n    args = args_define().args\n    args.file_path = str(tmpdir) + '/'\n    args.save_path = str(tmpdir) + '/'\n    uclsed = UCLSED(args, mock_dataset)\n    uclsed.fit()\n    ground_truth, predictions = uclsed.detection()\n    assert isinstance(ground_truth, np.ndarray)\n    assert isinstance(predictions, np.ndarray)\n\n# 测试UCLSED类的evaluate方法\ndef test_uclsed_evaluate(mock_dataset, tmpdir):\n    args = args_define().args\n    args.file_path = str(tmpdir) + '/'\n    args.save_path = str(tmpdir) + '/'\n    uclsed = UCLSED(args, mock_dataset)\n    uclsed.fit()\n    ground_truth, predictions = uclsed.detection()\n    val_f1, val_acc = uclsed.evaluate(ground_truth, predictions)\n    assert isinstance(val_f1, float)\n    assert isinstance(val_acc, float)"}
{"type": "test_file", "path": "SocialED/tests/test_dataloader.py", "content": "import unittest\nimport os\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom unittest.mock import patch, MagicMock\nfrom ..dataset.dataloader import (\n    DatasetLoader,\n    Event2012,\n    Event2018,\n    Event2012_100,\n    Event2018_100,\n    Mix_Data,\n    CrisisLexT6\n)\n\nclass TestDatasetLoader(unittest.TestCase):\n    def setUp(self):\n        \"\"\"Set up test fixtures\"\"\"\n        self.test_dir = os.path.join(os.path.dirname(__file__), \"test_data\")\n        os.makedirs(self.test_dir, exist_ok=True)\n        \n        # Create sample data\n        self.sample_data = np.array([\n            [1, \"text1\", 0, [\"word1\"], [\"filtered1\"], [\"entity1\"], 101, \"2023-01-01\", [], [\"tag1\"], []],\n            [2, \"text2\", 1, [\"word2\"], [\"filtered2\"], [\"entity2\"], 102, \"2023-01-02\", [], [\"tag2\"], []]\n        ])\n\n    def tearDown(self):\n        \"\"\"Clean up test fixtures\"\"\"\n        if os.path.exists(self.test_dir):\n            shutil.rmtree(self.test_dir)\n\n    def test_dataset_loader_initialization(self):\n        \"\"\"Test DatasetLoader initialization\"\"\"\n        loader = DatasetLoader(dataset=\"test_dataset\", dir_path=self.test_dir)\n        self.assertEqual(loader.dataset, \"test_dataset\")\n        self.assertEqual(loader.dir_path, self.test_dir)\n        self.assertEqual(len(loader.required_columns), 11)  # Check number of required columns\n\n    @patch('subprocess.run')\n    def test_download_and_cleanup(self, mock_run):\n        \"\"\"Test download_and_cleanup method\"\"\"\n        loader = DatasetLoader(dataset=\"test_dataset\")\n        \n        # Mock successful download\n        mock_run.return_value = MagicMock(returncode=0)\n        \n        with patch('os.path.exists') as mock_exists:\n            mock_exists.return_value = True\n            with patch('shutil.copy2') as mock_copy:\n                success = loader.download_and_cleanup(\n                    \"mock_url\",\n                    \"test_dataset\",\n                    self.test_dir\n                )\n                self.assertTrue(success)\n\n    def test_event2012_initialization(self):\n        \"\"\"Test Event2012 dataset initialization\"\"\"\n        dataset = Event2012(dir_path=self.test_dir)\n        self.assertEqual(dataset.dataset, \"Event2012\")\n\n    def test_event2018_initialization(self):\n        \"\"\"Test Event2018 dataset initialization\"\"\"\n        dataset = Event2018(dir_path=self.test_dir)\n        self.assertEqual(dataset.dataset, \"Event2018\")\n\n    @patch('numpy.load')\n    def test_load_data(self, mock_load):\n        \"\"\"Test data loading functionality\"\"\"\n        mock_load.return_value = self.sample_data\n        \n        # Test with Event2012\n        dataset = Event2012(dir_path=self.test_dir)\n        with patch('os.path.exists') as mock_exists:\n            mock_exists.return_value = True\n            df = dataset.load_data()\n            self.assertIsInstance(df, pd.DataFrame)\n            self.assertEqual(len(df), 2)\n            self.assertEqual(len(df.columns), 11)\n\n    def test_event2012_100_initialization(self):\n        \"\"\"Test Event2012_100 dataset initialization\"\"\"\n        dataset = Event2012_100(dir_path=self.test_dir)\n        self.assertEqual(dataset.dataset, \"Event2012_100\")\n\n    def test_event2018_100_initialization(self):\n        \"\"\"Test Event2018_100 dataset initialization\"\"\"\n        dataset = Event2018_100(dir_path=self.test_dir)\n        self.assertEqual(dataset.dataset, \"Event2018_100\")\n\n    def test_mix_data_initialization(self):\n        \"\"\"Test Mix_Data dataset initialization\"\"\"\n        dataset = Mix_Data(dir_path=self.test_dir)\n        self.assertEqual(dataset.dataset, \"Mix_Data\")\n\n    def test_crisis_lext6_initialization(self):\n        \"\"\"Test CrisisLexT6 dataset initialization\"\"\"\n        dataset = CrisisLexT6(dir_path=self.test_dir)\n        self.assertEqual(dataset.dataset, \"CrisisLexT6\")\n\n    @patch('numpy.load')\n    def test_data_loading_error_handling(self, mock_load):\n        \"\"\"Test error handling during data loading\"\"\"\n        mock_load.side_effect = FileNotFoundError\n        \n        dataset = Event2012(dir_path=self.test_dir)\n        with self.assertRaises(FileNotFoundError):\n            dataset.load_data()\n\n    def test_required_columns_consistency(self):\n        \"\"\"Test that all dataset classes use the same required columns\"\"\"\n        base_loader = DatasetLoader()\n        event2012 = Event2012()\n        event2018 = Event2018()\n        event2012_100 = Event2012_100()\n        event2018_100 = Event2018_100()\n        mix_data = Mix_Data()\n        crisis_lext6 = CrisisLexT6()\n        \n        self.assertEqual(base_loader.required_columns, event2012.required_columns)\n        self.assertEqual(base_loader.required_columns, event2018.required_columns)\n        self.assertEqual(base_loader.required_columns, event2012_100.required_columns)\n        self.assertEqual(base_loader.required_columns, event2018_100.required_columns)\n        self.assertEqual(base_loader.required_columns, mix_data.required_columns)\n        self.assertEqual(base_loader.required_columns, crisis_lext6.required_columns)\n\n    @patch('os.makedirs')\n    def test_directory_creation(self, mock_makedirs):\n        \"\"\"Test directory creation functionality\"\"\"\n        DatasetLoader(dataset=\"test_dataset\", dir_path=self.test_dir)\n        mock_makedirs.assert_called()\n\n    @patch('numpy.load')\n    def test_data_format(self, mock_load):\n        \"\"\"Test data format validation\"\"\"\n        # Test with invalid data format\n        mock_load.return_value = np.array([[1, 2, 3]])  # Invalid data format\n        \n        dataset = Event2012(dir_path=self.test_dir)\n        with patch('os.path.exists') as mock_exists:\n            mock_exists.return_value = True\n            with self.assertRaises(Exception):\n                df = dataset.load_data()\n\nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "SocialED/tests/test_bert.py", "content": "import os\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport pytest\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn import metrics\nfrom dataset.dataloader import DatasetLoader\nfrom detector.bert import *\n\n\n# Test data preprocessing\ndef test_preprocess(sample_dataset):\n    \"\"\"Test preprocessing of text data.\n    \n    Parameters\n    ----------\n    sample_dataset : DatasetLoader\n        Sample dataset for testing preprocessing.\n        \n    Returns\n    -------\n    None\n        Asserts that processed text column exists and contains strings.\n    \"\"\"\n    bert = BERT(sample_dataset)\n    df = bert.preprocess()\n    assert 'processed_text' in df.columns\n    assert df['processed_text'].apply(lambda x: isinstance(x, str)).all()\n\n# Test getting BERT embeddings\ndef test_get_bert_embeddings(sample_dataset):\n    \"\"\"Test extraction of BERT embeddings from text.\n    \n    Parameters\n    ----------\n    sample_dataset : DatasetLoader\n        Sample dataset for testing embeddings.\n        \n    Returns\n    -------\n    None\n        Asserts that embeddings are numpy arrays of correct shape.\n    \"\"\"\n    bert = BERT(sample_dataset)\n    bert.preprocess()\n    text = 'hello world'\n    embedding = bert.get_bert_embeddings(text)\n    assert isinstance(embedding, np.ndarray)\n    assert embedding.shape == (768,)\n\n# Test event detection\ndef test_detection(sample_dataset):\n    \"\"\"Test event detection functionality.\n    \n    Parameters\n    ----------\n    sample_dataset : DatasetLoader\n        Sample dataset for testing detection.\n        \n    Returns\n    -------\n    None\n        Asserts that predictions match ground truth length.\n    \"\"\"\n    bert = BERT(sample_dataset)\n    bert.preprocess()\n    ground_truths, predictions = bert.detection()\n    assert len(ground_truths) == len(predictions)\n\n# Test evaluation metrics\ndef test_evaluate(sample_dataset):\n    \"\"\"Test evaluation of detection results.\n    \n    Parameters\n    ----------\n    sample_dataset : DatasetLoader\n        Sample dataset for testing evaluation.\n        \n    Returns\n    -------\n    None\n        Asserts that evaluation metrics are valid floats.\n    \"\"\"\n    bert = BERT(sample_dataset)\n    bert.preprocess()\n    ground_truths, predictions = bert.detection()\n    ari, ami, nmi = bert.evaluate(ground_truths, predictions)\n    assert isinstance(ari, float)\n    assert isinstance(ami, float)\n    assert isinstance(nmi, float)\n\nif __name__ == \"__main__\":\n    pytest.main()"}
{"type": "source_file", "path": "SocialED/dataset/__init__.py", "content": "from .dataloader import DatasetLoader\nfrom .dataloader import MAVEN\nfrom .dataloader import CrisisNLP\nfrom .dataloader import Event2012\nfrom .dataloader import Event2018\nfrom .dataloader import Arabic_Twitter\nfrom .dataloader import CrisisLexT26\nfrom .dataloader import CrisisMMD\nfrom .dataloader import HumAID\nfrom .dataloader import KBP\nfrom .dataloader import Arabic_7\nfrom .dataloader import Event2012_100\nfrom .dataloader import Event2018_100\nfrom .dataloader import Mix_Data\nfrom .dataloader import CrisisLexT6\nfrom .dataloader import CrisisLexT7\n\n__all__ = [\n    'DatasetLoader',\n    'MAVEN',\n    'CrisisNLP',\n    'Event2012',\n    'Event2018',\n    'Arabic_Twitter',\n    'CrisisLexT26',\n    'CrisisMMD',\n    'HumAID',\n    'KBP',\n    'Arabic_7',\n    'Event2012_100',\n    'Event2018_100',\n    'Mix_Data',\n    'CrisisLexT6',\n    'CrisisLexT7'\n    ]"}
{"type": "source_file", "path": "SocialED/__init__.py", "content": "# -*- coding: utf-8 -*-\n\n\nfrom . import dataset\nfrom . import metrics\nfrom . import utils\nfrom . import detector\nfrom . import tests\n\n__all__ = ['dataset', 'metrics', 'utils', 'detector', 'tests']\n"}
{"type": "source_file", "path": "SocialED/detector/__init__.py", "content": "# __init__.py\n\nfrom .lda import LDA\nfrom .bilstm import BiLSTM\nfrom .word2vec import WORD2VEC\nfrom .glove import GloVe\nfrom .wmd import WMD\nfrom .bert import BERT\nfrom .sbert import SBERT\nfrom .eventx import EventX\nfrom .clkd import CLKD\nfrom .kpgnn import KPGNN\nfrom .finevent import FinEvent\nfrom .qsgnn import QSGNN\nfrom .hcrc import HCRC\nfrom .etgnn import ETGNN\nfrom .uclsed import UCLSED\nfrom .rplmsed import RPLMSED\nfrom .hisevent import HISEvent\nfrom .adpsemevent import ADPSEMEvent\nfrom .hypersed import HyperSED\n# List of all classes to be exported\n__all__ = [\n    \"LDA\",\n    \"BiLSTM\", # test 20s\n    \"WORD2VEC\",\n    \"GloVe\",# test 1min\n    \"WMD\", # test 10s\n    \"BERT\",# test 10s\n    \"SBERT\",# test 10s\n    \"EventX\",# test 10s\n    \"CLKD\",#\n    \"KPGNN\",\n    \"FinEvent\",\n    \"QSGNN\",\n    \"HCRC\",\n    \"ETGNN\",\n    \"UCLSED\",\n    \"RPLMSED\",\n    \"HISEvent\", #test160s\n    \"ADPSEMEvent\", #test160s\n    \"HyperSED\"\n]"}
{"type": "source_file", "path": "SocialED/detector/Hypersed.py", "content": "import sys\nimport os\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nimport re \nimport torch\nimport random\nimport json\nimport numpy as np\nimport math\nimport pickle\nfrom datetime import datetime\nfrom collections import Counter, namedtuple\nfrom queue import Queue\nfrom typing import List\nfrom random import sample\nfrom scipy import sparse\nfrom sentence_transformers import SentenceTransformer\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport scipy.sparse as sp\nfrom scipy.sparse import csr_matrix\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom munkres import Munkres\n#from math import log2, sqrt\nfrom tqdm import tqdm\nfrom torch_geometric.utils import negative_sampling, dropout_edge, add_self_loops, from_networkx\nfrom torch_scatter import scatter_sum, scatter_softmax\nimport pandas as pd\nimport networkx as nx\nfrom networkx.drawing.nx_pydot import graphviz_layout\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom itertools import combinations\n\nimport geoopt\nimport geoopt.manifolds.lorentz.math as lmath\nfrom geoopt.manifolds import PoincareBall\nfrom geoopt.manifolds.stereographic.math import mobius_matvec, project, expmap0, mobius_add, logmap0, dist0, dist\nfrom geoopt.tensor import ManifoldParameter\nfrom geoopt.optim import RiemannianAdam\nfrom os.path import exists\nfrom networkx.algorithms import cuts\nfrom itertools import chain\n\n\n\n# Set random seeds\nseed = 3047\nrandom.seed(seed)\ntorch.manual_seed(seed)\nnp.random.seed(seed)\n\n\n\nclass HyperSED:\n    r\"\"\"The HyperSED model for social event detection that uses hyperbolic graph neural networks\n    and deep structure inference for event detection.\n\n    .. note::\n        This detector uses hyperbolic graph neural networks and deep structure inference to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    devices : bool, optional\n        Whether to use GPU if available. Default: ``True``.\n    algorithm : str, optional\n        Algorithm name. Default: ``\"HyperSED\"``.\n    data_path : str, optional\n        Path to preprocessed data. Default: ``'../model/model_saved/hypersed/datasets/data_preprocess'``.\n    save_model_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/hypersed/datasets/data_preprocess/saved_models'``.\n    n_cluster_trials : int, optional\n        Number of clustering trials. Default: ``5``.\n    encode : str, optional\n        Text encoding method. Default: ``'SBERT'``.\n    edge_type : str, optional\n        Type of edges to use. Default: ``'e_as'``.\n    gpu : int, optional\n        GPU device ID to use. Default: ``0``.\n    num_epochs : int, optional\n        Number of training epochs. Default: ``50``.\n    patience : int, optional\n        Early stopping patience. Default: ``50``.\n    hgae : bool, optional\n        Whether to use hyperbolic GAE. Default: ``True``.\n    dsi : bool, optional\n        Whether to use deep structure inference. Default: ``True``.\n    pre_anchor : bool, optional\n        Whether to use pre-anchoring. Default: ``True``.\n    anchor_rate : int, optional\n        Anchor sampling rate. Default: ``20``.\n    plot : bool, optional\n        Whether to plot results. Default: ``False``.\n    thres : float, optional\n        Threshold value. Default: ``0.5``.\n    diag : float, optional\n        Diagonal value. Default: ``0.5``.\n    num_layers_gae : int, optional\n        Number of GAE layers. Default: ``2``.\n    hidden_dim_gae : int, optional\n        Hidden dimension for GAE. Default: ``128``.\n    out_dim_gae : int, optional\n        Output dimension for GAE. Default: ``2``.\n    t : float, optional\n        Temperature parameter. Default: ``1.0``.\n    r : float, optional\n        Curvature radius. Default: ``2.0``.\n    lr_gae : float, optional\n        Learning rate for GAE. Default: ``1e-3``.\n    w_decay : float, optional\n        Weight decay. Default: ``0.3``.\n    dropout : float, optional\n        Dropout rate. Default: ``0.4``.\n    nonlin : optional\n        Non-linear activation function. Default: ``None``.\n    use_attn : bool, optional\n        Whether to use attention. Default: ``False``.\n    use_bias : bool, optional\n        Whether to use bias. Default: ``True``.\n    decay_rate : optional\n        Learning rate decay rate. Default: ``None``.\n    num_layers : int, optional\n        Number of layers. Default: ``3``.\n    hidden_dim : int, optional\n        Hidden dimension. Default: ``64``.\n    out_dim : int, optional\n        Output dimension. Default: ``2``.\n    height : int, optional\n        Tree height. Default: ``2``.\n    max_nums : list, optional\n        Maximum numbers per level. Default: ``[300]``.\n    temperature : float, optional\n        Temperature for loss. Default: ``0.05``.\n    lr_pre : float, optional\n        Learning rate for pre-training. Default: ``1e-3``.\n    lr : float, optional\n        Learning rate. Default: ``1e-3``.\n    \"\"\"\n    \n    def __init__(self,\n                 dataset,\n                 devices=True,\n                 algorithm=\"HyperSED\",\n                 data_path='../model/model_saved/hypersed/datasets/data_preprocess',\n                 save_model_path='../model/model_saved/hypersed/datasets/data_preprocess/saved_models',\n                 n_cluster_trials=5,\n\n                 encode='SBERT',\n                 edge_type='e_as',\n                 gpu=0,\n                 num_epochs=50,\n                 patience=50,\n                 hgae=True,\n                 dsi=True,\n                 pre_anchor=True,\n                 anchor_rate=20,\n                 plot=False,\n                 thres=0.5,\n                 diag=0.5,\n                 # HGAE params \n                 num_layers_gae=2,\n                 hidden_dim_gae=128,\n                 out_dim_gae=2,\n                 t=1.,\n                 r=2.,\n                 lr_gae=1e-3,\n                 w_decay=0.3,\n                 dropout=0.4,\n                 nonlin=None,\n                 use_attn=False,\n                 use_bias=True,\n                 # DSI params\n                 decay_rate=None,\n                 num_layers=3,\n                 hidden_dim=64,\n                 out_dim=2,\n                 height=2,\n                 max_nums=[300],\n                 temperature=0.05,\n                 lr_pre=1e-3,\n                 lr=1e-3):\n\n        self.dataset = dataset \n        self.dataset_name = dataset.get_dataset_name()\n        self.devices = devices\n        self.algorithm = algorithm\n        self.data_path = data_path\n        self.save_model_path = save_model_path\n        self.n_cluster_trials = n_cluster_trials\n\n\n        self.encode = encode\n        self.edge_type = edge_type\n        self.gpu = gpu\n        self.num_epochs = num_epochs\n        self.patience = patience\n        self.hgae = hgae\n        self.dsi = dsi\n        self.pre_anchor = pre_anchor\n        self.anchor_rate = anchor_rate\n        self.plot = plot\n        self.thres = thres\n        self.diag = diag\n\n        # HGAE params\n        self.num_layers_gae = num_layers_gae\n        self.hidden_dim_gae = hidden_dim_gae\n        self.out_dim_gae = out_dim_gae\n        self.t = t\n        self.r = r\n        self.lr_gae = lr_gae\n        self.w_decay = w_decay\n        self.dropout = dropout\n        self.nonlin = nonlin\n        self.use_attn = use_attn\n        self.use_bias = use_bias\n\n        # DSI params\n        self.decay_rate = decay_rate\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n        self.height = height\n        self.max_nums = max_nums\n        self.temperature = temperature\n        self.lr_pre = lr_pre\n        self.lr = lr\n\n        # Set algorithm name\n        self.algorithm_name = '_'.join([name for name, value in vars(self).items() \n                                    if value and name in ['hgae', 'pre_anchor', 'dsi']])\n\n\n    def preprocess(self):\n        preprocessor = Preprocessor(self.dataset)\n        preprocessor.preprocess()\n\n\n    def fit(self):\n        # 检查 GPU 可用性并设置适当的设备\n        if self.devices and torch.cuda.is_available():\n            if self.gpu >= torch.cuda.device_count():\n                print(f\"Warning: GPU {self.gpu} not available, falling back to CPU\")\n                self.devices = False\n                self.gpu = -1\n            else:\n                torch.cuda.set_device(self.gpu)\n                print(f\"Using GPU: {torch.cuda.get_device_name(self.gpu)}\")\n        else:\n            self.devices = False\n            self.gpu = -1\n            print(\"Using CPU\")\n\n        self.trainer = Trainer(self)\n        self.trainer.fit()\n\n        \n    def detection(self):\n        ground_truths, predictions = self.trainer.detection()\n        torch.cuda.empty_cache()\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n\nclass Trainer(nn.Module):\n    def __init__(self, args, block=None):\n        super(Trainer, self).__init__()\n\n        self.args = args\n        self.block = block\n        self.data = TwitterDataSet(args, args.dataset_name, block)\n        self.device = f\"cuda:{args.gpu}\" if torch.cuda.is_available() and args.devices else \"cpu\"\n        self.save_model_path = f\"{args.save_model_path}/{args.algorithm}/{args.dataset_name}/best_model\"\n        self.manifold = Lorentz()\n\n        self.in_dim = self.data.data.feature.shape[1]\n        self.num_nodes = self.data.data.num_nodes\n        \n        if self.args.hgae:\n            # Part 1  Hyper Graph Encoder\n            self.gae = HyperGraphAutoEncoder(self.args, self.device, self.manifold, args.num_layers_gae, \n                                            self.in_dim, args.hidden_dim_gae, args.out_dim_gae, args.dropout, \n                                            args.nonlin, args.use_attn, args.use_bias).to(self.device)\n            if self.args.dsi:\n                self.in_dim = args.out_dim_gae\n\n        if self.args.dsi:\n            # Part 2  Hyper Structure Entropy\n            self.hyperSE = HyperSE(args=self.args, manifold=self.manifold, n_layers=args.num_layers, device=self.device, \n                                   in_features=self.in_dim, hidden_dim_enc=args.hidden_dim, hidden_features=args.hidden_dim, \n                                   num_nodes=self.num_nodes, height=args.height, temperature=args.temperature, embed_dim=args.out_dim,\n                                   dropout=args.dropout, nonlin=args.nonlin, decay_rate=args.decay_rate, \n                                   max_nums=args.max_nums, use_att=args.use_attn, use_bias=args.use_bias).to(self.device)\n\n        self.patience = self.args.patience\n\n\n    def forward(self, data, mode=\"val\"):\n        # for testing, with no loss\n        with torch.no_grad():\n            if self.args.hgae:\n                loss, feature = self.getGAEPre(data, mode)\n            else:\n                feature = data.anchor_feature if self.args.pre_anchor else data.feature\n            adj = data.anchor_edge_index_types.adj if self.args.pre_anchor else data.edge_index_types.adj\n            if self.args.dsi:\n                feature = self.hyperSE(feature, adj)\n\n        return feature.detach().cpu()\n\n\n    def fit(self):\n        \"\"\"Train the model\"\"\"\n        self.train()\n        time1 = datetime.now().strftime(\"%H:%M:%S\")\n        epochs = self.args.num_epochs\n        data = self.data.data\n        self.best_cluster = {'block_id': self.block, 'nmi': 0, 'ami': 0, 'ari': 0}\n\n        # training the new block\n        for epoch in tqdm(range(epochs), desc=\"Training Epochs\"):\n            if self.args.hgae:\n                self.gae.optimizer.zero_grad()\n            if self.args.dsi:\n                self.hyperSE.optimizer_pre.zero_grad()\n                if epoch > 0:\n                    self.hyperSE.optimizer.zero_grad()\n\n            # Part 1  Hyper Graph AutoEncoder\n            if self.args.hgae:\n                loss, feature = self.getGAEPre(data)\n            else:\n                feature = None\n\n            # Part 2  Hyper Structural Entropy\n            if self.args.dsi:\n                input_data = self.getOtherByedge(data, feature, epoch)\n                hse_loss = self.hyperSE.loss(input_data)\n                if self.args.hgae:\n                    loss = loss + hse_loss\n                else:\n                    loss = hse_loss\n            \n            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n            loss.backward()\n            if self.args.hgae:\n                self.gae.optimizer.step()\n            if self.args.dsi:\n                self.hyperSE.optimizer_pre.step()\n                if epoch > 0:\n                    self.hyperSE.optimizer.step()\n\n            # Evaluate current model\n            self.eval()\n            with torch.no_grad():\n                embeddings = self.forward(data, \"val\")\n\n                if self.args.dsi:\n                    manifold = self.hyperSE.manifold.cpu()\n                    tree = construct_tree(torch.tensor([i for i in range(embeddings.shape[0])]).long(),\n                                        manifold,\n                                        self.hyperSE.embeddings, self.hyperSE.ass_mat, height=self.args.height,\n                                        num_nodes=embeddings.shape[0])\n                    tree_graph = to_networkx_tree(tree, manifold, height=self.args.height)\n                    predicts = decoding_cluster_from_tree(manifold, tree_graph,\n                                                        data.num_classes, embeddings.shape[0],\n                                                        height=self.args.height)\n                else:\n                    fea = self.manifold.to_poincare(embeddings)\n                    Z_np = fea.detach().cpu().numpy()\n                    M = data.num_classes\n\n                    kmeans = PoincareKMeans(n_clusters=M, n_init=1, max_iter=200, tol=1e-10, verbose=True)\n                    kmeans.fit(Z_np)\n                    predicts = kmeans.labels_\n\n                trues = data.labels\n                nmis, amis, aris = [], [], []\n                if self.args.pre_anchor:\n                    predicts = getNewPredict(predicts, data.anchor_ass)\n                for step in range(self.args.n_cluster_trials):\n                    metrics = cluster_metrics(trues, predicts.astype(int))\n                    nmi, ami, ari = metrics.evaluateFromLabel()\n                    nmis.append(nmi)\n                    amis.append(ami)\n                    aris.append(ari)\n\n                nmi, ami, ari = np.mean(nmis), np.mean(amis), np.mean(aris)\n                \n                if nmi >= self.best_cluster['nmi'] and ami >= self.best_cluster['ami'] and ari >= self.best_cluster['ari']:\n                    self.patience = self.args.patience\n\n                    model_path = f'{self.save_model_path}/{self.block}'\n                    if not os.path.exists(model_path):\n                        os.makedirs(model_path)\n                    torch.save(self.state_dict(), f\"{model_path}/model.pt\")\n                    \n                    self.best_cluster['nmi'] = nmi\n                    self.best_cluster['ami'] = ami\n                    self.best_cluster['ari'] = ari\n                    self.best_cluster['predicts'] = predicts\n                    self.best_cluster['trues'] = trues\n\n                elif nmi < self.best_cluster['nmi'] and ami < self.best_cluster['ami'] and ari < self.best_cluster['ari']:\n                    self.patience -= 1\n\n                else:\n                    if nmi > self.best_cluster['nmi']:\n                        print(f'nmi: {nmi}, ami: {ami}, ari: {ari}')\n                    elif ami > self.best_cluster['ami']:\n                        print(f'ami: {ami}, nmi: {nmi}, ari: {ari}')\n                    elif ari > self.best_cluster['ari']:\n                        print(f'ari: {ari}, nmi: {nmi}, ami: {ami}')\n                    else:\n                        print()\n\n            print(f\"VALID {self.block} : NUM_MSG: {data.num_nodes}, NMI: {self.best_cluster['nmi']}, AMI: {self.best_cluster['ami']}, ARI: {self.best_cluster['ari']}\")\n\n            torch.cuda.empty_cache() \n\n            if self.patience < 0:\n                print(\"Run out of patience\")\n                break\n\n            self.train()\n\n        time2 = datetime.now().strftime(\"%H:%M:%S\")\n        self.time = {'t3': time1, 't4': time2}\n\n    def detection(self):\n        \"\"\"Detect communities and return results\"\"\"\n        return self.best_cluster['trues'], self.best_cluster['predicts']\n\n\n    def getGAEPre(self, data, mode=\"train\"):\n        # get hgae latent representation\n        if self.args.pre_anchor:\n            features = data.anchor_feature.clone()\n            adj_ori = data.anchor_edge_index_types.adj.clone()\n        else:\n            features = data.feature.clone()\n            adj_ori = data.edge_index_types.adj.clone()\n        \n        if mode == \"train\":\n            loss, adj, feature = self.gae.loss(features, adj_ori)\n        else:\n            adj, feature = self.gae.forward(features, adj_ori)\n            loss = None\n        \n        return loss, feature\n    \n\n    def getOtherByedge(self, data, gae_feature, epoch):\n        if self.args.hgae:\n            feature = gae_feature\n            if self.args.pre_anchor:\n                edge_index_types = data.anchor_edge_index_types\n            else:\n                edge_index_types = data.edge_index_types\n        else:\n            if self.args.pre_anchor:\n                feature = data.anchor_feature\n                edge_index_types = data.anchor_edge_index_types\n            else:\n                feature = data.feature\n                edge_index_types = data.edge_index_types\n        return DSIData(feature=feature, adj=edge_index_types.adj, weight=edge_index_types.weight, degrees=edge_index_types.degrees, \n                        neg_edge_index=edge_index_types.neg_edge_index, edge_index=edge_index_types.edge_index, device=self.device, \n                        pretrain=True if epoch == 0 else False)\n\n    # 离线模式学习\n    def offline_train(self):\n        pass\n\n    # 开始测试\n    def test(self, data, block_i):\n        model_path = f'{self.save_model_path}/{block_i}'\n        self.load_state_dict(torch.load(model_path + \"/model.pt\"))\n        self.eval()\n        with torch.no_grad():\n            embeddings = self.forward(data, \"test\")\n            manifold = self.hyperSE.manifold.cpu()\n\n            if self.args.dsi:\n                tree = construct_tree(torch.tensor([i for i in range(embeddings.shape[0])]).long(),\n                                    manifold,\n                                    self.hyperSE.embeddings, self.hyperSE.ass_mat, height=self.args.height,\n                                    num_nodes=embeddings.shape[0])\n                tree_graph = to_networkx_tree(tree, manifold, height=self.args.height)\n                if self.args.plot:\n                    labels = data.anchor_labels if self.args.pre_anchor else data.labels\n                    _, color_dict = plot_leaves(tree_graph, manifold, embeddings, labels, height=self.args.height,\n                                                save_path='/home/yuxiaoyan/paper/HyperSED/' + f\"{self.args.height}_{1}_true.pdf\")\n\n                predicts = decoding_cluster_from_tree(manifold, tree_graph,\n                                                    data.num_classes, embeddings.shape[0],\n                                                    height=self.args.height)\n            \n            else:\n                fea = self.manifold.to_poincare(embeddings)\n                Z_np = fea.detach().cpu().numpy()\n                M = data.num_classes\n\n                kmeans = PoincareKMeans(n_clusters=M, n_init=1, max_iter=200, tol=1e-10, verbose=True)\n                kmeans.fit(Z_np)\n                predicts = kmeans.labels_\n            \n\n            trues = data.labels\n\n            nmis, amis, aris = [], [], []\n            if self.args.pre_anchor:\n                predicts = getNewPredict(predicts, data.anchor_ass)\n            for step in range(self.args.n_cluster_trials):\n                metrics = cluster_metrics(trues, predicts)\n                nmi, ami, ari = metrics.evaluateFromLabel()\n                nmis.append(nmi)\n                amis.append(ami)\n                aris.append(ari)\n\n            nmi, ami, ari = np.mean(nmis), np.mean(amis), np.mean(aris)\n            metrics.get_new_predicts()\n            new_pred = metrics.new_predicts\n            plot_leaves(tree_graph, manifold, embeddings, new_pred, height=self.args.height,\n                        save_path='../model/model_saved/hypersed/' + f\"{self.args.height}_{1}_pred.pdf\",\n                        colors_dict=color_dict)\n\n        return nmi, ami, ari\n\n\n    # 获取训练集以及验证集数据\n    def getNTBlockData(self, datas, block_id):\n        # views = list(Views._fields)\n        data = datas[block_id]\n        features, labels, num_classes, num_features, num_nodes = data.feature, data.labels, data.num_classes, data.num_features, data.num_nodes\n\n        datasets = {}\n        # for view in views:\n        edge_index_type = data.edge_index_types\n        # 划分训练集、测试集以及验证集\n        pos_edges, neg_edges = mask_edges(edge_index_type.edge_index, edge_index_type.neg_edge_index, 0.1, 0.2 if block_id == 0 else 0)\n\n        train_adj, train_degrees, train_weight = getOtherByedge(pos_edges[0], num_nodes)\n        val_adj, val_train_degrees, val_train_weight = getOtherByedge(pos_edges[1], num_nodes)\n        train_edge = EdgeIndexTypes(adj=train_adj, degrees=train_degrees, weight=train_weight, edge_index=pos_edges[0], neg_edge_index=neg_edges[0])\n        val_edge = EdgeIndexTypes(adj=val_adj, degrees=val_train_degrees, weight=val_train_weight, edge_index=pos_edges[1], neg_edge_index=neg_edges[1])\n        if block_id == 0:\n            test_adj, test_train_degrees, test_train_weight = getOtherByedge(pos_edges[1], num_nodes)\n            test_edge = EdgeIndexTypes(adj=test_adj, degrees=test_train_degrees, weight=test_train_weight, edge_index=pos_edges[2], neg_edge_index=neg_edges[2])\n            datasets = {\"train\": train_edge, \"val\":  val_edge, \"test\":  test_edge}\n        else:\n            test_edge_index_type = EdgeIndexTypes(adj=edge_index_type.adj, degrees=edge_index_type.degrees, weight=edge_index_type.weight, edge_index=edge_index_type.edge_index, neg_edge_index=edge_index_type.neg_edge_index)\n            datasets = {\"train\": train_edge, \"val\":  val_edge, \"test\": test_edge_index_type}\n\n        dealed_data = DataPartition(features=features, labels=labels, num_nodes=num_nodes, num_classes=num_classes, num_features=num_features, views=datasets)\n        return dealed_data\n\n\n    def getOtherByedge(self, data, gae_feature, epoch):\n        if self.args.hgae:\n            feature = gae_feature\n            if self.args.pre_anchor:\n                edge_index_types = data.anchor_edge_index_types\n            else:\n                edge_index_types = data.edge_index_types\n        else:\n            if self.args.pre_anchor:\n                feature = data.anchor_feature\n                edge_index_types = data.anchor_edge_index_types\n            else:\n                feature = data.feature\n                edge_index_types = data.edge_index_types\n        return DSIData(feature=feature, adj=edge_index_types.adj, weight=edge_index_types.weight, degrees=edge_index_types.degrees, \n                        neg_edge_index=edge_index_types.neg_edge_index, edge_index=edge_index_types.edge_index, device=self.device, \n                        pretrain=True if epoch == 0 else False)\n    \n\nclass Preprocessor:\n    def __init__(self, dataset):\n        \"\"\"Initialize preprocessor\n        Args:\n            dataset: Dataset calss (e.g. Event2012, Event2018, etc.)\n            language: Language of the dataset (default 'English')\n        \"\"\"\n        self.dataset = dataset.load_data()\n        self.language = dataset.get_dataset_language()\n        self.dataset_name = dataset.get_dataset_name()\n        self.columns = ['tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n                       'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions']\n\n    def get_set_test_df(self, df):\n        \"\"\"Get closed set test dataframe\"\"\"\n        save_path = f'../model/model_saved/hypersed/{self.dataset_name}/data/'\n        if not exists(save_path):\n            os.makedirs(save_path)\n        \n        test_set_df_np_path = save_path + 'test_set.npy'\n        test_set_label_path = save_path + 'label.npy'\n\n        if not exists(test_set_df_np_path):\n            # Use 2012-style processing for all datasets\n            test_mask = torch.load(f'../model/model_saved/hypersed/{self.dataset_name}/masks/test_mask.pt').cpu().detach().numpy()\n            test_mask = list(np.where(test_mask==True)[0])\n            test_df = df.iloc[test_mask]\n            shuffled_index = np.random.permutation(test_df.index)\n            shuffled_df = test_df.reindex(shuffled_index)\n            shuffled_df.reset_index(drop=True, inplace=True)\n            \n            test_df_np = test_df.to_numpy()\n            labels = [int(label) for label in shuffled_df['event_id'].values]\n            \n            np.save(test_set_label_path, np.asarray(labels))\n            np.save(test_set_df_np_path, test_df_np)\n        \n\n    def get_set_messages_embeddings(self):\n        \"\"\"Get SBERT embeddings for closed set messages\"\"\"\n        save_path = f'../model/model_saved/hypersed/{self.dataset_name}/data/'\n        \n        SBERT_embedding_path = f'{save_path}/SBERT_embeddings.pkl'\n        if not exists(SBERT_embedding_path):\n            test_set_df_np_path = save_path + 'test_set.npy'\n            test_df_np = np.load(test_set_df_np_path, allow_pickle=True)\n            \n            test_df = pd.DataFrame(data=test_df_np, columns=self.columns)\n            print(\"Dataframe loaded.\")\n\n            processed_text = [preprocess_sentence(s) for s in test_df['text'].values]\n            print('message text contents preprocessed.')\n\n            embeddings = SBERT_embed(processed_text, language=self.language)\n\n            with open(SBERT_embedding_path, 'wb') as fp:\n                pickle.dump(embeddings, fp)\n            print('SBERT embeddings stored.')\n        return\n\n\n    def preprocess(self):\n        \"\"\"Main preprocessing function\"\"\"\n        # Load raw data using 2012-style processing\n        df_np = self.dataset\n        \n        print(\"Loaded data.\")\n        df = pd.DataFrame(data=df_np, columns=self.columns)\n        print(\"Data converted to dataframe.\")\n\n        save_dir = os.path.join(f'../model/model_saved/hypersed/{self.dataset_name}', 'masks')\n        os.makedirs(save_dir, exist_ok=True)\n        \n        # Split and save masks\n        self.split_and_save_masks(df, save_dir)\n        print(\"Generated and saved train/val/test masks.\")\n\n        self.get_set_test_df(df)\n        self.get_set_messages_embeddings()\n        self.construct_graph_all(self.dataset_name)\n\n        return\n\n    def split_and_save_masks(self, df, save_dir, train_size=0.7, val_size=0.1, test_size=0.2, random_seed=42):\n        \"\"\"\n        Splits the DataFrame into training, validation, and test sets, and saves the indices (masks) as .pt files.\n        \n        Parameters:\n        - df (pd.DataFrame): The DataFrame to be split\n        - save_dir (str): Directory to save the masks\n        - train_size (float): Proportion for training (default 0.7)\n        - val_size (float): Proportion for validation (default 0.1) \n        - test_size (float): Proportion for testing (default 0.2)\n        - random_seed (int): Random seed for reproducibility\n        \"\"\"\n        if train_size + val_size + test_size != 1.0:\n            raise ValueError(\"train_size + val_size + test_size must equal 1.0\")\n\n        if df.empty:\n            raise ValueError(\"The input DataFrame is empty.\")\n\n        print(f\"Total samples in DataFrame: {len(df)}\")\n        \n        # Set random seed\n        torch.manual_seed(random_seed)\n\n        # Split into train and temp\n        train_data, temp_data = train_test_split(df, train_size=train_size, random_state=random_seed)\n        \n        # Split temp into val and test\n        val_data, test_data = train_test_split(temp_data, \n                                             train_size=val_size/(val_size + test_size),\n                                             random_state=random_seed)\n\n        # Create boolean masks\n        full_train_mask = torch.zeros(len(df), dtype=torch.bool)\n        full_val_mask = torch.zeros(len(df), dtype=torch.bool)\n        full_test_mask = torch.zeros(len(df), dtype=torch.bool)\n\n        # Set indices\n        full_train_mask[train_data.index] = True\n        full_val_mask[val_data.index] = True  \n        full_test_mask[test_data.index] = True\n\n        print(f\"Training samples: {full_train_mask.sum()}\")\n        print(f\"Validation samples: {full_val_mask.sum()}\")\n        print(f\"Test samples: {full_test_mask.sum()}\")\n\n        # Save masks\n        mask_paths = {\n            'train_mask.pt': full_train_mask,\n            'val_mask.pt': full_val_mask, \n            'test_mask.pt': full_test_mask\n        }\n\n        for filename, mask in mask_paths.items():\n            mask_path = os.path.join(save_dir, filename)\n            if not os.path.exists(mask_path):\n                try:\n                    torch.save(mask, mask_path)\n                    print(f\"Saved {filename}\")\n                except Exception as e:\n                    print(f\"Error saving {filename}: {e}\")\n            else:\n                print(f\"{filename} already exists\")\n\n            # Verify saved file\n            if os.path.exists(mask_path):\n                saved_mask = torch.load(mask_path)\n                if saved_mask.numel() == 0:\n                    print(f\"Warning: {filename} is empty\")\n                else:\n                    print(f\"Verified {filename} with {saved_mask.numel()} elements\")\n\n        print(\"Mask generation completed\")\n\n    # Construct message graph\n    def get_best_threshold(self, path):\n        best_threshold_path = path + '/best_threshold.pkl'\n        if not os.path.exists(best_threshold_path):\n            embeddings_path = path + '/SBERT_embeddings.pkl'\n            with open(embeddings_path, 'rb') as f:\n                embeddings = pickle.load(f)\n            best_threshold = search_threshold(embeddings)\n            best_threshold = {'best_thres': best_threshold}\n            with open(best_threshold_path, 'wb') as fp:\n                pickle.dump(best_threshold, fp)\n            print('best threshold is stored.')\n\n        with open(best_threshold_path, 'rb') as f:\n            best_threshold = pickle.load(f)\n        print('best threshold loaded.')\n        return best_threshold\n\n\n    def construct_graph(self, df, embeddings, save_path, e_a=True, e_s=True):\n        \"\"\"Construct graph for given dataframe and embeddings\"\"\"\n        def safe_list(x):\n            \"\"\"Convert input to list safely\"\"\"\n            if isinstance(x, (list, tuple, set)):\n                return x\n            elif x is None:\n                return []\n            else:\n                return [x]\n\n        def safe_str_lower(x):\n            \"\"\"Convert input to lowercase string safely\"\"\"\n            try:\n                return str(x).lower()\n            except:\n                return str(x)\n\n        # Use unified columns\n        all_node_features = [\n            [str(u)] + \n            [str(each) for each in safe_list(um)] + \n            [safe_str_lower(h) for h in safe_list(hs)] + \n            (e if isinstance(e, list) else [str(e)])\n            for u, um, hs, e in zip(df['user_id'], df['user_mentions'], \n                                   df['hashtags'], df['entities'])\n        ]\n        \n        best_threshold = self.get_best_threshold(save_path)\n        best_threshold = best_threshold['best_thres']\n\n        global_edges = get_global_edges(all_node_features, embeddings, best_threshold, e_a=e_a, e_s=e_s)\n\n        corr_matrix = np.corrcoef(embeddings)\n        np.fill_diagonal(corr_matrix, 0)\n        weighted_global_edges = [(edge[0], edge[1], corr_matrix[edge[0], edge[1]]) \n                                for edge in global_edges if corr_matrix[edge[0], edge[1]] > 0]\n        \n        edge_types = 'e_as' if e_s and e_a else 'e_s' if e_s else 'e_a' if e_a else None\n\n        # Create adjacency matrix\n        num_nodes = embeddings.shape[0]\n        adj_matrix = np.zeros((num_nodes, num_nodes))\n        for node1, node2, weight in weighted_global_edges:\n            adj_matrix[node1, node2] = weight\n            adj_matrix[node2, node1] = weight\n            \n        sparse_adj_matrix = sparse.csr_matrix(adj_matrix)\n        return sparse_adj_matrix, edge_types\n\n    def construct_graph_all(self, dataset_name, e_a=True, e_s=True):\n        save_path = f'../model/model_saved/hypersed/{self.dataset_name}/data/'\n\n        # Unified columns\n        columns = ['tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n                'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions']\n\n        print('\\n\\n====================================================')\n        time1 = datetime.now().strftime(\"%H:%M:%S\")\n        print('Graph construct starting time:', time1)\n\n        # Load embeddings and data\n        with open(f'{save_path}/SBERT_embeddings.pkl', 'rb') as f:\n            embeddings = pickle.load(f)\n        \n        df_np = np.load(f'{save_path}/test_set.npy', allow_pickle=True)\n        df = pd.DataFrame(data=df_np, columns=columns)\n\n        # Construct graph\n        sparse_adj_matrix, edge_types = self.construct_graph(df, embeddings, save_path, e_a, e_s)\n        sparse.save_npz(f'{save_path}/message_graph_{edge_types}.npz', sparse_adj_matrix)\n\n        time2 = datetime.now().strftime(\"%H:%M:%S\")\n        print('Graph construct ending time:', time2)\n\n\n\ndef mobius_add(x, y):\n    \"\"\"Mobius addition in numpy.\"\"\"\n    xy = np.sum(x * y, 1, keepdims=True)\n    x2 = np.sum(x * x, 1, keepdims=True)\n    y2 = np.sum(y * y, 1, keepdims=True)\n    num = (1 + 2 * xy + y2) * x + (1 - x2) * y\n    den = 1 + 2 * xy + x2 * y2\n    return num / den\n\n\ndef mobius_mul(x, t):\n    \"\"\"Mobius multiplication in numpy.\"\"\"\n    normx = np.sqrt(np.sum(x * x, 1, keepdims=True))\n    return np.tanh(t * np.arctanh(normx)) * x / normx\n\n\ndef geodesic_fn(x, y, nb_points=100):\n    \"\"\"Get coordinates of points on the geodesic between x and y.\"\"\"\n    t = np.linspace(0, 1, nb_points)\n    x_rep = np.repeat(x.reshape((1, -1)), len(t), 0)\n    y_rep = np.repeat(y.reshape((1, -1)), len(t), 0)\n    t1 = mobius_add(-x_rep, y_rep)\n    t2 = mobius_mul(t1, t.reshape((-1, 1)))\n    return mobius_add(x_rep, t2)\n\n\ndef plot_geodesic(x, y, ax):\n    \"\"\"Plots geodesic between x and y.\"\"\"\n    points = geodesic_fn(x, y)\n    ax.plot(points[:, 0], points[:, 1], color='black', linewidth=0.3, alpha=1.)\n\n\ndef plot_leaves(tree, manifold, embeddings, labels, height, save_path=None, colors_dict=None):\n    fig = plt.figure(figsize=(15, 15))\n    ax = fig.add_subplot(111)\n    circle = plt.Circle((0, 0), 1.0, color='y', alpha=0.1)\n    ax.add_artist(circle)\n    for k in range(1, height + 1):\n        circle_k = plt.Circle((0, 0), k / (height + 1), color='b', alpha=0.05)\n        ax.add_artist(circle_k)\n    n = embeddings.shape[0]\n    colors_dict = get_colors(labels, color_seed=1234) if colors_dict is None else colors_dict\n    colors = [colors_dict[k] for k in labels]\n    embeddings = manifold.to_poincare(embeddings).numpy()\n    scatter = ax.scatter(embeddings[:n, 0], embeddings[:n, 1], c=colors, s=80, alpha=1.0)\n    # legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\n    # ax.add_artist(legend)\n    # ax.scatter(np.array([0]), np.array([0]), c='black')\n    for u, v in tree.edges():\n        x = manifold.to_poincare(tree.nodes[u]['coords']).numpy()\n        y = manifold.to_poincare(tree.nodes[v]['coords']).numpy()\n        if tree.nodes[u]['is_leaf'] is False:\n            c = 'black' if tree.nodes[u]['height'] == 0 else 'red'\n            m = '*' if tree.nodes[u]['height'] == 0 else 's'\n            ax.scatter(x[0], x[1], c=c, s=30, marker=m)\n        if tree.nodes[v]['is_leaf'] is False:\n            c = 'black' if tree.nodes[v]['height'] == 0 else 'red'\n            m = '*' if tree.nodes[u]['height'] == 0 else 's'\n            ax.scatter(y[0], y[1], c=c, s=30, marker=m)\n        plot_geodesic(y, x, ax)\n    ax.set_xlim(-1.05, 1.05)\n    ax.set_ylim(-1.05, 1.05)\n    ax.axis(\"off\")\n    plt.savefig(save_path, transparent=True, bbox_inches='tight', dpi=500)\n    plt.show()\n    return ax, colors_dict\n\n\ndef get_colors(y, color_seed=1234):\n    \"\"\"random color assignment for label classes.\"\"\"\n    np.random.seed(color_seed)\n    colors = {}\n    for k in np.unique(y):\n        r = np.random.random()\n        b = np.random.random()\n        g = np.random.random()\n        colors[k] = (r, g, b)\n    return colors\n\n\ndef plot_nx_graph(G: nx.Graph, root, save_path=None):\n    fig = plt.figure(figsize=(15, 15))\n    ax = fig.add_subplot(111)\n    pos = graphviz_layout(G, 'twopi')\n    nx.draw(G, pos, ax=ax, with_labels=True)\n    plt.savefig(save_path)\n    plt.show()\n\n\n\nDataset = namedtuple('Dataset', ['data', 'embedding', \"path\"])\n\nEdgeIndexTypes = namedtuple('EdgeIndexTypes', ['edge_index', 'weight', 'degrees', \n                                               'neg_edge_index', 'adj'])\n\nSingleBlockData = namedtuple('SingleBlockData', ['feature', 'num_features', 'labels', \n                                                 'num_nodes', 'num_classes', \n                                                 'edge_index_types', 'anchor_feature', 'num_anchors', \n                                                 'anchor_edge_index_types', 'anchor_ass', 'anchor_labels'])\n\nDSIData = namedtuple('DSIData', ['edge_index', 'device', 'pretrain', 'weight', 'adj', \n                                   'feature', 'degrees', 'neg_edge_index'])\n\nDataPartition = namedtuple('DataPartition', ['features', 'labels', 'num_classes', \n                                             'num_features', 'views', 'num_nodes'])\n\nViews = namedtuple('Views', ['userid', 'word', 'entity', 'all'])\n\nSingleBlockData.__annotations__ = {\n    'feature': np.ndarray,  # 假设feature是一个NumPy数组\n    'num_features': int,\n    'labels': np.ndarray,   # 假设labels也是一个NumPy数组\n    'num_nodes': int,\n    'num_classes': int,\n    'edge_index_types': EdgeIndexTypes,  # 假设edge_index_types是一个字符串列表\n    'adj': None\n}\n\n\n\ndef decoding_cluster_from_tree(manifold, tree: nx.Graph, num_clusters, num_nodes, height):\n    root = tree.nodes[num_nodes]\n    root_coords = root['coords']\n    dist_dict = {}  # for every height of tree\n    for u in tree.nodes():\n        if u != num_nodes:  # u is not root\n            h = tree.nodes[u]['height']\n            dist_dict[h] = dist_dict.get(h, {})\n            dist_dict[h].update({u: manifold.dist(root_coords, tree.nodes[u]['coords']).numpy()})\n\n    h = 1\n    sorted_dist_list = sorted(dist_dict[h].items(), reverse=False, key=lambda x: x[1])\n    count = len(sorted_dist_list)\n    group_list = [([u], dist) for u, dist in sorted_dist_list]  # [ ([u], dist_u) ]\n    while len(group_list) <= 1:\n        h = h + 1\n        sorted_dist_list = sorted(dist_dict[h].items(), reverse=False, key=lambda x: x[1])\n        count = len(sorted_dist_list)\n        group_list = [([u], dist) for u, dist in sorted_dist_list]\n\n    while count > num_clusters:\n        group_list, count = merge_nodes_once(manifold, root_coords, tree, group_list, count)\n\n    while count < num_clusters and h <= height:\n        h = h + 1   # search next level\n        pos = 0\n        while pos < len(group_list):\n            v1, d1 = group_list[pos]  # node to split\n            sub_level_set = []\n            v1_coord = tree.nodes[v1[0]]['coords']\n            for u, v in tree.edges(v1[0]):\n                if tree.nodes[v]['height'] == h:\n                    v_coords = tree.nodes[v]['coords']\n                    dist = manifold.dist(v_coords, v1_coord).cpu().numpy()\n                    sub_level_set.append(([v], dist))    # [ ([v], dist_v) ]\n            if len(sub_level_set) <= 1:\n                pos += 1\n                continue\n            sub_level_set = sorted(sub_level_set, reverse=False, key=lambda x: x[1])\n            count += len(sub_level_set) - 1\n            if count > num_clusters:\n                while count > num_clusters:\n                    sub_level_set, count = merge_nodes_once(manifold, v1_coord, tree, sub_level_set, count)\n                del group_list[pos]  # del the position node which will be split\n                group_list += sub_level_set    # Now count == num_clusters\n                break\n            elif count == num_clusters:\n                del group_list[pos]  # del the position node which will be split\n                group_list += sub_level_set\n                break\n            else:\n                del group_list[pos]\n                group_list += sub_level_set\n                pos += 1\n\n    cluster_dist = {}\n    # for i in range(num_clusters):\n    #     u_list, _ = group_list[i]\n    #     group = []\n    #     for u in u_list:\n    #         index = tree.nodes[u]['children'].tolist()\n    #         group += index\n    #     cluster_dist.update({k: i for k in group})\n    for i in range(len(group_list)):\n        u_list, _ = group_list[i]\n        group = []\n        for u in u_list:\n            index = tree.nodes[u]['children'].tolist()\n            group += index\n        cluster_dist.update({k: i for k in group})\n    results = sorted(cluster_dist.items(), key=lambda x: x[0])\n    results = np.array([x[1] for x in results])\n    return results\n\n\ndef merge_nodes_once(manifold, root_coords, tree, group_list, count):\n    # group_list should be ordered ascend\n    v1, v2 = group_list[-1], group_list[-2]\n    merged_node = v1[0] + v2[0]\n    merged_coords = torch.stack([tree.nodes[v]['coords'] for v in merged_node], dim=0)\n    merged_point = manifold.Frechet_mean(merged_coords)\n    merged_dist = manifold.dist(merged_point, root_coords).cpu().numpy()\n    merged_item = (merged_node, merged_dist)\n    del group_list[-2:]\n    group_list.append(merged_item)\n    group_list = sorted(group_list, reverse=False, key=lambda x: x[1])\n    count -= 1\n    return group_list, count\n\n\nclass cluster_metrics:\n    def __init__(self, trues, predicts):\n        self.true_label = trues\n        self.pred_label = predicts\n\n    def get_new_predicts(self):\n        # best mapping between true_label and predict label\n        l1 = list(set(self.true_label))\n        numclass1 = len(l1)\n\n        l2 = list(set(self.pred_label))\n        numclass2 = len(l2)\n        if numclass1 != numclass2:\n            print('Class Not equal, Error!!!!')\n            return 0, 0, 0, 0, 0, 0, 0\n\n        cost = np.zeros((numclass1, numclass2), dtype=int)\n        for i, c1 in enumerate(l1):\n            mps = [i1 for i1, e1 in enumerate(self.true_label) if e1 == c1]\n            for j, c2 in enumerate(l2):\n                mps_d = [i1 for i1 in mps if self.pred_label[i1] == c2]\n\n                cost[i][j] = len(mps_d)\n\n        # match two clustering results by Munkres algorithm\n        m = Munkres()\n        cost = cost.__neg__().tolist()\n\n        indexes = m.compute(cost)\n\n        # get the match results\n        new_predict = np.zeros(len(self.pred_label))\n        for i, c in enumerate(l1):\n            # correponding label in l2:\n            c2 = l2[indexes[i][1]]\n\n            # ai is the index with label==c2 in the pred_label list\n            ai = [ind for ind, elm in enumerate(self.pred_label) if elm == c2]\n            new_predict[ai] = c\n\n        self.new_predicts = new_predict\n\n    def clusterAcc(self):\n        # best mapping between true_label and predict label\n        l1 = list(set(self.true_label))\n        numclass1 = len(l1)\n\n        l2 = list(set(self.pred_label))\n        numclass2 = len(l2)\n        if numclass1 != numclass2:\n            print('Class Not equal, Error!!!!')\n            return 0, 0, 0, 0, 0, 0, 0\n\n        cost = np.zeros((numclass1, numclass2), dtype=int)\n        for i, c1 in enumerate(l1):\n            mps = [i1 for i1, e1 in enumerate(self.true_label) if e1 == c1]\n            for j, c2 in enumerate(l2):\n                mps_d = [i1 for i1 in mps if self.pred_label[i1] == c2]\n\n                cost[i][j] = len(mps_d)\n\n        # match two clustering results by Munkres algorithm\n        m = Munkres()\n        cost = cost.__neg__().tolist()\n\n        indexes = m.compute(cost)\n\n        # get the match results\n        new_predict = np.zeros(len(self.pred_label))\n        for i, c in enumerate(l1):\n            # correponding label in l2:\n            c2 = l2[indexes[i][1]]\n\n            # ai is the index with label==c2 in the pred_label list\n            ai = [ind for ind, elm in enumerate(self.pred_label) if elm == c2]\n            new_predict[ai] = c\n\n        self.new_predicts = new_predict\n        acc = metrics.accuracy_score(self.true_label, new_predict)\n        f1_macro = metrics.f1_score(self.true_label, new_predict, average='macro')\n        precision_macro = metrics.precision_score(self.true_label, new_predict, average='macro')\n        recall_macro = metrics.recall_score(self.true_label, new_predict, average='macro')\n        f1_micro = metrics.f1_score(self.true_label, new_predict, average='micro')\n        precision_micro = metrics.precision_score(self.true_label, new_predict, average='micro')\n        recall_micro = metrics.recall_score(self.true_label, new_predict, average='micro')\n        return acc, f1_macro, precision_macro, recall_macro, f1_micro, precision_micro, recall_micro\n\n    def evaluateFromLabel(self):\n        nmi = metrics.normalized_mutual_info_score(self.true_label, self.pred_label)\n        ami = metrics.adjusted_mutual_info_score(self.true_label, self.pred_label)\n        ari = metrics.adjusted_rand_score(self.true_label, self.pred_label)\n\n        return nmi, ami, ari\n\n\ndef cal_AUC_AP(scores, trues):\n    auc = metrics.roc_auc_score(trues, scores)\n    ap = metrics.average_precision_score(trues, scores)\n    return auc, ap\n\n\n\nclass Node:\n    def __init__(self, index: list, embeddings: torch.Tensor, coords=None,\n                 tree_index=None, is_leaf=False, height: int = None):\n        self.index = index  # T_alpha\n        self.embeddings = embeddings  # coordinates of nodes in T_alpha\n        self.children = []\n        self.coords = coords  # node coordinates\n        self.tree_index = tree_index\n        self.is_leaf = is_leaf\n        self.height = height\n\n\ndef construct_tree(nodes_list: torch.LongTensor, manifold, coords_list: dict,\n                   ass_list: dict, height, num_nodes):\n    nodes_count = num_nodes\n    que = Queue()\n    root = Node(nodes_list, coords_list[height][nodes_list].cpu(),\n                coords=coords_list[0].cpu(), tree_index=nodes_count, height=0)\n    que.put(root)\n\n    while not que.empty():\n        node = que.get()\n        L_nodes = node.index\n        k = node.height + 1\n        if k == height:\n            for i in L_nodes:\n                node.children.append(Node(i.reshape(-1), coords_list[height][i].cpu(), coords=coords_list[k][i].cpu(),\n                                          tree_index=i.item(), is_leaf=True, height=k))\n        else:\n            temp_ass = ass_list[k][L_nodes].cpu()\n            for j in range(temp_ass.shape[-1]):\n                temp_child = L_nodes[temp_ass[:, j].nonzero().flatten()]\n                if len(temp_child) > 0:\n                    nodes_count += 1\n                    child_node = Node(temp_child, coords_list[height][temp_child].cpu(),\n                                      coords=coords_list[k][j].cpu(),\n                                      tree_index=nodes_count, height=k)\n                    node.children.append(child_node)\n                    que.put(child_node)\n    return root\n\n\ndef to_networkx_tree(root: Node, manifold, height):\n    edges_list = []\n    nodes_list = []\n    que = Queue()\n    que.put(root)\n    nodes_list.append(\n        (\n            root.tree_index,\n            {'coords': root.coords.reshape(-1),\n             'is_leaf': root.is_leaf,\n             'children': root.index,\n             'height': root.height}\n        )\n    )\n\n    while not que.empty():\n        cur_node = que.get()\n        if cur_node.height == height:\n            break\n        for node in cur_node.children:\n            nodes_list.append(\n                (\n                    node.tree_index,\n                    {'coords': node.coords.reshape(-1),\n                     'is_leaf': node.is_leaf,\n                     'children': node.index,\n                     'height': node.height}\n                )\n            )\n            edges_list.append(\n                (\n                    cur_node.tree_index,\n                    node.tree_index,\n                    {'weight': torch.sigmoid(1. - manifold.dist(cur_node.coords, node.coords)).item()}\n                )\n            )\n            que.put(node)\n\n    graph = nx.Graph()\n    graph.add_nodes_from(nodes_list)\n    graph.add_edges_from(edges_list)\n    return graph\n\n\n\nMIN_NORM = 1e-15\nEPS = 1e-6\n\n\nclass HyperSE(nn.Module):\n    # def __init__(self, args, manifold, n_layers, device, in_features, hidden_features, num_nodes, height=3, temperature=0.2,\n    #              embed_dim=2, out_dim = 2, dropout=0.5, nonlin='relu', decay_rate=None, max_nums=None, use_att=True, use_bias=True):\n    def __init__(self, args, manifold, n_layers, device, in_features, hidden_dim_enc, hidden_features, num_nodes, height=3, temperature=0.2,\n                 embed_dim=2, dropout=0.5, nonlin='relu', decay_rate=None, max_nums=None, use_att=True, use_bias=True):\n        \n        super(HyperSE, self).__init__()\n        self.num_nodes = num_nodes\n        self.height = height\n        self.tau = temperature\n        self.manifold = manifold\n        self.device = device\n        self.encoder = LSENet(args, self.manifold, n_layers, in_features, hidden_dim_enc, hidden_features,\n                              num_nodes, height, temperature, embed_dim, dropout,\n                              nonlin, decay_rate, max_nums, use_att, use_bias)\n        self.optimizer_pre = RiemannianAdam(self.parameters(), lr=args.lr_pre, weight_decay=args.w_decay)\n        self.optimizer = RiemannianAdam(self.parameters(), lr=args.lr, weight_decay=args.w_decay)\n\n    def forward(self, features, adj):\n        features = features.to(self.device)\n        adj = adj.to(self.device)\n        adj = adj.to_dense()\n        embeddings, clu_mat = self.encoder(features, adj)\n        self.embeddings = {}\n        self.num_nodes = features.shape[0]\n        for height, x in embeddings.items():\n            self.embeddings[height] = x.detach()\n        ass_mat = {self.height: torch.eye(self.num_nodes).to(self.device)}\n        for k in range(self.height - 1, 0, -1):\n            ass_mat[k] = ass_mat[k + 1] @ clu_mat[k + 1]\n        for k, v in ass_mat.items():\n            idx = v.max(1)[1]\n            t = torch.zeros_like(v)\n            t[torch.arange(t.shape[0]), idx] = 1.\n            ass_mat[k] = t\n        self.ass_mat = ass_mat\n        return self.embeddings[self.height]\n\n    def loss(self, input_data: DSIData):\n\n        device = input_data.device\n        weight = input_data.weight.to(self.device)\n        adj = input_data.adj.to(self.device)\n        degrees = input_data.degrees.to(self.device)\n        features = input_data.feature.to(self.device)\n        edge_index = input_data.edge_index.to(self.device)\n        neg_edge_index = input_data.neg_edge_index.to(self.device)\n        pretrain = input_data.pretrain\n        self.num_nodes = features.shape[0]\n\n        embeddings, clu_mat = self.encoder(features, adj.to_dense())\n\n        se_loss = 0\n        vol_G = weight.sum()\n        ass_mat = {self.height: torch.eye(self.num_nodes).to(self.device)}\n        vol_dict = {self.height: degrees, 0: vol_G.unsqueeze(0)}\n        for k in range(self.height - 1, 0, -1):\n            ass_mat[k] = ass_mat[k + 1] @ clu_mat[k + 1]\n            vol_dict[k] = torch.einsum('ij, i->j', ass_mat[k], degrees)\n\n        edges = torch.cat([edge_index, neg_edge_index], dim=-1)\n        prob = self.manifold.dist(embeddings[self.height][edges[0]], embeddings[self.height][edges[1]])\n        prob = torch.sigmoid((2. - prob) / 1.)\n        label = torch.cat([torch.ones(edge_index.shape[-1]), torch.zeros(neg_edge_index.shape[-1])]).to(self.device)\n        lp_loss = F.binary_cross_entropy(prob, label)\n\n        if pretrain:\n            return self.manifold.dist0(embeddings[0]) + lp_loss\n\n        for k in range(1, self.height + 1):\n            vol_parent = torch.einsum('ij, j->i', clu_mat[k], vol_dict[k - 1])  # (N_k, )\n            log_vol_ratio_k = torch.log2((vol_dict[k] + EPS) / (vol_parent + EPS))  # (N_k, )\n            ass_i = ass_mat[k][edge_index[0]]   # (E, N_k)\n            ass_j = ass_mat[k][edge_index[1]]\n            weight_sum = torch.einsum('en, e->n', ass_i * ass_j, weight)  # (N_k, )\n            delta_vol = vol_dict[k] - weight_sum    # (N_k, )\n            se_loss += torch.sum(delta_vol * log_vol_ratio_k)\n        se_loss = -1 / vol_G * se_loss\n        return se_loss + self.manifold.dist0(embeddings[0]) + lp_loss\n    \n\n\nclass LorentzGraphConvolution(nn.Module):\n    \"\"\"\n    Hyperbolic graph convolution layer.\n    \"\"\"\n\n    def __init__(self, manifold, in_features, out_features, use_bias, dropout, use_att, nonlin=None):\n        super(LorentzGraphConvolution, self).__init__()\n        self.linear = LorentzLinear(manifold, in_features, out_features, use_bias, dropout, nonlin=nonlin)\n        self.agg = LorentzAgg(manifold, out_features, dropout, use_att)\n\n    def forward(self, x, edge_index):\n        h = self.linear(x)\n        h = self.agg(h, edge_index)\n        return h\n\n\nclass LorentzLinear(nn.Module):\n    def __init__(self,\n                 manifold,\n                 in_features,\n                 out_features,\n                 bias=True,\n                 dropout=0.1,\n                 scale=10,\n                 fixscale=False,\n                 nonlin=None):\n        super().__init__()\n        self.manifold = manifold\n        self.nonlin = nonlin\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bias = bias\n        self.weight = nn.Linear(\n            self.in_features, self.out_features, bias=bias)\n        self.reset_parameters()\n        self.dropout = nn.Dropout(dropout)\n        self.scale = nn.Parameter(torch.ones(()) * math.log(scale), requires_grad=not fixscale)\n\n    def forward(self, x):\n        if self.nonlin is not None:\n            x = self.nonlin(x)\n        x = self.weight(self.dropout(x))\n        x_narrow = x.narrow(-1, 1, x.shape[-1] - 1)\n        time = x.narrow(-1, 0, 1).sigmoid() * self.scale.exp() + 1.1\n        scale = (time * time - 1) / \\\n            (x_narrow * x_narrow).sum(dim=-1, keepdim=True).clamp_min(1e-8)\n        x = torch.cat([time, x_narrow * scale.sqrt()], dim=-1)\n        return x\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.out_features)\n        step = self.in_features\n        nn.init.uniform_(self.weight.weight, -stdv, stdv)\n        with torch.no_grad():\n            for idx in range(0, self.in_features, step):\n                self.weight.weight[:, idx] = 0\n        if self.bias:\n            nn.init.constant_(self.weight.bias, 0)\n\n\nclass LorentzAgg(nn.Module):\n    \"\"\"\n    Lorentz aggregation layer.\n    \"\"\"\n\n    def __init__(self, manifold, in_features, dropout, use_att):\n        super(LorentzAgg, self).__init__()\n        self.manifold = manifold\n\n        self.in_features = in_features\n        self.dropout = dropout\n        self.use_att = use_att\n        if self.use_att:\n            self.key_linear = LorentzLinear(manifold, in_features, in_features)\n            self.query_linear = LorentzLinear(manifold, in_features, in_features)\n            self.bias = nn.Parameter(torch.zeros(()) + 20)\n            self.scale = nn.Parameter(torch.zeros(()) + math.sqrt(in_features))\n\n    def forward(self, x, adj):\n        if self.use_att:\n            query = self.query_linear(x)\n            key = self.key_linear(x)\n            att_adj = 2 + 2 * self.manifold.cinner(query, key)\n            att_adj = att_adj / self.scale + self.bias\n            att_adj = torch.sigmoid(att_adj)\n            att_adj = torch.mul(adj.to_dense(), att_adj)\n            support_t = torch.matmul(att_adj, x)\n        else:\n            support_t = torch.matmul(adj, x)\n\n        denorm = (-self.manifold.inner(None, support_t, keepdim=True))\n        denorm = denorm.abs().clamp_min(1e-8).sqrt()\n        output = support_t / denorm\n        return output\n\n\nclass LorentzAssignment(nn.Module):\n    def __init__(self, manifold, in_features, hidden_features, num_assign, dropout,\n                 bias=False, use_att=False, nonlin=None, temperature=0.2):\n        super(LorentzAssignment, self).__init__()\n        self.manifold = manifold\n        self.num_assign = num_assign\n        self.proj = nn.Sequential(LorentzLinear(manifold, in_features, hidden_features,\n                                                     bias=bias, dropout=dropout, nonlin=None),\n                                  # LorentzLinear(manifold, hidden_features, hidden_features,\n                                  #               bias=bias, dropout=dropout, nonlin=nonlin)\n                                  )\n        self.assign_linear = LorentzGraphConvolution(manifold, hidden_features, num_assign + 1, use_att=use_att,\n                                                     use_bias=bias, dropout=dropout, nonlin=nonlin)\n        self.temperature = temperature\n        self.key_linear = LorentzLinear(manifold, in_features, in_features)\n        self.query_linear = LorentzLinear(manifold, in_features, in_features)\n        self.bias = nn.Parameter(torch.zeros(()) + 20)\n        self.scale = nn.Parameter(torch.zeros(()) + math.sqrt(hidden_features))\n\n    def forward(self, x, adj):\n        ass = self.assign_linear(self.proj(x), adj).narrow(-1, 1, self.num_assign)\n        query = self.query_linear(x)\n        key = self.key_linear(x)\n        att_adj = 2 + 2 * self.manifold.cinner(query, key)\n        att_adj = att_adj / self.scale + self.bias\n        att = torch.sigmoid(att_adj)\n        # att = torch.mul(adj.to_dense(), att)\n        att = torch.mul(adj, att)\n        ass = torch.matmul(att, ass)   # (N_k, N_{k-1})\n        logits = torch.log_softmax(ass, dim=-1)\n        return logits\n\n\nclass LSENetLayer(nn.Module):\n    def __init__(self, manifold, in_features, hidden_features, num_assign, dropout,\n                 bias=False, use_att=False, nonlin=None, temperature=0.2):\n        super(LSENetLayer, self).__init__()\n        self.manifold = manifold\n        self.assignor = LorentzAssignment(manifold, in_features, hidden_features, num_assign, use_att=use_att, bias=bias,\n                                          dropout=dropout, nonlin=nonlin, temperature=temperature)\n        self.temperature = temperature\n\n    def forward(self, x, adj):\n        ass = self.assignor(x, adj)\n        support_t = ass.exp().t() @ x\n        denorm = (-self.manifold.inner(None, support_t, keepdim=True))\n        denorm = denorm.abs().clamp_min(1e-8).sqrt()\n        x_assigned = support_t / denorm\n        adj = ass.exp().t() @ adj @ ass.exp()\n        adj = adj - torch.eye(adj.shape[0]).to(adj.device) * adj.diag()\n        adj = gumbel_sigmoid(adj, tau=self.temperature)\n        return x_assigned, adj, ass.exp()\n\n\n\n## K-Means in the Poincare Disk model\n\nclass PoincareKMeans(object):\n    def __init__(self,n_clusters=8,n_init=20,max_iter=300,tol=1e-8,verbose=True):\n        self.n_clusters = n_clusters\n        self.n_init = n_init\n        self.max_iter = max_iter\n        self.tol = tol\n        self.verbose =  verbose\n        self.labels_=None\n        self.cluster_centers_ = None\n           \n\n    def fit(self,X):\n        n_samples = X.shape[0]\n        self.inertia = None\n\n        for run_it in range(self.n_init):\n            centroids = X[sample(range(n_samples),self.n_clusters),:]\n            for it in range(self.max_iter):\n                distances = self._get_distances_to_clusters(X,centroids)\n                labels = np.argmin(distances,axis=1)\n\n                new_centroids = np.zeros((self.n_clusters,2))\n                for i in range(self.n_clusters):\n                    indices = np.where(labels==i)[0]\n                    if len(indices)>0:\n                        new_centroids[i,:] = self._hyperbolic_centroid(X[indices,:])\n                    else:\n                        new_centroids[i,:] = X[sample(range(n_samples),1),:]\n                m = np.ravel(centroids-new_centroids, order='K')\n                diff = np.dot(m,m)\n                centroids = new_centroids.copy()\n                if(diff<self.tol):\n                    break\n                \n            distances = self._get_distances_to_clusters(X,centroids)\n            labels = np.argmin(distances,axis=1)\n            inertia = np.sum([np.sum(distances[np.where(labels==i)[0],i]**2) for i in range(self.n_clusters)])\n            if (self.inertia == None) or (inertia<self.inertia):\n                self.inertia = inertia\n                self.labels_ = labels.copy()\n                self.cluster_centers_ = centroids.copy()\n                \n            if self.verbose:\n                print(\"Iteration: {} - Best Inertia: {}\".format(run_it,self.inertia))\n                          \n    def fit_predict(self,X):\n        self.fit(X)\n        return self.labels_\n\n    def fit_transform(self,X):\n        self.fit(X)\n        return self.transform(X)\n    \n    def predict(self,X):\n        distances = self.transform(X)\n        return np.argmin(distances,axis=1)\n        \n    def transform(self,X):\n        return self._get_distances_to_clusters(X,self.cluster_centers_)\n    \n    def _get_distances_to_clusters(self,X,clusters):\n        n_samples,n_clusters = X.shape[0],clusters.shape[0]\n        \n        distances = np.zeros((n_samples,n_clusters))\n        for i in range(n_clusters):\n            centroid = np.tile(clusters[i,:],(n_samples,1))\n            den1 = 1-np.linalg.norm(X,axis=1)**2\n            den2 = 1-np.linalg.norm(centroid,axis=1)**2\n            the_num = np.linalg.norm(X-centroid,axis=1)**2\n            distances[:,i] = np.arccosh(1+2*the_num/(den1*den2))\n        \n        return distances\n      \n    def _poinc_to_minsk(self,points):\n        minsk_points = np.zeros((points.shape[0],3))\n        minsk_points[:,0] = np.apply_along_axis(arr=points,axis=1,func1d=lambda v: 2*v[0]/(1-v[0]**2-v[1]**2))\n        minsk_points[:,1] = np.apply_along_axis(arr=points,axis=1,func1d=lambda v: 2*v[1]/(1-v[0]**2-v[1]**2))\n        minsk_points[:,2] = np.apply_along_axis(arr=points,axis=1,func1d=lambda v: (1+v[0]**2+v[1]**2)/(1-v[0]**2-v[1]**2))\n        return minsk_points\n\n    def _minsk_to_poinc(self,points):\n        poinc_points = np.zeros((points.shape[0],2))\n        poinc_points[:,0] = points[:,0]/(1+points[:,2])\n        poinc_points[:,1] = points[:,1]/(1+points[:,2])\n        return poinc_points\n\n    def _hyperbolic_centroid(self,points):\n        minsk_points = self._poinc_to_minsk(points)\n        minsk_centroid = np.mean(minsk_points,axis=0)\n        normalizer = np.sqrt(np.abs(minsk_centroid[0]**2+minsk_centroid[1]**2-minsk_centroid[2]**2))\n        minsk_centroid = minsk_centroid/normalizer\n        return self._minsk_to_poinc(minsk_centroid.reshape((1,3)))[0]\n\n\n\n\n\nclass LSENet(nn.Module):\n    def __init__(self, args, manifold, n_layers, in_features, hidden_dim_enc, hidden_features, num_nodes, height=3, temperature=0.1,\n                 embed_dim=64, dropout=0.5, nonlin='relu', decay_rate=None, max_nums=None, use_att=True, use_bias=True):\n        super(LSENet, self).__init__()\n        if max_nums is not None:\n            assert len(max_nums) == height - 1, \"length of max_nums must equal height-1.\"\n        self.args = args\n        self.manifold = manifold\n        self.nonlin = select_activation(nonlin) if nonlin is not None else None\n        self.temperature = temperature\n        self.num_nodes = num_nodes\n        self.height = height\n        self.scale = nn.Parameter(torch.tensor([0.999]), requires_grad=True)\n        self.embed_layer = GraphEncoder(self.manifold, n_layers, in_features + 1, hidden_dim_enc, embed_dim + 1, \n                                        use_att=use_att, use_bias=use_bias, dropout=dropout, nonlin=self.nonlin)\n        \n        self.layers = nn.ModuleList([])\n        if max_nums is None:\n            decay_rate = int(np.exp(np.log(num_nodes) / height)) if decay_rate is None else decay_rate\n            max_nums = [int(num_nodes / (decay_rate ** i)) for i in range(1, height)]\n        for i in range(height - 1):\n            self.layers.append(LSENetLayer(self.manifold, embed_dim + 1, hidden_features, max_nums[i],\n                                           bias=use_bias, use_att=use_att, dropout=dropout,\n                                           nonlin=self.nonlin, temperature=self.temperature))\n\n    def forward(self, z, edge_index):\n\n        if not self.args.hgae:\n            o = torch.zeros_like(z).to(z.device)\n            z = torch.cat([o[:, 0:1], z], dim=1)\n            z = self.manifold.expmap0(z)\n        z = self.embed_layer(z, edge_index)\n        z = self.normalize(z)\n\n        self.tree_node_coords = {self.height: z}\n        self.assignments = {}\n\n        edge = edge_index.clone()\n        ass = None\n        for i, layer in enumerate(self.layers):\n            z, edge, ass = layer(z, edge)\n            self.tree_node_coords[self.height - i - 1] = z\n            self.assignments[self.height - i] = ass\n\n        self.tree_node_coords[0] = self.manifold.Frechet_mean(z)\n        self.assignments[1] = torch.ones(ass.shape[-1], 1).to(z.device)\n\n        return self.tree_node_coords, self.assignments\n\n    def normalize(self, x):\n        x = self.manifold.to_poincare(x).to(self.scale.device)\n        x = F.normalize(x, p=2, dim=-1) * self.scale.clamp(1e-2, 0.999)\n        x = self.manifold.from_poincare(x)\n        return x\n    \n\n\nclass Lorentz(geoopt.Lorentz):\n    def __init__(self, k=1.0, learnable=False):\n        super(Lorentz, self).__init__(k, learnable)\n\n    def cinner(self, x, y):\n        x = x.clone()\n        x.narrow(-1, 0, 1).mul_(-1)\n        return x @ y.transpose(-1, -2)\n\n    def to_poincare(self, x, dim=-1):\n        x = x.to(self.device)\n        dn = x.size(dim) - 1\n        return x.narrow(dim, 1, dn) / (x.narrow(dim, 0, 1) + torch.sqrt(self.k))\n\n    def from_poincare(self, x, dim=-1, eps=1e-6):\n        x_norm_square = torch.sum(x * x, dim=dim, keepdim=True)\n        res = (\n                torch.sqrt(self.k)\n                * torch.cat((1 + x_norm_square, 2 * x), dim=dim)\n                / (1.0 - x_norm_square + eps)\n        )\n        return res\n\n    def Frechet_mean(self, x, weights=None, keepdim=False):\n        if weights is None:\n            z = torch.sum(x, dim=0, keepdim=True)\n        else:\n            z = torch.sum(x * weights, dim=0, keepdim=keepdim)\n        denorm = self.inner(None, z, keepdim=keepdim)\n        denorm = denorm.abs().clamp_min(1e-8).sqrt()\n        z = z / denorm\n        return z\n\n\nclass Poincare(geoopt.PoincareBall):\n    def __init__(self, c=1.0, learnable=False):\n        super(Poincare, self).__init__(c=c, learnable=learnable)\n\n    def from_lorentz(self, x, dim=-1):\n        x = x.to(self.c.device)\n        dn = x.size(dim) - 1\n        return x.narrow(dim, 1, dn) / (x.narrow(dim, 0, 1) + torch.sqrt(self.c))\n\n    def to_lorentz(self, x, dim=-1, eps=1e-6):\n        x = x.to(self.c.device)\n        x_norm_square = torch.sum(x * x, dim=dim, keepdim=True)\n        res = (\n                torch.sqrt(self.c)\n                * torch.cat((1 + x_norm_square, 2 * x), dim=dim)\n                / (1.0 - x_norm_square + eps)\n        )\n        return res\n\n    def Frechet_mean(self, embeddings, weights=None, keepdim=False):\n        z = self.to_lorentz(embeddings)\n        if weights is None:\n            z = torch.sum(z, dim=0, keepdim=True)\n        else:\n            z = torch.sum(z * weights, dim=0, keepdim=keepdim)\n        denorm = lmath.inner(z, z, keepdim=keepdim)\n        denorm = denorm.abs().clamp_min(1e-8).sqrt()\n        z = z / denorm\n        z = self.from_lorentz(z).to(embeddings.device)\n        return z\n\n\n\n\n\ndef mask_edges(edge_index, neg_edges, val_prop=0.05, test_prop=0.1):\n    n = len(edge_index[0])\n    n_val = int(val_prop * n)\n    n_test = int(test_prop * n)\n    edge_val, edge_test, edge_train = edge_index[:, :n_val], edge_index[:, n_val:n_val + n_test], edge_index[:, n_val + n_test:]\n    val_edges_neg, test_edges_neg = neg_edges[:, :n_val], neg_edges[:, n_val: n_test + n_val]\n    train_edges_neg = torch.cat([neg_edges, val_edges_neg, test_edges_neg], dim=-1)\n    if test_prop == 0:\n        return (edge_train, edge_val), (train_edges_neg, val_edges_neg)\n    else:\n        return (edge_train, edge_val, edge_test), (train_edges_neg, val_edges_neg, test_edges_neg)\n\n\nclass TwitterDataSet:\n    def __init__(self, args, dataset_name = \"Event2012\", block=None):\n\n        self.args = args\n        self.dataset_name = dataset_name\n\n        self.path: str = f\"../model/model_saved/hypersed/{self.dataset_name}/data\"\n        self.data = self.get_data(self.path)\n        print('Got data.')\n\n\n    def get_data(self, path):\n\n        with open(path + f\"/{self.args.encode}_embeddings.pkl\", 'rb') as file:\n            feature = pickle.load(file)\n\n        temp = np.load(path + '/' + 'label.npy', allow_pickle=True)\n        labels = np.asarray([int(each) for each in temp])\n        num_classes = len(np.unique(labels))\n        num_features = feature.shape[1]\n        num_nodes = feature.shape[0]\n\n        cur_path = f\"{path}/message_graph_{self.args.edge_type}.npz\"\n        matrix = sparse.load_npz(cur_path)\n        source_nodes, target_nodes = matrix.nonzero()\n        edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n        weight = torch.tensor(matrix.data, dtype=torch.float)\n        degrees = scatter_sum(weight, edge_index[0])\n        neg_edge_index = negative_sampling(edge_index)\n        adj = index2adjacency(num_nodes, edge_index, weight, is_sparse=True)\n        edge_index_type = EdgeIndexTypes(edge_index=edge_index, weight=weight, degrees=degrees, \n                                         neg_edge_index=neg_edge_index, adj=adj)\n        \n        if self.args.pre_anchor:\n            anchor_matrix, anchor_fea, anchor_ass, anchor_labels = get_euc_anchors(feature, adj, self.args.anchor_rate, \n                                                                                   self.args.diag, labels)\n            num_anchors = anchor_fea.shape[0]\n            anchor_edge_index = torch.nonzero(anchor_matrix, as_tuple=False).t()\n            # anchor_edge_index = torch.tensor([anchor_source_nodes, anchor_target_nodes], dtype=torch.long)\n            anchor_weight = anchor_matrix[anchor_edge_index[0], anchor_edge_index[1]]\n            anchor_degrees = scatter_sum(anchor_weight, anchor_edge_index[0])\n            anchor_neg_edge_index = negative_sampling(anchor_edge_index)\n            anchor_adj = index2adjacency(num_anchors, anchor_edge_index, anchor_weight, is_sparse=True)\n            anchor_edge_index_type = EdgeIndexTypes(edge_index=anchor_edge_index, weight=anchor_weight, \n                                                    degrees=anchor_degrees, neg_edge_index=anchor_neg_edge_index, \n                                                    adj=anchor_adj)\n        else:\n            num_anchors, anchor_fea, anchor_edge_index_type, anchor_ass, anchor_labels = None, None, None, None, None\n        \n        return SingleBlockData(feature=feature, num_features=num_features, labels=labels, \n                               num_nodes=num_nodes, num_classes=num_classes, \n                               edge_index_types=edge_index_type, anchor_feature=anchor_fea, num_anchors=num_anchors,\n                               anchor_edge_index_types=anchor_edge_index_type, anchor_ass=anchor_ass, anchor_labels=anchor_labels)\n\n\n    def init_incr_data(self):\n\n        for i in sorted(os.listdir(self.path), key=int):\n            if i == '0':\n                continue\n        # for i in (19,20):    # for test\n            self.datas.append(self.get_block_data(os.path.join(self.path, str(i))))\n\n\n\nclass GraphEncoder(nn.Module):\n    def __init__(self, manifold, n_layers, in_features, n_hidden, out_dim,\n                 dropout, nonlin=None, use_att=False, use_bias=False):\n        super(GraphEncoder, self).__init__()\n        self.manifold = manifold\n        self.layers = nn.ModuleList([])\n        self.layers.append(LorentzGraphConvolution(self.manifold, in_features,\n                                                   n_hidden, use_bias, dropout=dropout, use_att=use_att, nonlin=None))\n        for i in range(n_layers - 2):\n            self.layers.append(LorentzGraphConvolution(self.manifold, n_hidden,\n                                                       n_hidden, use_bias, dropout=dropout, use_att=use_att, nonlin=nonlin))\n        self.layers.append(LorentzGraphConvolution(self.manifold, n_hidden,\n                                                       out_dim, use_bias, dropout=dropout, use_att=use_att, nonlin=nonlin))\n\n    def forward(self, x, edge_index):\n        for layer in self.layers:\n            x = layer(x, edge_index)\n        return x\n\n\nclass FermiDiracDecoder(nn.Module):\n    def __init__(self, \n                 args,\n                 manifold):\n        super(FermiDiracDecoder, self).__init__()\n        \n        self.args = args\n        self.manifold = manifold\n        self.r = self.args.r\n        self.t = self.args.t\n\n    def forward(self, x):\n        \n        N = x.shape[0]\n        dist = torch.zeros((N, N), device=x.device)  # 初始化一个 N x N 的结果张量\n        \n        for i in range(N):\n            # 计算第 i 行的所有距离\n            dist[i, :] = self.manifold.dist2(x[i].unsqueeze(0), x)\n\n        probs = torch.sigmoid((self.r - dist) / self.t)\n        adj_pred = torch.sigmoid(probs)\n\n        return adj_pred\n\n\nclass HyperGraphAutoEncoder(nn.Module):\n    def __init__(self, args, device, manifold, n_layers, in_features, n_hidden, out_dim, dropout, nonlin, use_att, use_bias):\n        super(HyperGraphAutoEncoder, self).__init__()\n\n        self.args = args\n        self.device = device\n        self.manifold = manifold\n        self.scale = nn.Parameter(torch.tensor([0.999]), requires_grad=True)\n\n        self.encoder = GraphEncoder(self.manifold, n_layers, in_features + 1, n_hidden, out_dim + 1, \n                                    dropout, nonlin, use_att, use_bias)\n        self.decoder = FermiDiracDecoder(self.args, self.manifold)\n        self.optimizer = RiemannianAdam(self.parameters(), lr=self.args.lr_gae, weight_decay=args.w_decay)\n\n\n    def forward(self, x, adj):\n        x = x.to(self.device)\n        adj = adj.to(self.device)\n        \n        o = torch.zeros_like(x).to(x.device)\n        x = torch.cat([o[:, 0:1], x], dim=1)\n        x = self.manifold.expmap0(x)\n        z = self.encoder(x, adj)\n        z = self.normalize(z)\n        adj_pred = self.decoder(z)\n        \n        return adj_pred, z\n\n    \n    def loss(self, x, adj):\n        x = x.to(self.device)\n        \n        # 获取预测值和隐藏表示\n        adj_pred, z = self.forward(x, adj)\n        \n        # 确保预测值在[0,1]范围内\n        adj_pred = torch.clamp(adj_pred, min=0.0, max=1.0)\n\n        # 转换为稀疏矩阵并计算权重\n        adj = tensor_to_sparse(adj, (adj.shape[0], adj.shape[1]))\n        pos_weight = float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()\n        norm = adj.shape[0] * adj.shape[0] / float(\n            (adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n        \n        # 创建标签\n        adj_label = adj + sp.eye(adj.shape[0])\n        adj_label = sparse_to_tuple(adj_label)\n        adj_label = torch.sparse.FloatTensor(torch.LongTensor(adj_label[0].T),\n                                            torch.FloatTensor(adj_label[1]),\n                                            torch.Size(adj_label[2]))\n        \n        # 转换为密集张量并确保值在[0,1]范围内\n        adj_label_dense = adj_label.to_dense()\n        adj_label_dense = torch.clamp(adj_label_dense, min=0.0, max=1.0)\n        \n        # 计算权重掩码\n        weight_mask = adj_label_dense.view(-1) == 1\n        weight_tensor = torch.ones(weight_mask.size(0))\n        weight_tensor[weight_mask] = pos_weight\n        \n        # 移动到正确的设备\n        adj_label_dense = adj_label_dense.to(self.device)\n        weight_tensor = weight_tensor.to(self.device)\n        \n        # 计算损失\n        loss = norm * F.binary_cross_entropy(\n            adj_pred.view(-1),\n            adj_label_dense.view(-1),\n            weight=weight_tensor,\n            reduction='mean'\n        )\n        \n        return loss, adj_pred, z\n    \n\n    def normalize(self, x):\n        x = self.manifold.to_poincare(x)\n        x = x.to(self.device)\n        x = F.normalize(x, p=2, dim=-1) * self.scale.clamp(1e-2, 0.999)\n        x = self.manifold.from_poincare(x)\n        return x\n\n\n\n\ndef select_activation(activation):\n    if activation == 'elu':\n        return F.elu\n    elif activation == 'relu':\n        return F.relu\n    elif activation == 'sigmoid':\n        return F.sigmoid\n    elif activation is None:\n        return None\n    else:\n        raise NotImplementedError('the non_linear_function is not implemented')\n\ndef Frechet_mean_poincare(manifold, embeddings, weights=None, keepdim=False):\n    z = manifold.from_poincare(embeddings)\n    if weights is None:\n        z = torch.sum(z, dim=0, keepdim=True)\n    else:\n        z = torch.sum(z * weights, dim=0, keepdim=keepdim)\n    denorm = manifold.inner(None, z, keepdim=keepdim)\n    denorm = denorm.abs().clamp_min(1e-8).sqrt()\n    z = z / denorm\n    z = manifold.to_poincare(z).to(embeddings.device)\n    return z\n\ndef sample_gumbel(shape, eps=1e-20):\n    U = torch.rand(shape)\n    return -torch.log(-torch.log(U + eps) + eps)\n\ndef gumbel_softmax_sample(logits, temperature=1):\n    y = logits + sample_gumbel(logits.size()).to(logits.device)\n    return torch.nn.functional.softmax(y / temperature, dim=-1)\n\ndef gumbel_softmax(logits, temperature=0.2, hard=False):\n    \"\"\"\n    ST-gumple-softmax\n    input: [*, n_class]\n    return: flatten --> [*, n_class] an one-hot vector\n    \"\"\"\n    y = gumbel_softmax_sample(logits, temperature)\n\n    if not hard:\n        return y\n\n    shape = y.size()\n    _, ind = y.max(dim=-1)\n    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n    y_hard.scatter_(1, ind.view(-1, 1), 1)\n    y_hard = y_hard.view(*shape)\n    # Set gradients w.r.t. y_hard gradients w.r.t. y\n    y_hard = (y_hard - y).detach() + y\n    return y_hard\n\ndef gumbel_sigmoid(logits, tau: float = 1, hard: bool = False, threshold: float = 0.5):\n    gumbels = (\n        -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()\n    )  # ~Gumbel(0, 1)\n    gumbels = (logits + gumbels) / tau  # ~Gumbel(logits, tau)\n    y_soft = gumbels.sigmoid()\n\n    if hard:\n        # Straight through.\n        indices = (y_soft > threshold).nonzero(as_tuple=True)\n        y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format)\n        y_hard[indices[0], indices[1]] = 1.0\n        ret = y_hard - y_soft.detach() + y_soft\n    else:\n        # Reparametrization trick.\n        ret = y_soft\n    return ret\n\ndef graph_top_K(dense_adj, k):\n    assert k < dense_adj.shape[-1]\n    _, indices = dense_adj.topk(k=k, dim=-1)\n    mask = torch.zeros(dense_adj.shape).bool().to(dense_adj.device)\n    mask[torch.arange(dense_adj.shape[0])[:, None], indices] = True\n    sparse_adj = torch.masked_fill(dense_adj, ~mask, value=0.)\n    return sparse_adj\n\ndef adjacency2index(adjacency, weight=False, topk=False, k=10):\n    \"\"\"_summary_\n\n    Args:\n        adjacency (torch.tensor): [N, N] matrix\n    return:\n        edge_index: [2, E]\n        edge_weight: optional\n    \"\"\"\n    if topk and k:\n        adj = graph_top_K(adjacency, k)\n    else:\n        adj = adjacency\n    edge_index = torch.nonzero(adj).t().contiguous()\n    if weight:\n        weight = adjacency[edge_index[0], edge_index[1]].reshape(-1)\n        return edge_index, weight\n    else:\n        return edge_index\n\ndef index2adjacency(N, edge_index, weight=None, is_sparse=True):\n    adjacency = torch.zeros(N, N).to(edge_index.device)\n    m = edge_index.shape[1]\n    if weight is None:\n        adjacency[edge_index[0], edge_index[1]] = 1\n    else:\n        adjacency[edge_index[0], edge_index[1]] = weight.reshape(-1)\n    if is_sparse:\n        weight = weight if weight is not None else torch.ones(m).to(edge_index.device)\n        adjacency = torch.sparse_coo_tensor(indices=edge_index, values=weight, size=(N, N))\n    return adjacency\n\ndef sparse_to_tuple(sparse_mx):\n    if not sp.isspmatrix_coo(sparse_mx):\n        sparse_mx = sparse_mx.tocoo()\n    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n    values = sparse_mx.data\n    shape = sparse_mx.shape\n    return coords, values, shape\n\ndef tensor_to_sparse(sparse_tensor, size = (500, 500)):\n    # 获取稀疏张量的索引和值\n    indices_np = sparse_tensor.coalesce().indices().numpy()\n    values_np = sparse_tensor.coalesce().values().numpy()\n\n    # 将索引转换为行和列\n    row = indices_np[0]\n    col = indices_np[1]\n\n    # 创建 csr_matrix\n    csr = csr_matrix((values_np, (row, col)), shape=size)\n    return csr\n\ndef getOtherByedge(edge_index, num_nodes):\n    weight = torch.ones(edge_index.shape[1])\n    degrees = scatter_sum(weight, edge_index[0])\n    adj = index2adjacency(num_nodes, edge_index, weight, is_sparse=True)\n\n    return adj, degrees, weight\n\ndef getNewPredict(predicts, C):\n    N = predicts.shape[0]\n    P = np.zeros(N)\n    C = C.cpu().numpy()\n    for i in range(C.shape[0]):\n        j = np.where(C[i] == 1)[0][0]\n        P[i] = predicts[j]\n\n    return P\n\ndef getC(Z, M):\n    N = Z.size(0)\n    Z_np = Z.detach().cpu().numpy()\n\n    # 随机选择M个数据点作为初始锚点，并且确保每个聚类簇中至少有一个数据点\n    initial_indices = np.random.choice(N, M, replace=False)\n    initial_anchors = Z_np[initial_indices]\n\n    # 对得到的特征进行kmeans聚类，使用初始锚点\n    kmeans = KMeans(n_clusters=M, init=initial_anchors, n_init=1, max_iter=200, tol=1e-10)\n    kmeans.fit(Z_np)\n    labels = kmeans.labels_\n    labels = torch.tensor(labels, device=Z.device, dtype=torch.long)\n\n    C = torch.zeros(N, M, device=Z.device)\n    C[torch.arange(N, dtype=torch.long), labels] = 1\n\n    return C\n\ndef getNewPredict(predicts, C):\n    P = np.zeros(C.shape[0])\n    C = C.cpu().numpy()\n    for i in range(C.shape[0]):\n        j = np.where(C[i] == 1)[0][0]\n        P[i] = predicts[j]\n\n    return P\n\ndef get_anchor(Z, A, M):\n    N = Z.size(0)\n    Z_np = Z.detach().cpu().numpy()\n\n    kmeans = PoincareKMeans(n_clusters=M, n_init=1, max_iter=200, tol=1e-10, verbose=True)\n    kmeans.fit(Z_np)\n    labels = kmeans.labels_\n    labels = torch.tensor(labels, device=Z.device, dtype=torch.long)\n\n    C = torch.zeros(N, M, device=Z.device)\n    C[torch.arange(N, dtype=torch.long), labels] = 1\n\n    # 计算锚点图的邻接矩阵\n    A_anchor = C.T @ A @ C\n    A_anchor.fill_diagonal_(0)\n\n    # 计算锚点的表示\n    X_anchor = torch.zeros(M, Z.size(1), device=Z.device)\n    for i in range(M):\n        cluster_points = Z[torch.where(C[:, i] == 1)]\n        if cluster_points.size(0) > 0:  # 检查簇中是否存在数据点\n            X_anchor[i] = cluster_points.mean(dim=0)\n\n    return A_anchor, X_anchor, C\n\nclass Artanh(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        x = x.clamp(-1 + 1e-15, 1 - 1e-15)\n        ctx.save_for_backward(x)\n        z = x.double()\n        return (torch.log_(1 + z).sub_(torch.log_(1 - z))).mul_(0.5).to(x.dtype)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        return grad_output / (1 - input ** 2)\n\ndef artanh(x):\n    return Artanh.apply(x)\n\ndef tanh(x, clamp=15):\n    return x.clamp(-clamp, clamp).tanh()\n\n\n\ndef mobius_add(x, y, c, dim=-1, eps=1e-5):\n    x2 = x.pow(2).sum(dim=dim, keepdim=True)\n    y2 = y.pow(2).sum(dim=dim, keepdim=True)\n    xy = (x * y).sum(dim=dim, keepdim=True)\n    num = (1 + 2 * c * xy + c * y2) * x + (1 - c * x2) * y\n    denom = 1 + 2 * c * xy + c ** 2 * x2 * y2\n    return num / denom.clamp_min(1e-15)\n\ndef hyperbolic_distance1(p1, p2, c=1):\n    sqrt_c = c ** 0.5\n    dist_c = artanh(\n        sqrt_c * mobius_add(-p1, p2, c, dim=-1).norm(dim=-1, p=2, keepdim=False)\n    )\n    dist = dist_c * 2 / sqrt_c\n    return dist ** 2\n\ndef hyperbolic_distance(z_u, z_h, eps=1e-5):\n    norm_z_u = torch.norm(z_u, p=2, dim=-1)\n    norm_z_h = torch.norm(z_h, p=2, dim=-1)\n\n    # Ensure norms are less than 1 to satisfy the Poincaré ball constraint\n    norm_z_u = torch.clamp(norm_z_u, max=1 - eps)\n    norm_z_h = torch.clamp(norm_z_h, max=1 - eps)\n\n    # Compute the squared Euclidean distance\n    euclidean_dist_sq = torch.sum((z_u - z_h) ** 2, dim=-1)\n\n    # Compute the hyperbolic distance\n    numerator = 2 * euclidean_dist_sq\n    denominator = (1 - norm_z_u ** 2) * (1 - norm_z_h ** 2)\n    arg_acosh = 1 + numerator / denominator\n\n    # Ensure the argument of acosh is >= 1\n    arg_acosh = torch.clamp(arg_acosh, min=1 + eps)\n\n    return torch.acosh(arg_acosh)\n\n\ndef contrastive_loss(manifold, z_u, z_h, z_h_all, temperature=0.5):\n\n    dist1 = manifold.dist2(z_u, z_h)\n    dist = manifold.dist2(z_u, z_h_all)\n\n    loss = -torch.log(torch.exp(dist1 / temperature) / torch.exp(dist / temperature).sum())\n\n    return loss.mean()\n\n\ndef L_ConV(manifold, z_u, z_h, z_e, N_s, temperature=0.5):\n    \"\"\"\n    Compute the overall contrastive loss.\n    \"\"\"\n    loss = 0.0\n    for i in range(N_s):\n        loss += (contrastive_loss(manifold, z_u[i], z_h[i], z_h, temperature) +\n                 contrastive_loss(manifold, z_h[i], z_e[i], z_e, temperature) +\n                 contrastive_loss(manifold, z_e[i], z_u[i], z_u, temperature))\n    return loss / (3 * N_s)\n\n\ndef get_agg_feauture(manifold, x1, x2, x3):\n    \n    x = torch.zeros_like(x1)\n\n    for i in range(x1.shape[0]):\n        x_chunk = torch.stack((x1[i], x2[i], x3[i]), dim=0)  # (3, hidden)\n        x[i] = manifold.Frechet_mean(x_chunk, keepdim=False)  # (hidden,)\n    \n    return x\n\n\ndef get_euc_anchors(features, adj, anchor_rate, diag, true_labels):\n\n    num_nodes = features.shape[0]\n    num_anchor = int(num_nodes / anchor_rate)\n\n    kmeans = KMeans(n_clusters=num_anchor, n_init=10, random_state=1)\n    anchor_result = kmeans.fit(features)\n    anchor_predictions = anchor_result.labels_\n\n    labels = get_cluster_labels(anchor_predictions, true_labels)\n\n    anchor_predictions = torch.tensor(anchor_predictions, device=features.device, dtype=torch.long)\n\n    C = torch.zeros(num_nodes, num_anchor, device=features.device)\n    C[torch.arange(num_nodes, dtype=torch.long), anchor_predictions] = 1\n\n    anchor_fea = torch.zeros(num_anchor, features.size(1), device=features.device)\n    for i in range(num_anchor):\n        cluster_points = features[torch.where(C[:, i] == 1)]\n        if cluster_points.size(0) > 0:\n            anchor_fea[i] = cluster_points.mean(dim=0)\n    \n    anchor_adj = C.T @ adj.to_dense() @ C\n    # anchor_adj.fill_diagonal_(diag)\n\n    return anchor_adj, anchor_fea, C, labels\n\n\ndef get_cluster_labels(cluster_predictions, true_labels):\n    unique_clusters = set(cluster_predictions)\n    cluster_labels = {}\n    labels = []\n\n    for cluster in unique_clusters:\n        cluster_indices = [i for i, pred in enumerate(cluster_predictions) if pred == cluster]\n        labels_in_cluster = [true_labels[idx] for idx in cluster_indices]\n        most_common_label = Counter(labels_in_cluster).most_common(1)[0][0]\n        cluster_labels[cluster] = most_common_label\n        labels.append(most_common_label)\n\n    unique_cluster_labels = set(cluster_labels.values())\n\n    return labels\n\n\ndef get_euc_anchors_alladj(features, adj, anchor_rate, diag, thres):\n\n    num_nodes = features.shape[0]\n    num_anchor = int(num_nodes / anchor_rate)\n\n    kmeans = KMeans(n_clusters=num_anchor, n_init=10, random_state=1)\n    anchor_result = kmeans.fit(features)\n    anchor_predictions = anchor_result.labels_\n\n    anchor_predictions = torch.tensor(anchor_predictions, device=features.device, dtype=torch.long)\n\n    C = torch.zeros(num_nodes, num_anchor, device=features.device)\n    C[torch.arange(num_nodes, dtype=torch.long), anchor_predictions] = 1\n\n    anchor_fea = torch.zeros(num_anchor, features.size(1), device=features.device)\n    for i in range(num_anchor):\n        cluster_points = features[torch.where(C[:, i] == 1)]\n        if cluster_points.size(0) > 0:\n            anchor_fea[i] = cluster_points.mean(dim=0)\n\n    corr_matrix_np = np.corrcoef(features, rowvar=True)\n    adjacency_matrix_np = np.where(corr_matrix_np > thres, corr_matrix_np, 0)\n    adj = torch.tensor(adjacency_matrix_np, dtype=torch.float32)\n\n    anchor_adj = C.T @ adj @ C\n    # anchor_adj.fill_diagonal_(diag)\n\n    return anchor_adj, anchor_fea, C\n\n\ndef get_euc_anchors_alladj_as(features, adj, anchor_rate, diag, thres):\n\n    num_nodes = features.shape[0]\n    num_anchor = int(num_nodes / anchor_rate)\n\n    kmeans = KMeans(n_clusters=num_anchor, n_init=10, random_state=1)\n    anchor_result = kmeans.fit(features)\n    anchor_predictions = anchor_result.labels_\n\n    anchor_predictions = torch.tensor(anchor_predictions, device=features.device, dtype=torch.long)\n\n    C = torch.zeros(num_nodes, num_anchor, device=features.device)\n    C[torch.arange(num_nodes, dtype=torch.long), anchor_predictions] = 1\n\n    anchor_fea = torch.zeros(num_anchor, features.size(1), device=features.device)\n    for i in range(num_anchor):\n        cluster_points = features[torch.where(C[:, i] == 1)]\n        if cluster_points.size(0) > 0:\n            anchor_fea[i] = cluster_points.mean(dim=0)\n\n    corr_matrix_np = np.corrcoef(features, rowvar=True)\n    adjacency_matrix_np = np.where(corr_matrix_np > thres, corr_matrix_np, 0)\n    adj = torch.tensor(adjacency_matrix_np + adj, dtype=torch.float32)\n\n    anchor_adj = C.T @ adj @ C\n    # anchor_adj.fill_diagonal_(diag)\n\n    return anchor_adj, anchor_fea, C\n\ndef replaceAtUser(text):\n    \"\"\" Replaces \"@user\" with \"\" \"\"\"\n    text = re.sub('@[^\\s]+|RT @[^\\s]+', '', text)\n    return text\n\n\ndef removeUnicode(text):\n    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)', r'', text)\n    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n    return text\n\n\ndef replaceURL(text):\n    \"\"\" Replaces url address with \"url\" \"\"\"\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'url', text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\n\ndef replaceMultiExclamationMark(text):\n    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n    text = re.sub(r\"(\\!)\\1+\", '!', text)\n    return text\n\n\ndef replaceMultiQuestionMark(text):\n    \"\"\" Replaces repetitions of question marks \"\"\"\n    text = re.sub(r\"(\\?)\\1+\", '?', text)\n    return text\n\n\ndef removeEmoticons(text):\n    \"\"\" Removes emoticons from text \"\"\"\n    text = re.sub(\n        ':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:',\n        '', text)\n    return text\n\ndef removeNewLines(text):\n    text = re.sub('\\n', '', text)\n    return text\n\n\ndef preprocess_sentence(s):\n    return removeNewLines(replaceAtUser(\n        removeEmoticons(replaceMultiQuestionMark(replaceMultiExclamationMark(removeUnicode(replaceURL(s)))))))\n\ndef preprocess_french_sentence(s):\n    return removeNewLines(\n        replaceAtUser(removeEmoticons(replaceMultiQuestionMark(replaceMultiExclamationMark(replaceURL(s))))))\n\ndef SBERT_embed(s_list, language):\n    '''\n    Use Sentence-BERT to embed sentences.\n    s_list: a list of sentences/ tokens to be embedded.\n    language: the language of the sentences ('English', 'French', 'Arabic').\n    output: the embeddings of the sentences/ tokens.\n    '''\n    # Model paths or names for each language\n    model_map = {\n        'English': '../model/model_needed/all-MiniLM-L6-v2',\n        'French': '../model/model_needed/distiluse-base-multilingual-cased-v1',\n        'Arabic': '../model/model_needed/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Default model for Hugging Face\n    hf_model_map = {\n        'English': 'sentence-transformers/all-MiniLM-L6-v2',\n        'French': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n        'Arabic': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Print language and model being used\n    print(f\"Embedding sentences in language: {language}\")\n    \n    # Determine model path\n    model_path = model_map.get(language)\n    if not model_path:\n        raise ValueError(f\"Unsupported language: {language}. Supported languages are: {', '.join(model_map.keys())}\")\n\n    print(f\"Using model: {model_path}\")\n\n    # Load the model, downloading if necessary\n    try:\n        model = SentenceTransformer(model_path)\n        print(f\"Successfully loaded model from local path: {model_path}\")\n    except Exception as e:\n        print(f\"Model {model_path} not found locally. Attempting to download from Hugging Face...\")\n        model = SentenceTransformer(hf_model_map[language])\n        print(f\"Model downloaded from Hugging Face: {hf_model_map[language]}\")\n\n    # Compute embeddings\n    embeddings = model.encode(s_list, convert_to_tensor=True, normalize_embeddings=True)\n    print(f\"Computed embeddings for {len(s_list)} sentences/tokens.\")\n    \n    return embeddings.cpu()\n\n\ndef compute_argmin(C, all_1dSEs):\n    N = len(all_1dSEs)\n    min_val = float('inf')\n    min_i = None\n    \n    for j, i in enumerate(C):\n        sum_val = 1/N * np.sum(all_1dSEs) - all_1dSEs[j]\n        \n        if sum_val < min_val:\n            min_val = sum_val\n            min_i = i\n    \n    return min_i\n\ndef search_threshold(embeddings, start=0.6, end=0.4, step=-0.05):\n    all_1dSEs = []\n    \n    corr_matrix = np.corrcoef(embeddings)\n    np.fill_diagonal(corr_matrix, 0)\n    \n    for i in tqdm(np.arange(start, end, step)):\n        threshold = i\n        edges = [(s, d, corr_matrix[s, d]) for s, d in np.ndindex(corr_matrix.shape) if corr_matrix[s, d] >= threshold]\n        g = nx.Graph()\n        g.add_weighted_edges_from(edges)\n        seg = SE(g)\n        all_1dSEs.append(seg.calc_1dSE())\n    \n    best_threshold = compute_argmin(np.arange(start, end, step), all_1dSEs)\n    print('best threshold:', best_threshold)\n    \n    return best_threshold\n\n\nclass SE:\n    def __init__(self, graph: nx.Graph):\n        self.graph = graph.copy()\n        self.vol = self.get_vol()\n        self.division = {}  # {comm1: [node11, node12, ...], comm2: [node21, node22, ...], ...}\n        self.struc_data = {}  # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }\n        self.struc_data_2d = {} # {comm1: {comm2: [vol_after_merge, cut_after_merge, comm_node_SE_after_merge, leaf_nodes_SE_after_merge], comm3: [], ...}, ...}\n\n    def get_vol(self):\n        '''\n        get the volume of the graph\n        '''\n        return cuts.volume(self.graph, self.graph.nodes, weight = 'weight')\n\n    def calc_1dSE(self):\n        '''\n        get the 1D SE of the graph\n        '''\n        SE = 0\n        for n in self.graph.nodes:\n            d = cuts.volume(self.graph, [n], weight = 'weight')\n            SE += - (d / self.vol) * math.log2(d / self.vol)\n            #SE += - (d / self.vol) * log2(d / self.vol)\n        return SE\n\n    def update_1dSE(self, original_1dSE, new_edges):\n        '''\n        get the updated 1D SE after new edges are inserted into the graph\n        '''\n    \n        affected_nodes = []\n        for edge in new_edges:\n            affected_nodes += [edge[0], edge[1]]\n        affected_nodes = set(affected_nodes)\n\n        original_vol = self.vol\n        original_degree_dict = {node:0 for node in affected_nodes}\n        for node in affected_nodes.intersection(set(self.graph.nodes)):\n            original_degree_dict[node] = self.graph.degree(node, weight = 'weight')\n\n        # insert new edges into the graph\n        self.graph.add_weighted_edges_from(new_edges)\n\n        self.vol = self.get_vol()\n        updated_vol = self.vol\n        updated_degree_dict = {}\n        for node in affected_nodes:\n            updated_degree_dict[node] = self.graph.degree(node, weight = 'weight')\n        \n        updated_1dSE = (original_vol / updated_vol) * (original_1dSE - math.log2(original_vol / updated_vol))\n        for node in affected_nodes:\n            d_original = original_degree_dict[node]\n            d_updated = updated_degree_dict[node]\n            if d_original != d_updated:\n                if d_original != 0:\n                    updated_1dSE += (d_original / updated_vol) * math.log2(d_original / updated_vol)\n                updated_1dSE -= (d_updated / updated_vol) * math.log2(d_updated / updated_vol)\n\n        return updated_1dSE\n\n    def get_cut(self, comm):\n        '''\n        get the sum of the degrees of the cut edges of community comm\n        '''\n        return cuts.cut_size(self.graph, comm, weight = 'weight')\n\n    def get_volume(self, comm):\n        '''\n        get the volume of community comm\n        '''\n        return cuts.volume(self.graph, comm, weight = 'weight')\n\n    def calc_2dSE(self):\n        '''\n        get the 2D SE of the graph\n        '''\n        SE = 0\n        for comm in self.division.values():\n            g = self.get_cut(comm)\n            v = self.get_volume(comm)\n            SE += - (g / self.vol) * math.log2(v / self.vol)\n            for node in comm:\n                d = self.graph.degree(node, weight = 'weight')\n                SE += - (d / self.vol) * math.log2(d / v)\n        return SE\n\n    def show_division(self):\n        print(self.division)\n\n    def show_struc_data(self):\n        print(self.struc_data)\n    \n    def show_struc_data_2d(self):\n        print(self.struc_data_2d)\n        \n    def print_graph(self):\n        fig, ax = plt.subplots()\n        nx.draw(self.graph, ax=ax, with_labels=True)\n        plt.show()\n    \n    def update_struc_data(self):\n        '''\n        calculate the volume, cut, communitiy mode SE, and leaf nodes SE of each cummunity, \n        then store them into self.struc_data\n        '''\n        self.struc_data = {} # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }\n        for vname in self.division.keys():\n            comm = self.division[vname]\n            volume = self.get_volume(comm)\n            cut = self.get_cut(comm)\n            if volume == 0:\n                vSE = 0\n            else:\n                vSE = - (cut / self.vol) * math.log2(volume / self.vol)\n            vnodeSE = 0\n            for node in comm:\n                d = self.graph.degree(node, weight = 'weight')\n                if d != 0:\n                    vnodeSE -= (d / self.vol) * math.log2(d / volume)\n            self.struc_data[vname] = [volume, cut, vSE, vnodeSE]\n\n    def update_struc_data_2d(self):\n        '''\n        calculate the volume, cut, communitiy mode SE, and leaf nodes SE after merging each pair of cummunities, \n        then store them into self.struc_data_2d\n        '''\n        self.struc_data_2d = {} # {(comm1, comm2): [vol_after_merge, cut_after_merge, comm_node_SE_after_merge, leaf_nodes_SE_after_merge], (comm1, comm3): [], ...}\n        comm_num = len(self.division)\n        for i in range(comm_num):\n            for j in range(i + 1, comm_num):\n                v1 = list(self.division.keys())[i]\n                v2 = list(self.division.keys())[j]\n                if v1 < v2:\n                    k = (v1, v2)\n                else:\n                    k = (v2, v1)\n\n                comm_merged = self.division[v1] + self.division[v2]\n                gm = self.get_cut(comm_merged)\n                vm = self.struc_data[v1][0] + self.struc_data[v2][0]\n                if self.struc_data[v1][0] == 0 or self.struc_data[v2][0] == 0:\n                    vmSE = self.struc_data[v1][2] + self.struc_data[v2][2]\n                    vmnodeSE = self.struc_data[v1][3] + self.struc_data[v2][3]\n                else:\n                    vmSE = - (gm / self.vol) * math.log2(vm / self.vol)\n                    vmnodeSE = self.struc_data[v1][3] - (self.struc_data[v1][0]/ self.vol) * math.log2(self.struc_data[v1][0] / vm) + \\\n                            self.struc_data[v2][3] - (self.struc_data[v2][0]/ self.vol) * math.log2(self.struc_data[v2][0] / vm)\n                self.struc_data_2d[k] = [vm, gm, vmSE, vmnodeSE]\n\n    def init_division(self):\n        '''\n        initialize self.division such that each node assigned to its own community\n        '''\n        self.division = {}\n        for node in self.graph.nodes:\n            new_comm = node\n            self.division[new_comm] = [node]\n            self.graph.nodes[node]['comm'] = new_comm\n\n    def add_isolates(self):\n        '''\n        add any isolated nodes into graph\n        '''\n        all_nodes = list(chain(*list(self.division.values())))\n        all_nodes.sort()\n        edge_nodes = list(self.graph.nodes)\n        edge_nodes.sort()\n        if all_nodes != edge_nodes:\n            for node in set(all_nodes)-set(edge_nodes):\n                self.graph.add_node(node)\n\n    def update_division_MinSE(self):\n        '''\n        greedily update the encoding tree to minimize 2D SE\n        '''\n        def Mg_operator(v1, v2):\n            '''\n            MERGE operator. It calculates the delta SE caused by mergeing communities v1 and v2, \n            without actually merging them, i.e., the encoding tree won't be changed\n            '''\n            v1SE = self.struc_data[v1][2] \n            v1nodeSE = self.struc_data[v1][3]\n\n            v2SE = self.struc_data[v2][2]\n            v2nodeSE = self.struc_data[v2][3]\n\n            if v1 < v2:\n                k = (v1, v2)\n            else:\n                k = (v2, v1)\n            vm, gm, vmSE, vmnodeSE = self.struc_data_2d[k]\n            delta_SE = vmSE + vmnodeSE - (v1SE + v1nodeSE + v2SE + v2nodeSE)\n            return delta_SE\n\n        # continue merging any two communities that can cause the largest decrease in SE, \n        # until the SE can't be further reduced\n        while True: \n            comm_num = len(self.division)\n            delta_SE = 99999\n            vm1 = None\n            vm2 = None\n            for i in range(comm_num):\n                for j in range(i + 1, comm_num):\n                    v1 = list(self.division.keys())[i]\n                    v2 = list(self.division.keys())[j]\n                    new_delta_SE = Mg_operator(v1, v2)\n                    if new_delta_SE < delta_SE:\n                        delta_SE = new_delta_SE\n                        vm1 = v1\n                        vm2 = v2\n\n            if delta_SE < 0:\n                # Merge v2 into v1, and update the encoding tree accordingly\n                for node in self.division[vm2]:\n                    self.graph.nodes[node]['comm'] = vm1\n                self.division[vm1] += self.division[vm2]\n                self.division.pop(vm2)\n\n                volume = self.struc_data[vm1][0] + self.struc_data[vm2][0]\n                cut = self.get_cut(self.division[vm1])\n                vmSE = - (cut / self.vol) * math.log2(volume / self.vol)\n                vmnodeSE = self.struc_data[vm1][3] - (self.struc_data[vm1][0]/ self.vol) * math.log2(self.struc_data[vm1][0] / volume) + \\\n                        self.struc_data[vm2][3] - (self.struc_data[vm2][0]/ self.vol) * math.log2(self.struc_data[vm2][0] / volume)\n                self.struc_data[vm1] = [volume, cut, vmSE, vmnodeSE]\n                self.struc_data.pop(vm2)\n\n                struc_data_2d_new = {}\n                for k in self.struc_data_2d.keys():\n                    if k[0] == vm2 or k[1] == vm2:\n                        continue\n                    elif k[0] == vm1 or k[1] == vm1:\n                        v1 = k[0]\n                        v2 = k[1]\n                        comm_merged = self.division[v1] + self.division[v2]\n                        gm = self.get_cut(comm_merged)\n                        vm = self.struc_data[v1][0] + self.struc_data[v2][0]\n                        if self.struc_data[v1][0] == 0 or self.struc_data[v2][0] == 0:\n                            vmSE = self.struc_data[v1][2] + self.struc_data[v2][2]\n                            vmnodeSE = self.struc_data[v1][3] + self.struc_data[v2][3]\n                        else:\n                            vmSE = - (gm / self.vol) * math.log2(vm / self.vol)\n                            vmnodeSE = self.struc_data[v1][3] - (self.struc_data[v1][0]/ self.vol) * math.log2(self.struc_data[v1][0] / vm) + \\\n                                    self.struc_data[v2][3] - (self.struc_data[v2][0]/ self.vol) * math.log2(self.struc_data[v2][0] / vm)\n                        struc_data_2d_new[k] = [vm, gm, vmSE, vmnodeSE]\n                    else:\n                        struc_data_2d_new[k] = self.struc_data_2d[k]\n                self.struc_data_2d = struc_data_2d_new\n            else:\n                break\n\n\ndef get_graph_edges(attributes):\n    attr_nodes_dict = {}\n    for i, l in enumerate(attributes):\n        for attr in l:\n            if attr not in attr_nodes_dict:\n                attr_nodes_dict[attr] = [i] # node indexing starts from 1\n            else:\n                attr_nodes_dict[attr].append(i)\n\n    for attr in attr_nodes_dict.keys():\n        attr_nodes_dict[attr].sort()\n\n    graph_edges = []\n    for l in attr_nodes_dict.values():\n        graph_edges += list(combinations(l, 2))\n    return list(set(graph_edges))\n\ndef get_knn_edges(embeddings, best_threshold):\n    corr_matrix = np.corrcoef(embeddings)\n    np.fill_diagonal(corr_matrix, 0)\n    knn_edges = [(s, d, corr_matrix[s, d]) for s, d in np.ndindex(corr_matrix.shape) if corr_matrix[s, d] >= best_threshold]\n        \n    return list(set(knn_edges))\n\ndef get_global_edges(attributes, embeddings, best_threshold, e_a = True, e_s = True):\n    graph_edges, knn_edges = [], []\n    if e_a == True:\n        graph_edges = get_graph_edges(attributes)\n    if e_s == True:\n        knn_edges = get_knn_edges(embeddings, best_threshold)\n    return list(set(knn_edges + graph_edges))\n\n\n\n\n    "}
{"type": "source_file", "path": "SocialED/dataset/dataloader.py", "content": "import numpy as np\nimport os\nfrom git import Repo, GitCommandError\nimport pandas as pd\nimport shutil\nfrom uuid import uuid4\nfrom datetime import datetime\nimport subprocess\nimport tempfile\n\nclass DatasetLoader:\n    r\"\"\"Base class for loading social event detection datasets.\n\n    .. note::\n        This is the base dataset loader class that provides common functionality for loading\n        and preprocessing social event detection datasets. All specific dataset loaders should\n        inherit from this class.\n\n    Parameters\n    ----------\n    dataset : str, optional\n        Name of the dataset to load.\n        Default: ``None``.\n    dir_path : str, optional\n        Custom directory path to load data from.\n        Default: ``None``.\n\n    Attributes\n    ----------\n    required_columns : list\n        Required columns that must be present in loaded datasets.\n    repo_url : str\n        URL of the repository containing the datasets.\n    target_folder : str \n        Target folder name for downloaded data.\n    \"\"\"\n    def __init__(self, dataset=None, dir_path=None):\n        self.dir_path = dir_path\n        self.dataset = dataset\n        self.default_root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), \"../dataset/data\"))\n        print(f\"Data root path: {self.default_root_path}\")  # 调试信息\n        os.makedirs(self.default_root_path, exist_ok=True)\n        \n        self.required_columns = [\n            'tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n            'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions'\n        ]\n        self.repo_url = \"https://github.com/ChenBeici/SocialED_datasets.git\"\n        self.target_folder = \"npy_data\"\n\n    def download_and_cleanup(self, repo_url, dataset_name, local_target_folder):\n        # 创建临时目录\n        local_repo_path = os.path.join(os.path.dirname(__file__), \"tmp\", str(uuid4()))\n        try:\n            print(f\"Downloading {dataset_name}.npy from {repo_url}\")\n            \n            # 克隆仓库\n            subprocess.run(['git', 'clone', '--branch', 'main', repo_url, local_repo_path], check=True)\n            \n            # 确保目标目录存在\n            os.makedirs(local_target_folder, exist_ok=True)\n            print(f\"Target directory: {local_target_folder}\")  # 调试信息\n            \n            # 搜索.npy文件\n            npy_files = []\n            for root, dirs, files in os.walk(local_repo_path):\n                for file in files:\n                    if file == f'{dataset_name}.npy':\n                        npy_files.append(os.path.join(root, file))\n            \n            if npy_files:\n                target_file = os.path.join(local_target_folder, f'{dataset_name}.npy')\n                print(f\"Copying from {npy_files[0]} to {target_file}\")  # 调试信息\n                shutil.copy2(npy_files[0], target_file)\n                return True\n            else:\n                print(f\"Error: {dataset_name}.npy not found in repository\")\n                return False\n                \n        except Exception as e:\n            print(f\"Error during download: {str(e)}\")\n            return False\n        finally:\n            if os.path.exists(local_repo_path):\n                shutil.rmtree(local_repo_path)\n\n    def download(self):\n        local_target_folder = os.path.join(self.default_root_path, self.dataset)\n        return self.download_and_cleanup(\n            self.repo_url,\n            self.dataset,\n            local_target_folder\n        )\n\n    def load_data(self):\n        \"\"\"Temporary implementation that returns empty dataset\"\"\"\n        print(f\"Loading {self.dataset} dataset (mock data)\")\n        return {\n            'texts': [],\n            'labels': [],\n            'metadata': {'name': self.dataset}\n        }\n\n    def get_dataset_language(self):\n        \"\"\"\n        Determine the language based on the current dataset.\n        \n        Returns:\n            str: The language of the dataset ('English', 'French', 'Arabic').\n        \"\"\"\n        dataset_language_map = {\n            'MAVEN': 'English',\n            'Event2012': 'English', \n            'Event2018': 'French',\n            'Arabic_Twitter': 'Arabic',\n            'CrisisLexT26': 'English',\n            'CrisisLexT6': 'English', \n            'CrisisMMD': 'English',\n            'CrisisNLP': 'English',\n            'HumAID': 'English',\n            'Mix_Data': 'English',\n            'KBP': 'English',\n            'Event2012_100': 'English',\n            'Event2018_100': 'French',\n            'Arabic_7': 'Arabic'\n        }\n        \n        language = dataset_language_map.get(self.dataset)\n        if not language:\n            raise ValueError(f\"Unsupported dataset: {self.dataset}. Supported datasets are: {', '.join(dataset_language_map.keys())}\")\n        return language\n\n    def get_dataset_name(self):\n        \"\"\"\n        Get the name of the current dataset.\n        \n        Returns:\n            str: The name of the dataset.\n        \"\"\"\n        return self.dataset\n    \n\n\n    def get_dataset_info(self):\n        \"\"\"\n        Get the info of the current dataset.\n        \n        Returns:\n            list: The info of the dataset.\n        \"\"\"\n\n        df = self.load_data().sort_values(by='created_at').reset_index()\n        print(self.get_dataset_name())\n        print(self.get_dataset_language())\n        print(\"Columns:\", df.columns.tolist())\n\n        print(\"First row:\")\n        print(df.iloc[0].to_dict())\n        print(\"DataFrame shape:\", df.shape)\n\n\n        \n        #return end\n\n\nclass MAVEN(DatasetLoader):\n    r\"\"\"The MAVEN dataset for social event detection.\n\n    .. note::\n        This dataset contains English language social media posts related to various events.\n        The dataset provides text content and event labels for social event detection tasks.\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='MAVEN', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"MAVEN dataset loaded successfully.\")\n\n        return df\n\nclass CrisisNLP(DatasetLoader):\n    r\"\"\"The CrisisNLP dataset for social event detection.\n\n    .. note::\n        This dataset contains English language social media posts related to crisis events.\n        The dataset provides text content and event labels for crisis event detection tasks.\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='CrisisNLP', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"CrisisNLP dataset loaded successfully.\")\n        return df\n\nclass Event2012(DatasetLoader):\n    r\"\"\"The Event2012 dataset for social event detection.\n\n    .. note::\n        This dataset contains English language social media posts from 2012.\n        The dataset provides text content and event labels for social event detection tasks.\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='Event2012', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"Event2012 dataset loaded successfully.\")\n        return df\n\n\nclass Event2018(DatasetLoader):\n    r\"\"\"The Event2018 dataset for social event detection.\n\n    .. note::\n        This dataset contains French language social media posts from 2018.\n        The dataset provides text content and event labels for social event detection tasks.\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='Event2018', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"Event2018 dataset loaded successfully.\")\n        return df\n\n\nclass Arabic_Twitter(DatasetLoader):\n    r\"\"\"The Arabic Twitter dataset for social event detection.\n\n    .. note::\n        This dataset contains Arabic language tweets related to various events.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='Arabic_Twitter', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"Arabic Twitter dataset loaded successfully.\")\n        return df\n\nclass CrisisLexT26(DatasetLoader):\n    r\"\"\"The CrisisLexT26 dataset for social event detection.\n\n    .. note::\n        This dataset contains tweets related to 26 different crisis events.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='CrisisLexT26', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"CrisisLexT26 dataset loaded successfully.\")\n        return df\n\nclass CrisisMMD(DatasetLoader):\n    r\"\"\"The CrisisMMD dataset for social event detection.\n\n    .. note::\n        This dataset contains multimodal crisis-related social media data.\n        The dataset provides text, images and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='CrisisMMD', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"CrisisMMD dataset loaded successfully.\")\n        return df\n\nclass HumAID(DatasetLoader):\n    r\"\"\"The HumAID dataset for social event detection.\n\n    .. note::\n        This dataset contains tweets related to humanitarian crises and disasters.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='HumAID', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"HumAID dataset loaded successfully.\")\n        return df\n\nclass KBP(DatasetLoader):\n    r\"\"\"The KBP dataset for social event detection.\n\n    .. note::\n        This dataset contains knowledge base population event data.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='KBP', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"KBP dataset loaded successfully.\")\n        return df\n\nclass Arabic_7(DatasetLoader):\n    r\"\"\"The Arabic_7 dataset for social event detection.\n\n    .. note::\n        This dataset contains Arabic language social media posts for 7 event types.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='Arabic_7', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"Arabic_7 dataset loaded successfully.\")\n        return df\n\nclass Event2012_100(DatasetLoader):\n    r\"\"\"The Event2012_100 dataset for social event detection.\n\n    .. note::\n        This dataset contains tweets from 2012 related to 100 different events.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='Event2012_100', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"Event2012_100 dataset loaded successfully.\")\n        return df\n\nclass Event2018_100(DatasetLoader):\n    r\"\"\"The Event2018_100 dataset for social event detection.\n\n    .. note::\n        This dataset contains tweets from 2018 related to 100 different events.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='Event2018_100', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"Event2018_100 dataset loaded successfully.\")\n        return df\n\nclass Mix_Data(DatasetLoader):\n    r\"\"\"The Mix_Data dataset for social event detection.\n\n    .. note::\n        This dataset contains a mixture of social media data from various sources.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='Mix_Data', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"Mix_Data dataset loaded successfully.\")\n        return df\n\nclass CrisisLexT6(DatasetLoader):\n    r\"\"\"The CrisisLexT6 dataset for social event detection.\n\n    .. note::\n        This dataset contains tweets related to 6 different crisis events.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='CrisisLexT6', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"CrisisLexT6 dataset loaded successfully.\")\n        return df\n\nclass CrisisLexT7(DatasetLoader):\n    r\"\"\"The CrisisLexT7 dataset for social event detection.\n\n    .. note::\n        This dataset contains tweets related to 7 different crisis events.\n        The dataset provides text content and event labels for social event detection tasks.\n\n    \"\"\"\n    def __init__(self, dir_path=None):\n        super().__init__(dataset='CrisisLexT7', dir_path=dir_path)\n    \n    def load_data(self):\n        dataset_path = os.path.join(self.default_root_path, self.dataset)\n        print(f\"Dataset path: {dataset_path}\")  \n        \n        if not os.path.exists(dataset_path) or not os.listdir(dataset_path):\n            print(f\"Directory {dataset_path} does not exist or is empty, downloading...\")\n            if not self.download():\n                raise RuntimeError(\"Failed to download dataset\")\n        \n        file_path = os.path.join(dataset_path, f'{self.dataset}.npy')\n        print(f\"Loading file from: {file_path}\")  \n        \n        if not os.path.exists(file_path):\n            print(f\"File not found at: {file_path}\")\n            print(f\"Directory contents: {os.listdir(dataset_path) if os.path.exists(dataset_path) else 'Directory does not exist'}\")\n            raise FileNotFoundError(f\"Data file not found at {file_path}\")\n        \n        data = np.load(file_path, allow_pickle=True)\n        df = pd.DataFrame(data, columns=self.required_columns)\n        print(\"CrisisLexT7 dataset loaded successfully.\")\n        return df\n\n\n\nif __name__ == \"__main__\":\n    # Test MAVEN dataset\n    #maven = MAVEN()\n    #dataset = MAVEN().load_data()\n    print(Event2018().get_dataset_name())\n    print(Event2018().get_dataset_language())\n"}
{"type": "source_file", "path": "SocialED/detector/sbert.py", "content": "import os\nimport pandas as pd\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport logging\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n# Setup logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\nclass SBERT:\n    r\"\"\"The SBERT model for social event detection that uses Sentence-BERT \n    for text embedding and event detection.\n\n    .. note::\n        This detector uses Sentence-BERT to generate text embeddings for identifying events \n        in social media data. The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    model_name : str, optional\n        Path or name of the SBERT model to use.\n        Default: ``'../model/model_needed/paraphrase-MiniLM-L6-v2'``\n    df : pandas.DataFrame, optional\n        Processed dataframe. Default: ``None``\n    train_df : pandas.DataFrame, optional\n        Training dataframe. Default: ``None``\n    test_df : pandas.DataFrame, optional\n        Test dataframe. Default: ``None``\n    \"\"\"\n    def __init__(self,\n                 dataset,\n                 model_name='../model/model_needed/paraphrase-MiniLM-L6-v2',\n                 df=None,\n                 train_df=None,\n                 test_df=None, ):\n        self.dataset = dataset.load_data()\n        if os.path.exists(model_name):\n            self.model_name = model_name\n        else:\n            self.model_name = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n        self.df = df\n        self.train_df = train_df\n        self.test_df = test_df\n        self.model = SentenceTransformer(self.model_name)\n\n    def preprocess(self):\n        \"\"\"\n        Data preprocessing: tokenization, stop words removal, etc.\n        \"\"\"\n        df = self.dataset\n        df['processed_text'] = df['filtered_words'].apply(\n            lambda x: ' '.join([str(word).lower() for word in x]) if isinstance(x, list) else '')\n        self.df = df\n        return df\n\n    def get_sbert_embeddings(self, text):\n        \"\"\"\n        Get SBERT embeddings for a given text.\n        \"\"\"\n        return self.model.encode(text)\n\n    def fit(self):\n        pass\n\n    def detection(self):\n        \"\"\"\n        Detect events by comparing SBERT embeddings.\n        \"\"\"\n        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=42)\n        self.train_df = train_df\n        self.test_df = test_df\n\n        logging.info(\"Calculating SBERT embeddings for the training set...\")\n        train_df['sbert_embedding'] = train_df['processed_text'].apply(self.get_sbert_embeddings)\n        logging.info(\"SBERT embeddings calculated for the training set.\")\n\n        logging.info(\"Calculating SBERT embeddings for the test set...\")\n        test_df['sbert_embedding'] = test_df['processed_text'].apply(self.get_sbert_embeddings)\n        logging.info(\"SBERT embeddings calculated for the test set.\")\n\n        train_embeddings = np.stack(self.train_df['sbert_embedding'].values)\n        test_embeddings = np.stack(self.test_df['sbert_embedding'].values)\n\n        predictions = []\n        for test_emb in test_embeddings:\n            distances = np.linalg.norm(train_embeddings - test_emb, axis=1)\n            closest_idx = np.argmin(distances)\n            predictions.append(self.train_df.iloc[closest_idx]['event_id'])\n\n        ground_truths = self.test_df['event_id'].tolist()\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/bilstm.py", "content": "import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport os\nimport pandas as pd\nfrom collections import Counter\nfrom itertools import combinations\nfrom time import time\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport argparse\nimport logging\nimport spacy\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n\n\nclass BiLSTM:\n    r\"\"\"The BiLSTM model for social event detection that uses bidirectional LSTM \n    to detect events in social media data.\n\n    .. note::\n        This detector uses bidirectional LSTM to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    lr : float, optional\n        Learning rate for optimizer. Default: ``1e-3``.\n    batch_size : int, optional\n        Batch size for training. Default: ``1000``.\n    dropout_keep_prob : float, optional\n        Dropout keep probability. Default: ``0.8``.\n    embedding_size : int, optional\n        Size of word embeddings. Default: ``300``.\n    max_size : int, optional\n        Maximum vocabulary size. Default: ``5000``.\n    seed : int, optional\n        Random seed for reproducibility. Default: ``1``.\n    num_hidden_nodes : int, optional\n        Number of LSTM hidden nodes. Default: ``32``.\n    hidden_dim2 : int, optional\n        Size of second hidden layer. Default: ``64``.\n    num_layers : int, optional\n        Number of LSTM layers. Default: ``1``.\n    bi_directional : bool, optional\n        Whether to use bidirectional LSTM. Default: ``True``.\n    pad_index : int, optional\n        Index used for padding. Default: ``0``.\n    num_epochs : int, optional\n        Number of training epochs. Default: ``20``.\n    margin : int, optional\n        Margin for triplet loss. Default: ``3``.\n    max_len : int, optional\n        Maximum sequence length. Default: ``10``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/Bilstm/'``.\n    \"\"\"\n    def __init__(self, dataset,\n                 lr=1e-3,\n                 batch_size=1000,\n                 dropout_keep_prob=0.8,\n                 embedding_size=300,\n                 max_size=5000,\n                 seed=1,\n                 num_hidden_nodes=32,\n                 hidden_dim2=64,\n                 num_layers=1,\n                 bi_directional=True,\n                 pad_index=0,\n                 num_epochs=20,\n                 margin=3,\n                 max_len=10,\n                 file_path='../model/model_saved/Bilstm/'):\n        self.dataset = dataset.load_data()\n        self.lr = lr\n        self.batch_size = batch_size\n        self.dropout_keep_prob = dropout_keep_prob\n        self.embedding_size = embedding_size\n        self.max_size = max_size\n        self.seed = seed\n        self.num_hidden_nodes = num_hidden_nodes\n        self.hidden_dim2 = hidden_dim2\n        self.num_layers = num_layers\n        self.bi_directional = bi_directional\n        self.pad_index = pad_index\n        self.num_epochs = num_epochs\n        self.margin = margin\n        self.max_len = max_len\n        self.file_path = file_path\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.df = None\n        self.train_df = None\n        self.test_df = None\n        self.word2idx = None\n        self.idx2word = None\n        self.weight = None\n\n    # Add the rest of the class methods here, like preprocess, fit, detection, etc.\n\n    def preprocess(self):\n        \"\"\"\n        Data preprocessing: tokenization, stop words removal, etc.\n        \"\"\"\n        self.split()\n        df = self.dataset[['event_id', 'words', 'filtered_words']].copy()\n\n        # Tokenize tweets\n        # f_batch_text = df.iloc[:, 5]\n        f_batch_text = df.iloc[:, 2]\n        logging.info(\"Extracted tweets.\")\n\n        # Count unique words (converted to lowercases)\n        words = Counter()\n        for tweet in f_batch_text.values:\n            words.update(w.lower() for w in tweet)\n\n        # Convert words from counter to list (sorted by frequencies from high to low)\n        words = [key for key, _ in words.most_common()]\n        words = ['_PAD', '_UNK'] + words\n        logging.info('Extracted unique words.')\n\n        # Construct a mapping of words to indices and vice versa\n        self.word2idx = {o: i for i, o in enumerate(words)}\n        self.idx2word = {i: o for i, o in enumerate(words)}\n\n        # Save\n        os.makedirs(self.file_path, exist_ok=True)\n        np.save(self.file_path + 'word2idx.npy', self.word2idx)\n        np.save(self.file_path + 'idx2word.npy', self.idx2word)\n        logging.info('Constructed and saved word2idx and idx2word maps.')\n\n        # Load\n        self.word2idx = np.load(self.file_path + 'word2idx.npy', allow_pickle='TRUE').item()\n        logging.info('word2idx map loaded.')\n\n        df[\"wordsidx\"] = df.words.apply(\n            lambda tweet: [self.word2idx.get(w.lower(), self.word2idx['_UNK']) for w in tweet])\n        logging.info('Tokenized tweets in the df to word indices.')\n\n        self.df = df\n        return df\n\n    def split(self):\n        \"\"\"\n        Split the dataset into training, validation, and test sets.\n        \"\"\"\n        train_ratio = 0.7\n        test_ratio = 0.2\n        val_ratio = 0.1\n\n        df = self.dataset\n\n        train_data, temp_data = train_test_split(df, test_size=(1 - train_ratio), random_state=42)\n        test_size = test_ratio / (test_ratio + val_ratio)\n        test_data, val_data = train_test_split(temp_data, test_size=test_size, random_state=42)\n\n        os.makedirs(self.file_path + '/split_indices/', exist_ok=True)\n        np.save(self.file_path + '/split_indices/train_indices_7170.npy', train_data.index.to_numpy())\n        np.save(self.file_path + '/split_indices/test_indices_2048.npy', test_data.index.to_numpy())\n        np.save(self.file_path + '/split_indices/val_indices_1024.npy', val_data.index.to_numpy())\n\n        os.makedirs(self.file_path + '/split_data/', exist_ok=True)\n        train_data.to_numpy().dump(self.file_path + '/split_data/train_data_7170.npy')\n        test_data.to_numpy().dump(self.file_path + '/split_data/test_data_2048.npy')\n        val_data.to_numpy().dump(self.file_path + '/split_data/val_data_1024.npy')\n\n        self.train_df = train_data\n        self.test_df = test_data\n        self.val_df = val_data\n\n        logging.info(\n            f\"Data split completed: {len(train_data)} train, {len(test_data)} test, {len(val_data)} validation samples.\")\n\n    def load_embeddings(self):\n        \"\"\"\n        Load pre-trained word embeddings.\n        \"\"\"\n        # Initialize weight matrix with zeros\n        self.weight = np.zeros((len(self.word2idx), self.embedding_size), dtype=np.float64)\n\n        # Load pre-trained word2vec model\n        start = time()\n        nlp = spacy.load(\"en_core_web_lg\")\n        logging.info('Word2vec model took {:.2f} mins to load.'.format((time() - start) / 60))\n\n        # Update word embeddings to weight\n        for i in range(len(self.word2idx)):\n            w = self.idx2word.get(i)\n            token = nlp(w)\n            if token.has_vector:\n                self.weight[i] = token.vector\n        logging.info('Word embeddings extracted. Shape: {}'.format(self.weight.shape))\n\n        # Save and load word embeddings\n        np.save(self.file_path + 'word_embeddings.npy', self.weight)\n        logging.info('Word embeddings saved.')\n        self.weight = np.load(self.file_path + 'word_embeddings.npy')\n        logging.info('Word embeddings loaded. Shape: {}'.format(self.weight.shape))\n        self.weight = torch.tensor(self.weight, dtype=torch.float)\n\n    def train(self, model, train_iterator, optimizer, loss_func, log_interval=40):\n        \"\"\"\n        Train the BiLSTM model.\n        \"\"\"\n        n_batches = len(train_iterator)\n        epoch_loss = 0\n        for i, batch in enumerate(train_iterator):\n            optimizer.zero_grad()\n            text, text_lengths = batch['text']\n            predictions = model(text, text_lengths)\n            loss, num_triplets = loss_func(predictions, batch['label'])\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            if i % log_interval == 0:\n                print(\n                    f'\\tBatch: [{i}/{n_batches} ({100. * (i + 1) / n_batches:.0f}%)]\\tLoss: {epoch_loss / (i + 1):.4f}\\tNum_triplets: {num_triplets}')\n        return epoch_loss / n_batches\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n    def run_train(self, epochs, model, train_iterator, test_iterator, optimizer, loss_func):\n        \"\"\"\n        Run the training and evaluation process for the BiLSTM model.\n        \"\"\"\n        all_nmi, all_ami, all_ari, all_predictions, all_labels = [], [], [], [], []\n\n        for epoch in range(epochs):\n            # Train the model\n            start = time()\n            print(f'Epoch {epoch}. Training.')\n            train_loss = self.train(model, train_iterator, optimizer, loss_func)\n            print(f'\\tTrain Loss: {train_loss:.4f}')\n            print(f'\\tThis epoch took {(time() - start) / 60:.2f} mins to train.')\n\n            # Evaluate the model\n            start = time()\n            print(f'Epoch {epoch}. Evaluating.')\n            model.eval()\n            with torch.no_grad():\n                for i, batch in enumerate(test_iterator):\n                    assert i == 0  # cluster all the test tweets at once\n                    text, text_lengths = batch['text']\n                    predictions = model(text, text_lengths)\n\n                    assert predictions.shape[0] == batch['label'].shape[0]\n                    n_classes = len(set(batch['label'].tolist()))\n                    kmeans = KMeans(n_clusters=n_classes, n_init=10, random_state=0).fit(predictions)\n                    predictions = kmeans.labels_\n\n                    validate_nmi = metrics.normalized_mutual_info_score(batch['label'], predictions)\n                    validate_ami = metrics.adjusted_mutual_info_score(batch['label'], predictions)\n                    validate_ari = metrics.adjusted_rand_score(batch['label'], predictions)\n\n            all_nmi.append(validate_nmi)\n            all_ami.append(validate_ami)\n            all_ari.append(validate_ari)\n            all_predictions.append(predictions)\n            all_labels.append(batch['label'])\n\n            print(f'\\tVal. NMI: {validate_nmi:.4f}')\n            print(f'\\tVal. AMI: {validate_ami:.4f}')\n            print(f'\\tVal. ARI: {validate_ari:.4f}')\n            print(f'\\tThis epoch took {(time() - start) / 60:.2f} mins to evaluate.')\n\n        return all_nmi, all_ami, all_ari, all_predictions, all_labels\n\n    def fit(self):\n        \"\"\"\n        Fit the model on the training data and save the best model.\n        \"\"\"\n        self.load_embeddings()\n\n        # Split training and test datasets\n        train_mask = list(np.load(self.file_path + '/split_indices/train_indices_7170.npy', allow_pickle=True))\n        test_mask = list(np.load(self.file_path + '/split_indices/test_indices_2048.npy', allow_pickle=True))\n\n        train_data = VectorizeData(self.df.iloc[train_mask, :].copy().reset_index(drop=True), self.max_len)\n        test_data = VectorizeData(self.df.iloc[test_mask, :].copy().reset_index(drop=True), self.max_len)\n\n        # Construct training and test iterator\n        train_iterator = DataLoader(train_data, batch_size=self.batch_size, shuffle=True)\n        test_iterator = DataLoader(test_data, batch_size=len(test_data), shuffle=True)\n\n        # Loss function\n        loss_func = OnlineTripletLoss(self.margin, RandomNegativeTripletSelector(self.margin))\n\n        # Model\n        lstm_model = LSTM(self.embedding_size, self.weight, self.num_hidden_nodes, self.hidden_dim2,\n                          self.num_layers, self.bi_directional, self.dropout_keep_prob, self.pad_index,\n                          self.batch_size)\n\n        # Optimizer\n        optimizer = torch.optim.Adam(lstm_model.parameters(), lr=self.lr)\n\n        # Train and evaluation\n        all_nmi, all_ami, all_ari, all_predictions, all_labels = self.run_train(self.num_epochs, lstm_model,\n                                                                                train_iterator, test_iterator,\n                                                                                optimizer, loss_func)\n\n        best_epoch = [i for i, j in enumerate(all_nmi) if j == max(all_nmi)][0]\n        print(\"all_nmi: \", all_nmi)\n        print(\"all_ami: \", all_ami)\n        print(\"all_ari: \", all_ari)\n        print(\"\\nTraining completed. Best results at epoch \", best_epoch)\n\n        # Save the best model\n        self.best_model_path = os.path.join(self.file_path, \"best_model.pth\")\n        torch.save(lstm_model.state_dict(), self.best_model_path)\n        print(f\"Best model saved at {self.best_model_path}\")\n\n        self.best_epoch = best_epoch\n        self.best_model = lstm_model\n\n    def detection(self):\n        \"\"\"\n        Detect events using the best trained model on the test data.\n        \"\"\"\n        # Load the best model\n        lstm_model = LSTM(self.embedding_size, self.weight, self.num_hidden_nodes, self.hidden_dim2,\n                          self.num_layers, self.bi_directional, self.dropout_keep_prob, self.pad_index,\n                          self.batch_size)\n        lstm_model.load_state_dict(torch.load(self.best_model_path))\n        lstm_model.eval()\n\n        # Load test data\n        test_mask = list(np.load(self.file_path + '/split_indices/test_indices_2048.npy', allow_pickle=True))\n        test_data = VectorizeData(self.df.iloc[test_mask, :].copy().reset_index(drop=True), self.max_len)\n        test_iterator = DataLoader(test_data, batch_size=len(test_data), shuffle=False)\n\n        with torch.no_grad():\n            for i, batch in enumerate(test_iterator):\n                assert i == 0  # Process all test tweets at once\n                text, text_lengths = batch['text']\n                predictions = lstm_model(text, text_lengths)\n\n                assert predictions.shape[0] == batch['label'].shape[0]\n                n_classes = len(set(batch['label'].tolist()))\n                kmeans = KMeans(n_clusters=n_classes, n_init=10, random_state=0).fit(predictions)\n                predictions = kmeans.labels_\n\n                ground_truths = batch['label']\n\n        return ground_truths, predictions\n\n\nclass LSTM(nn.Module):\n    # define all the layers used in model\n    def __init__(self, embedding_dim, weight, lstm_units, hidden_dim, lstm_layers,\n                 bidirectional, dropout, pad_index, batch_size):\n        super().__init__()\n        # self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n        # use pretrained embeddings\n        self.embedding = nn.Embedding.from_pretrained(weight, padding_idx=pad_index)\n        self.lstm = nn.LSTM(embedding_dim,\n                            lstm_units,\n                            num_layers=lstm_layers,\n                            bidirectional=bidirectional,\n                            batch_first=True)\n        num_directions = 2 if bidirectional else 1\n        self.fc1 = nn.Linear(lstm_units * num_directions, hidden_dim)\n        # self.fc2 = nn.Linear(hidden_dim, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout)\n        self.lstm_layers = lstm_layers\n        self.num_directions = num_directions\n        self.lstm_units = lstm_units\n\n    def init_hidden(self, batch_size):\n        h, c = (Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.lstm_units)),\n                Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.lstm_units)))\n        return h, c\n\n    def forward(self, text, text_lengths):\n        batch_size = text.shape[0]\n        h_0, c_0 = self.init_hidden(batch_size)\n\n        embedded = self.embedding(text)\n        packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n        # output of shape (batch, seq_len, num_directions * hidden_size): tensor containing the \n        # output features (h_t) from the last layer of the LSTM, for each t.\n        output, (h_n, c_n) = self.lstm(packed_embedded, (h_0, c_0))\n        output_unpacked, output_lengths = pad_packed_sequence(output, batch_first=True)\n        # get the hidden state of the last time step \n        out = output_unpacked[:, -1, :]\n        rel = self.relu(out)\n        dense1 = self.fc1(rel)\n        # drop = self.dropout(dense1)\n        # preds = self.fc2(drop)\n        preds = self.dropout(dense1)\n        return preds\n\n\nclass VectorizeData(Dataset):\n    def __init__(self, df, max_len):\n\n        self.df = df\n        self.maxlen = max_len\n        self.df[\"lengths\"] = self.df.wordsidx.apply(lambda x: self.maxlen if len(x) > self.maxlen else len(x))\n        self.df = self.df[self.df[\"lengths\"] > 0].reset_index(drop=True)\n        self.df[\"wordsidxpadded\"] = self.df.wordsidx.apply(self.pad_data)\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        x = self.df.wordsidxpadded[idx]\n        lens = self.df.lengths[idx]  # truncated tweet length\n        y = self.df.event_id[idx]\n        sample = {'text': (x, lens), 'label': y}\n        return sample\n\n    def pad_data(self, tweet):\n        padded = np.zeros((self.maxlen,), dtype=np.int64)\n        if len(tweet) > self.maxlen:\n            padded[:] = tweet[:self.maxlen]\n        else:\n            padded[:len(tweet)] = tweet\n        return padded\n\n\nclass OnlineTripletLoss(nn.Module):\n    \"\"\"\n    Online Triplets loss\n    Takes a batch of embeddings and corresponding labels.\n    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n    triplets\n    \"\"\"\n\n    def __init__(self, margin, triplet_selector):\n        super(OnlineTripletLoss, self).__init__()\n        self.margin = margin\n        self.triplet_selector = triplet_selector\n\n    def forward(self, embeddings, target):\n        triplets = self.triplet_selector.get_triplets(embeddings, target)\n\n        if embeddings.is_cuda:\n            triplets = triplets.cuda()\n\n        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(ap_distances - an_distances + self.margin)\n\n        return losses.mean(), len(triplets)\n\n\ndef pdist(vectors):\n    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n        dim=1).view(-1, 1)\n    return distance_matrix\n\n\nclass TripletSelector:\n    \"\"\"\n    Implementation should return indices of anchors, positive and negative samples\n    return np array of shape [N_triplets x 3]\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def get_triplets(self, embeddings, labels):\n        raise NotImplementedError\n\n\nclass FunctionNegativeTripletSelector(TripletSelector):\n    \"\"\"\n    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n    Margin should match the margin used in triplet loss.\n    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n    and return a negative index for that pair\n    \"\"\"\n\n    def __init__(self, margin, negative_selection_fn, cpu=True):\n        super(FunctionNegativeTripletSelector, self).__init__()\n        self.cpu = cpu\n        self.margin = margin\n        self.negative_selection_fn = negative_selection_fn\n\n    def get_triplets(self, embeddings, labels):\n        if self.cpu:\n            embeddings = embeddings.cpu()\n        distance_matrix = pdist(embeddings)\n        distance_matrix = distance_matrix.cpu()\n\n        labels = labels.cpu().data.numpy()\n        triplets = []\n\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            negative_indices = np.where(np.logical_not(label_mask))[0]\n            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n            anchor_positives = np.array(anchor_positives)\n\n            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n                loss_values = ap_distance - distance_matrix[\n                    torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n                loss_values = loss_values.data.cpu().numpy()\n                hard_negative = self.negative_selection_fn(loss_values)\n                if hard_negative is not None:\n                    hard_negative = negative_indices[hard_negative]\n                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n\n        if len(triplets) == 0:\n            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n\n        triplets = np.array(triplets)\n\n        return torch.LongTensor(triplets)\n\n\ndef random_hard_negative(loss_values):\n    hard_negatives = np.where(loss_values > 0)[0]\n    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n\n\ndef hardest_negative(loss_values):\n    hard_negative = np.argmax(loss_values)\n    return hard_negative if loss_values[hard_negative] > 0 else None\n\n\ndef HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                              negative_selection_fn=hardest_negative,\n                                                                                              cpu=cpu)\n\n\ndef RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                             negative_selection_fn=random_hard_negative,\n                                                                                             cpu=cpu)\n\n"}
{"type": "source_file", "path": "SocialED/detector/kpgnn.py", "content": "import numpy as np\nimport json\nimport argparse\nfrom torch.utils.data import Dataset\nimport dgl\nimport dgl.function as fn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom itertools import combinations\nimport time\nfrom time import localtime, strftime\nimport os\nimport pickle\nfrom scipy import sparse\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport sys\nimport pandas as pd\nimport spacy\nfrom datetime import datetime\nimport networkx as nx\nfrom dgl.dataloading import MultiLayerNeighborSampler, NodeDataLoader\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n\n\n\nclass KPGNN():\n    r\"\"\"The KPGNN model for social event detection that uses knowledge-preserving graph neural networks\n    for event detection.\n\n    .. note::\n        This detector uses graph neural networks with knowledge preservation to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    See :cite:`wang2020kpgnn` for details.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    n_epochs : int, optional\n        Number of training epochs. Default: ``15``.\n    n_infer_epochs : int, optional\n        Number of inference epochs. Default: ``0``.\n    window_size : int, optional\n        Size of sliding window. Default: ``3``.\n    patience : int, optional\n        Early stopping patience. Default: ``5``.\n    margin : float, optional\n        Margin for triplet loss. Default: ``3.0``.\n    lr : float, optional\n        Learning rate for optimizer. Default: ``1e-3``.\n    batch_size : int, optional\n        Batch size for training. Default: ``200``.\n    n_neighbors : int, optional\n        Number of neighbors to sample. Default: ``800``.\n    hidden_dim : int, optional\n        Hidden layer dimension. Default: ``8``.\n    out_dim : int, optional\n        Output dimension. Default: ``32``.\n    num_heads : int, optional\n        Number of attention heads. Default: ``4``.\n    use_residual : bool, optional\n        Whether to use residual connections. Default: ``True``.\n    validation_percent : float, optional\n        Percentage of data for validation. Default: ``0.2``.\n    use_hardest_neg : bool, optional\n        Whether to use hardest negative mining. Default: ``False``.\n    use_dgi : bool, optional\n        Whether to use deep graph infomax. Default: ``False``.\n    remove_obsolete : int, optional\n        Number of epochs before removing obsolete data. Default: ``2``.\n    is_incremental : bool, optional\n        Whether to use incremental learning. Default: ``False``.\n    use_cuda : bool, optional\n        Whether to use GPU acceleration. Default: ``False``.\n    data_path : str, optional\n        Path to save model data. Default: ``'../model/model_saved/kpgnn/kpgnn_incremental_test'``.\n    mask_path : str, optional\n        Path to mask file. Default: ``None``.\n    resume_path : str, optional\n        Path to resume training from. Default: ``None``.\n    resume_point : int, optional\n        Epoch to resume from. Default: ``0``.\n    resume_current : bool, optional\n        Whether to resume from current state. Default: ``True``.\n    log_interval : int, optional\n        Number of steps between logging. Default: ``10``.\n    \"\"\"\n    def __init__(\n        self,\n        dataset,\n        n_epochs=15,\n        n_infer_epochs=0,\n        window_size=3,\n        patience=5,\n        margin=3.0,\n        lr=1e-3,\n        batch_size=200,\n        n_neighbors=800,\n        hidden_dim=8,\n        out_dim=32,\n        num_heads=4,\n        use_residual=True,\n        validation_percent=0.2,\n        use_hardest_neg=False,\n        use_dgi=False,\n        remove_obsolete=2,\n        is_incremental=False,\n        use_cuda=False,\n        data_path='../model/model_saved/kpgnn/kpgnn_incremental_test',\n        mask_path=None,\n        resume_path=None,\n        resume_point=0,\n        resume_current=True,\n        log_interval=10\n    ):        \n        # 数据集\n        self.dataset = dataset.load_data()\n\n        # 训练参数\n        self.n_epochs = n_epochs\n        self.n_infer_epochs = n_infer_epochs\n        self.lr = lr\n        self.batch_size = batch_size\n        self.patience = patience\n        self.margin = margin\n        self.validation_percent = validation_percent\n        self.log_interval = log_interval\n\n        # 模型结构参数\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n        self.num_heads = num_heads\n        self.use_residual = use_residual\n        self.n_neighbors = n_neighbors\n        self.window_size = window_size\n\n        # 训练策略\n        self.use_hardest_neg = use_hardest_neg\n        self.use_dgi = use_dgi\n        self.remove_obsolete = remove_obsolete\n        self.is_incremental = is_incremental\n\n        # 硬件与路径\n        self.use_cuda = use_cuda\n        self.data_path = data_path\n        self.mask_path = mask_path\n        self.resume_path = resume_path\n        self.resume_point = resume_point\n        self.resume_current = resume_current\n        \n        self.resume_path = None\n        self.model = None\n        self.loss_fn = None\n        self.loss_fn_dgi = None\n        self.metrics = None\n        self.train_indices = None\n        self.indices_to_remove = None\n        self.embedding_save_path = None\n        self.data_split = None\n\n\n    def preprocess(self):\n        preprocessor = Preprocessor(self.dataset)\n        preprocessor.generate_initial_features(self.dataset)\n        preprocessor.custom_message_graph(self.dataset)\n\n    def fit(self):\n        use_cuda = self.use_cuda and torch.cuda.is_available()\n        print(\"Using CUDA:\", use_cuda)\n        os.makedirs(self.data_path, exist_ok=True)\n\n        # make dirs and save args\n        if self.resume_path is None:  # build a new dir if training from scratch\n            self.embedding_save_path = self.data_path + '/embeddings_' + strftime(\"%m%d%H%M%S\", localtime())\n            os.mkdir(self.embedding_save_path)\n\n        # resume training using original dir\n        else:\n            self.embedding_save_path = self.resume_path\n        print(\"embedding_save_path: \", self.embedding_save_path)\n\n        # with open(self.embedding_save_path + '/args.txt', 'w') as f:\n        #     json.dump(self.__dict__, f, indent=2)\n\n        # Load data splits\n        self.data_split = np.load(self.data_path + '/data_split.npy')\n\n        # Loss\n        if self.use_hardest_neg:\n            self.loss_fn = OnlineTripletLoss(self.margin, HardestNegativeTripletSelector(self.margin))\n        else:\n            self.loss_fn = OnlineTripletLoss(self.margin, RandomNegativeTripletSelector(self.margin))\n        if self.use_dgi:\n            self.loss_fn_dgi = torch.nn.BCEWithLogitsLoss()\n\n        self.metrics = [AverageNonzeroTripletsMetric()]\n\n        train_i = 0\n        print(\"1embedding_save_path: \", self.embedding_save_path)\n        if ((self.resume_path is not None) and (self.resume_point == 0) and (\n                self.resume_current)) or self.resume_path is None:\n            if not self.use_dgi:\n                print(\"12embedding_save_path: \", self.embedding_save_path)\n                # 在调用 initial_maintain 之前打印参数\n                print(\"Before calling initial_maintain:\")\n                print(\"train_i:\", train_i)\n                print(\"i:\", 0)\n                print(\"data_split:\", self.data_split)\n                print(\"metrics:\", self.metrics)\n                print(\"embedding_save_path:\", self.embedding_save_path)\n                print(\"loss_fn:\", self.loss_fn)\n                print(\"model:\", self.model)\n\n                self.train_indices, self.indices_to_remove, self.model = KPGNN_model(self).initial_maintain(train_i, 0,\n                                                                                                      self.data_split,\n                                                                                                      self.metrics,\n                                                                                                      self.embedding_save_path,\n                                                                                                      self.loss_fn,\n                                                                                                      self.model)\n            else:\n                self.train_indices, self.indices_to_remove, self.model = KPGNN_model(self).initial_maintain(train_i, 0,\n                                                                                                      self.data_split,\n                                                                                                      self.metrics,\n                                                                                                      self.embedding_save_path,\n                                                                                                      self.loss_fn,\n                                                                                                      None,\n                                                                                                      self.loss_fn_dgi)\n\n    def detection(self):\n        train_i = 0\n        if self.is_incremental:\n            # Initialize the model, train_indices and indices_to_remove to avoid errors\n            if self.resume_path is not None:\n                self.model = None\n                self.train_indices = None\n                self.indices_to_remove = []\n\n            # iterate through all blocks\n            for i in range(1, self.data_split.shape[0]):\n                # Inference (prediction)\n                # Resume model from the previous, i.e., (i-1)th block or continue the new experiment. Otherwise (to resume from other blocks) skip this step.\n                if ((self.resume_path is not None) and (self.resume_point == i - 1) and (\n                        not self.resume_current)) or self.resume_path is None:\n                    if not self.use_dgi:\n                        self.model = KPGNN_model(self).infer(train_i, i, self.data_split, self.metrics,\n                                                       self.embedding_save_path, self.loss_fn, self.train_indices,\n                                                       self.model, None,\n                                                       self.indices_to_remove)\n                    else:\n                        self.model = KPGNN_model(self).infer(train_i, i, self.data_split, self.metrics,\n                                                       self.embedding_save_path, self.loss_fn, self.train_indices,\n                                                       self.model,\n                                                       self.loss_fn_dgi, self.indices_to_remove)\n                # Maintain\n                # Resume model from the current, i.e., ith block or continue the new experiment. Otherwise (to resume from other blocks) skip this step.\n                if ((self.resume_path is not None) and (self.resume_point == i) and (\n                        self.resume_current)) or self.resume_path is None:\n                    if i % self.window_size == 0:\n                        train_i = i\n                        if not self.use_dgi:\n                            self.train_indices, self.indices_to_remove, self.model = KPGNN_model(self).initial_maintain(\n                                train_i, i, self.data_split, self.metrics,\n                                self.embedding_save_path, self.loss_fn, self.model)\n                        else:\n                            self.train_indices, self.indices_to_remove, self.model = KPGNN_model(self).initial_maintain(\n                                train_i, i, self.data_split, self.metrics,\n                                self.embedding_save_path, self.loss_fn, self.model,\n                                self.loss_fn_dgi)\n\n        data = SocialDataset(self.data_path, 0)\n        g = dgl.DGLGraph(data.matrix)\n        g.readonly()\n        features = torch.FloatTensor(data.features)\n        labels = torch.LongTensor(data.labels)\n\n        predictions = []\n        ground_truths = []\n        self.detection_path = '../model/model_saved/kpgnn/detection_split/'\n        os.makedirs(self.detection_path, exist_ok=True)\n\n        test_indices = generateMasks(len(labels), self.data_split, 1, 0, 0.2, self.detection_path,\n                                     num_indices_to_remove=0)\n\n        g.ndata['features'] = features\n\n        _, extract_features, extract_labels = extract_embeddings(g, self.model, len(labels), labels)\n\n        # Extract labels\n        test_indices = torch.load(self.detection_path + '/test_indices.pt')\n\n        labels_true = extract_labels[test_indices]\n        # Extract features\n        X = extract_features[test_indices, :]\n        assert labels_true.shape[0] == X.shape[0]\n        n_test_tweets = X.shape[0]\n\n        # Get the total number of classes\n        n_classes = len(set(list(labels_true)))\n\n        # kmeans clustering\n        kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n        predictions = kmeans.labels_\n        ground_truths = labels_true\n\n        return predictions, ground_truths\n\n    def evaluate(self, predictions, ground_truths):\n        ars = metrics.adjusted_rand_score(ground_truths, predictions)\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n\n        print(f\"Model Adjusted Rand Index (ARI): {ars}\")\n        print(f\"Model Adjusted Mutual Information (AMI): {ami}\")\n        print(f\"Model Normalized Mutual Information (NMI): {nmi}\")\n        return ars, ami, nmi\n\nclass Preprocessor:\n    def __init__(self, dataset):\n        pass\n\n    # generate_initial_features\n    def generate_initial_features(self, dataset):\n        save_path = '../model/model_saved/kpgnn/data/Event2012/kpgnn/'\n        df = dataset\n\n        os.makedirs(save_path, exist_ok=True)\n        print(\"Data converted to dataframe.\")\n        print(type(df))\n        print(df.shape)\n        print(df.head(10))\n\n        d_features = self.documents_to_features(df)\n        print(\"Document features generated.\")\n        t_features = self.df_to_t_features(df)\n        print(\"Time features generated.\")\n        combined_features = np.concatenate((d_features, t_features), axis=1)\n        print(\"Concatenated document features and time features.\")\n\n        np.save(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy', combined_features)\n        print(\"Initial features saved.\")\n\n    def documents_to_features(self, df):\n        nlp = spacy.load(\"en_core_web_lg\")\n        print(\"df.filtered_words.head(10)\", df.filtered_words.head(10))\n        features = df.filtered_words.apply(lambda x: nlp(' '.join(x)).vector).values\n        print(\"features.head(10)\", features, \"\\n\", \"np.stack(features, axis=0)\", np.stack(features, axis=0))\n        return np.stack(features, axis=0)\n\n    def extract_time_feature(self, t_str):\n        t = datetime.fromisoformat(str(t_str))\n        OLE_TIME_ZERO = datetime(1899, 12, 30)\n        delta = t - OLE_TIME_ZERO\n        return [(float(delta.days) / 100000.), (float(delta.seconds) / 86400)]  # 86,400 seconds in day\n\n    # encode the times-tamps of all the messages in the dataframe\n    def df_to_t_features(self, df):\n        t_features = np.asarray([self.extract_time_feature(t_str) for t_str in df['created_at']])\n        return t_features\n\n    # custom_message_graph\n    def custom_message_graph(self, dataset):\n        save_path = '../model/model_saved/kpgnn/kpgnn_incremental_test/'\n        '''\n        if os.path.exists(save_path):\n            pass\n        else:\n            os.mkdir(save_path)\n        '''\n        os.makedirs(save_path, exist_ok=True)\n\n        df = dataset\n        print(\"Data loaded.\")\n\n        # sort data by time\n        df = df.sort_values(by='created_at').reset_index()\n\n        # append date\n        df['date'] = [d.date() for d in df['created_at']]\n\n        # load features\n        # the dimension of feature is 300 in this dataset\n        f = np.load('../model/model_saved/kpgnn/data/Event2012/kpgnn/features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy')\n        # generate test graphs, features, and labels\n        message, data_split, all_graph_mins = self.construct_incremental_dataset(df, save_path, f, True)\n        with open(save_path + \"node_edge_statistics.txt\", \"w\") as text_file:\n            text_file.write(message)\n        np.save(save_path + 'data_split.npy', np.asarray(data_split))\n        print(\"Data split: \", data_split)\n        np.save(save_path + 'all_graph_mins.npy', np.asarray(all_graph_mins))\n        print(\"Time sepnt on heterogeneous -> homogeneous graph conversions: \", all_graph_mins)\n\n    def construct_graph_from_df(self, df, G=None):\n        if G is None:\n            G = nx.Graph()\n        for _, row in df.iterrows():\n            tid = 't_' + str(row['tweet_id'])\n            G.add_node(tid)\n            G.nodes[tid]['tweet_id'] = True  # right-hand side value is irrelevant for the lookup\n\n            user_ids = row['user_mentions']\n            user_ids.append(row['user_id'])\n            user_ids = ['u_' + str(each) for each in user_ids]\n            # print(user_ids)\n            G.add_nodes_from(user_ids)\n            for each in user_ids:\n                G.nodes[each]['user_id'] = True\n\n            entities = row['entities']\n            # entities = ['e_' + each for each in entities]\n            # print(entities)\n            G.add_nodes_from(entities)\n            for each in entities:\n                G.nodes[each]['entity'] = True\n\n            words = row['filtered_words']\n            words = ['w_' + each for each in words]\n            # print(words)\n            G.add_nodes_from(words)\n            for each in words:\n                G.nodes[each]['word'] = True\n\n            edges = []\n            edges += [(tid, each) for each in user_ids]\n            edges += [(tid, each) for each in entities]\n            edges += [(tid, each) for each in words]\n            G.add_edges_from(edges)\n\n        return G\n\n    def networkx_to_dgl_graph(self, G, save_path=None):\n        message = ''\n        print('Start converting heterogeneous networkx graph to homogeneous dgl graph.')\n        message += 'Start converting heterogeneous networkx graph to homogeneous dgl graph.\\n'\n        all_start = time.time()\n\n        print('\\tGetting a list of all nodes ...')\n        message += '\\tGetting a list of all nodes ...\\n'\n        start = time.time()\n        all_nodes = list(G.nodes)\n        mins = (time.time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # print('All nodes: ', all_nodes)\n        # print('Total number of nodes: ', len(all_nodes))\n\n        print('\\tGetting adjacency matrix ...')\n        message += '\\tGetting adjacency matrix ...\\n'\n        start = time.time()\n        A = nx.to_numpy_array(G)  # Returns the graph adjacency matrix as a NumPy matrix.\n        mins = (time.time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # compute commuting matrices\n        print('\\tGetting lists of nodes of various types ...')\n        message += '\\tGetting lists of nodes of various types ...\\n'\n        start = time.time()\n        tid_nodes = list(nx.get_node_attributes(G, 'tweet_id').keys())\n        userid_nodes = list(nx.get_node_attributes(G, 'user_id').keys())\n        word_nodes = list(nx.get_node_attributes(G, 'word').keys())\n        entity_nodes = list(nx.get_node_attributes(G, 'entity').keys())\n        del G\n        mins = (time.time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\tConverting node lists to index lists ...')\n        message += '\\tConverting node lists to index lists ...\\n'\n        start = time.time()\n        #  find the index of target nodes in the list of all_nodes\n        indices_tid = [all_nodes.index(x) for x in tid_nodes]\n        indices_userid = [all_nodes.index(x) for x in userid_nodes]\n        indices_word = [all_nodes.index(x) for x in word_nodes]\n        indices_entity = [all_nodes.index(x) for x in entity_nodes]\n        del tid_nodes\n        del userid_nodes\n        del word_nodes\n        del entity_nodes\n        mins = (time.time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------tweet-user-tweet----------------------\n        print('\\tStart constructing tweet-user-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-user matrix ...')\n        message += '\\tStart constructing tweet-user-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-user ' \\\n                   'matrix ...\\n '\n        start = time.time()\n        w_tid_userid = A[np.ix_(indices_tid, indices_userid)]\n        #  return a N(indices_tid)*N(indices_userid) matrix, representing the weight of edges between tid and userid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time.time()\n        s_w_tid_userid = sparse.csr_matrix(w_tid_userid)  # matrix compression\n        del w_tid_userid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time.time()\n        s_w_userid_tid = s_w_tid_userid.transpose()\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-user * user-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-user * user-tweet ...\\n'\n        start = time.time()\n        s_m_tid_userid_tid = s_w_tid_userid * s_w_userid_tid  # homogeneous message graph\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time.time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_userid_tid.npz\", s_m_tid_userid_tid)\n            print(\"Sparse binary userid commuting matrix saved.\")\n            del s_m_tid_userid_tid\n        del s_w_tid_userid\n        del s_w_userid_tid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------tweet-ent-tweet------------------------\n        print('\\tStart constructing tweet-ent-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-ent matrix ...')\n        message += '\\tStart constructing tweet-ent-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-ent matrix ' \\\n                   '...\\n '\n        start = time.time()\n        w_tid_entity = A[np.ix_(indices_tid, indices_entity)]\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time.time()\n        s_w_tid_entity = sparse.csr_matrix(w_tid_entity)\n        del w_tid_entity\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time.time()\n        s_w_entity_tid = s_w_tid_entity.transpose()\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-ent * ent-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-ent * ent-tweet ...\\n'\n        start = time.time()\n        s_m_tid_entity_tid = s_w_tid_entity * s_w_entity_tid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time.time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_entity_tid.npz\", s_m_tid_entity_tid)\n            print(\"Sparse binary entity commuting matrix saved.\")\n            del s_m_tid_entity_tid\n        del s_w_tid_entity\n        del s_w_entity_tid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------tweet-word-tweet----------------------\n        print('\\tStart constructing tweet-word-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-word matrix ...')\n        message += '\\tStart constructing tweet-word-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-word ' \\\n                   'matrix ...\\n '\n        start = time.time()\n        w_tid_word = A[np.ix_(indices_tid, indices_word)]\n        del A\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time.time()\n        s_w_tid_word = sparse.csr_matrix(w_tid_word)\n        del w_tid_word\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time.time()\n        s_w_word_tid = s_w_tid_word.transpose()\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-word * word-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-word * word-tweet ...\\n'\n        start = time.time()\n        s_m_tid_word_tid = s_w_tid_word * s_w_word_tid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time.time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_word_tid.npz\", s_m_tid_word_tid)\n            print(\"Sparse binary word commuting matrix saved.\")\n            del s_m_tid_word_tid\n        del s_w_tid_word\n        del s_w_word_tid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------compute tweet-tweet adjacency matrix----------------------\n        print('\\tComputing tweet-tweet adjacency matrix ...')\n        message += '\\tComputing tweet-tweet adjacency matrix ...\\n'\n        start = time.time()\n        if save_path is not None:\n            s_m_tid_userid_tid = sparse.load_npz(save_path + \"s_m_tid_userid_tid.npz\")\n            print(\"Sparse binary userid commuting matrix loaded.\")\n            s_m_tid_entity_tid = sparse.load_npz(save_path + \"s_m_tid_entity_tid.npz\")\n            print(\"Sparse binary entity commuting matrix loaded.\")\n            s_m_tid_word_tid = sparse.load_npz(save_path + \"s_m_tid_word_tid.npz\")\n            print(\"Sparse binary word commuting matrix loaded.\")\n\n        s_A_tid_tid = s_m_tid_userid_tid + s_m_tid_entity_tid\n        del s_m_tid_userid_tid\n        del s_m_tid_entity_tid\n        s_bool_A_tid_tid = (s_A_tid_tid + s_m_tid_word_tid).astype('bool')  # confirm the connect between tweets\n        del s_m_tid_word_tid\n        del s_A_tid_tid\n        mins = (time.time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n        all_mins = (time.time() - all_start) / 60\n        print('\\tOver all time elapsed: ', all_mins, ' mins\\n')\n        message += '\\tOver all time elapsed: '\n        message += str(all_mins)\n        message += ' mins\\n'\n\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_bool_A_tid_tid.npz\", s_bool_A_tid_tid)\n            print(\"Sparse binary adjacency matrix saved.\")\n            s_bool_A_tid_tid = sparse.load_npz(save_path + \"s_bool_A_tid_tid.npz\")\n            print(\"Sparse binary adjacency matrix loaded.\")\n\n        # create corresponding dgl graph\n        G = dgl.DGLGraph(s_bool_A_tid_tid)\n        print('We have %d nodes.' % G.number_of_nodes())\n        print('We have %d edges.' % G.number_of_edges())\n        print()\n        message += 'We have '\n        message += str(G.number_of_nodes())\n        message += ' nodes.'\n        message += 'We have '\n        message += str(G.number_of_edges())\n        message += ' edges.\\n'\n\n        return all_mins, message\n\n    def construct_incremental_dataset(self, df, save_path, features, test=True):\n        # If test equals true, construct the initial graph using test_ini_size tweets\n        # and increment the graph by test_incr_size tweets each day\n        test_ini_size = 500\n        test_incr_size = 100\n\n        # save data splits for training/validate/test mask generation\n        data_split = []\n        # save time spent for the heterogeneous -> homogeneous conversion of each graph\n        all_graph_mins = []\n        message = \"\"\n        # extract distinct dates\n        distinct_dates = df.date.unique()  # 2012-11-07\n        # print(\"Distinct dates: \", distinct_dates)\n        print(\"Number of distinct dates: \", len(distinct_dates))\n        print()\n        message += \"Number of distinct dates: \"\n        message += str(len(distinct_dates))\n        message += \"\\n\"\n\n        # split data by dates and construct graphs\n        # first week -> initial graph (20254 tweets)\n        print(\"Start constructing initial graph ...\")\n        message += \"\\nStart constructing initial graph ...\\n\"\n        ini_df = df.loc[df['date'].isin(distinct_dates[:7])]  # find top 7 dates\n        if test:\n            ini_df = ini_df[:test_ini_size]  # top test_ini_size dates\n        G = self.construct_graph_from_df(ini_df)\n        path = save_path + '0/'\n        if os.path.exists(path):\n            pass\n        else:\n            os.mkdir(path)\n        grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n        message += graph_message\n        print(\"Initial graph saved\")\n        message += \"Initial graph saved\\n\"\n        # record the total number of tweets\n        data_split.append(ini_df.shape[0])\n        # record the time spent for graph conversion\n        all_graph_mins.append(grap_mins)\n        # extract and save the labels of corresponding tweets\n        y = ini_df['event_id'].values\n        y = [int(each) for each in y]\n        np.save(path + 'labels.npy', np.asarray(y))\n        print(\"Labels saved.\")\n        message += \"Labels saved.\\n\"\n        # extract and save the features of corresponding tweets\n        indices = ini_df['index'].values.tolist()\n        x = features[indices, :]\n        np.save(path + 'features.npy', x)\n        print(\"Features saved.\")\n        message += \"Features saved.\\n\\n\"\n\n        # subsequent days -> insert tweets day by day (skip the last day because it only contains one tweet)\n        for i in range(7, len(distinct_dates) - 1):\n            print(\"Start constructing graph \", str(i - 6), \" ...\")\n            message += \"\\nStart constructing graph \"\n            message += str(i - 6)\n            message += \" ...\\n\"\n            incr_df = df.loc[df['date'] == distinct_dates[i]]\n            if test:\n                incr_df = incr_df[:test_incr_size]\n\n            # All/Relevant Message Strategy: keeping all the messages when constructing the graphs \n            # (for the Relevant Message Strategy, the unrelated messages will be removed from the graph later on).\n            # G = construct_graph_from_df(incr_df, G) \n\n            # Latest Message Strategy: construct graph using only the data of the day\n            G = self.construct_graph_from_df(incr_df)\n\n            path = save_path + str(i - 6) + '/'\n            if os.path.exists(path):\n                pass\n            else:\n                os.mkdir(path)\n\n            grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n            message += graph_message\n            print(\"Graph \", str(i - 6), \" saved\")\n            message += \"Graph \"\n            message += str(i - 6)\n            message += \" saved\\n\"\n            # record the total number of tweets\n            data_split.append(incr_df.shape[0])\n            # record the time spent for graph conversion\n            all_graph_mins.append(grap_mins)\n            # extract and save the labels of corresponding tweets\n            # y = np.concatenate([y, incr_df['event_id'].values], axis = 0)\n            y = [int(each) for each in incr_df['event_id'].values]\n            np.save(path + 'labels.npy', y)\n            print(\"Labels saved.\")\n            message += \"Labels saved.\\n\"\n            # extract and save the features of corresponding tweets\n            indices = incr_df['index'].values.tolist()\n            x = features[indices, :]\n            # x = np.concatenate([x, x_incr], axis = 0)\n            np.save(path + 'features.npy', x)\n            print(\"Features saved.\")\n            message += \"Features saved.\\n\"\n        return message, data_split, all_graph_mins\n\nclass KPGNN_model():\n    def __init__(self,args):\n        super(KPGNN_model, self).__init__()\n        self.args = args\n        pass\n\n    # Inference(prediction)\n    def infer(self, train_i, i, data_split, metrics, embedding_save_path, loss_fn, train_indices=None, model=None,\n              loss_fn_dgi=None, indices_to_remove=[]):\n\n        save_path_i = embedding_save_path + '/block_' + str(i)\n        if not os.path.isdir(save_path_i):\n            os.mkdir(save_path_i)\n\n        data = SocialDataset(self.args.data_path, i)\n        features = torch.FloatTensor(data.features)\n        labels = torch.LongTensor(data.labels)\n        print(\"labels1:\", labels)\n        in_feats = features.shape[1]\n\n        g = dgl.graph((data.matrix.row, data.matrix.col))\n        num_isolated_nodes = graph_statistics(g, save_path_i)\n\n        if self.args.remove_obsolete == 1:\n            if ((self.args.resume_path is not None) and (not self.args.resume_current) and (i == self.args.resume_point + 1) and (\n                    i > self.args.window_size)) \\\n                    or (indices_to_remove == [] and i > self.args.window_size):\n                temp_i = max(((i - 1) // self.args.window_size) * self.args.window_size, 0)\n                indices_to_remove = np.load(\n                    embedding_save_path + '/block_' + str(temp_i) + '/indices_to_remove.npy').tolist()\n\n            if indices_to_remove != []:\n                data.remove_obsolete_nodes(indices_to_remove)\n                features = torch.FloatTensor(data.features)\n                labels = torch.LongTensor(data.labels)\n                print(\"labels2:\", labels)\n                g = dgl.graph((data.matrix.row, data.matrix.col))\n                num_isolated_nodes = graph_statistics(g, save_path_i)\n\n        if self.args.mask_path is None:\n            mask_path = save_path_i + '/masks'\n            if not os.path.isdir(mask_path):\n                os.mkdir(mask_path)\n            test_indices = generateMasks(len(labels), data_split, train_i, i, self.args.validation_percent, mask_path,\n                                         len(indices_to_remove))\n        else:\n            test_indices = torch.load(self.args.mask_path + '/block_' + str(i) + '/masks/test_indices.pt')\n\n        if self.args.use_cuda:\n            features, labels = features.cuda(), labels.cuda()\n            print(\"labels3:\", labels)\n            test_indices = test_indices.cuda()\n\n        g.ndata['features'] = features\n\n        if (self.args.resume_path is not None) and (not self.args.resume_current) and (i == self.args.resume_point + 1):\n            if self.args.use_dgi:\n                model = DGI(in_feats, self.args.hidden_dim, self.args.out_dim, self.args.num_heads, self.args.use_residual)\n            else:\n                model = GAT(in_feats, self.args.hidden_dim, self.args.out_dim, self.args.num_heads, self.args.use_residual)\n\n            if self.args.use_cuda:\n                model.cuda()\n\n            model_path = embedding_save_path + '/block_' + str(self.args.resume_point) + '/models/best.pt'\n            model.load_state_dict(torch.load(model_path))\n            print(\"Resumed model from the previous block.\")\n\n            self.args.resume_path = None\n\n        if train_indices is None:\n            if self.args.remove_obsolete == 0 or self.args.remove_obsolete == 1:\n                temp_i = max(((i - 1) // self.args.window_size) * self.args.window_size, 0)\n                train_indices = torch.load(embedding_save_path + '/block_' + str(temp_i) + '/masks/train_indices.pt')\n            else:\n                if self.args.n_infer_epochs != 0:\n                    print(\n                        \"==================================\\n'continue training then predict' is unimplemented under remove_obsolete mode 2, will skip infer epochs.\\n===================================\\n\")\n                    self.args.n_infer_epochs = 0\n\n        all_test_nmi = []\n        time_predict = []\n\n        message = \"\\n------------ Directly predict on block \" + str(i) + \" ------------\\n\"\n        print(message)\n        with open(save_path_i + '/log.txt', 'a') as f:\n            f.write(message)\n        start = time.time()\n\n        extract_nids, extract_features, extract_labels = extract_embeddings(g, model, len(labels), labels)\n        test_nmi = evaluate_model(extract_features, extract_labels, test_indices, -1, num_isolated_nodes, save_path_i,\n                                  False)\n        seconds_spent = time.time() - start\n        message = '\\nDirect prediction took {:.2f} seconds'.format(seconds_spent)\n        print(message)\n        with open(save_path_i + '/log.txt', 'a') as f:\n            f.write(message)\n        all_test_nmi.append(test_nmi)\n        time_predict.append(seconds_spent)\n        np.save(save_path_i + '/time_predict.npy', np.asarray(time_predict))\n\n        optimizer = optim.Adam(model.parameters(), lr=self.args.lr, weight_decay=1e-4)\n\n        if self.args.n_infer_epochs != 0:\n            message = \"\\n------------ Continue training then predict on block \" + str(i) + \" ------------\\n\"\n            print(message)\n            with open(save_path_i + '/log.txt', 'a') as f:\n                f.write(message)\n        seconds_infer_batches = []\n        mins_infer_epochs = []\n\n        sampler = MultiLayerNeighborSampler([self.args.n_neighbors] * 2)\n        dataloader = NodeDataLoader(\n            g, train_indices, sampler,\n            batch_size=self.args.batch_size,\n            shuffle=True,\n            drop_last=False,\n            num_workers=4)\n\n        for epoch in range(self.args.n_infer_epochs):\n            start_epoch = time.time()\n            losses = []\n            total_loss = 0\n            if self.args.use_dgi:\n                losses_triplet = []\n                losses_dgi = []\n            for metric in metrics:\n                metric.reset()\n\n            for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n                start_batch = time.time()\n                batch_features = blocks[0].srcdata['features']\n                model.train()\n\n                if self.args.use_dgi:\n                    pred, ret = model(blocks, batch_features)\n                else:\n                    pred = model(blocks, batch_features)\n\n                batch_labels = labels[output_nodes]\n                loss_outputs = loss_fn(pred, batch_labels)\n                loss = loss_outputs[0] if isinstance(loss_outputs, (tuple, list)) else loss_outputs\n                if self.args.use_dgi:\n                    n_samples = len(output_nodes)\n                    lbl_1 = torch.ones(n_samples)\n                    lbl_2 = torch.zeros(n_samples)\n                    lbl = torch.cat((lbl_1, lbl_2), 0)\n                    if self.args.use_cuda:\n                        lbl = lbl.cuda()\n                    losses_triplet.append(loss.item())\n                    loss_dgi = loss_fn_dgi(ret, lbl)\n                    losses_dgi.append(loss_dgi.item())\n                    loss += loss_dgi\n                    losses.append(loss.item())\n                else:\n                    losses.append(loss.item())\n                total_loss += loss.item()\n\n                for metric in metrics:\n                    metric(pred, batch_labels, loss_outputs)\n\n                if batch_id % self.args.log_interval == 0:\n                    message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                        batch_id * self.args.batch_size, train_indices.shape[0],\n                        100. * batch_id / (train_indices.shape[0] // self.args.batch_size), np.mean(losses))\n                    if self.args.use_dgi:\n                        message += '\\tLoss_triplet: {:.6f}'.format(np.mean(losses_triplet))\n                        message += '\\tLoss_dgi: {:.6f}'.format(np.mean(losses_dgi))\n                    for metric in metrics:\n                        message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n                    print(message)\n                    with open(save_path_i + '/log.txt', 'a') as f:\n                        f.write(message)\n                    losses = []\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                batch_seconds_spent = time.time() - start_batch\n                seconds_infer_batches.append(batch_seconds_spent)\n\n            total_loss /= (batch_id + 1)\n            message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch + 1, self.args.n_infer_epochs, total_loss)\n            for metric in metrics:\n                message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n            mins_spent = (time.time() - start_epoch) / 60\n            message += '\\nThis epoch took {:.2f} mins'.format(mins_spent)\n            message += '\\n'\n            print(message)\n            with open(save_path_i + '/log.txt', 'a') as f:\n                f.write(message)\n            mins_infer_epochs.append(mins_spent)\n\n            # 验证\n            extract_nids, extract_features, extract_labels = extract_embeddings(g, model, len(labels), labels)\n            test_nmi = evaluate_model(extract_features, extract_labels, test_indices, epoch, num_isolated_nodes,\n                                      save_path_i, False)\n            all_test_nmi.append(test_nmi)\n            # end one epoch\n\n        # Save model (fine-tuned from the above continue training process)\n        model_path = save_path_i + '/models'\n        os.mkdir(model_path)\n        p = model_path + '/best.pt'\n        torch.save(model.state_dict(), p)\n        print('Model saved.')\n\n        # Save all test nmi\n        np.save(save_path_i + '/all_test_nmi.npy', np.asarray(all_test_nmi))\n        print('Saved all test nmi.')\n        # Save time spent on epochs\n        np.save(save_path_i + '/mins_infer_epochs.npy', np.asarray(mins_infer_epochs))\n        print('Saved mins_infer_epochs.')\n        # Save time spent on batches\n        np.save(save_path_i + '/seconds_infer_batches.npy', np.asarray(seconds_infer_batches))\n        print('Saved seconds_infer_batches.')\n\n        return model\n\n    # Train on initial/maintenance graphs, t == 0 or t % window_size == 0 in this paper\n    def initial_maintain(self, train_i, i, data_split, metrics, embedding_save_path, loss_fn, model=None, loss_fn_dgi=None):\n        # 在调用 initial_maintain 之前打印参数\n        print(\"After calling initial_maintain:\")\n        print(\"train_i:\", train_i)\n        print(\"i:\", i)\n        print(\"data_split:\", data_split)\n        print(\"metrics:\", metrics)\n        print(\"embedding_save_path:\", embedding_save_path)\n        print(\"loss_fn:\", loss_fn)\n        print(\"model:\", model)\n\n            \n        save_path_i = embedding_save_path + '/block_' + str(i)\n        if not os.path.isdir(save_path_i):\n            os.mkdir(save_path_i)\n\n        # load data\n        data = SocialDataset(self.args.data_path, i)\n        features = torch.FloatTensor(data.features)\n        labels = torch.LongTensor(data.labels)\n        in_feats = features.shape[1]  # feature dimension\n\n        # Construct graph that contains message blocks 0, ..., i if remove_obsolete = 0 or 1; graph that only contains message block i if remove_obsolete = 2\n        g = dgl.DGLGraph(data.matrix)\n        num_isolated_nodes = graph_statistics(g, save_path_i)\n\n        # if remove_obsolete is mode 1, resume or generate indices_to_remove, then remove obsolete nodes from the graph\n        if self.args.remove_obsolete == 1:\n\n            # Resume indices_to_remove from the current block\n            if (self.args.resume_path is not None) and self.args.resume_current and (i == self.args.resume_point) and (i != 0):\n                indices_to_remove = np.load(save_path_i + '/indices_to_remove.npy').tolist()\n\n            elif i == 0:  # generate empty indices_to_remove for initial block\n                indices_to_remove = []\n                # save indices_to_remove\n                np.save(save_path_i + '/indices_to_remove.npy', np.asarray(indices_to_remove))\n\n            #  update graph\n            else:  # generate indices_to_remove for maintenance block\n                # get the indices of all training nodes\n                num_all_train_nodes = np.sum(data_split[:i + 1])\n                all_train_indices = np.arange(0, num_all_train_nodes).tolist()\n                # get the number of old training nodes added before this maintenance\n                num_old_train_nodes = np.sum(data_split[:i + 1 - self.args.window_size])\n                # indices_to_keep: indices of nodes that are connected to the new training nodes added at this maintenance\n                # (include the indices of the new training nodes)\n                indices_to_keep = list(set(data.matrix.indices[data.matrix.indptr[num_old_train_nodes]:]))\n                # indices_to_remove is the difference between the indices of all training nodes and indices_to_keep\n                indices_to_remove = list(set(all_train_indices) - set(indices_to_keep))\n                # save indices_to_remove\n                np.save(save_path_i + '/indices_to_remove.npy', np.asarray(indices_to_remove))\n\n            if indices_to_remove != []:\n                # remove obsolete nodes from the graph\n                data.remove_obsolete_nodes(indices_to_remove)\n                features = torch.FloatTensor(data.features)\n                labels = torch.LongTensor(data.labels)\n                # Reconstruct graph\n                g = dgl.DGLGraph(data.matrix)  # graph that contains tweet blocks 0, ..., i\n                num_isolated_nodes = graph_statistics(g, save_path_i)\n\n        else:\n\n            indices_to_remove = []\n\n        # generate or load training/validate/test masks\n        if (self.args.resume_path is not None) and self.args.resume_current and (\n                i == self.args.resume_point):  # Resume masks from the current block\n\n            train_indices = torch.load(save_path_i + '/masks/train_indices.pt')\n            validation_indices = torch.load(save_path_i + '/masks/validation_indices.pt')\n        if self.args.mask_path is None:\n\n            mask_path = save_path_i + '/masks'\n            if not os.path.isdir(mask_path):\n                os.mkdir(mask_path)\n            train_indices, validation_indices = generateMasks(len(labels), data_split, train_i, i,\n                                                              self.args.validation_percent,\n                                                              mask_path, len(indices_to_remove))\n\n        else:\n            train_indices = torch.load(self.args.mask_path + '/block_' + str(i) + '/masks/train_indices.pt')\n            validation_indices = torch.load(self.args.mask_path + '/block_' + str(i) + '/masks/validation_indices.pt')\n\n        # Suppress warning\n        g.set_n_initializer(dgl.init.zero_initializer)\n\n        if self.args.use_cuda:\n            features, labels = features.cuda(), labels.cuda()\n            train_indices, validation_indices = train_indices.cuda(), validation_indices.cuda()\n\n        g.ndata['features'] = features\n\n        if (self.args.resume_path is not None) and self.args.resume_current and (\n                i == self.args.resume_point):  # Resume model from the current block\n\n            # Declare model\n            if self.args.use_dgi:\n                model = DGI(in_feats, self.args.hidden_dim, self.args.out_dim, self.args.num_heads, self.args.use_residual)\n            else:\n                model = GAT(in_feats, self.args.hidden_dim, self.args.out_dim, self.args.num_heads, self.args.use_residual)\n\n            if self.args.use_cuda:\n                model.cuda()\n\n            # Load model from resume_point\n            model_path = embedding_save_path + '/block_' + str(self.args.resume_point) + '/models/best.pt'\n            model.load_state_dict(torch.load(model_path))\n            print(\"Resumed model from the current block.\")\n\n            # Use resume_path as a flag\n            self.args.resume_path = None\n\n        elif model is None:  # Construct the initial model\n            # Declare model\n            if self.args.use_dgi:\n                model = DGI(in_feats, self.args.hidden_dim, self.args.out_dim, self.args.num_heads, self.args.use_residual)\n            else:\n                model = GAT(in_feats, self.args.hidden_dim, self.args.out_dim, self.args.num_heads, self.args.use_residual)\n\n            if self.args.use_cuda:\n                model.cuda()\n\n        # Optimizer\n        optimizer = optim.Adam(model.parameters(), lr=self.args.lr, weight_decay=1e-4)\n\n        # Start training\n        message = \"\\n------------ Start initial training / maintaining using blocks 0 to \" + str(i) + \" ------------\\n\"\n        print(message)\n        with open(save_path_i + '/log.txt', 'a') as f:\n            f.write(message)\n        # record the highest validation nmi ever got for early stopping\n        best_vali_nmi = 1e-9\n        best_epoch = 0\n        wait = 0\n        # record validation nmi of all epochs before early stop\n        all_vali_nmi = []\n        # record the time spent in seconds on each batch of all training/maintaining epochs\n        seconds_train_batches = []\n        # record the time spent in mins on each epoch\n        mins_train_epochs = []\n        g.readonly()\n\n        sampler = MultiLayerNeighborSampler([self.args.n_neighbors] * 2)  # self.args.n_hops 应该是 2\n\n        # 创建 DataLoader\n        dataloader = NodeDataLoader(\n            g, train_indices, sampler,\n            batch_size=self.args.batch_size,\n            shuffle=True,\n            drop_last=False,\n            num_workers=4)  # 设置为适当的 num_workers\n\n        for epoch in range(self.args.n_epochs):\n            start_epoch = time.time()\n            model.train()\n            total_loss = 0\n            losses = []\n            if self.args.use_dgi:\n                losses_triplet = []\n                losses_dgi = []\n\n            for metric in metrics:\n                metric.reset()\n\n            for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n                start_batch = time.time()\n\n                blocks = [block.int().to(torch.device('cuda' if self.args.use_cuda else 'cpu')) for block in blocks]\n                batch_features = blocks[0].srcdata['features']\n                batch_labels = labels[output_nodes]\n\n                # forward\n                if self.args.use_dgi:\n                    pred, ret = model(blocks, batch_features)\n                else:\n                    blocks[0].srcdata['h'] = batch_features\n                    # print(blocks[0].srcdata)\n\n                    pred = model(blocks, batch_features)\n\n                loss_outputs = loss_fn(pred, batch_labels)\n                loss = loss_outputs[0] if isinstance(loss_outputs, (tuple, list)) else loss_outputs\n\n                if self.args.use_dgi:\n                    n_samples = len(output_nodes)\n                    lbl_1 = torch.ones(n_samples)\n                    lbl_2 = torch.zeros(n_samples)\n                    lbl = torch.cat((lbl_1, lbl_2), 0)\n                    if self.args.use_cuda:\n                        lbl = lbl.cuda()\n                    losses_triplet.append(loss.item())\n                    loss_dgi = loss_fn_dgi(ret, lbl)\n                    losses_dgi.append(loss_dgi.item())\n                    loss += loss_dgi\n                    losses.append(loss.item())\n                else:\n                    losses.append(loss.item())\n\n                total_loss += loss.item()\n\n                for metric in metrics:\n                    metric(pred, batch_labels, loss_outputs)\n\n                if batch_id % self.args.log_interval == 0:\n                    message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                        batch_id * self.args.batch_size, train_indices.shape[0],\n                        100. * batch_id / (len(dataloader)), np.mean(losses))\n                    if self.args.use_dgi:\n                        message += '\\tLoss_triplet: {:.6f}'.format(np.mean(losses_triplet))\n                        message += '\\tLoss_dgi: {:.6f}'.format(np.mean(losses_dgi))\n                    for metric in metrics:\n                        message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n                    print(message)\n                    with open(save_path_i + '/log.txt', 'a') as f:\n                        f.write(message)\n                    losses = []\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                batch_seconds_spent = time.time() - start_batch\n                seconds_train_batches.append(batch_seconds_spent)\n\n            total_loss /= (batch_id + 1)\n            message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch + 1, self.args.n_epochs, total_loss)\n            for metric in metrics:\n                message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n            mins_spent = (time.time() - start_epoch) / 60\n            message += '\\nThis epoch took {:.2f} mins'.format(mins_spent)\n            message += '\\n'\n            print(message)\n            with open(save_path_i + '/log.txt', 'a') as f:\n                f.write(message)\n            mins_train_epochs.append(mins_spent)\n\n            # Validation\n            # Infer the representations of all tweets\n            g.readonly()\n            extract_nids, extract_features, extract_labels = extract_embeddings(g, model, len(labels), labels)\n            # Save the representations of all tweets\n            # save_embeddings(extract_nids, extract_features, extract_labels, extract_train_tags, save_path_i, epoch)\n            # Evaluate the model: conduct kMeans clustering on the validation and report NMI\n            validation_nmi = evaluate_model(extract_features, extract_labels, validation_indices, epoch,\n                                            num_isolated_nodes,\n                                            save_path_i, True)\n            all_vali_nmi.append(validation_nmi)\n\n            # Early stop\n            if validation_nmi > best_vali_nmi:\n                best_vali_nmi = validation_nmi\n                best_epoch = epoch\n                wait = 0\n                # Save model\n                model_path = save_path_i + '/models'\n                if (epoch == 0) and (not os.path.isdir(model_path)):\n                    os.mkdir(model_path)\n                p = model_path + '/best.pt'\n                torch.save(model.state_dict(), p)\n                print('Best model saved after epoch ', str(epoch))\n            else:\n                wait += 1\n            if wait == self.args.patience:\n                print('Saved all_mins_spent')\n                print('Early stopping at epoch ', str(epoch))\n                print('Best model was at epoch ', str(best_epoch))\n                break\n            # end one epoch\n\n        # Save all validation nmi\n        np.save(save_path_i + '/all_vali_nmi.npy', np.asarray(all_vali_nmi))\n        # Save time spent on epochs\n        np.save(save_path_i + '/mins_train_epochs.npy', np.asarray(mins_train_epochs))\n        print('Saved mins_train_epochs.')\n        # Save time spent on batches\n        np.save(save_path_i + '/seconds_train_batches.npy', np.asarray(seconds_train_batches))\n        print('Saved seconds_train_batches.')\n\n        # Load the best model of the current block\n        best_model_path = save_path_i + '/models/best.pt'\n        model.load_state_dict(torch.load(best_model_path))\n        print(\"Best model loaded.\")\n\n        if self.args.remove_obsolete == 2:\n            return None, indices_to_remove, model\n        return train_indices, indices_to_remove, model\n\ndef graph_statistics(G, save_path):\n    message = '\\nGraph statistics:\\n'\n\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    ave_degree = (num_edges / 2) // num_nodes\n    in_degrees = G.in_degrees()\n    isolated_nodes = torch.zeros([in_degrees.size()[0]], dtype=torch.long)\n    isolated_nodes = (in_degrees == isolated_nodes)\n    torch.save(isolated_nodes, save_path + '/isolated_nodes.pt')\n    num_isolated_nodes = torch.sum(isolated_nodes).item()\n\n    message += 'We have ' + str(num_nodes) + ' nodes.\\n'\n    message += 'We have ' + str(num_edges / 2) + ' in-edges.\\n'\n    message += 'Average degree: ' + str(ave_degree) + '\\n'\n    message += 'Number of isolated nodes: ' + str(num_isolated_nodes) + '\\n'\n    print(message)\n    with open(save_path + \"/graph_statistics.txt\", \"a\") as f:\n        f.write(message)\n\n    return num_isolated_nodes\n\ndef generateMasks(length, data_split, train_i, i, validation_percent=0.2, save_path=None, num_indices_to_remove=0):\n    \"\"\"\n        Intro:\n        This function generates train and validation indices for initial/maintenance epochs and test indices for inference(prediction) epochs\n        If remove_obsolete mode 0 or 1:\n        For initial/maintenance epochs:\n        - The first (train_i + 1) blocks (blocks 0, ..., train_i) are used as training set (with explicit labels)\n        - Randomly sample validation_percent of the training indices as validation indices\n        For inference(prediction) epochs:\n        - The (i + 1)th block (block i) is used as test set\n        Note that other blocks (block train_i + 1, ..., i - 1) are also in the graph (without explicit labels, only their features and structural info are leveraged)\n        If remove_obsolete mode 2:\n        For initial/maintenance epochs:\n        - The (i + 1) = (train_i + 1)th block (block train_i = i) is used as training set (with explicit labels)\n        - Randomly sample validation_percent of the training indices as validation indices\n        For inference(prediction) epochs:\n        - The (i + 1)th block (block i) is used as test set\n\n        :param length: the length of label list\n        :param data_split: loaded splited data (generated in custom_message_graph.py)\n        :param train_i, i: flag, indicating for initial/maintenance stage if train_i == i and inference stage for others\n        :param validation_percent: the percent of validation data occupied in whole dataset\n        :param save_path: path to save data\n        :param num_indices_to_remove: number of indices ought to be removed\n\n        :returns train indices, validation indices or test indices\n    \"\"\"\n\n    # verify total number of nodes\n    assert length == data_split[i]\n\n    # If is in initial/maintenance epochs, generate train and validation indices\n    if train_i == i:\n        # randomly shuffle the graph indices\n        train_indices = torch.randperm(length)\n        # get total number of validation indices\n        n_validation_samples = int(length * validation_percent)\n        # sample n_validation_samples validation indices and use the rest as training indices\n        validation_indices = train_indices[:n_validation_samples]\n        train_indices = train_indices[n_validation_samples:]\n        if save_path is not None:\n            torch.save(validation_indices, save_path +\n                        '/validation_indices.pt')\n            torch.save(train_indices, save_path + '/train_indices.pt')\n            validation_indices = torch.load(\n                save_path + '/validation_indices.pt')\n            train_indices = torch.load(save_path + '/train_indices.pt')\n        return train_indices, validation_indices\n    # If is in inference(prediction) epochs, generate test indices\n    else:\n        test_indices = torch.range(\n            0, (data_split[i] - 1), dtype=torch.long)\n        if save_path is not None:\n            torch.save(test_indices, save_path + '/test_indices.pt')\n            test_indices = torch.load(save_path + '/test_indices.pt')\n        return test_indices\n\ndef extract_embeddings(g, model, num_all_samples, labels):\n\n    sampler = MultiLayerNeighborSampler([1000, 1000])\n    dataloader = NodeDataLoader(\n        g, torch.arange(g.num_nodes()), sampler,\n        batch_size=num_all_samples,\n        shuffle=False,\n        drop_last=False,\n        num_workers=4)  # 设置合适的 num_workers\n\n    with torch.no_grad():\n        model.eval()\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            batch_features = blocks[0].srcdata['features']\n\n            extract_features = model(blocks, batch_features)\n\n            extract_nids = output_nodes.to(device=extract_features.device, dtype=torch.long)\n            extract_labels = labels[extract_nids]\n\n            assert batch_id == 0\n            extract_nids = extract_nids.cpu().numpy()\n            extract_features = extract_features.cpu().numpy()\n            extract_labels = extract_labels.cpu().numpy()\n\n        A = np.arange(num_all_samples)\n        assert (A == extract_nids).all()\n\n    return extract_nids, extract_features, extract_labels\n\ndef save_embeddings(extract_nids, extract_features, extract_labels, extract_train_tags, path, counter):\n    np.savetxt(path + '/features_' + str(counter) + '.tsv', extract_features, delimiter='\\t')\n    np.savetxt(path + '/labels_' + str(counter) + '.tsv', extract_labels, fmt='%i', delimiter='\\t')\n    with open(path + '/labels_tags_' + str(counter) + '.tsv', 'w') as f:\n        f.write('label\\tmessage_id\\ttrain_tag\\n')\n        for (label, mid, train_tag) in zip(extract_labels, extract_nids, extract_train_tags):\n            f.write(\"%s\\t%s\\t%s\\n\" % (label, mid, train_tag))\n    print(\"Embeddings after inference epoch \" + str(counter) + \" saved.\")\n    print()\n\ndef intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\ndef run_kmeans(extract_features, extract_labels, indices, isoPath=None):\n    # Extract the features and labels of the test tweets\n    indices = indices.cpu().detach().numpy()\n\n    if isoPath is not None:\n        # Remove isolated points\n        temp = torch.load(isoPath)\n        temp = temp.cpu().detach().numpy()\n        non_isolated_index = list(np.where(temp != 1)[0])\n        indices = intersection(indices, non_isolated_index)\n\n    # Extract labels\n    labels_true = extract_labels[indices]\n    # Extract features\n    X = extract_features[indices, :]\n    assert labels_true.shape[0] == X.shape[0]\n    n_test_tweets = X.shape[0]\n\n    # Get the total number of classes     \n    n_classes = len(set(list(labels_true)))\n\n    # k-means clustering\n    kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n    labels = kmeans.labels_\n    print(\"n_classes:\", n_classes)\n    print(\"labels_true list:\", labels_true.tolist())\n    print(\"labels_pred list:\", labels.tolist())\n\n    nmi = metrics.normalized_mutual_info_score(labels_true, labels)\n\n    # Return number of test tweets, number of classes covered by the test tweets, and kMeans cluatering NMI\n    return (n_test_tweets, n_classes, nmi)\n\ndef evaluate_model(extract_features, extract_labels, indices, epoch, num_isolated_nodes, save_path, is_validation=True):\n    message = ''\n    message += '\\nEpoch '\n    message += str(epoch)\n    message += '\\n'\n\n    # with isolated nodes\n    n_tweets, n_classes, nmi = run_kmeans(extract_features, extract_labels, indices)\n    if is_validation:\n        mode = 'validation'\n    else:\n        mode = 'test'\n    message += '\\tNumber of ' + mode + ' tweets: '\n    message += str(n_tweets)\n    message += '\\n\\tNumber of classes covered by ' + mode + ' tweets: '\n    message += str(n_classes)\n    message += '\\n\\t' + mode + ' NMI: '\n    message += str(nmi)\n    if num_isolated_nodes != 0:\n        # without isolated nodes\n        message += '\\n\\tWithout isolated nodes:'\n        n_tweets, n_classes, nmi = run_kmeans(extract_features, extract_labels, indices,\n                                              save_path + '/isolated_nodes.pt')\n        message += '\\tNumber of ' + mode + ' tweets: '\n        message += str(n_tweets)\n        message += '\\n\\tNumber of classes covered by ' + mode + ' tweets: '\n        message += str(n_classes)\n        message += '\\n\\t' + mode + ' NMI: '\n        message += str(nmi)\n    message += '\\n'\n\n    with open(save_path + '/evaluate.txt', 'a') as f:\n        f.write(message)\n    print(message)\n\n    return nmi\n\nclass Metric:\n    def __init__(self):\n        pass\n\n    def __call__(self, outputs, target, loss):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def value(self):\n        raise NotImplementedError\n\n    def name(self):\n        raise NotImplementedError\n\nclass AccumulatedAccuracyMetric(Metric):\n    \"\"\"\n    Works with classification model\n    \"\"\"\n\n    def __init__(self):\n        self.correct = 0\n        self.total = 0\n\n    def __call__(self, outputs, target, loss):\n        pred = outputs[0].data.max(1, keepdim=True)[1]\n        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()\n        self.total += target[0].size(0)\n        return self.value()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n\n    def value(self):\n        return 100 * float(self.correct) / self.total\n\n    def name(self):\n        return 'Accuracy'\n\nclass AverageNonzeroTripletsMetric(Metric):\n    '''\n    Counts average number of nonzero triplets found in minibatches\n    '''\n\n    def __init__(self):\n        self.values = []\n\n    def __call__(self, outputs, target, loss):\n        self.values.append(loss[1])\n        return self.value()\n\n    def reset(self):\n        self.values = []\n\n    def value(self):\n        return np.mean(self.values)\n\n    def name(self):\n        return 'Average nonzero triplets'\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, use_residual=False):\n        super(GATLayer, self).__init__()\n        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n        self.use_residual = use_residual\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        \"\"\"Reinitialize learnable parameters.\"\"\"\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n\n    def edge_attention(self, edges):\n        # Edge UDF for equation (2)\n        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n        a = self.attn_fc(z2)\n        return {'e': F.leaky_relu(a)}\n\n    def message_func(self, edges):\n        # Message UDF for equation (3) & (4)\n        return {'z': edges.src['z'], 'e': edges.data['e']}\n\n    def reduce_func(self, nodes):\n        # Reduce UDF for equation (3) & (4)\n        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n        return {'h': h}\n\n    def forward(self, block):\n        h = block.srcdata['h']\n        z = self.fc(h)\n        block.srcdata['z'] = z\n        block.dstdata['z'] = z[:block.num_dst_nodes()]  # 确保 dstdata['z'] 也被正确设置\n\n        block.apply_edges(self.edge_attention)\n        block.update_all(self.message_func, self.reduce_func)\n\n        if self.use_residual:\n            return z[:block.num_dst_nodes()] + block.dstdata['h']\n        else:\n            return block.dstdata['h']\n\nclass MultiHeadGATLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, num_heads, merge='cat', use_residual=False):\n        super(MultiHeadGATLayer, self).__init__()\n        self.heads = nn.ModuleList()\n        for i in range(num_heads):\n            self.heads.append(GATLayer(in_dim, out_dim, use_residual))\n        self.merge = merge\n\n    def forward(self, block):\n        head_outs = [attn_head(block) for attn_head in self.heads]\n        if self.merge == 'cat':\n            return torch.cat(head_outs, dim=1)\n        else:\n            return torch.mean(torch.stack(head_outs), dim=0)\n\nclass GAT(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, use_residual=False):\n        super(GAT, self).__init__()\n        self.layer1 = MultiHeadGATLayer(in_dim, hidden_dim, num_heads, 'cat', use_residual)\n        self.layer2 = MultiHeadGATLayer(hidden_dim * num_heads, out_dim, 1, 'cat', use_residual)\n\n    def forward(self, blocks, features):\n        blocks[0].srcdata['h'] = features\n\n        h = self.layer1(blocks[0])\n        h = F.elu(h)\n        blocks[1].srcdata['h'] = h\n\n        h = self.layer2(blocks[1])\n\n        return h\n\nclass AvgReadout(nn.Module):\n    def __init__(self):\n        super(AvgReadout, self).__init__()\n\n    def forward(self, seq):\n        return torch.mean(seq, 0)\n\nclass Discriminator(nn.Module):\n    def __init__(self, n_h):\n        super(Discriminator, self).__init__()\n        self.f_k = nn.Bilinear(n_h, n_h, 1)\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Bilinear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n        c_x = torch.unsqueeze(c, 0)\n        c_x = c_x.expand_as(h_pl)\n        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)\n        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)\n        if s_bias1 is not None:\n            sc_1 += s_bias1\n        if s_bias2 is not None:\n            sc_2 += s_bias2\n        logits = torch.cat((sc_1, sc_2), 0)\n        # print(\"testing, shape of logits: \", logits.size())\n        return logits\n\nclass DGI(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, use_residual=False):\n        super(DGI, self).__init__()\n        self.gat = GAT(in_dim, hidden_dim, out_dim, num_heads, use_residual)\n        self.read = AvgReadout()\n        self.sigm = nn.Sigmoid()\n        self.disc = Discriminator(out_dim)\n\n    def forward(self, nf):\n        h_1 = self.gat(nf, False)\n        c = self.read(h_1)\n        c = self.sigm(c)\n        h_2 = self.gat(nf, True)\n        ret = self.disc(c, h_1, h_2)\n        return h_1, ret\n\n    # Detach the return variables\n    def embed(self, nf):\n        h_1 = self.gat(nf, False)\n        return h_1.detach()\n\nclass OnlineTripletLoss(nn.Module):\n    \"\"\"\n    Online Triplets loss\n    Takes a batch of embeddings and corresponding labels.\n    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n    triplets\n    \"\"\"\n\n    def __init__(self, margin, triplet_selector):\n        super(OnlineTripletLoss, self).__init__()\n        self.margin = margin\n        self.triplet_selector = triplet_selector\n\n    def forward(self, embeddings, target):\n        triplets = self.triplet_selector.get_triplets(embeddings, target)\n\n        if embeddings.is_cuda:\n            triplets = triplets.cuda()\n\n        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(ap_distances - an_distances + self.margin)\n\n        return losses.mean(), len(triplets)\n\ndef pdist(vectors):\n    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n        dim=1).view(-1, 1)\n    return distance_matrix\n\nclass TripletSelector:\n    \"\"\"\n    Implementation should return indices of anchors, positive and negative samples\n    return np array of shape [N_triplets x 3]\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def get_triplets(self, embeddings, labels):\n        raise NotImplementedError\n\nclass FunctionNegativeTripletSelector(TripletSelector):\n    \"\"\"\n    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n    Margin should match the margin used in triplet loss.\n    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n    and return a negative index for that pair\n    \"\"\"\n\n    def __init__(self, margin, negative_selection_fn, cpu=True):\n        super(FunctionNegativeTripletSelector, self).__init__()\n        self.cpu = cpu\n        self.margin = margin\n        self.negative_selection_fn = negative_selection_fn\n\n    def get_triplets(self, embeddings, labels):\n        if self.cpu:\n            embeddings = embeddings.cpu()\n        distance_matrix = pdist(embeddings)\n        distance_matrix = distance_matrix.cpu()\n\n        labels = labels.cpu().data.numpy()\n        triplets = []\n\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            negative_indices = np.where(np.logical_not(label_mask))[0]\n            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n            anchor_positives = np.array(anchor_positives)\n\n            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n                loss_values = ap_distance - distance_matrix[\n                    torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n                loss_values = loss_values.data.cpu().numpy()\n                hard_negative = self.negative_selection_fn(loss_values)\n                if hard_negative is not None:\n                    hard_negative = negative_indices[hard_negative]\n                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n\n        if len(triplets) == 0:\n            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n\n        triplets = np.array(triplets)\n\n        return torch.LongTensor(triplets)\n\ndef random_hard_negative(loss_values):\n    hard_negatives = np.where(loss_values > 0)[0]\n    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n\ndef hardest_negative(loss_values):\n    hard_negative = np.argmax(loss_values)\n    return hard_negative if loss_values[hard_negative] > 0 else None\n\ndef HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                              negative_selection_fn=hardest_negative,\n                                                                                              cpu=cpu)\n\ndef RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                             negative_selection_fn=random_hard_negative,\n                                                                                             cpu=cpu)\n\nclass SocialDataset(Dataset):\n    def __init__(self, path, index):\n        self.features = np.load(path + '/' + str(index) + '/features.npy')\n        temp = np.load(path + '/' + str(index) + '/labels.npy', allow_pickle=True)\n        self.labels = np.asarray([int(each) for each in temp])\n        self.matrix = self.load_adj_matrix(path, index)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n    def load_adj_matrix(self, path, index):\n        s_bool_A_tid_tid = sparse.load_npz(path + '/' + str(index) + '/s_bool_A_tid_tid.npz')\n        print(\"Sparse binary adjacency matrix loaded.\")\n        return s_bool_A_tid_tid\n\n    # Used by remove_obsolete mode 1\n    def remove_obsolete_nodes(self, indices_to_remove=None):  # indices_to_remove: list\n        # torch.range(0, (self.labels.shape[0] - 1), dtype=torch.long)\n        if indices_to_remove is not None:\n            all_indices = np.arange(0, self.labels.shape[0]).tolist()\n            indices_to_keep = list(set(all_indices) - set(indices_to_remove))\n            self.features = self.features[indices_to_keep, :]\n            self.labels = self.labels[indices_to_keep]\n            self.matrix = self.matrix[indices_to_keep, :]  # keep row\n            self.matrix = self.matrix[:, indices_to_keep]  # keep column\n            #  remove nodes from matrix\n\n"}
{"type": "source_file", "path": "SocialED/metrics/__init__.py", "content": "from .metric import *\n"}
{"type": "source_file", "path": "SocialED/detector/finevent.py", "content": "import numpy as np\nimport pandas as pd\nimport spacy\nfrom datetime import datetime\nimport torch\nfrom typing import Any, Dict, List\nimport math\nimport os\nimport dgl\nimport dgl.function as fn\nimport gc\nfrom itertools import combinations\nfrom scipy import sparse\nfrom scipy.sparse import coo_matrix\nfrom scipy.sparse import csr_matrix\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom torch.utils.data import Dataset\nfrom torch.functional import Tensor\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nfrom torch.nn import Linear, BatchNorm1d, Sequential, ModuleList, ReLU, Dropout\nfrom torch_geometric.data import Data\nfrom torch_geometric.loader import NeighborSampler\nimport random\nimport argparse\nfrom time import localtime, strftime, time\nimport networkx as nx\nimport json\nimport torch.optim as optim\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\n\nclass FinEvent:\n    r\"\"\"The FinEvent model for social event detection that uses graph neural networks\n    and reinforcement learning for adaptive event detection.\n\n    .. note::\n        This detector uses graph neural networks and reinforcement learning to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    n_epochs : int, optional\n        Number of training epochs. Default: ``1``.\n    window_size : int, optional\n        Size of sliding window for incremental learning. Default: ``3``.\n    patience : int, optional\n        Number of epochs to wait before early stopping. Default: ``5``.\n    margin : float, optional\n        Margin for triplet loss. Default: ``3.0``.\n    lr : float, optional\n        Learning rate. Default: ``1e-3``.\n    batch_size : int, optional\n        Mini-batch size. Default: ``50``.\n    hidden_dim : int, optional\n        Hidden layer dimension. Default: ``128``.\n    out_dim : int, optional\n        Output dimension. Default: ``64``.\n    heads : int, optional\n        Number of attention heads. Default: ``4``.\n    validation_percent : float, optional\n        Percentage of data for validation. Default: ``0.2``.\n    use_hardest_neg : bool, optional\n        Whether to use hardest negative mining. Default: ``False``.\n    is_shared : bool, optional\n        Whether to use shared parameters. Default: ``False``.\n    inter_opt : str, optional\n        Integration option for multi-view features. Default: ``'cat_w_avg'``.\n    is_initial : bool, optional\n        Whether to initialize model. Default: ``True``.\n    sampler : str, optional\n        Type of sampler to use. Default: ``'RL_sampler'``.\n    cluster_type : str, optional\n        Clustering algorithm to use. Default: ``'kmeans'``.\n    threshold_start0 : list, optional\n        Initial thresholds for RL-0. Default: ``[[0.2], [0.2], [0.2]]``.\n    RL_step0 : float, optional\n        Step size for RL-0. Default: ``0.02``.\n    RL_start0 : int, optional\n        Starting point for RL-0. Default: ``0``.\n    eps_start : float, optional\n        Initial epsilon for RL-1. Default: ``0.001``.\n    eps_step : float, optional\n        Step size for epsilon in RL-1. Default: ``0.02``.\n    min_Pts_start : int, optional\n        Initial minimum points for RL-1. Default: ``2``.\n    min_Pts_step : int, optional\n        Step size for minimum points in RL-1. Default: ``1``.\n    use_cuda : bool, optional\n        Whether to use GPU acceleration. Default: ``True``.\n    data_path : str, optional\n        Path to data directory. Default: ``'../model/model_saved/finevent/incremental_test/'``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/finevent/'``.\n    mask_path : str, optional\n        Path to attention mask file. Default: ``None``.\n    log_interval : int, optional\n        Number of steps between logging. Default: ``10``.\n    \"\"\"\n\n    def __init__(self,\n                 # Hyper-parameters\n                 dataset,\n                 n_epochs=1,\n                 window_size=3,\n                 patience=5,\n                 margin=3.0,\n                 lr=1e-3,\n                 batch_size=50,\n                 hidden_dim=128,\n                 out_dim=64,\n                 heads=4,\n                 validation_percent=0.2,\n                 use_hardest_neg=False,\n                 is_shared=False,\n                 inter_opt='cat_w_avg',\n                 is_initial=True,\n                 sampler='RL_sampler',\n                 cluster_type='kmeans',\n\n                 # RL-0\n                 threshold_start0=[[0.2], [0.2], [0.2]],\n                 RL_step0=0.02,\n                 RL_start0=0,\n\n                 # RL-1\n                 eps_start=0.001,\n                 eps_step=0.02,\n                 min_Pts_start=2,\n                 min_Pts_step=1,\n\n                 # Other arguments\n                 use_cuda=True,\n                 data_path='../model/model_saved/finevent/incremental_test/',\n                 file_path='../model/model_saved/finevent/',\n                 mask_path=None,\n                 log_interval=10):\n            \n            self.dataset = dataset\n            \n            # Hyper-parameters\n            self.n_epochs = n_epochs\n            self.window_size = window_size\n            self.patience = patience\n            self.margin = margin\n            self.lr = lr\n            self.batch_size = batch_size\n            self.hidden_dim = hidden_dim\n            self.out_dim = out_dim\n            self.heads = heads\n            self.validation_percent = validation_percent\n            self.use_hardest_neg = use_hardest_neg\n            self.is_shared = is_shared\n            self.inter_opt = inter_opt\n            self.is_initial = is_initial\n            self.sampler = sampler\n            self.cluster_type = cluster_type\n\n            # RL-0\n            self.threshold_start0 = threshold_start0\n            self.RL_step0 = RL_step0\n            self.RL_start0 = RL_start0\n\n            # RL-1\n            self.eps_start = eps_start\n            self.eps_step = eps_step\n            self.min_Pts_start = min_Pts_start\n            self.min_Pts_step = min_Pts_step\n\n            # Other arguments\n            self.use_cuda = use_cuda\n            self.data_path = data_path\n            self.file_path = file_path\n            self.mask_path = mask_path\n            self.log_interval = log_interval\n\n    def preprocess(self):\n        preprocessor = Preprocessor()\n        preprocessor.generate_initial_features(self.dataset)\n        preprocessor.construct_graph(self.dataset)\n        preprocessor.save_edge_index()\n\n    def fit(self):\n        args=self\n        embedding_save_path = args.data_path + '/embeddings'\n        os.makedirs(embedding_save_path, exist_ok=True)\n        print('embedding save path: ', embedding_save_path)\n\n        # record hyper-parameters\n        # with open(embedding_save_path + '/args.txt', 'w') as f:\n        #     json.dump(args.__dict__, f, indent=2)\n\n        print('Batch Size:', args.batch_size)\n        print('Intra Agg Mode:', args.is_shared)\n        print('Inter Agg Mode:', args.inter_opt)\n        print('Reserve node config?', args.is_initial)\n\n        data_split = np.load(args.data_path + '/data_split.npy')\n\n        if args.use_hardest_neg:\n            loss_fn = OnlineTripletLoss(args.margin, HardestNegativeTripletSelector(args.margin))\n        else:\n            loss_fn = OnlineTripletLoss(args.margin, RandomNegativeTripletSelector(args.margin))\n\n        # define metrics\n        BCL_metrics = [AverageNonzeroTripletsMetric()]\n\n        # define detection stage\n        Streaming = FinEvent_model(args)\n\n        # pre-train stage: train on initial graph\n        train_i = 0\n        self.model, self.RL_thresholds = Streaming.initial_maintain(train_i=train_i,\n                                                                    i=0,\n                                                                    metrics=BCL_metrics,\n                                                                    embedding_save_path=embedding_save_path,\n                                                                    loss_fn=loss_fn,\n                                                                    model=None)\n\n        # detection-maintenance stage: incremental training and detection\n        for i in range(1, data_split.shape[0]):\n            # infer every block\n            self.model = Streaming.inference(train_i=train_i,\n                                             i=i,\n                                             metrics=BCL_metrics,\n                                             embedding_save_path=embedding_save_path,\n                                             loss_fn=loss_fn,\n                                             model=self.model,\n                                             RL_thresholds=self.RL_thresholds)\n\n            # maintenance in window size and desert the last block\n            if i % args.window_size == 0 and i != data_split.shape[0] - 1:\n                train_i = i\n                self.model, self.RL_thresholds = Streaming.initial_maintain(train_i=train_i,\n                                                                            i=i,\n                                                                            metrics=BCL_metrics,\n                                                                            embedding_save_path=embedding_save_path,\n                                                                            loss_fn=loss_fn,\n                                                                            model=None)\n\n    def detection(self):\n        args=self\n        \"\"\"\n        :param eval_data_path: Path to the detection data\n        :param eval_metrics: List of detection metrics\n        :param embedding_save_path: Path to save embeddings if needed\n        :param best_model_path: Path to the best trained model\n        :param loss_fn: Loss function used during detection\n        :return: None\n        \"\"\"\n\n        start_time = time()\n\n        # Load detection data\n        print(\"Loading detection data...\")\n        relation_ids = ['entity', 'userid', 'word']\n        homo_data = create_offline_homodataset(args.data_path, [0, 0])\n        multi_r_data = create_multi_relational_graph(args.data_path, relation_ids, [0, 0])\n        print(\"detection data loaded. Time elapsed: {:.2f} seconds\".format(time() - start_time))\n\n        device = torch.device('cuda' if torch.cuda.is_available() and args.use_cuda else 'cpu')\n\n        # Load the best trained model\n        print(\"Loading the best trained model...\")\n        best_model_path = args.data_path + 'embeddings/block_0/models/best.pt'\n        feat_dim = homo_data.x.size(1)\n        num_relations = len(multi_r_data)\n\n        self.model = MarGNN((feat_dim, args.hidden_dim, args.out_dim, args.heads),\n                            num_relations=num_relations, inter_opt=args.inter_opt, is_shared=args.is_shared)\n\n        state_dict = torch.load(best_model_path)\n        self.model.load_state_dict(state_dict)\n        self.model.to(device)  # 将模型移动到指定设备（如果使用GPU）\n\n        # 设置模型为评估模式\n        self.model.eval()\n        print(\"Best model loaded and set to eval mode. Time elapsed: {:.2f} seconds\".format(time() - start_time))\n\n        RL_thresholds = torch.FloatTensor(args.threshold_start0)\n        filtered_multi_r_data = torch.load('../model/model_saved/finevent/multi_remain_data.pt')\n\n        # Sampling nodes\n        print(\"Sampling nodes...\")\n        sampler = MySampler(args.sampler)\n        test_num_samples = homo_data.test_mask.size(0)\n        num_batches = int(test_num_samples / args.batch_size) + 1\n\n        extract_features = []\n\n        for batch in range(num_batches):\n            print(f\"Processing batch {batch + 1}/{num_batches}...\")\n            i_start = args.batch_size * batch\n            i_end = min((batch + 1) * args.batch_size, test_num_samples)\n            batch_nodes = homo_data.test_mask[i_start:i_end]\n            batch_labels = homo_data.y[batch_nodes]\n            adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, sizes=[-1, -1],\n                                         batch_size=args.batch_size)\n\n            # Perform prediction\n            with torch.no_grad():\n                pred = self.model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n\n            extract_features.append(pred.cpu().detach())\n            print(f\"Batch {batch + 1} processed.\")\n\n        extract_features = torch.cat(extract_features, dim=0)\n\n        all_nodes = homo_data.test_mask\n        ground_truths = homo_data.y[all_nodes]\n\n        X = extract_features.cpu().detach().numpy()\n        assert ground_truths.shape[0] == X.shape[0]\n\n        # Get the total number of classes\n        n_classes = len(set(ground_truths.tolist()))\n\n        # k-means clustering\n        print(\"Performing k-means clustering...\")\n        kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n        predictions = kmeans.labels_\n        print(\"k-means clustering done. Time elapsed: {:.2f} seconds\".format(time() - start_time))\n\n        print(\"Detection complete. Total time elapsed: {:.2f} seconds\".format(time() - start_time))\n        return ground_truths, predictions\n\n\n    def evaluate(self, predictions, ground_truths):\n        ars = metrics.adjusted_rand_score(ground_truths, predictions)\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n\n        print(f\"Model Adjusted Rand Index (ARI): {ars}\")\n        print(f\"Model Adjusted Mutual Information (AMI): {ami}\")\n        print(f\"Model Normalized Mutual Information (NMI): {nmi}\")\n        return ars, ami, nmi\n\n\nclass FinEvent_model(FinEvent):\n    def __init__(self, args) -> None:\n        # register args\n        super().__init__(dataset=\"\")\n        self.args = args\n\n    def inference(self,\n                  train_i, i,\n                  metrics,\n                  embedding_save_path,\n                  loss_fn,\n                  model,\n                  RL_thresholds=None,\n                  loss_fn_dgi=None):\n\n        model = MarGNN()\n        # make dir for graph i\n        # ./incremental_0808//embeddings_0403005348/block_xxx\n        save_path_i = embedding_save_path + '/block_' + str(i)\n        if not os.path.isdir(save_path_i):\n            os.mkdir(save_path_i)\n\n        # load data\n        relation_ids: List[str] = ['entity', 'userid', 'word']\n        homo_data = create_homodataset(self.args.data_path, [train_i, i], self.args.validation_percent)\n        multi_r_data = create_multi_relational_graph(self.args.data_path, relation_ids, [train_i, i])\n\n        print('embedding save path: ', embedding_save_path)\n        num_relations = len(multi_r_data)\n\n        device = torch.device('cuda:0' if torch.cuda.is_available() and self.args.use_cuda else 'cpu')\n\n        # input dimension (300 in our paper)\n        features = homo_data.x\n        feat_dim = features.size(1)\n\n        # prepare graph configs for node filtering\n        if self.args.is_initial:\n            print('prepare node configures...')\n            pre_node_dist(multi_r_data, homo_data.x, save_path_i)\n            filter_path = save_path_i\n        else:\n            filter_path = save_path_i\n\n        if model is None:\n            assert 'Cannot find pre-trained model'\n\n        # directly predict\n        message = \"\\n------------ Directly predict on block \" + str(i) + \" ------------\\n\"\n        print(message)\n        print('RL Threshold using in this block:', RL_thresholds)\n\n        model.eval()\n\n        test_indices, labels = homo_data.test_mask, homo_data.y\n        test_num_samples = test_indices.size(0)\n\n        sampler = MySampler(self.args.sampler)\n\n        # filter neighbor in advance to fit with neighbor sampling\n        filtered_multi_r_data = RL_neighbor_filter(self,multi_r_data, RL_thresholds,\n                                                   filter_path) if RL_thresholds is not None and self.args.sampler == 'RL_sampler' else multi_r_data\n\n        # batch testing\n        extract_features = torch.FloatTensor([])\n        num_batches = int(test_num_samples / self.args.batch_size) + 1\n        with torch.no_grad():\n            for batch in range(num_batches):\n\n                start_batch = time()\n\n                # split batch\n                i_start = self.args.batch_size * batch\n                i_end = min((batch + 1) * self.args.batch_size, test_num_samples)\n                batch_nodes = test_indices[i_start:i_end]\n                if not len(batch_nodes):\n                    continue\n                # sampling neighbors of batch nodes\n                adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, sizes=[-1, -1],\n                                             batch_size=self.args.batch_size)\n\n                pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n\n                batch_seconds_spent = time() - start_batch\n\n                # for we haven't shuffle the test indices(see utils.py),\n                # the output embeddings can be simply stacked together\n                extract_features = torch.cat((extract_features, pred.cpu().detach()), dim=0)\n\n                del pred\n                gc.collect()\n\n\n        nmi, ami, ari, = evaluate_model(extract_features,\n                                        labels,\n                                        indices=test_indices,\n                                        epoch=-1,  # just for test\n                                        num_isolated_nodes=0,\n                                        save_path=save_path_i,\n                                        is_validation=False,\n                                        cluster_type=self.args.cluster_type,\n                                        )\n\n        k_score = {\"NMI\": nmi, \"AMI\": ami, \"ARI\": ari}\n        del homo_data, multi_r_data, features, filtered_multi_r_data\n        torch.cuda.empty_cache()\n\n        return model, k_score\n\n    # train on initial/maintenance graphs, t == 0 or t % window_size == 0 in this paper\n\n    def initial_maintain(self,\n                         train_i, i,\n                         metrics,\n                         embedding_save_path,\n                         loss_fn,\n                         model=None,\n                         loss_fn_dgi=None):\n        \"\"\"\n        :param i:\n        :param data_split:\n        :param metrics:\n        :param embedding_save_path:\n        :param loss_fn:\n        :param model:\n        :param loss_fn_dgi:\n        :return:\n        \"\"\"\n\n        # make dir for graph i\n        # ./incremental_0808//embeddings_0403005348/block_xxx\n        save_path_i = embedding_save_path + '/block_' + str(i)\n        if not os.path.isdir(save_path_i):\n            os.mkdir(save_path_i)\n\n        # load data\n        relation_ids: List[str] = ['entity', 'userid', 'word']\n        homo_data = create_homodataset(self.args.data_path, [train_i, i], self.args.validation_percent)\n        multi_r_data = create_multi_relational_graph(self.args.data_path, relation_ids, [train_i, i])\n        num_relations = len(multi_r_data)\n\n        device = torch.device('cuda' if torch.cuda.is_available() and self.args.use_cuda else 'cpu')\n\n        # input dimension (300 in our paper)\n        num_dim = homo_data.x.size(0)\n        feat_dim = homo_data.x.size(1)\n\n        # prepare graph configs for node filtering\n        if self.args.is_initial:\n            print('prepare node configures...')\n            pre_node_dist(multi_r_data, homo_data.x, save_path_i)\n            filter_path = save_path_i\n        else:\n            filter_path = self.args.data_path + str(i)\n\n        if model is None:  # pre-training stage in our paper\n            # print('Pre-Train Stage...')\n            model = MarGNN((feat_dim, self.args.hidden_dim, self.args.out_dim, self.args.heads),\n                           num_relations=num_relations, inter_opt=self.args.inter_opt, is_shared=self.args.is_shared)\n\n        # define sampler\n        sampler = MySampler(self.args.sampler)\n        # load model to device\n        model.to(device)\n\n        # initialize RL thresholds\n        # RL_threshold: [[.5], [.5], [.5]]\n        RL_thresholds = torch.FloatTensor(self.args.threshold_start0)\n\n        # define optimizer\n        optimizer = optim.Adam(model.parameters(), lr=self.args.lr, weight_decay=1e-4)\n\n        # record training log\n        message = \"\\n------------ Start initial training / maintaining using block \" + str(i) + \" ------------\\n\"\n        print(message)\n        with open(save_path_i + '/log.txt', 'a') as f:\n            f.write(message)\n\n        # record the highest validation nmi ever got for early stopping\n        best_vali_nmi = 1e-9\n        best_epoch = 0\n        wait = 0\n        # record validation nmi of all epochs before early stop\n        all_vali_nmi = []\n        # record the time spent in seconds on each batch of all training/maintaining epochs\n        seconds_train_batches = []\n        # record the time spent in mins on each epoch\n        mins_train_epochs = []\n\n        # step13: start training\n        for epoch in range(self.args.n_epochs):\n            start_epoch = time()\n            losses = []\n            total_loss = 0.0\n\n            for metric in metrics:\n                metric.reset()\n\n            # Multi-Agent\n\n            # filter neighbor in advance to fit with neighbor sampling\n            filtered_multi_r_data = RL_neighbor_filter(self ,multi_r_data, RL_thresholds, filter_path) if epoch >= self.args.RL_start0 and self.args.sampler == 'RL_sampler' else multi_r_data\n\n            #filtered_multi_r_data = torch.load(self.args.file_path + 'multi_remain_data.pt')\n\n            print(f\"Epoch {epoch + 1}/{self.args.n_epochs} - Starting training...\")\n            model.train()\n\n            train_num_samples, valid_num_samples = homo_data.train_mask.size(0), homo_data.val_mask.size(0)\n            #train_num_samples, valid_num_samples = homo_data.train_mask.size(0) // 100, homo_data.val_mask.size(0)\n            all_num_samples = train_num_samples + valid_num_samples\n\n            # batch training\n            num_batches = int(train_num_samples / self.args.batch_size) + 1\n            for batch in range(num_batches):\n                start_batch = time()\n\n                # split batch\n                i_start = self.args.batch_size * batch\n                i_end = min((batch + 1) * self.args.batch_size, train_num_samples)\n                batch_nodes = homo_data.train_mask[i_start:i_end]\n                batch_labels = homo_data.y[batch_nodes]\n\n                print(\n                    f\"Epoch {epoch + 1}/{self.args.n_epochs} - Batch {batch + 1}/{num_batches}: Processing nodes {i_start} to {i_end}...\")\n\n                # sampling neighbors of batch nodes\n                adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, sizes=[-1, -1],\n                                             batch_size=self.args.batch_size)\n                optimizer.zero_grad()\n\n                pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n                loss_outputs = loss_fn(pred, batch_labels)\n                loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n\n                losses.append(loss.item())\n                total_loss += loss.item()\n\n                for metric in metrics:\n                    metric(pred, batch_labels, loss_outputs)\n\n                if batch % self.args.log_interval == 0:\n                    message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(batch * self.args.batch_size,\n                                                                              train_num_samples, 100. * batch / ((\n                                                                                                                         train_num_samples // self.args.batch_size) + 1),\n                                                                              np.mean(losses))\n\n                    for metric in metrics:\n                        message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n\n                    print(message)  # 输出到控制台\n                    with open(save_path_i + '/log.txt', 'a') as f:\n                        f.write(message)\n                    losses = []\n\n                del pred, loss_outputs\n                gc.collect()\n\n                print(\n                    f\"Epoch {epoch + 1}/{self.args.n_epochs} - Batch {batch + 1}/{num_batches}: Performing backward pass...\")\n                loss.backward()\n                optimizer.step()\n\n                batch_seconds_spent = time() - start_batch\n                seconds_train_batches.append(batch_seconds_spent)\n\n                del loss\n                gc.collect()\n\n            # step14: print loss\n            total_loss /= (batch + 1)\n            message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch + 1, self.args.n_epochs, total_loss)\n            for metric in metrics:\n                message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n            mins_spent = (time() - start_epoch) / 60\n            message += '\\nThis epoch took {:.2f} mins'.format(mins_spent)\n            message += '\\n'\n            print(message)\n            with open(save_path_i + '/log.txt', 'a') as f:\n                f.write(message)\n            mins_train_epochs.append(mins_spent)\n\n            # validation\n            # infer the representations of all tweets\n            model.eval()\n\n            # we recommand to forward all nodes and select the validation indices instead\n            extract_features = torch.FloatTensor([])\n\n            num_batches = int(all_num_samples / self.args.batch_size) + 1\n\n            # all mask are then splited into mini-batch in order\n            all_mask = torch.arange(0, num_dim, dtype=torch.long)\n\n            for batch in range(num_batches):\n                start_batch = time()\n\n                # split batch\n                i_start = self.args.batch_size * batch\n                i_end = min((batch + 1) * self.args.batch_size, all_num_samples)\n                batch_nodes = all_mask[i_start:i_end]\n                batch_labels = homo_data.y[batch_nodes]\n\n                # sampling neighbors of batch nodes\n                adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, sizes=[-1, -1],\n                                             batch_size=self.args.batch_size)\n\n                pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n\n                extract_features = torch.cat((extract_features, pred.cpu().detach()), dim=0)\n\n                del pred\n                gc.collect()\n\n            # save_embeddings(extract_features, save_path_i)\n            epoch = epoch + 1\n            # evaluate the model: conduct kMeans clustering on the validation and report NMI\n            validation_nmi = evaluate_model(extract_features[homo_data.val_mask],\n                                            homo_data.y,\n                                            indices=homo_data.val_mask,\n                                            epoch=epoch,\n                                            num_isolated_nodes=0,\n                                            save_path=save_path_i,\n                                            is_validation=True,\n                                            cluster_type=self.args.cluster_type)\n            all_vali_nmi.append(validation_nmi)\n\n            # step16: early stop\n            if validation_nmi > best_vali_nmi:\n                best_vali_nmi = validation_nmi\n                best_epoch = epoch\n                wait = 0\n                # save model\n                model_path = save_path_i + '/models'\n                if (epoch == 1) and (not os.path.isdir(model_path)):\n                    os.mkdir(model_path)\n                p = model_path + '/best.pt'\n                torch.save(model.state_dict(), p)\n                print('Best model saved after epoch ', str(epoch))\n            else:\n                wait += 1\n            if wait >= self.args.patience:\n                print('Saved all_mins_spent')\n                print('Early stopping at epoch ', str(epoch))\n                print('Best model was at epoch ', str(best_epoch))\n                break\n            # end one epoch\n\n        # save all validation nmi\n        np.save(save_path_i + '/all_vali_nmi.npy', np.asarray(all_vali_nmi))\n        # save time spent on epochs\n        np.save(save_path_i + '/mins_train_epochs.npy', np.asarray(mins_train_epochs))\n        print('Saved mins_train_epochs.')\n        # save time spent on batches\n        np.save(save_path_i + '/seconds_train_batches.npy', np.asarray(seconds_train_batches))\n        print('Saved seconds_train_batches.')\n\n        # load the best model of the current block\n        best_model_path = save_path_i + '/models/best.pt'\n        model.load_state_dict(torch.load(best_model_path))\n        print(\"Best model loaded.\")\n\n        del homo_data, multi_r_data\n        torch.cuda.empty_cache()\n\n        return model, RL_thresholds\n\nclass Preprocessor:\n    def __init__(self):\n        pass\n\n    def documents_to_features(self, df):\n        self.nlp = spacy.load(\"en_core_web_lg\")\n        features = df.filtered_words.apply(lambda x: self.nlp(' '.join(x)).vector).values\n        return np.stack(features, axis=0)\n\n    def extract_time_feature(self, t_str):\n        t = datetime.fromisoformat(str(t_str))\n        OLE_TIME_ZERO = datetime(1899, 12, 30)\n        delta = t - OLE_TIME_ZERO\n        return [(float(delta.days) / 100000.), (float(delta.seconds) / 86400)]  # 86,400 seconds in day\n\n    def df_to_t_features(self, df):\n        t_features = np.asarray([self.extract_time_feature(t_str) for t_str in df['created_at']])\n        return t_features\n\n    def generate_initial_features(self, df, save_path='../model/model_saved/finevent/'):\n        os.makedirs(save_path, exist_ok=True)\n        print(df.shape)\n        print(df.head(10))\n        d_features = self.documents_to_features(df)\n        print(\"Document features generated.\")\n        t_features = self.df_to_t_features(df)\n        print(\"Time features generated.\")\n        combined_features = np.concatenate((d_features, t_features), axis=1)\n        print(\"Concatenated document features and time features.\")\n        np.save(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy', combined_features)\n        print(\"Initial features saved.\")\n        combined_features = np.load(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy')\n        print(\"Initial features loaded.\")\n        print(combined_features.shape)\n\n    def construct_graph_from_df(self, df, G=None):\n        if G is None:\n            G = nx.Graph()\n        for _, row in df.iterrows():\n            tid = 't_' + str(row['tweet_id'])\n            G.add_node(tid)\n            G.nodes[tid]['tweet_id'] = True  # right-hand side value is irrelevant for the lookup\n\n            user_ids = row['user_mentions']\n            user_ids.append(row['user_id'])\n            user_ids = ['u_' + str(each) for each in user_ids]\n            G.add_nodes_from(user_ids)\n            for each in user_ids:\n                G.nodes[each]['user_id'] = True\n\n            entities = row['entities']\n            G.add_nodes_from(entities)\n            for each in entities:\n                G.nodes[each]['entity'] = True\n\n            words = row['filtered_words']\n            words = ['w_' + each for each in words]\n            G.add_nodes_from(words)\n            for each in words:\n                G.nodes[each]['word'] = True\n\n            edges = []\n            edges += [(tid, each) for each in user_ids]\n            edges += [(tid, each) for each in entities]\n            edges += [(tid, each) for each in words]\n            G.add_edges_from(edges)\n\n        return G\n\n    def construct_incremental_dataset(self, df, save_path, features, test=True):\n        test_ini_size = 500\n        test_incr_size = 100\n\n        data_split = []\n        all_graph_mins = []\n        message = \"\"\n        distinct_dates = df.date.unique()\n        print(\"Number of distinct dates: \", len(distinct_dates))\n        print()\n        message += \"Number of distinct dates: \"\n        message += str(len(distinct_dates))\n        message += \"\\n\"\n\n        print(\"Start constructing initial graph ...\")\n        message += \"\\nStart constructing initial graph ...\\n\"\n        ini_df = df\n        G = self.construct_graph_from_df(ini_df)\n        path = save_path + '0/'\n        os.makedirs(path, exist_ok=True)\n        grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n        message += graph_message\n        print(\"Initial graph saved\")\n        message += \"Initial graph saved\\n\"\n        data_split.append(ini_df.shape[0])\n        all_graph_mins.append(grap_mins)\n        y = ini_df['event_id'].values\n        y = [int(each) for each in y]\n        np.save(path + 'labels.npy', np.asarray(y))\n        print(\"Labels saved.\")\n        message += \"Labels saved.\\n\"\n        indices = ini_df['index'].values.tolist()\n        x = features[indices, :]\n        np.save(path + 'features.npy', x)\n        print(\"Features saved.\")\n        message += \"Features saved.\\n\\n\"\n\n        for i in range(7, len(distinct_dates) - 1):\n            print(\"Start constructing graph \", str(i - 6), \" ...\")\n            message += \"\\nStart constructing graph \"\n            message += str(i - 6)\n            message += \" ...\\n\"\n            incr_df = df.loc[df['date'] == distinct_dates[i]]\n            if test:\n                incr_df = incr_df[:test_incr_size]\n            G = self.construct_graph_from_df(incr_df)\n            path = save_path + str(i - 6) + '/'\n            os.makedirs(path, exist_ok=True)\n            grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n            message += graph_message\n            print(\"Graph \", str(i - 6), \" saved\")\n            message += \"Graph \"\n            message += str(i - 6)\n            message += \" saved\\n\"\n            all_graph_mins.append(grap_mins)\n            y = [int(each) for each in incr_df['event_id'].values]\n            np.save(path + 'labels.npy', y)\n            print(\"Labels saved.\")\n            message += \"Labels saved.\\n\"\n            indices = incr_df['index'].values.tolist()\n            x = features[indices, :]\n            np.save(path + 'features.npy', x)\n            print(\"Features saved.\")\n            message += \"Features saved.\\n\"\n\n        return message, data_split, all_graph_mins\n\n    def construct_graph(self, df, save_path='../model/model_saved/finevent/incremental_test/'):\n        os.makedirs(save_path, exist_ok=True)\n\n        df = df.sort_values(by='created_at').reset_index()\n        df['date'] = [d.date() for d in df['created_at']]\n        f = np.load('../model/model_saved/finevent/features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy')\n        message, data_split, all_graph_mins = self.construct_incremental_dataset(df, save_path, f, True)\n        with open(save_path + \"node_edge_statistics.txt\", \"w\") as text_file:\n            text_file.write(message)\n        np.save(save_path + 'data_split.npy', np.asarray(data_split))\n        np.save(save_path + 'all_graph_mins.npy', np.asarray(all_graph_mins))\n        print(\"Time spent on heterogeneous -> homogeneous graph conversions: \", all_graph_mins)\n\n    def networkx_to_dgl_graph(self, G, save_path=None):\n        message = ''\n        print('Start converting heterogeneous networkx graph to homogeneous dgl graph.')\n        message += 'Start converting heterogeneous networkx graph to homogeneous dgl graph.\\n'\n        all_start = time()\n\n        print('\\tGetting a list of all nodes ...')\n        message += '\\tGetting a list of all nodes ...\\n'\n        start = time()\n        all_nodes = list(G.nodes)\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\tGetting adjacency matrix ...')\n        message += '\\tGetting adjacency matrix ...\\n'\n        start = time()\n        A = nx.to_numpy_array(G)  # 使用稀疏矩阵\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\tGetting lists of nodes of various types ...')\n        message += '\\tGetting lists of nodes of various types ...\\n'\n        start = time()\n        tid_nodes = list(nx.get_node_attributes(G, 'tweet_id').keys())\n        userid_nodes = list(nx.get_node_attributes(G, 'user_id').keys())\n        word_nodes = list(nx.get_node_attributes(G, 'word').keys())\n        entity_nodes = list(nx.get_node_attributes(G, 'entity').keys())\n        del G\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\tConverting node lists to index lists ...')\n        message += '\\tConverting node lists to index lists ...\\n'\n        start = time()\n        indices_tid = [all_nodes.index(x) for x in tid_nodes]\n        indices_userid = [all_nodes.index(x) for x in userid_nodes]\n        indices_word = [all_nodes.index(x) for x in word_nodes]\n        indices_entity = [all_nodes.index(x) for x in entity_nodes]\n        del tid_nodes\n        del userid_nodes\n        del word_nodes\n        del entity_nodes\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------tweet-user-tweet----------------------\n        print('\\tStart constructing tweet-user-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-user matrix ...')\n        message += '\\tStart constructing tweet-user-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-user matrix ...\\n'\n        start = time()\n        w_tid_userid = A[indices_tid, :][:, indices_userid]\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_userid = csr_matrix(w_tid_userid)  # matrix compression\n        del w_tid_userid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_userid_tid = s_w_tid_userid.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-user * user-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-user * user-tweet ...\\n'\n        start = time()\n        s_m_tid_userid_tid = s_w_tid_userid * s_w_userid_tid  # homogeneous message graph\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_userid_tid.npz\", s_m_tid_userid_tid)\n            print(\"Sparse binary userid commuting matrix saved.\")\n            del s_m_tid_userid_tid\n        del s_w_tid_userid\n        del s_w_userid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------tweet-ent-tweet------------------------\n        print('\\tStart constructing tweet-ent-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-ent matrix ...')\n        message += '\\tStart constructing tweet-ent-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-ent matrix ...\\n'\n        start = time()\n        w_tid_entity = A[indices_tid, :][:, indices_entity]\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_entity = csr_matrix(w_tid_entity)\n        del w_tid_entity\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_entity_tid = s_w_tid_entity.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-ent * ent-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-ent * ent-tweet ...\\n'\n        start = time()\n        s_m_tid_entity_tid = s_w_tid_entity * s_w_entity_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_entity_tid.npz\", s_m_tid_entity_tid)\n            print(\"Sparse binary entity commuting matrix saved.\")\n            del s_m_tid_entity_tid\n        del s_w_tid_entity\n        del s_w_entity_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------tweet-word-tweet----------------------\n        print('\\tStart constructing tweet-word-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-word matrix ...')\n        message += '\\tStart constructing tweet-word-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-word matrix ...\\n'\n        start = time()\n        w_tid_word = A[indices_tid, :][:, indices_word]\n        del A\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_word = csr_matrix(w_tid_word)\n        del w_tid_word\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_word_tid = s_w_tid_word.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-word * word-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-word * word-tweet ...\\n'\n        start = time()\n        s_m_tid_word_tid = s_w_tid_word * s_w_word_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_word_tid.npz\", s_m_tid_word_tid)\n            print(\"Sparse binary word commuting matrix saved.\")\n            del s_m_tid_word_tid\n        del s_w_tid_word\n        del s_w_word_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # ----------------------compute tweet-tweet adjacency matrix----------------------\n        print('\\tComputing tweet-tweet adjacency matrix ...')\n        message += '\\tComputing tweet-tweet adjacency matrix ...\\n'\n        start = time()\n        if save_path is not None:\n            s_m_tid_userid_tid = sparse.load_npz(save_path + \"s_m_tid_userid_tid.npz\")\n            print(\"Sparse binary userid commuting matrix loaded.\")\n            s_m_tid_entity_tid = sparse.load_npz(save_path + \"s_m_tid_entity_tid.npz\")\n            print(\"Sparse binary entity commuting matrix loaded.\")\n            s_m_tid_word_tid = sparse.load_npz(save_path + \"s_m_tid_word_tid.npz\")\n            print(\"Sparse binary word commuting matrix loaded.\")\n\n        s_A_tid_tid = s_m_tid_userid_tid + s_m_tid_entity_tid\n        del s_m_tid_userid_tid\n        del s_m_tid_entity_tid\n        s_bool_A_tid_tid = (s_A_tid_tid + s_m_tid_word_tid).astype(bool)  # confirm the connect between tweets\n        del s_m_tid_word_tid\n        del s_A_tid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n        all_mins = (time() - all_start) / 60\n        print('\\tOver all time elapsed: ', all_mins, ' mins\\n')\n        message += '\\tOver all time elapsed: '\n        message += str(all_mins)\n        message += ' mins\\n'\n\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_bool_A_tid_tid.npz\", s_bool_A_tid_tid)\n            print(\"Sparse binary adjacency matrix saved.\")\n            s_bool_A_tid_tid = sparse.load_npz(save_path + \"s_bool_A_tid_tid.npz\")\n            print(\"Sparse binary adjacency matrix loaded.\")\n\n        # create corresponding dgl graph\n        G = dgl.from_scipy(s_bool_A_tid_tid)\n        print('We have %d nodes.' % G.number_of_nodes())\n        print('We have %d edges.' % G.number_of_edges())\n        print()\n        message += 'We have '\n        message += str(G.number_of_nodes())\n        message += ' nodes.'\n        message += 'We have '\n        message += str(G.number_of_edges())\n        message += ' edges.\\n'\n\n        return all_mins, message\n\n    def save_edge_index(self, data_path='../model/model_saved/finevent/incremental_test'):\n        relation_ids = ['entity', 'userid', 'word']\n        for i in range(22):\n            save_multi_relational_graph(data_path, relation_ids, [0, i])\n            print('edge index saved')\n        print('all edge index saved')\n\n\n\n# gen_dataset\ndef sparse_trans(datapath='incremental_test/0/s_m_tid_userid_tid.npz'):\n    relation = sparse.load_npz(datapath)\n    all_edge_index = torch.tensor([], dtype=int)\n    for node in range(relation.shape[0]):\n        neighbor = torch.IntTensor(relation[node].toarray()).squeeze()\n        # del self_loop in advance\n        neighbor[node] = 0\n        neighbor_idx = neighbor.nonzero()\n        neighbor_sum = neighbor_idx.size(0)\n        loop = torch.tensor(node).repeat(neighbor_sum, 1)\n        edge_index_i_j = torch.cat((loop, neighbor_idx), dim=1).t()\n        # edge_index_j_i = torch.cat((neighbor_idx, loop), dim=1).t()\n        self_loop = torch.tensor([[node], [node]])\n        all_edge_index = torch.cat((all_edge_index, edge_index_i_j, self_loop), dim=1)\n        del neighbor, neighbor_idx, loop, self_loop, edge_index_i_j\n    return all_edge_index\n\n\ndef coo_trans(datapath='incremental_test/0/s_m_tid_userid_tid.npz'):\n    relation: csr_matrix = sparse.load_npz(datapath)\n    relation: coo_matrix = relation.tocoo()\n    sparse_edge_index = torch.LongTensor([relation.row, relation.col])\n    return sparse_edge_index\n\n\ndef create_dataset(loadpath, relation, mode):\n    features = np.load(os.path.join(loadpath, str(mode[1]), 'features.npy'))\n    features = torch.FloatTensor(features)\n    print('features loaded')\n    labels = np.load(os.path.join(loadpath, str(mode[1]), 'labels.npy'))\n    print('labels loaded')\n    labels = torch.LongTensor(labels)\n    relation_edge_index = coo_trans(os.path.join(loadpath, str(mode[1]), 's_m_tid_%s_tid.npz' % relation))\n    print('edge index loaded')\n    data = Data(x=features, edge_index=relation_edge_index, y=labels)\n    data_split = np.load(os.path.join(loadpath, 'data_split.npy'))\n    train_i, i = mode[0], mode[1]\n    if train_i == i:\n        data.train_mask, data.val_mask = generateMasks(len(labels), data_split, train_i, i)\n    else:\n        data.test_mask = generateMasks(len(labels), data_split, train_i, i)\n\n    return data\n\n\ndef create_homodataset(loadpath, mode, valid_percent=0.2):\n    features = np.load(os.path.join(loadpath, str(mode[1]), 'features.npy'))\n    features = torch.FloatTensor(features)\n    print('features loaded')\n    labels = np.load(os.path.join(loadpath, str(mode[1]), 'labels.npy'))\n    print('labels loaded')\n    labels = torch.LongTensor(labels)\n    data = Data(x=features, edge_index=None, y=labels)\n    data_split = np.load(os.path.join(loadpath, 'data_split.npy'))\n    train_i, i = mode[0], mode[1]\n    if train_i == i:\n        data.train_mask, data.val_mask = generateMasks(len(labels), data_split, train_i, i, valid_percent)\n    else:\n        data.test_mask = generateMasks(len(labels), data_split, train_i, i)\n\n    return data\n\n\ndef create_offline_homodataset(loadpath, mode):\n    features = np.load(os.path.join(loadpath, str(mode[1]), 'features.npy'))\n    features = torch.FloatTensor(features)\n    print('features loaded')\n    labels = np.load(os.path.join(loadpath, str(mode[1]), 'labels.npy'))\n    print('labels loaded')\n    labels = torch.LongTensor(labels)\n    # relation_edge_index = sparse_trans(os.path.join(loadpath, str(mode[1]), 's_bool_A_tid_tid.npz'))\n    # print('edge index loaded')\n    data = Data(x=features, edge_index=None, y=labels)\n    data.train_mask, data.val_mask, data.test_mask = gen_offline_masks(len(labels))\n\n    return data\n\n\ndef create_multi_relational_graph(loadpath, relations, mode):\n    # multi_relation_edge_index = [sparse_trans(os.path.join(loadpath, str(mode[1]), 's_m_tid_%s_tid.npz' % relation)) for relation in relations]\n    multi_relation_edge_index = [torch.load(loadpath + '/' + str(mode[1]) + '/edge_index_%s.pt' % relation) for relation\n                                 in relations]\n    print('sparse trans...')\n    print('edge index loaded')\n\n    return multi_relation_edge_index\n\n\ndef save_multi_relational_graph(loadpath, relations, mode):\n    for relation in relations:\n        relation_edge_index = sparse_trans(os.path.join(loadpath, str(mode[1]), 's_m_tid_%s_tid.npz' % relation))\n        print('%s have saved' % (os.path.join(loadpath, str(mode[1]), 's_m_tid_%s_tid.npz' % relation)))\n        torch.save(relation_edge_index, loadpath + '/' + str(mode[1]) + '/edge_index_%s.pt' % relation)\n\n\n# utils\ndef intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\n\ndef run_hdbscan(extract_features, extract_labels, indices, is_validation, isoPath=None, ):\n    # 2018:min_cluster_size = 5, copy = True, alpha = 0.8\n    # 2012:min_cluster_size = 8\n\n    indices = indices.cpu().detach().numpy()\n\n    if isoPath is not None:\n        # Remove isolated points\n        temp = torch.load(isoPath)\n        temp = temp.cpu().detach().numpy()\n        non_isolated_index = list(np.where(temp != 1)[0])\n        indices = intersection(indices, non_isolated_index)\n\n    # Extract labels\n    extract_labels = extract_labels.cpu().numpy()\n    labels_true = extract_labels[indices]\n\n    # Extract features\n    # X = extract_features[indices, :]\n    X = extract_features.cpu().detach().numpy()\n    assert labels_true.shape[0] == X.shape[0]\n    nmi, ami, ari = 0, 0, 0\n    for eps in [0.2, 0.3, 0.5, 0.7, 1, 1.2, 1.5, 1.7, 2, 2.2, 2.5, 2.7, 3, 3.2, 3.5, 3.7, 4, 4.2, 4.5, 4.7, 5]:\n        hdb = DBSCAN(eps=eps, min_samples=8)\n        hdb.fit(X)\n\n        labels = hdb.labels_\n        _nmi = metrics.normalized_mutual_info_score(labels_true, labels)\n        _ami = metrics.adjusted_mutual_info_score(labels_true, labels)\n        _ari = metrics.adjusted_rand_score(labels_true, labels)\n        print(f\"_nmi:{_nmi}\\t _ami:{_ami}\\t _ari:{_ari}\\n\")\n        if _nmi > nmi:\n            nmi = _nmi\n            ami = _ami\n            ari = _ari\n\n    return nmi, ami, ari\n\n\ndef run_kmeans(extract_features, extract_labels, indices, isoPath=None):\n    # Extract the features and labels of the test tweets\n    indices = indices.cpu().detach().numpy()\n\n    if isoPath is not None:\n        # Remove isolated points\n        temp = torch.load(isoPath)\n        temp = temp.cpu().detach().numpy()\n        non_isolated_index = list(np.where(temp != 1)[0])\n        indices = intersection(indices, non_isolated_index)\n\n    # Extract labels\n    extract_labels = extract_labels.cpu().numpy()\n    labels_true = extract_labels[indices]\n\n    # Extract features\n    # X = extract_features[indices, :]\n    X = extract_features.cpu().detach().numpy()\n    assert labels_true.shape[0] == X.shape[0]\n    n_test_tweets = X.shape[0]  # 100\n\n    # Get the total number of classes\n    n_classes = len(set(labels_true.tolist()))\n\n    # k-means clustering\n    kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n    labels = kmeans.labels_\n\n    nmi = metrics.normalized_mutual_info_score(labels_true, labels)\n    ami = metrics.adjusted_mutual_info_score(labels_true, labels)\n    ari = metrics.adjusted_rand_score(labels_true, labels)\n\n    # Return number of test tweets, number of classes covered by the test tweets, and kMeans cluatering NMI\n    return n_test_tweets, n_classes, nmi, ami, ari\n\n\ndef evaluate_model(extract_features, extract_labels, indices,\n                   epoch, num_isolated_nodes, save_path, is_validation=True,\n                   cluster_type='kmeans'):\n    message = ''\n    message += '\\nEpoch '\n    message += str(epoch)\n    message += '\\n'\n\n    # with isolated nodes\n    if cluster_type == 'kmeans':\n        n_tweets, n_classes, nmi, ami, ari = run_kmeans(extract_features, extract_labels, indices)\n    elif cluster_type == 'dbscan':\n        pass\n\n    if is_validation:\n        mode = 'validation'\n    else:\n        mode = 'test'\n    message += '\\tNumber of ' + mode + ' tweets: '\n    message += str(n_tweets)\n    message += '\\n\\tNumber of classes covered by ' + mode + ' tweets: '\n    message += str(n_classes)\n    message += '\\n\\t' + mode + ' NMI: '\n    message += str(nmi)\n    message += '\\n\\t' + mode + ' AMI: '\n    message += str(ami)\n    message += '\\n\\t' + mode + ' ARI: '\n    message += str(ari)\n    if cluster_type == 'dbscan':\n        message += '\\n\\t' + mode + ' best_eps: '\n        # message += str(best_eps)\n        message += '\\n\\t' + mode + ' best_min_Pts: '\n        # message += str(best_min_Pts)\n\n    if num_isolated_nodes != 0:\n        # without isolated nodes\n        message += '\\n\\tWithout isolated nodes:'\n        n_tweets, n_classes, nmi, ami, ari = run_kmeans(extract_features, extract_labels, indices,\n                                                        save_path + '/isolated_nodes.pt')\n        message += '\\tNumber of ' + mode + ' tweets: '\n        message += str(n_tweets)\n        message += '\\n\\tNumber of classes covered by ' + mode + ' tweets: '\n        message += str(n_classes)\n        message += '\\n\\t' + mode + ' NMI: '\n        message += str(nmi)\n        message += '\\n\\t' + mode + ' AMI: '\n        message += str(ami)\n        message += '\\n\\t' + mode + ' ARI: '\n        message += str(ari)\n    message += '\\n'\n\n    with open(save_path + '/evaluate.txt', 'a') as f:\n        f.write(message)\n    print(message)\n\n    np.save(save_path + '/%s_metric.npy' % mode, np.asarray([nmi, ami, ari]))\n\n    return nmi\n\n\ndef generateMasks(length, data_split, train_i, i, validation_percent=0.2, save_path=None, remove_obsolete=2):\n    \"\"\"\n    Intro:\n    This function generates train and validation indices for initial/maintenance epochs and test indices for inference(prediction) epochs\n    If remove_obsolete mode 0 or 1:\n    For initial/maintenance epochs:\n    - The first (train_i + 1) blocks (blocks 0, ..., train_i) are used as training set (with explicit labels)\n    - Randomly sample validation_percent of the training indices as validation indices\n    For inference(prediction) epochs:\n    - The (i + 1)th block (block i) is used as test set\n    Note that other blocks (block train_i + 1, ..., i - 1) are also in the graph (without explicit labels, only their features and structural info are leveraged)\n    If remove_obsolete mode 2:\n    For initial/maintenance epochs:\n    - The (i + 1) = (train_i + 1)th block (block train_i = i) is used as training set (with explicit labels)\n    - Randomly sample validation_percent of the training indices as validation indices\n    For inference(prediction) epochs:\n    - The (i + 1)th block (block i) is used as test set\n\n    :param length: the length of label list\n    :param data_split: loaded splited data (generated in custom_message_graph.py)\n    :param train_i, i: flag, indicating for initial/maintenance stage if train_i == i and inference stage for others\n    :param validation_percent: the percent of validation data occupied in whole dataset\n    :param save_path: path to save data\n    :param num_indices_to_remove: number of indices ought to be removed\n    :returns train indices, validation indices or test indices\n    \"\"\"\n    print(length)\n    print(data_split[i])\n\n    # step1: verify total number of nodes\n    assert length == data_split[i]  # 500\n\n    # step2.0: if is in initial/maintenance epochs, generate train and validation indices\n    if train_i == i:\n        # step3: randomly shuffle the graph indices\n        train_indices = torch.randperm(length)\n\n        # step4: get total number of validation indices\n        n_validation_samples = int(length * validation_percent)\n\n        # step5: sample n_validation_samples validation indices and use the rest as training indices\n        validation_indices = train_indices[:n_validation_samples]\n        train_indices = train_indices[n_validation_samples:]\n\n        # print(save_path) #./incremental_0808//embeddings_0403100832/block_0/masks\n        # step6: save indices\n        if save_path is not None:\n            torch.save(train_indices, save_path + '/train_indices.pt')\n            torch.save(validation_indices, save_path + '/validation_indices.pt')\n\n        return train_indices, validation_indices\n        # step2.1: if is in inference(prediction) epochs, generate test indices\n    else:\n        test_indices = torch.arange(0, (data_split[i]), dtype=torch.long)\n        if save_path is not None:\n            torch.save(test_indices, save_path + '/test_indices.pt')\n\n        return test_indices\n\n\ndef gen_offline_masks(length, validation_percent=0.2, test_percent=0.1):\n    test_length = int(length * test_percent)\n    valid_length = int(length * validation_percent)\n    train_length = length - valid_length - test_length\n\n    samples = torch.randperm(length)\n    train_indices = samples[:train_length]\n    valid_indices = samples[train_length:train_length + valid_length]\n    test_indices = samples[train_length + valid_length:]\n\n    return train_indices, valid_indices, test_indices\n\n\ndef save_embeddings(extracted_features, save_path):\n    torch.save(extracted_features, save_path + '/final_embeddings.pt')\n    print('extracted features saved.')\n\n\n# Mysampler\nclass MySampler(object):\n\n    def __init__(self, sampler) -> None:\n        super().__init__()\n\n        self.sampler = sampler\n\n    def sample(self, multi_relational_edge_index: List[Tensor], node_idx, sizes, batch_size):\n\n        if self.sampler == 'RL_sampler':\n            return self._RL_sample(multi_relational_edge_index, node_idx, sizes, batch_size)\n        elif self.sampler == 'random_sampler':\n            return self._random_sample(multi_relational_edge_index, node_idx, batch_size)\n        elif self.sampler == 'const_sampler':\n            return self._const_sample(multi_relational_edge_index, node_idx, batch_size)\n\n    def _RL_sample(self, multi_relational_edge_index: List[Tensor], node_idx, sizes, batch_size):\n\n        outs = []\n        all_n_ids = []\n        for id, edge_index in enumerate(multi_relational_edge_index):\n            loader = NeighborSampler(edge_index=edge_index,\n                                     sizes=sizes,\n                                     node_idx=node_idx,\n                                     return_e_id=False,\n                                     batch_size=batch_size,\n                                     num_workers=0)\n            for id, (_, n_ids, adjs) in enumerate(loader):\n                # print(adjs)\n                outs.append(adjs)\n                all_n_ids.append(n_ids)\n\n            # print(id)\n            assert id == 0\n\n        return outs, all_n_ids\n\n    def _random_sample(self, multi_relational_edge_index: List[Tensor], node_idx, batch_size):\n\n        outs = []\n        all_n_ids = []\n\n        sizes = [random.randint(10, 100), random.randint(10, 50)]\n        for edge_index in multi_relational_edge_index:\n            loader = NeighborSampler(edge_index=edge_index,\n                                     sizes=sizes,\n                                     node_idx=node_idx,\n                                     return_e_id=False,\n                                     batch_size=batch_size,\n                                     num_workers=0)\n            for id, (_, n_ids, adjs) in enumerate(loader):\n                # print(adjs)\n                outs.append(adjs)\n                all_n_ids.append(n_ids)\n\n            # print(id)\n            assert id == 0\n\n        return outs, all_n_ids\n\n    def _const_sample(self, multi_relational_edge_index: List[Tensor], node_idx, batch_size):\n\n        outs = []\n        all_n_ids = []\n        sizes = [25, 15]\n        for edge_index in multi_relational_edge_index:\n\n            loader = NeighborSampler(edge_index=edge_index,\n                                     sizes=sizes,\n                                     node_idx=node_idx,\n                                     return_e_id=False,\n                                     batch_size=batch_size,\n                                     num_workers=0)\n            for id, (_, n_ids, adjs) in enumerate(loader):\n                # print(adjs)\n                outs.append(adjs)\n                all_n_ids.append(n_ids)\n\n            # print(id)\n            assert id == 0\n\n        return outs, all_n_ids\n\n\n# Metrics\nclass Metric:\n    def __init__(self):\n        pass\n\n    def __call__(self, outputs, target, loss):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def value(self):\n        raise NotImplementedError\n\n    def name(self):\n        raise NotImplementedError\n\n\nclass AccumulatedAccuracyMetric(Metric):\n    \"\"\"\n    Works with classification model\n    \"\"\"\n\n    def __init__(self):\n        self.correct = 0\n        self.total = 0\n\n    def __call__(self, outputs, target, loss):\n        pred = outputs[0].data.max(1, keepdim=True)[1]\n        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()\n        self.total += target[0].size(0)\n        return self.value()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n\n    def value(self):\n        return 100 * float(self.correct) / self.total\n\n    def name(self):\n        return 'Accuracy'\n\n\nclass AverageNonzeroTripletsMetric(Metric):\n    '''\n    Counts average number of nonzero triplets found in minibatches\n    '''\n\n    def __init__(self):\n        self.values = []\n\n    def __call__(self, outputs, target, loss):\n        self.values.append(loss[1])\n        return self.value()\n\n    def reset(self):\n        self.values = []\n\n    def value(self):\n        return np.mean(self.values)\n\n    def name(self):\n        return 'Average nonzero triplets'\n\n\n# model\nclass MarGNN(nn.Module):\n    def __init__(self, GNN_args, num_relations, inter_opt, is_shared=False):\n        super(MarGNN, self).__init__()\n\n        self.num_relations = num_relations\n        self.inter_opt = inter_opt\n        self.is_shared = is_shared\n        if not self.is_shared:\n            self.intra_aggs = torch.nn.ModuleList([Intra_AGG(GNN_args) for _ in range(self.num_relations)])\n        else:\n            self.intra_aggs = Intra_AGG(GNN_args)  # shared parameters\n\n        if self.inter_opt == 'cat_w_avg_mlp' or 'cat_wo_avg_mlp':\n            in_dim, hid_dim, out_dim, heads = GNN_args\n            mlp_args = self.num_relations * out_dim, out_dim\n            self.inter_agg = Inter_AGG(mlp_args)\n        else:\n            self.inter_agg = Inter_AGG()\n\n    def forward(self, x, adjs, n_ids, device, RL_thresholds):\n\n        # RL_threshold: tensor([[.5], [.5], [.5]])\n        if RL_thresholds is None:\n            RL_thresholds = torch.FloatTensor([[1.], [1.], [1.]])\n        if not isinstance(RL_thresholds, Tensor):\n            RL_thresholds = torch.FloatTensor(RL_thresholds)\n\n        RL_thresholds = RL_thresholds.to(device)\n\n        features = []\n        for i in range(self.num_relations):\n            if not self.is_shared:\n                # print('Intra Aggregation of relation %d' % i)\n                features.append(self.intra_aggs[i](x[n_ids[i]], adjs[i], device))\n            else:\n                # shared parameters.\n                # print('Shared Intra Aggregation...')\n                features.append(self.intra_aggs(x[n_ids[i]], adjs[i], device))\n\n        features = torch.stack(features, dim=0)\n\n        features = self.inter_agg(features, RL_thresholds, self.inter_opt)\n\n        return features\n\n\n# env\ndef RL_neighbor_filter_full(multi_r_data, RL_thresholds, features, save_path=None):\n    multi_remain_data = []\n    multi_r_score = []\n\n    for i, r_data in enumerate(multi_r_data):\n        r_data: Tensor\n        unique_nodes = r_data[1].unique()\n        num_nodes = unique_nodes.size(0)\n        remain_node_index = torch.tensor([])\n        node_scores = []\n        for node in range(num_nodes):\n            # get neighbors' index\n            neighbors_idx = torch.where(r_data[1] == node)[0]\n            # get neighbors\n            neighbors = r_data[0, neighbors_idx]\n            num_neighbors = neighbors.size(0)\n            neighbors_features = features[neighbors, :]\n            target_features = features[node, :]\n            # calculate euclid distance with broadcast\n            dist: Tensor = torch.norm(neighbors_features - target_features, p=2, dim=1)\n            # smaller is better and we use 'top p' in our paper \n            # => (threshold * num_neighbors)\n            # see RL_neighbor_filter for details\n            sorted_neighbors, sorted_index = dist.sort(descending=False)\n\n            if num_neighbors <= 5:\n                remain_node_index = torch.cat((remain_node_index, neighbors_idx))\n                continue  # add limitations\n\n            threshold = float(RL_thresholds[i])\n\n            num_kept_neighbors = math.ceil(num_neighbors * threshold) + 1\n            filtered_neighbors_idx = neighbors_idx[sorted_index[:num_kept_neighbors]]\n            remain_node_index = torch.cat((remain_node_index, filtered_neighbors_idx))\n\n            filtered_neighbors_scores = sorted_neighbors[:num_kept_neighbors].mean()\n            node_scores.append(filtered_neighbors_scores)\n\n        remain_node_index = remain_node_index.type('torch.LongTensor')\n        edge_index = r_data[:, remain_node_index]\n        multi_remain_data.append(edge_index)\n\n        node_scores = torch.FloatTensor(node_scores)  # from list\n        avg_node_scores = node_scores.sum(dim=1) / num_nodes\n        multi_r_score.append(avg_node_scores)\n\n    return multi_remain_data, multi_r_score\n\n\ndef multi_forward_agg(args, foward_args, iter_epoch):\n    # args prepare\n    model, homo_data, all_num_samples, num_dim, sampler, multi_r_data, filtered_multi_r_data, device, RL_thresholds = foward_args\n\n    if filtered_multi_r_data is None:\n        filtered_multi_r_data = multi_r_data\n\n    extract_features = torch.FloatTensor([])\n\n    num_batches = int(all_num_samples / args.batch_size) + 1\n\n    # all mask are then splited into mini-batch in order\n    all_mask = torch.arange(0, num_dim, dtype=torch.long)\n\n    # multiple forward with RL training\n    for _ in range(iter_epoch):\n\n        # batch training\n        for batch in range(num_batches):\n            start_batch = time()\n\n            # split batch\n            i_start = args.batch_size * batch\n            i_end = min((batch + 1) * args.batch_size, all_num_samples)\n            batch_nodes = all_mask[i_start:i_end]\n            batch_labels = homo_data.y[batch_nodes]\n\n            # sampling neighbors of batch nodes\n            adjs, n_ids = sampler.sample(filtered_multi_r_data, node_idx=batch_nodes, sizes=[-1, -1],\n                                         batch_size=args.batch_size)\n\n            pred = model(homo_data.x, adjs, n_ids, device, RL_thresholds)\n\n            extract_features = torch.cat((extract_features, pred.cpu().detach()), dim=0)\n\n            del pred\n\n        # RL trainig\n        filtered_multi_r_data, multi_r_scores = RL_neighbor_filter_full(filtered_multi_r_data, RL_thresholds,\n                                                                        extract_features)\n        # return new RL thresholds\n\n    return RL_thresholds\n\n\n# layer\nclass GAT(nn.Module):\n    '''\n        adopt this module when using mini-batch\n    '''\n\n    def __init__(self, in_dim, hid_dim, out_dim, heads) -> None:\n        super(GAT, self).__init__()\n        self.GAT1 = GATConv(in_channels=in_dim, out_channels=hid_dim, heads=heads, add_self_loops=False)\n        self.GAT2 = GATConv(in_channels=hid_dim * heads, out_channels=out_dim, add_self_loops=False)\n        self.layers = ModuleList([self.GAT1, self.GAT2])\n        self.norm = BatchNorm1d(heads * hid_dim)\n\n    def forward(self, x, adjs, device):\n        for i, (edge_index, _, size) in enumerate(adjs):\n            # x: Tensor, edge_index: Tensor\n            x, edge_index = x.to(device), edge_index.to(device)\n            x_target = x[:size[1]]  # Target nodes are always placed first.\n            x = self.layers[i]((x, x_target), edge_index)\n            if i == 0:\n                x = self.norm(x)\n                x = F.elu(x)\n                x = F.dropout(x, training=self.training)\n            del edge_index\n\n        return x\n\n\nclass Intra_AGG(nn.Module):\n\n    def __init__(self, GAT_args):\n        super(Intra_AGG, self).__init__()\n\n        in_dim, hid_dim, out_dim, heads = GAT_args\n\n        self.gnn = GAT(in_dim, hid_dim, out_dim, heads)\n\n    def forward(self, x, adjs, device):\n        x = self.gnn(x, adjs, device)\n\n        return x\n\n\nclass Inter_AGG(nn.Module):\n\n    def __init__(self, mlp_args=None):\n        super(Inter_AGG, self).__init__()\n\n        if mlp_args is not None:\n            hid_dim, out_dim = mlp_args\n            self.mlp = nn.Sequential(\n                Linear(hid_dim, hid_dim),\n                BatchNorm1d(hid_dim),\n                ReLU(inplace=True),\n                Dropout(),\n                Linear(hid_dim, out_dim),\n            )\n\n    def forward(self, features, thresholds, inter_opt):\n\n        batch_size = features[0].size(0)\n        features = torch.transpose(features, dim0=0, dim1=1)\n        if inter_opt == 'cat_wo_avg':\n            features = features.reshape(batch_size, -1)\n        elif inter_opt == 'cat_w_avg':\n            # weighted average and concatenate\n            features = torch.mul(features, thresholds).reshape(batch_size, -1)\n        elif inter_opt == 'cat_w_avg_mlp':\n            features = torch.mul(features, thresholds).reshape(batch_size, -1)\n            features = self.mlp(features)\n        elif inter_opt == 'cat_wo_avg_mlp':\n            features = torch.mul(features, thresholds).reshape(batch_size, -1)\n            features = self.mlp(features)\n        elif inter_opt == 'add_wo_avg':\n            features = features.sum(dim=1)\n        elif inter_opt == 'add_w_avg':\n            features = torch.mul(features, thresholds).sum(dim=1)\n\n\n        return features\n\n\n# neighborRL\ndef pre_node_dist(multi_r_data, features, save_path=None):\n    \"\"\"This is used to culculate the similarity between node and \n    its neighbors in advance in order to avoid the repetitive computation.\n\n    Args:\n        multi_r_data ([type]): [description]\n        features ([type]): [description]\n        save_path ([type], optional): [description]. Defaults to None.\n    \"\"\"\n\n    relation_config: Dict[str, Dict[int, Any]] = {}\n    for relation_id, r_data in enumerate(multi_r_data):\n        node_config: Dict[int, Any] = {}\n        r_data: Tensor\n        unique_nodes = r_data[1].unique()\n        num_nodes = unique_nodes.size(0)\n        for node in range(num_nodes):\n            # get neighbors' index\n            neighbors_idx = torch.where(r_data[1] == node)[0]\n            # get neighbors\n            neighbors = r_data[0, neighbors_idx]\n            num_neighbors = neighbors.size(0)\n            neighbors_features = features[neighbors, :]\n            target_features = features[node, :]\n            # calculate euclid distance with broadcast\n            dist: Tensor = torch.norm(neighbors_features - target_features, p=2, dim=1)\n            # smaller is better and we use 'top p' in our paper \n            # (threshold * num_neighbors) see RL_neighbor_filter for details\n            sorted_neighbors, sorted_index = dist.sort(descending=False)\n            node_config[node] = {'neighbors_idx': neighbors_idx,\n                                 'sorted_neighbors': sorted_neighbors,\n                                 'sorted_index': sorted_index,\n                                 'num_neighbors': num_neighbors}\n        relation_config['relation_%d' % relation_id] = node_config\n\n    if save_path is not None:\n        save_path = os.path.join(save_path, 'relation_config.npy')\n        # print(save_path)\n        np.save(save_path, relation_config)\n\n\ndef RL_neighbor_filter(args,multi_r_data, RL_thresholds, load_path):\n\n    load_path = os.path.join(load_path, 'relation_config.npy')\n    relation_config = np.load(load_path, allow_pickle=True)\n    relation_config = relation_config.tolist()\n    relations = list(relation_config.keys())\n    multi_remain_data = []\n\n    for i in range(len(relations)):\n        print(f\"Processing relation {i + 1}/{len(relations)}: {relations[i]}\")\n        edge_index: Tensor = multi_r_data[i]\n        unique_nodes = edge_index[1].unique()\n        num_nodes = unique_nodes.size(0)\n        remain_node_index = torch.tensor([])\n\n        for node in range(num_nodes):\n            if node % 1000 == 0:  # 每处理1000个节点输出一次进度\n                print(f\"  Processing node {node}/{num_nodes}\")\n\n            # extract config\n            neighbors_idx = relation_config[relations[i]][node]['neighbors_idx']\n            num_neighbors = relation_config[relations[i]][node]['num_neighbors']\n            sorted_neighbors = relation_config[relations[i]][node]['sorted_neighbors']\n            sorted_index = relation_config[relations[i]][node]['sorted_index']\n\n            if num_neighbors <= 5:\n                remain_node_index = torch.cat((remain_node_index, neighbors_idx))\n                continue  # add limitations\n\n            threshold = float(RL_thresholds[i])\n\n            num_kept_neighbors = math.ceil(num_neighbors * threshold) + 1\n            filtered_neighbors_idx = neighbors_idx[sorted_index[:num_kept_neighbors]]\n\n            # 修正超出范围的索引\n            valid_indices = filtered_neighbors_idx[filtered_neighbors_idx < edge_index.size(1)]\n            remain_node_index = torch.cat((remain_node_index, valid_indices))\n\n        remain_node_index = remain_node_index.type('torch.LongTensor')\n        # print(remain_node_index)\n\n        # Debugging print statements\n        max_index = remain_node_index.max().item()\n        edge_size = edge_index.size(1)\n        print(f\"Max remain_node_index: {max_index}\")\n        print(f\"Edge index size: {edge_size}\")\n\n        # 修正索引超出范围的情况\n        if max_index >= edge_size:\n            remain_node_index = remain_node_index[remain_node_index < edge_size]\n\n        edge_index = edge_index[:, remain_node_index]\n        multi_remain_data.append(edge_index)\n        print(f\"Finished processing relation {relations[i]}\")\n\n    # 保存 multi_remain_data\n    save_path = os.path.join(args.file_path, 'multi_remain_data.pt')\n    torch.save(multi_remain_data, save_path)\n    print(f\"Filtered multi_r_data saved successfully at {save_path}\")\n\n    return multi_remain_data\n\n\n# TripletLoss\nclass AvgReadout(nn.Module):\n    def __init__(self):\n        super(AvgReadout, self).__init__()\n\n    def forward(self, seq):\n        return torch.mean(seq, 0)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, n_h):\n        super(Discriminator, self).__init__()\n        self.f_k = nn.Bilinear(n_h, n_h, 1)\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Bilinear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n        c_x = torch.unsqueeze(c, 0)\n        c_x = c_x.expand_as(h_pl)\n        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)\n        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)\n        if s_bias1 is not None:\n            sc_1 += s_bias1\n        if s_bias2 is not None:\n            sc_2 += s_bias2\n        logits = torch.cat((sc_1, sc_2), 0)\n        # print(\"testing, shape of logits: \", logits.size())\n        return logits\n\n\nclass OnlineTripletLoss(nn.Module):\n    \"\"\"\n    Online Triplets loss\n    Takes a batch of embeddings and corresponding labels.\n    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of triplets.\n    \"\"\"\n\n    def __init__(self, margin, triplet_selector):\n        super(OnlineTripletLoss, self).__init__()\n        self.margin = margin\n        self.triplet_selector = triplet_selector\n\n    def forward(self, embeddings, target):\n        # 确保 embeddings 至少是二维\n        if embeddings.dim() == 1:\n            embeddings = embeddings.unsqueeze(1)\n\n        triplets = self.triplet_selector.get_triplets(embeddings, target)\n\n        # 打印 embeddings 和 triplets 的形状用于调试\n        print(f\"embeddings shape: {embeddings.shape}\")\n        print(f\"triplets shape: {triplets.shape}\")\n\n        # 检查 triplets 是否为空\n        if triplets.numel() == 0:\n            return torch.tensor(0.0, requires_grad=True), 0\n\n        # 确保 triplets 的索引在 embeddings 的范围内\n        if (triplets >= embeddings.size(0)).any():\n            raise IndexError(\"triplets index out of range of embeddings\")\n\n        # 计算 anchor-positive 和 anchor-negative 的距离\n        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(ap_distances - an_distances + self.margin)\n\n        return losses.mean(), len(triplets)\n\n\ndef pdist(vectors):\n    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n        dim=1).view(-1, 1)\n    return distance_matrix\n\n\nclass TripletSelector:\n    \"\"\"\n    Implementation should return indices of anchors, positive and negative samples\n    return np array of shape [N_triplets x 3]\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def get_triplets(self, embeddings, labels):\n        raise NotImplementedError\n\n\nclass FunctionNegativeTripletSelector(TripletSelector):\n    \"\"\"\n    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n    Margin should match the margin used in triplet loss.\n    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n    and return a negative index for that pair\n    \"\"\"\n\n    def __init__(self, margin, negative_selection_fn, cpu=True):\n        super(FunctionNegativeTripletSelector, self).__init__()\n        self.cpu = cpu\n        self.margin = margin\n        self.negative_selection_fn = negative_selection_fn\n\n    def get_triplets(self, embeddings, labels):\n        if self.cpu:\n            embeddings = embeddings.cpu()\n        distance_matrix = pdist(embeddings)\n        distance_matrix = distance_matrix.cpu()\n\n        labels = labels.cpu().data.numpy()\n        triplets = []\n\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            negative_indices = np.where(np.logical_not(label_mask))[0]\n            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n            anchor_positives = np.array(anchor_positives)\n\n            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n                loss_values = ap_distance - distance_matrix[\n                    torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n                loss_values = loss_values.data.cpu().numpy()\n                hard_negative = self.negative_selection_fn(loss_values)\n                if hard_negative is not None:\n                    hard_negative = negative_indices[hard_negative]\n                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n\n        # if len(triplets) == 0:\n        #    triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n\n        triplets = np.array(triplets)\n\n        return torch.LongTensor(triplets)\n\n\ndef random_hard_negative(loss_values):\n    hard_negatives = np.where(loss_values > 0)[0]\n    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n\n\ndef hardest_negative(loss_values):\n    hard_negative = np.argmax(loss_values)\n    return hard_negative if loss_values[hard_negative] > 0 else None\n\n\ndef HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                              negative_selection_fn=hardest_negative,\n                                                                                              cpu=cpu)\n\n\ndef RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                             negative_selection_fn=random_hard_negative,\n                                                                                             cpu=cpu)\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/word2vec.py", "content": "import argparse\nimport os\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport logging\nfrom sklearn.cluster import KMeans\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n# Setup logging\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nclass WORD2VEC:\n    r\"\"\"The Word2Vec model for social event detection that uses word embeddings \n    to detect events in social media data.\n\n    .. note::\n        This detector uses word embeddings to identify semantic relationships\n        and detect events in social media data. The model requires a dataset\n        object with a load_data() method.\n\n    See :cite:`mikolov2013efficient` for details.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    vector_size : int, optional\n        Dimensionality of word vectors. Default: ``100``.\n    window : int, optional\n        Maximum distance between current and predicted word. Default: ``5``.\n    min_count : int, optional\n        Minimum word frequency. Default: ``1``.\n    sg : int, optional\n        Training algorithm: Skip-gram (1) or CBOW (0). Default: ``1``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/Word2vec/word2vec_model.model'``.\n    \"\"\"\n    def __init__(self,\n                 dataset,\n                 vector_size=100, \n                 window=5,\n                 min_count=1,\n                 sg=1,\n                 file_path='../model/model_saved/Word2vec/word2vec_model.model'):\n\n        self.dataset = dataset.load_data()\n        self.vector_size = vector_size\n        self.window = window\n        self.min_count = min_count\n        self.sg = sg\n        self.file_path = file_path\n        self.df = None\n        self.train_df = None\n        self.test_df = None\n        self.word2vec_model = None\n\n    def preprocess(self):\n        \"\"\"\n        Data preprocessing: tokenization, stop words removal, etc.\n        \"\"\"\n        df = self.dataset[['filtered_words', 'event_id']].copy()\n        df['processed_text'] = df['filtered_words'].apply(\n            lambda x: [str(word).lower() for word in x] if isinstance(x, list) else [])\n        self.df = df\n        return df\n\n    def fit(self):\n        \"\"\"\n        Train the Word2Vec model and save it to a file.\n        \"\"\"\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(self.file_path), exist_ok=True)\n\n        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=42)\n        self.train_df = train_df\n        self.test_df = test_df\n\n        sentences = train_df['processed_text'].tolist()\n\n        logging.info(\"Training Word2Vec model...\")\n        word2vec_model = Word2Vec(sentences=sentences, vector_size=self.vector_size, window=self.window,\n                                  min_count=self.min_count, sg=self.sg)\n        logging.info(\"Word2Vec model trained successfully.\")\n\n        # Save the trained model to a file\n        word2vec_model.save(self.file_path)\n        logging.info(f\"Word2Vec model saved to {self.file_path}\")\n\n        self.word2vec_model = word2vec_model\n        return word2vec_model\n\n    def load_model(self):\n        \"\"\"\n        Load the Word2Vec model from a file.\n        \"\"\"\n        logging.info(f\"Loading Word2Vec model from {self.file_path}...\")\n        word2vec_model = Word2Vec.load(self.file_path)\n        logging.info(\"Word2Vec model loaded successfully.\")\n\n        self.word2vec_model = word2vec_model\n        return word2vec_model\n\n    def document_vector(self, document):\n        \"\"\"\n        Create a document vector by averaging the Word2Vec embeddings of its words.\n        \"\"\"\n        words = [word for word in document if word in self.word2vec_model.wv]\n        if words:\n            return np.mean(self.word2vec_model.wv[words], axis=0)\n        else:\n            return np.zeros(self.vector_size)\n\n    def detection(self):\n        \"\"\"\n        Detect events by representing each document as the average Word2Vec embedding of its words.\n        \"\"\"\n        self.load_model()  # Ensure the model is loaded before making detections\n\n        test_vectors = self.test_df['processed_text'].apply(self.document_vector)\n        predictions = np.stack(test_vectors)\n\n        ground_truths = self.test_df['event_id'].tolist()\n        kmeans = KMeans(n_clusters=len(set(ground_truths)), random_state=42)\n        predictions = kmeans.fit_predict(predictions)\n\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        return ari, ami, nmi\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/eventx.py", "content": "# eventx original paper:\n# Bang Liu, Fred X Han, Di Niu, Linglong Kong, Kunfeng Lai, and Yu Xu. 2020. Story Forest: Extracting Events and Telling Stories from Breaking News. TKDD 14, 3 (2020), 1–28.\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport networkx as nx\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics.pairwise import linear_kernel\nimport random\nfrom sklearn import metrics\nfrom collections import Counter\nimport os\nimport pickle\nfrom sklearn.model_selection import train_test_split\nimport logging\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nclass EventX:\n    r\"\"\"The EventX model for social event detection that extracts events from breaking news\n    using keyword co-occurrence and graph-based clustering.\n\n    .. note::\n        This detector uses keyword co-occurrence and graph-based clustering to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    See :cite:`liu2020story` for details.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/eventX/'``.\n    num_repeats : int, optional\n        Number of times to repeat keyword extraction. Default: ``5``.\n    min_cooccur_time : int, optional\n        Minimum number of times keywords must co-occur. Default: ``2``.\n    min_prob : float, optional\n        Minimum probability threshold for keyword selection. Default: ``0.15``.\n    max_kw_num : int, optional\n        Maximum number of keywords to extract per document. Default: ``3``.\n    \"\"\"\n    def __init__(self,\n                 dataset,\n                 file_path='../model/model_saved/eventX/',\n                 num_repeats=5,\n                 min_cooccur_time=2,\n                 min_prob=0.15,\n                 max_kw_num=3):\n        self.dataset = dataset.load_data()\n        self.file_path = file_path\n        self.mask_path = None\n        self.num_repeats = num_repeats\n        self.min_cooccur_time = min_cooccur_time\n        self.min_prob = min_prob\n        self.max_kw_num = max_kw_num\n\n    # construct offline df\n    def preprocess(self):\n\n        self.split()\n        self.mask_path = '../model/model_saved/eventX/split_indices/test_indices.npy'\n        df = self.dataset\n        logging.info(\"Loaded all_df_words_ents_mid.\")\n\n        test_mask = np.load(self.mask_path, allow_pickle=True)\n        df = df.iloc[test_mask, :]\n        logging.info(\"Test df extracted.\")\n\n        np.save(self.file_path + 'corpus_offline.npy', df.values)\n        logging.info(\"corpus_offline saved.\")\n\n        self.df = self.dataset[['event_id','filtered_words','tweet_id','entities']].copy()\n        logging.info(\"Data preprocessed.\")\n\n    def split(self):\n        \"\"\"\n        Split the dataset into training, validation, and test sets.\n        \"\"\"\n        train_ratio = 0.7\n        test_ratio = 0.2\n        val_ratio = 0.1\n\n        df = self.dataset\n\n        train_data, temp_data = train_test_split(df, test_size=(1 - train_ratio), random_state=42)\n        test_size = test_ratio / (test_ratio + val_ratio)\n        test_data, val_data = train_test_split(temp_data, test_size=test_size, random_state=42)\n\n        os.makedirs(self.file_path + '/split_indices/', exist_ok=True)\n        np.save(self.file_path + '/split_indices/train_indices.npy', train_data.index.to_numpy())\n        np.save(self.file_path + '/split_indices/test_indices.npy', test_data.index.to_numpy())\n        np.save(self.file_path + '/split_indices/val_indices.npy', val_data.index.to_numpy())\n\n        os.makedirs(self.file_path + '/split_data/', exist_ok=True)\n        train_data.to_numpy().dump(self.file_path + '/split_data/train_data.npy')\n        test_data.to_numpy().dump(self.file_path + '/split_data/test_data.npy')\n        val_data.to_numpy().dump(self.file_path + '/split_data/val_data.npy')\n\n        self.train_df = train_data\n        self.test_df = test_data\n        self.val_df = val_data\n\n        logging.info(\n            f\"Data split completed: {len(train_data)} train, {len(test_data)} test, {len(val_data)} validation samples.\")\n\n    def fit(self):\n        pass\n\n\n    def detection(self):\n        kw_pair_dict, kw_dict = self.construct_dict(self.df, self.file_path)\n        m_kw_pair_dict, m_kw_dict, map_index_to_kw, map_kw_to_index = self.map_dicts(kw_pair_dict, kw_dict,\n                                                                                     self.file_path)\n        G = self.construct_kw_graph(kw_pair_dict, kw_dict, self.min_cooccur_time, self.min_prob)\n        communities = []\n        self.detect_kw_communities_iter(G, communities, kw_pair_dict, kw_dict, self.max_kw_num)\n        m_communities = self.map_communities(communities, map_kw_to_index)\n\n        logging.info(\"Model fitted.\")\n        m_tweets, ground_truths = self.map_tweets(self.df, self.file_path)\n        predictions = self.classify_docs(m_tweets, m_communities, map_kw_to_index, self.file_path)\n\n        logging.info(\"Events detected.\")\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n    def construct_dict(self, df, dir_path=None):\n        kw_pair_dict = {}\n        kw_dict = {}\n\n        for _, row in df.iterrows():\n            tweet_id = str(row['tweet_id'])\n            entities = row['entities']\n            entities = ['_'.join(tup) for tup in entities]\n            for each in entities:\n                if each not in kw_dict.keys():\n                    kw_dict[each] = []\n                kw_dict[each].append(tweet_id)\n\n            words = row['filtered_words']\n            for each in words:\n                if each not in kw_dict.keys():\n                    kw_dict[each] = []\n                kw_dict[each].append(tweet_id)\n\n            for r in itertools.combinations(entities + words, 2):\n                r = list(r)\n                r.sort()\n                pair = (r[0], r[1])\n                if pair not in kw_pair_dict.keys():\n                    kw_pair_dict[pair] = []\n                kw_pair_dict[pair].append(tweet_id)\n\n        if dir_path is not None:\n            pickle.dump(kw_dict, open(dir_path + '/kw_dict.pickle', 'wb'))\n            pickle.dump(kw_pair_dict, open(dir_path + '/kw_pair_dict.pickle', 'wb'))\n\n        return kw_pair_dict, kw_dict\n\n    def map_dicts(self, kw_pair_dict, kw_dict, dir_path=None):\n        map_index_to_kw = {}\n        m_kw_dict = {}\n        for i, k in enumerate(kw_dict.keys()):\n            map_index_to_kw['k' + str(i)] = k\n            m_kw_dict['k' + str(i)] = kw_dict[k]\n        map_kw_to_index = {v: k for k, v in map_index_to_kw.items()}\n        m_kw_pair_dict = {}\n        for _, pair in enumerate(kw_pair_dict.keys()):\n            m_kw_pair_dict[(map_kw_to_index[pair[0]], map_kw_to_index[pair[1]])] = kw_pair_dict[pair]\n\n        if dir_path is not None:\n            pickle.dump(m_kw_pair_dict, open(dir_path + '/m_kw_pair_dict.pickle', 'wb'))\n            pickle.dump(m_kw_dict, open(dir_path + '/m_kw_dict.pickle', 'wb'))\n            pickle.dump(map_index_to_kw, open(dir_path + '/map_index_to_kw.pickle', 'wb'))\n            pickle.dump(map_kw_to_index, open(dir_path + '/map_kw_to_index.pickle', 'wb'))\n\n        return m_kw_pair_dict, m_kw_dict, map_index_to_kw, map_kw_to_index\n\n    def construct_kw_graph(self, kw_pair_dict, kw_dict, min_cooccur_time, min_prob):\n        G = nx.Graph()\n        G.add_nodes_from(list(kw_dict.keys()))\n        for pair, co_tid_list in kw_pair_dict.items():\n            if len(co_tid_list) > min_cooccur_time:\n                if (len(co_tid_list) / len(kw_dict[pair[0]]) > min_prob) and (\n                        len(co_tid_list) / len(kw_dict[pair[1]]) > min_prob):\n                    G.add_edge(*pair)\n        return G\n\n    def detect_kw_communities_iter(self, G, communities, kw_pair_dict, kw_dict, max_kw_num=3):\n        connected_components = [c for c in nx.connected_components(G)]\n        while len(connected_components) >= 1:\n            c = connected_components[0]\n            if len(c) < max_kw_num:\n                communities.append(c)\n                G.remove_nodes_from(c)\n            else:\n                c_sub_G = G.subgraph(c).copy()\n                d = nx.edge_betweenness_centrality(c_sub_G)\n                max_value = max(d.values())\n                edges = [key for key, value in d.items() if value == max_value]\n                if len(edges) > 1:\n                    probs = []\n                    for e in edges:\n                        e = list(e)\n                        e.sort()\n                        pair = (e[0], e[1])\n                        co_len = len(kw_pair_dict[pair])\n                        e_prob = (co_len / len(kw_dict[pair[0]]) + co_len / len(kw_dict[pair[1]])) / 2\n                        probs.append(e_prob)\n                    min_prob = min(probs)\n                    min_index = [i for i, j in enumerate(probs) if j == min_prob]\n                    edge_to_remove = edges[min_index[0]]\n                else:\n                    edge_to_remove = edges[0]\n                G.remove_edge(*edge_to_remove)\n            connected_components = [c for c in nx.connected_components(G)]\n\n    def map_communities(self, communities, map_kw_to_index):\n        m_communities = []\n        for cluster in communities:\n            m_cluster = ' '.join(map_kw_to_index[kw] for kw in cluster)\n            m_communities.append(m_cluster)\n        return m_communities\n\n    def classify_docs(self, test_tweets, m_communities, map_kw_to_index, dir_path=None):\n        m_test_tweets = []\n        for doc in test_tweets:\n            m_doc = ' '.join(map_kw_to_index[kw] for kw in doc)     \n            m_test_tweets.append(m_doc)\n\n        vectorizer = TfidfVectorizer()\n        X = vectorizer.fit_transform(m_communities + m_test_tweets)\n        train_size = len(m_communities)\n        test_size = len(m_test_tweets)\n        classes = []\n        for i in range(test_size):\n            cosine_similarities = linear_kernel(X[train_size + i], X[:train_size]).flatten()\n            max_similarity = cosine_similarities[cosine_similarities.argsort()[-1]]\n            related_clusters = [i for i, sim in enumerate(cosine_similarities) if sim == max_similarity]\n            if len(related_clusters) == 1:\n                classes.append(related_clusters[0])\n            else:\n                classes.append(random.choice(related_clusters))\n\n        if dir_path is not None:\n            np.save(dir_path + '/classes.npy', classes)\n\n        return classes\n\n    def map_tweets(self, df, dir_path=None):\n        m_tweets = []\n        ground_truths = []\n        for _, row in df.iterrows():\n            entities = row['entities']\n            entities = ['_'.join(tup) for tup in entities]\n            words = row['filtered_words']\n            m_tweets.append(entities + words)\n            ground_truths.append(row['event_id'])\n\n        if dir_path is not None:\n            with open(os.path.join(dir_path, 'm_tweets.pkl'), 'wb') as f:\n                pickle.dump(m_tweets, f)\n            with open(os.path.join(dir_path, 'ground_truths.pkl'), 'wb') as f:\n                pickle.dump(ground_truths, f)\n\n        return m_tweets, ground_truths\n\n\n# recursive version, can cause RecursionError when running on large graphs. Changed to iterative version below.\ndef detect_kw_communities(G, communities, kw_pair_dict, kw_dict, max_kw_num=3):\n    connected_components = [c for c in nx.connected_components(G)]\n    if len(connected_components) >= 1:\n        c = connected_components[0]\n        if len(c) < max_kw_num:\n            communities.append(c)\n            G.remove_nodes_from(c)\n        else:\n            c_sub_G = G.subgraph(c).copy()\n            d = nx.edge_betweenness_centrality(c_sub_G)\n            max_value = max(d.values())\n            edges = [key for key, value in d.items() if value == max_value]\n            # If two edges have the same betweenness score, the one with lower conditional probability will be removed\n            if len(edges) > 1:\n                probs = []\n                for e in edges:\n                    e = list(e)\n                    e.sort()\n                    pair = (e[0], e[1])\n                    co_len = len(kw_pair_dict[pair])\n                    e_prob = (co_len / len(kw_dict[pair[0]]) + co_len / len(kw_dict[pair[1]])) / 2\n                    probs.append(e_prob)\n                min_prob = min(probs)\n                min_index = [i for i, j in enumerate(probs) if j == min_prob]\n                edge_to_remove = edges[min_index[0]]\n            else:\n                edge_to_remove = edges[0]\n            G.remove_edge(*edge_to_remove)\n        detect_kw_communities(G, communities, kw_pair_dict, kw_dict, max_kw_num)\n    else:\n        return\n\n\n\n\n"}
{"type": "source_file", "path": "SocialED/metrics/metric.py", "content": "# -*- coding: utf-8 -*-\n\"\"\"Evaluation metrics for clustering and classification in social event detection.\"\"\"\n\nimport numpy as np\nimport torch\nfrom sklearn import metrics\nimport networkx as nx\n\n\n\ndef eval_nmi(ground_truths, predictions):\n    \"\"\"\n    Normalized Mutual Information (NMI) score for clustering evaluation.\n\n    Parameters\n    ----------\n    ground_truths : array-like\n        Ground truth labels.\n    predictions : array-like \n        Predicted cluster labels.\n\n    Returns\n    -------\n    nmi : float\n        Normalized Mutual Information score.\n    \"\"\"\n    nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n    return nmi\n\n\ndef eval_ami(ground_truths, predictions):\n    \"\"\"\n    Adjusted Mutual Information (AMI) score for clustering evaluation.\n\n    Parameters\n    ----------\n    ground_truths : array-like\n        Ground truth labels.\n    predictions : array-like\n        Predicted cluster labels.\n\n    Returns\n    -------\n    ami : float\n        Adjusted Mutual Information score.\n    \"\"\"\n    ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n    return ami\n\n\ndef eval_ari(ground_truths, predictions):\n    \"\"\"\n    Adjusted Rand Index (ARI) score for clustering evaluation.\n\n    Parameters\n    ----------\n    ground_truths : array-like\n        Ground truth labels.\n    predictions : array-like\n        Predicted cluster labels.\n\n    Returns\n    -------\n    ari : float\n        Adjusted Rand Index score.\n    \"\"\"\n    ari = metrics.adjusted_rand_score(ground_truths, predictions)\n    return ari\n\n\ndef eval_f1(ground_truths, predictions):\n    \"\"\"\n    F1 score for classification evaluation.\n\n    Parameters\n    ----------\n    ground_truths : array-like\n        Ground truth labels.\n    predictions : array-like\n        Predicted labels.\n\n    Returns\n    -------\n    f1 : float\n        F1 score.\n    \"\"\"\n    f1 = metrics.f1_score(ground_truths, predictions, average='macro')\n    return f1\n\n\ndef eval_acc(ground_truths, predictions):\n    \"\"\"\n    Accuracy score for classification evaluation.\n\n    Parameters\n    ----------\n    ground_truths : array-like\n        Ground truth labels.\n    predictions : array-like\n        Predicted labels.\n\n    Returns\n    -------\n    acc : float\n        Accuracy score.\n    \"\"\"\n    acc = metrics.accuracy_score(ground_truths, predictions)\n    return acc\n\n"}
{"type": "source_file", "path": "SocialED/detector/rplmsed.py", "content": "#!/usr/bin/env python\n# encoding: utf-8\nimport gc\nimport torch.optim as optim\nfrom ignite.contrib.handlers import ProgressBar\nfrom ignite.engine import Engine, Events\nfrom ignite.handlers import EarlyStopping, ModelCheckpoint\nfrom ignite.metrics import RunningAverage, Average\nfrom sklearn.metrics import accuracy_score\nfrom torch.optim.lr_scheduler import StepLR\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom collections import namedtuple, OrderedDict, Counter\nfrom typing import Any, List\nimport math\nimport os\nimport random\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom sklearn import metrics, manifold\nfrom sklearn.cluster import KMeans, DBSCAN, OPTICS\nfrom hdbscan import HDBSCAN\nfrom matplotlib import pyplot as plt\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport datetime\nimport itertools\nimport scipy as sp\nfrom sklearn.model_selection import train_test_split\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n\n\n\nclass RPLMSED:\n    r\"\"\"The RPLMSED model for social event detection that uses pre-trained language models\n    with prompt learning for event detection.\n\n    .. note::\n        This detector uses prompt learning with pre-trained language models to identify events \n        in social media data. The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    plm_path : str, optional\n        Path to pre-trained language model. Default: ``'../model/model_needed/base_plm_model/roberta-large'``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/rplmsed/'``.\n    plm_tuning : bool, optional\n        Whether to fine-tune PLM. Default: ``False``.\n    use_ctx_att : bool, optional\n        Whether to use context attention. Default: ``False``.\n    offline : bool, optional\n        Whether to use offline mode. Default: ``True``.\n    ctx_att_head_num : int, optional\n        Number of context attention heads. Default: ``2``.\n    pmt_feats : tuple, optional\n        Prompt feature indices to use. Default: ``(0,1,2,4)``.\n    batch_size : int, optional\n        Batch size for training. Default: ``128``.\n    lmda1 : float, optional\n        Lambda 1 hyperparameter. Default: ``0.010``.\n    lmda2 : float, optional\n        Lambda 2 hyperparameter. Default: ``0.005``.\n    tao : float, optional\n        Temperature parameter. Default: ``0.90``.\n    optimizer : str, optional\n        Optimizer to use. Default: ``'Adam'``.\n    lr : float, optional\n        Learning rate. Default: ``2e-5``.\n    weight_decay : float, optional\n        Weight decay for optimizer. Default: ``1e-5``.\n    momentum : float, optional\n        Momentum for optimizer. Default: ``0.9``.\n    step_lr_gamma : float, optional\n        Learning rate decay factor. Default: ``0.98``.\n    max_epochs : int, optional\n        Maximum training epochs. Default: ``1``.\n    ckpt_path : str, optional\n        Path to save checkpoints. Default: ``'../model/model_saved/rplmsed/ckpt/'``.\n    eva_data : str, optional\n        Path to evaluation data. Default: ``'../model/model_saved/rplmsed/Eva_data/'``.\n    early_stop_patience : int, optional\n        Early stopping patience. Default: ``2``.\n    early_stop_monitor : str, optional\n        Metric to monitor for early stopping. Default: ``'loss'``.\n    SAMPLE_NUM_TWEET : int, optional\n        Number of tweets to sample. Default: ``60``.\n    WINDOW_SIZE : int, optional\n        Size of sliding window. Default: ``3``.\n    device : str, optional\n        Device to use for computation. Default: ``\"cuda:0\" if available else \"cpu\"``.\n    \"\"\"\n    \n    def __init__(self, dataset,\n                 plm_path='../model/model_needed/base_plm_model/roberta-large',\n                 file_path='../model/model_saved/rplmsed/',\n                 plm_tuning=False,\n                 use_ctx_att=False,\n                 offline=True,\n                 ctx_att_head_num=2,\n                 pmt_feats=(0,1,2,4),\n                 batch_size=128,\n                 lmda1=0.010,\n                 lmda2=0.005,\n                 tao=0.90,\n                 optimizer='Adam',\n                 lr=2e-5,\n                 weight_decay=1e-5,\n                 momentum=0.9,\n                 step_lr_gamma=0.98,\n                 max_epochs=1,\n                 ckpt_path='../model/model_saved/rplmsed/ckpt/',\n                 eva_data=\"../model/model_saved/rplmsed/Eva_data/\",\n                 early_stop_patience=2,\n                 early_stop_monitor='loss',\n                 SAMPLE_NUM_TWEET=60,\n                 WINDOW_SIZE=3,\n                 device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\"):\n        self.dataset = dataset\n        self.plm_path = plm_path\n        self.file_path = file_path\n        self.plm_tuning = plm_tuning\n        self.use_ctx_att = use_ctx_att\n        self.offline = offline\n        self.ctx_att_head_num = ctx_att_head_num\n        self.pmt_feats = pmt_feats\n        self.batch_size = batch_size\n        self.lmda1 = lmda1\n        self.lmda2 = lmda2\n        self.tao = tao\n        self.optimizer = optimizer\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.momentum = momentum\n        self.step_lr_gamma = step_lr_gamma\n        self.max_epochs = max_epochs\n        self.ckpt_path = ckpt_path\n        self.eva_data = eva_data\n        self.early_stop_patience = early_stop_patience\n        self.early_stop_monitor = early_stop_monitor\n        self.SAMPLE_NUM_TWEET = SAMPLE_NUM_TWEET\n        self.WINDOW_SIZE = WINDOW_SIZE\n        self.device = device\n\n    def preprocess(self):\n        preprocessor = Preprocessor(self)\n        preprocessor.preprocess_all(self.dataset)\n\n    def fit(self):\n        torch.manual_seed(2357)\n        args.model_name = os.path.basename(os.path.normpath(args.plm_path))\n        dataset_name = os.path.basename(args.dataset)\n        args.dataset_name = dataset_name.replace(\".npy\", \"\")\n\n        if 'cuda' in args.device:\n            torch.cuda.manual_seed(2357)\n\n        tokenizer = AutoTokenizer.from_pretrained(args.plm_path)\n        data_blocks = load_data_blocks(args.dataset, args, tokenizer)\n        self.model = start_run(args, data_blocks)\n\n    def detection(self):\n        blk = torch.load(f'{args.file_path}cache/cache_long_tail/roberta-large-data.npy')\n        test = blk['test']\n\n        msg_tags = np.array(test['tw_to_ev'], dtype=np.int32)\n        tst_num = msg_tags.shape[0]\n        msg_feats = torch.zeros((tst_num, self.model.feat_size()), device='cpu')\n        ref_num = torch.zeros((tst_num,), dtype=torch.long, device='cpu')\n\n        msg_feats = msg_feats / (ref_num + torch.eq(ref_num, 0).float()).unsqueeze(-1)\n        msg_feats = msg_feats.numpy()\n\n        n_clust = len(test['ev_to_idx'])\n        k_means_score = run_kmeans(msg_feats, n_clust, msg_tags)\n\n        k_means = KMeans(init=\"k-means++\", n_clusters=n_clust, n_init=40, random_state=0)\n        k_means.fit(msg_feats)\n\n        predictions = k_means.labels_\n        ground_truths = msg_tags\n        return predictions, ground_truths\n\n    def evaluate(self, predictions, ground_truths):\n        ars = metrics.adjusted_rand_score(ground_truths, predictions)\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n\n        print(f\"Model Adjusted Rand Index (ARI): {ars}\")\n        print(f\"Model Adjusted Mutual Information (AMI): {ami}\")\n        print(f\"Model Normalized Mutual Information (NMI): {nmi}\")\n        return ars, ami, nmi\n\n\n\nCOLUMNS = [\n    'tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n    'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions'\n]\nDataItem = namedtuple('DataItem', COLUMNS)\n\nclass Preprocessor:\n    def __init__(self):\n        pass\n\n    def preprocess_all(self, dataset):\n        os.makedirs('../model/model_saved/rplmsed/cache', exist_ok=True)\n        print(f\"load data  ... \")\n        df = dataset\n        np_data = df.to_numpy()\n        print(\"\\tDone\")\n\n        blk_data = self.pre_process(np_data)\n        print(f\"save data to '../model/model_saved/rplmsed/cache/offline.npy' ... \", end='')\n        np.save(f'../model/model_saved/rplmsed/cache/offline.npy', blk_data)\n        print(\"\\tDone\")\n\n    def to_sparse_matrix(self, feat_to_tw, tw_num, tao=0):\n        tw_adj = sp.sparse.coo_matrix((tw_num, tw_num), dtype=np.int8)\n        tw_adj = tw_adj.todok()  # convert to dok\n        for f in feat_to_tw.keys():\n            for i in feat_to_tw[f]:\n                for j in feat_to_tw[f]:\n                    tw_adj[i, j] += 1\n\n        tw_adj = tw_adj > tao\n        tw_adj = tw_adj.tocsr().astype(np.int8)\n        return tw_adj\n\n    def build_entity_adj(self, data):\n        tw_num = len(data)\n        feat_to_tw = {}\n        for i, it in enumerate(data):\n            feats = it.entities\n            feats = [e for e, t in feats]\n\n            for f in feats:\n                f = f.lower()\n                if f not in feat_to_tw:\n                    feat_to_tw[f] = set()\n                feat_to_tw[f].add(i)\n\n        return self.to_sparse_matrix(feat_to_tw, tw_num)\n\n    def build_hashtag_adj(self, data):\n        tw_num = len(data)\n        feat_to_tw = {}\n        for i, it in enumerate(data):\n            feats = it.hashtags\n\n            for f in feats:\n                f = f.lower()\n                if f not in feat_to_tw:\n                    feat_to_tw[f] = set()\n                feat_to_tw[f].add(i)\n\n        return self.to_sparse_matrix(feat_to_tw, tw_num)\n\n    def build_words_adj(self, data):\n        tw_num = len(data)\n        feat_to_tw = {}\n        for i, it in enumerate(data):\n            feats = it.words\n\n            for f in feats:\n                f = f.lower()\n                if f not in feat_to_tw:\n                    feat_to_tw[f] = set()\n                feat_to_tw[f].add(i)\n\n        return self.to_sparse_matrix(feat_to_tw, tw_num)\n\n    def build_user_adj(self, data):\n        tw_num = len(data)\n        feat_to_tw = {}\n        for i, it in enumerate(data):\n            feats = it.user_mentions\n            feats.append(it.user_id)\n\n            for f in feats:\n                if f not in feat_to_tw:\n                    feat_to_tw[f] = set()\n                feat_to_tw[f].add(i)\n\n        return self.to_sparse_matrix(feat_to_tw, tw_num)\n\n    def build_creat_at_adj(self, data):\n        tw_num = len(data)\n        tw_feat_idx = []\n        feat_to_idx = {}\n        for i, it in enumerate(data):\n            feats = it.created_at\n            feats = [e for e, t in feats]\n\n            for f in feats:\n                if f not in feat_to_idx:\n                    feat_to_idx[f] = len(feat_to_idx)\n                f_idx = feat_to_idx[f]\n\n                tw_feat_idx.append([i, f_idx])\n\n        tw_feat_val = np.ones((len(tw_feat_idx),), dtype=np.int32)\n        tw_feat_idx = np.array(tw_feat_idx, dtype=np.int64).T\n\n        feat_num = len(feat_to_idx)\n        tw_feat_mat = sp.sparse.coo_matrix(\n            (tw_feat_val, (tw_feat_idx[0, :], tw_feat_idx[1, :])),\n            shape=(tw_num, feat_num),\n            dtype=np.int8)\n\n        tw_adj = tw_feat_mat @ tw_feat_mat.T\n        return tw_adj\n\n    def tweet_to_event(self, data):\n        ev_ids = sorted(set(it.event_id for it in data))\n        ev_to_idx = {eid: i for i, eid in enumerate(ev_ids)}\n        tw_to_ev = [ev_to_idx[it.event_id] for it in data]\n        return tw_to_ev, ev_to_idx\n\n    def build_feats_adj(self, data, feats):\n        feats_adj = [func(data) for f, func in feats]\n        return feats_adj\n\n    def build_feat_adj(self, data, cols):\n        tw_num = len(data)\n        tw_feat_idx = []\n        feat_to_idx = {}\n        cols = [DataItem._fields.index(c) for c in cols] if isinstance(cols, list) else [DataItem._fields.index(cols)]\n        for i, it in enumerate(data):\n            feats = [\n                list(itertools.chain(*it[c])) if isinstance(it[c], list) or isinstance(it[c], tuple) else [it[c]]\n                for c in cols  # 特征列\n            ]\n            feats = [f for cf in feats for f in cf]\n\n            for f in feats:\n                if f not in feat_to_idx:\n                    feat_to_idx[f] = len(feat_to_idx)\n                f_idx = feat_to_idx[f]\n\n                tw_feat_idx.append([i, f_idx])\n\n        tw_feat_val = np.ones((len(tw_feat_idx),), dtype=np.int32)\n        tw_feat_idx = np.array(tw_feat_idx, dtype=np.int64).T\n\n        feat_num = len(feat_to_idx)\n        tw_feat_mat = sp.sparse.coo_matrix(\n            (tw_feat_val, (tw_feat_idx[0, :], tw_feat_idx[1, :])),\n            shape=(tw_num, feat_num),\n            dtype=np.int8)\n\n        tw_adj = tw_feat_mat @ tw_feat_mat.T\n        return tw_adj\n\n    def get_time_relation(self, tw_i, tw_j, delta: datetime.timedelta = datetime.timedelta(hours=4)):\n        a, b = tw_i.created_at, tw_j.created_at\n        return int(a - b < delta) if a > b else int(b - a < delta)\n\n    def make_train_samples(self, tw_adj, tw_to_ev, data):\n        tw_adj_num = len(tw_adj)\n        tw_num = len(tw_to_ev)\n        ev_num = max(tw_to_ev) + 1\n\n        tw_ev_mat = np.zeros(shape=(tw_num, ev_num), dtype=np.int8)\n        for i, e in enumerate(tw_to_ev):\n            tw_ev_mat[i, e] = 1\n\n        eye = sp.sparse.eye(tw_num, tw_num, dtype=np.int8)\n        adj = tw_adj[0] - eye\n        for f in range(1, tw_adj_num):\n            adj = adj + (tw_adj[f] - eye)\n\n        adj = np.asarray(adj.todense())\n\n        pairs = []\n        for i in range(tw_num):\n            ev_idx = tw_to_ev[i]\n            ev_tw_vec = tw_ev_mat[:, ev_idx]\n            ev_tw_num = ev_tw_vec.sum()\n            if ev_tw_num < 5:\n                # print(f\"outlier or small events: {i} -- {tw_to_ev[i]}--{ev_tw_num[tw_to_ev[i]]}\")\n                continue\n\n            adj_i_tw = adj[i, :]\n            adj_i_tw_score = np.exp(adj_i_tw - (1. - ev_tw_vec) * 1e12)\n\n            pos_idx, = np.nonzero(ev_tw_vec)\n            p = sp.special.softmax(adj_i_tw_score.take(pos_idx))\n\n            pos_idx = np.random.choice(pos_idx, size=args.SAMPLE_NUM_TWEET, p=p)\n            # (tag, event, (tweet_a, tweet_b), [feats,])\n            pos_pairs = [\n                (\n                    int(tw_to_ev[i] == tw_to_ev[j]), tw_to_ev[i], (i, j),\n                    list(1 if tw_adj[f][i, j] > 0 else 0 for f in range(tw_adj_num)) + [\n                        self.get_time_relation(data[i], data[j])]\n                )\n                for j in pos_idx\n            ]\n            pairs.extend(pos_pairs)\n\n            neg_idx, = np.nonzero(1 - ev_tw_vec)\n            adj_i_tw_score = np.exp(adj_i_tw - ev_tw_vec * 1e12)\n\n            p = sp.special.softmax(adj_i_tw_score.take(neg_idx))\n\n            neg_idx = np.random.choice(neg_idx, size=args.SAMPLE_NUM_TWEET, p=p)\n\n            # (tag, event, (tweet_a, tweet_b), [feats,])\n            neg_pairs = [\n                (\n                    int(tw_to_ev[i] == tw_to_ev[j]), tw_to_ev[i], (i, j),\n                    list(1 if tw_adj[f][i, j] > 0 else 0 for f in range(tw_adj_num)) + [\n                        self.get_time_relation(data[i], data[j])]\n                )\n                for j in neg_idx\n            ]\n            pairs.extend(neg_pairs)\n\n        return pairs\n\n    def make_ref_samples(self, tw_adj, tw_to_ev, data):\n        tw_adj_num = len(tw_adj)\n        tw_num = len(tw_to_ev)\n\n        pairs = []\n        adj = tw_adj[0]\n        for f in range(1, tw_adj_num):\n            adj = adj + tw_adj[f]\n\n        adj = np.asarray(adj.todense())\n        eye = np.eye(tw_num, tw_num, dtype=np.int8)\n        adj = adj * (1 - eye) + eye\n\n        tw_idx = np.arange(tw_num)\n        for i in range(tw_num):\n            p = sp.special.softmax(np.exp(adj[i]))\n\n            ref_idx = np.random.choice(tw_idx, size=args.SAMPLE_NUM_TWEET * 3, p=p)\n            # (tag, event, (tweet_a, tweet_b), [feats,])\n            ref_pairs = [\n                (\n                    int(tw_to_ev[i] == tw_to_ev[j]),\n                    tw_to_ev[i], (i, j),\n                    list(1 if tw_adj[f][i, j] > 0 else 0 for f in range(tw_adj_num)) + [\n                        self.get_time_relation(data[i], data[j])]\n                )\n                for j in ref_idx\n            ]\n            pairs.extend(ref_pairs)\n\n        return pairs\n\n    def process_block(self, block):\n        blk = {}\n\n        FEAT_COLS = [\n            (\"entities\", self.build_entity_adj),\n            (\"hashtags\", self.build_hashtag_adj),\n            (\"user\", self.build_user_adj),  # user_mentions and user_id\n            (\"words\", self.build_words_adj),\n\n            # (\"create_at\", self.build_creat_at_adj)\n        ]\n\n        for name in [\"train\", \"test\", \"valid\"]:\n            data = block[name]\n            tw_to_ev, ev_to_idx = self.tweet_to_event(data)\n            tw_adj = self.build_feats_adj(data, FEAT_COLS)\n\n            blk[name] = {\n                \"data\": data,\n                \"tw_to_ev\": tw_to_ev,\n                \"ev_to_idx\": ev_to_idx,\n                \"tw_adj\": tw_adj\n            }\n\n            if name == \"train\" or name == \"valid\":\n                if data:\n                    blk[name][\"samples\"] = self.make_train_samples(tw_adj, tw_to_ev, data)\n                else:\n                    blk[name][\"samples\"] = []\n\n            if name == \"test\":\n                if data:\n                    blk[name][\"samples\"] = self.make_ref_samples(tw_adj, tw_to_ev, data)\n                else:\n                    blk[name][\"samples\"] = []\n\n        return blk\n\n    def split_train_test_validation(self, data: List):\n        block = []\n        off_dataset = []\n        for i in range(len(data)):\n            if i == 0:\n                data_size = len(data[i])\n                valid_size = math.ceil(data_size * 0.2)\n                test_size = math.ceil(data_size * 0.1)  # Add test size\n                train, temp = train_test_split(data[i], test_size=valid_size + test_size, random_state=42, shuffle=True)\n                valid, test = train_test_split(temp, test_size=test_size, random_state=42, shuffle=True)\n                block.append({\"train\": train, \"test\": test, \"valid\": valid})\n\n                print(f\"Block {i}: Train size: {len(train)}, Valid size: {len(valid)}, Test size: {len(test)}\")\n\n                off_test_size = math.ceil(data_size * 0.2)\n                off_valid_size = math.ceil(data_size * 0.1)\n                off_train, off_test = train_test_split(data[i], test_size=off_test_size, random_state=42, shuffle=True)\n                off_train, off_valid = train_test_split(off_train, test_size=off_valid_size, random_state=42,\n                                                        shuffle=True)\n\n                print(\"create offline dataset ...\", end=\"\\t\")\n                off_dataset.append(self.process_block({\"train\": off_train, \"test\": off_test, \"valid\": off_valid}))\n                print(\"done\")\n\n                print(f\"save data to '{args.file_path}cache/offline.npy' ... \", end='')\n                np.save(args.file_path + 'cache/offline.npy', off_dataset)\n                print(\"\\tDone\")\n\n            elif i % args.WINDOW_SIZE == 0:\n                sub_data = []\n                for j in range(args.WINDOW_SIZE):\n                    sub_data += data[i - j]\n                sub_data_size = len(sub_data)\n                sub_valid_size = math.ceil(sub_data_size * 0.2)\n                train, valid = train_test_split(sub_data, test_size=sub_valid_size, random_state=42, shuffle=True)\n                block.append({\"train\": train, \"test\": data[i], \"valid\": valid})\n                print(f\"Block {i}: Train size: {len(train)}, Valid size: {len(valid)}, Test size: {len(data[i])}\")\n            else:\n                block.append({\"train\": [], \"test\": data[i], \"valid\": []})\n                print(f\"Block {i}: Train size: 0, Valid size: 0, Test size: {len(data[i])}\")\n\n        return block\n\n    def split_into_blocks(self, data):\n        data = [DataItem(*it) for it in data]\n        data = sorted(data, key=lambda it: it.created_at)\n        groups = itertools.groupby(data, key=lambda it: it.created_at.timetuple().tm_yday)\n        groups = {k: list(g) for k, g in groups}\n\n        days = sorted(groups.keys())\n        blk0 = [groups[d] for d in days[:7]]\n        blk0 = [it for b in blk0 for it in b]\n\n        print(f\"Initial Block 0: {len(blk0)} items\")\n\n        day_blk = [groups[d] for d in days[7:-1]]\n        for idx, blk in enumerate(day_blk, start=1):\n            print(f\"Block {idx}: {len(blk)} items\")\n\n        blocks = [blk0] + day_blk\n        datacount = [len(sublist) for sublist in blocks]\n\n        print(f\"save block datas counts into '{args.file_path}cache/datacount.npy' \", end='')\n        os.makedirs(f'{args.file_path}cache', exist_ok=True)\n        np.save(f'{args.file_path}cache/datacount.npy', datacount)\n        print(\"done\")\n\n        return self.split_train_test_validation(blocks)\n\n    def pre_process(self, data):\n        print(\"split data into blocks... \")\n        blocks = self.split_into_blocks(data)\n        print(\"\\tDone\")\n\n        print(\"process blocks..., \", end='')\n        data_blocks = []\n        for i, blk in enumerate(blocks):\n            print(i, end=\" \")\n            blk = self.process_block(blk)\n            data_blocks.append(blk)\n\n        print(\"\\tDone\")\n        return data_blocks\n\n\n\n\n\ndef get_model(args):\n    return PairPfxTuningEncoder(\n        len(args.pmt_feats), args.plm_path, args.plm_tuning,\n        use_ctx_att=args.use_ctx_att, ctx_att_head_num=args.ctx_att_head_num)\n\n\ndef initialize(model, args, num_train_batch):\n    # parameters = model.parameters()  # 优化器的初始化\n    parameters = [\n        {\n            'name': 'pair_cls',\n            'params': model.pair_cls.parameters(),\n            'lr': args.lr\n        }, {\n            'name': 'pfx_embedding',\n            'params': model.pfx_embedding.parameters(),\n            'lr': args.lr\n        }\n    ]\n\n    if args.plm_tuning:\n        parameters.append(\n            {\n                'name': 'encoder',\n                'params': model.plm.parameters(),\n                'lr': args.lr / 100.\n            }\n        )\n\n    if args.optimizer == 'Adam':\n        optimizer = optim.Adam(parameters, lr=args.lr, weight_decay=args.weight_decay)\n    elif args.optimizer == 'AdamW':\n        optimizer = optim.AdamW(parameters, lr=args.lr, weight_decay=args.weight_decay)\n    elif args.optimizer == 'RAdam':\n        optimizer = optim.RAdam(parameters, lr=args.lr, weight_decay=args.weight_decay)\n    elif args.optimizer == 'SGD':\n        optimizer = optim.SGD(parameters, lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum)\n    else:\n        raise Exception(\"unsupported optimizer %s\" % args.optimizer)\n\n    lr_scheduler = None\n    if args.step_lr_gamma > 0:\n        lr_scheduler = StepLR(optimizer, step_size=num_train_batch, gamma=args.step_lr_gamma)\n\n    return optimizer, lr_scheduler\n\n\ndef batch_to_tensor(batch, args):\n    tags = [tag for tag, evt, a, b, fix, tok, _ in batch]\n    events = [evt for tag, evt, a, b, fix, tok, _ in batch]\n    prefix = [fix for tag, evt, a, b, fix, tok, _ in batch]\n    toks = [tok for tag, evt, a, b, fix, tok, _ in batch]\n    typs = [typ for tag, evt, a, b, fix, tok, typ in batch]\n\n    max_len = min(max([len(it) for it in toks]), 200)\n    toks = [pad_seq(it, pad=args.pad_tok_id, max_len=max_len) for it in toks]\n    toks = torch.tensor(toks, dtype=torch.long)\n    typs = [pad_seq(it, pad=args.pad_tok_id, max_len=max_len) for it in typs]\n    typs = torch.tensor(typs, dtype=torch.long)\n    tags = torch.tensor(tags, dtype=torch.long)\n    events = torch.tensor(events, dtype=torch.long)\n    prefix = torch.tensor(prefix, dtype=torch.long)\n\n    return toks, typs, prefix, tags, events\n\n\n# loss functions\n# cls_loss = torch.nn.BCEWithLogitsLoss()\n\ndef create_trainer(model, optimizer, lr_scheduler, args):\n    evt_proto = torch.zeros((args.train_evt_num, model.feat_size()), device=args.device, requires_grad=False)\n    cls_loss = torch.nn.BCEWithLogitsLoss()\n\n    # update event cluster center prototype by training batch\n    def update_evt_proto(feats, events, alpha):\n        proto = torch.zeros_like(evt_proto)\n        proto.index_add_(0, events, feats)\n\n        ev_idx, ev_idx_inv, ev_count = torch.unique(events, return_inverse=True, return_counts=True)\n        proto_a = torch.index_select(proto, dim=0, index=ev_idx) / ev_count.unsqueeze(-1)\n        proto_b = torch.index_select(evt_proto, dim=0, index=ev_idx)\n\n        source = alpha * proto_a + (1 - alpha) * proto_b\n        # source = proto_a\n        source.detach_()\n        source.requires_grad = False\n\n        evt_proto.index_copy_(0, ev_idx, source)\n\n        return proto_a\n\n    # training logic for iteration\n    def _train_step(_, batch):\n        data = batch_to_tensor(batch, args)\n        dist_loss = torch.nn.PairwiseDistance()\n\n        toks, typs, prefix, tags, events = [x.to(args.device) for x in data]\n        mask = torch.not_equal(toks, args.pad_tok_id).to(args.device)\n\n        model.train()\n        logit, left_feat = model(toks, typs, prefix, mask)\n\n        loss = cls_loss(logit, tags.float())\n        pred = torch.gt(logit, 0.)\n\n        feats = left_feat\n        evt_feats = update_evt_proto(feats, events, 0.8)\n        protos = evt_proto.index_select(0, events)\n\n        intra_dist = dist_loss(feats, protos)\n        intra_dist_loss = intra_dist.mean()\n\n        rand_idx = torch.randperm(evt_feats.size(0), device=args.device)\n        rand_evt_feats = evt_feats.index_select(0, rand_idx)\n        inter_dist_loss = torch.nn.functional.pairwise_distance(evt_feats, rand_evt_feats)\n\n        inter_dist_loss = torch.maximum(10 - inter_dist_loss, torch.zeros_like(inter_dist_loss)).mean()\n\n        if args.lmda1 > 0.:\n            loss = loss + args.lmda1 * inter_dist_loss + args.lmda2 * intra_dist_loss\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        del toks, prefix, mask\n        acc = accuracy_score(tags.cpu(), pred.cpu())\n\n        return loss, acc, inter_dist_loss, intra_dist_loss\n\n    # Define trainer engine\n    trainer = Engine(_train_step)\n\n    # metrics for trainer\n    RunningAverage(output_transform=lambda x: x[0]).attach(trainer, 'loss')\n    RunningAverage(output_transform=lambda x: x[1]).attach(trainer, 'acc')\n    RunningAverage(output_transform=lambda x: x[2]).attach(trainer, 'inter')\n    RunningAverage(output_transform=lambda x: x[3]).attach(trainer, 'intra')\n\n    # Add progress bar showing trainer metrics\n    mtcs = ['loss', 'acc', 'inter', 'intra']\n    ProgressBar(persist=True).attach(trainer, mtcs)\n    return trainer\n\n\ndef create_evaluator(model, args):\n    cls_loss = torch.nn.BCEWithLogitsLoss()\n\n    def _validation_step(_, batch):\n        model.eval()\n        with torch.no_grad():\n            data = batch_to_tensor(batch, args)\n\n            toks, typs, prefix, tags, events = [x.to(args.device) for x in data]\n            mask = torch.not_equal(toks, args.pad_tok_id).to(args.device)\n\n            logit, left_feat = model(toks, typs, prefix, mask)\n\n            loss = cls_loss(logit, tags.float())\n            pred = torch.gt(logit, 0.)\n\n            acc = accuracy_score(tags.cpu(), pred.cpu())\n\n            return loss, acc\n\n    evaluator = Engine(_validation_step)\n    Average(lambda x: x[0]).attach(evaluator, 'loss')\n    Average(lambda x: x[1]).attach(evaluator, 'acc')\n\n    ProgressBar(persist=True).attach(evaluator)\n    return evaluator\n\n\ndef create_tester(model, args, msg_feats, ref_num):\n    cls_loss = torch.nn.BCEWithLogitsLoss()\n\n    def _test_step(_, batch):\n        model.eval()\n        with torch.no_grad():\n            data = batch_to_tensor(batch, args)\n\n            toks, typs, prefix, tags, events = [x.to(args.device) for x in data]\n            mask = torch.not_equal(toks, args.pad_tok_id).to(args.device)\n\n            idx = [a for tag, evt, a, b, fix, tok, _ in batch]\n            idx = torch.tensor(idx, dtype=torch.long).to(args.device)\n\n            me = [True if a == b else False for tag, evt, a, b, fix, tok, _ in batch]\n            me = torch.tensor(me, dtype=torch.long).to(args.device)\n\n            logit, left_feat = model(toks, typs, prefix, mask)\n\n            loss = cls_loss(logit, tags.float())\n            pred = torch.gt(logit, 0.)\n\n            # top_k_values, top_k_indices = torch.topk(torch.sigmoid(logit), k=90, largest=True)#\n\n            msk = torch.gt(torch.sigmoid(logit), args.tao)\n\n            acc = accuracy_score(tags.cpu(), pred.cpu())\n            msk = torch.logical_or(msk, me)\n\n            msk_idx, = torch.nonzero(msk, as_tuple=True)\n            idx = torch.index_select(idx, dim=0, index=msk_idx)\n            # idx = torch.index_select(idx, dim=0, index=top_k_indices)#\n            ## feats = (pfx_feat + left_feat) / 2.\n            feats = left_feat\n            feat = torch.index_select(feats, dim=0, index=msk_idx)\n            evt = torch.index_select(events, dim=0, index=msk_idx)\n\n            # feat = torch.index_select(feats, dim=0, index=top_k_indices)#\n            # evt = torch.index_select(events, dim=0, index=top_k_indices)#\n\n            msg_feats.index_add_(0, idx.cpu(), feat.cpu())\n            ref_num.index_add_(0, idx.cpu(), torch.ones_like(evt, device='cpu'))\n\n            return loss, acc\n\n    tester = Engine(_test_step)\n\n    Average(lambda x: x[0]).attach(tester, 'loss')\n    Average(lambda x: x[1]).attach(tester, 'acc')\n\n    ProgressBar(persist=True).attach(tester)\n    return tester\n\n\ndef test_on_block(model, cfg, blk, b=0):\n    test = blk['test']\n    print(\"Length of test['samples']:\", len(test['samples']))\n\n    msg_tags = np.array(test['tw_to_ev'], dtype=np.int32)\n\n    tst_num = msg_tags.shape[0]\n    msg_feats = torch.zeros((tst_num, model.feat_size()), device='cpu')  # cfg.feat_dim\n    ref_num = torch.zeros((tst_num,), dtype=torch.long, device='cpu')\n\n    train, valid = blk['train'], blk['valid']\n    cfg.train_evt_num = len(train['ev_to_idx'])\n    # print(\"cfg.train_evt_num:\", cfg.train_evt_num)\n    test_gen, test_batch_num = data_generator(test['samples'], cfg.batch_size)\n    tester = create_tester(model, cfg, msg_feats, ref_num)\n\n    print(\"Evaluate model on test data ...\")\n    test_state = tester.run(test_gen, epoch_length=test_batch_num)\n\n    print(\"Available metrics:\", test_state.metrics.keys())  # Add debug print to check available metrics\n    test_metrics = [(m, test_state.metrics[m]) for m in ['loss', 'acc']]\n    test_metrics = \", \".join([(\"%s: %.4f\" % (m, v)) for m, v in test_metrics])\n    print(f\"Test: {test_metrics}\\n\", flush=True)\n\n    # clustering & report\n    msg_feats = msg_feats / (ref_num + torch.eq(ref_num, 0).float()).unsqueeze(-1)\n    msg_feats = msg_feats.numpy()\n    n_clust = len(test['ev_to_idx'])\n\n    if not os.path.exists(cfg.eva_data):\n        os.makedirs(cfg.eva_data)\n\n    Evaluate_datas = {'msg_feats': msg_feats, 'msg_tags': msg_tags, 'n_clust': n_clust}\n    if cfg.offline:\n        print(f\"save Evaluate_datas_offline to '{cfg.eva_data}/evaluate_data_long_tail.npy'\", end='\\t')\n        np.save(f'{cfg.eva_data}/evaluate_data_long_tail.npy', Evaluate_datas)\n    else:\n        print(f\"save Evaluate_datas{b} to '{cfg.eva_data}/evaluate_data_M{b}.npy'\", end='\\t')\n        e_path = cfg.eva_data + f'/evaluate_data_M{b}.npy'\n        np.save(e_path, Evaluate_datas)\n\n    print('done')\n\n    k_means_score = run_kmeans(msg_feats, n_clust, msg_tags)\n    dbscan_score = run_hdbscan(msg_feats, msg_tags)\n\n    del msg_feats\n\n    return k_means_score, dbscan_score\n\n\ndef load_ckpt(model, args, ckpt, b):\n    print(f\"Load best ckpt for block {b} from '{ckpt}'\")\n\n    ckpt = torch.load(ckpt, map_location=args.device)\n    model.load_state_dict(ckpt['model'], strict=False)\n\n    ckpt_args = ckpt['args']\n    ckpt_args.dataset = args.dataset\n    ckpt_args.plm_path = args.plm_path\n    ckpt_args.batch_size = args.batch_size\n    ckpt_args.device = args.device\n    ckpt_args.tao = args.tao\n\n    return model, ckpt_args\n\n\ndef start_run(cfg, blocks):\n    tokenizer = AutoTokenizer.from_pretrained(args.plm_path)\n    cfg.pad_tok_id = tokenizer.pad_token_id\n    model = get_model(cfg).to(cfg.device)\n    # print settings\n    print_table([(k, str(v)[0:60]) for k, v in vars(cfg).items()])\n    kmeans_scores, dbscan_scores = [], []\n    ckpt_list = []\n\n    for b, blk in enumerate(blocks):\n        print(f\"Processing block-{b}...\", flush=True)\n        print(f\"Block-{b} content keys: {blk.keys()}\")\n\n        train, valid, test = (blk[n] for n in ('train', 'valid', 'test'))\n        print(\n            f\"Train samples: {len(train['samples'])}, Valid samples: {len(valid['samples'])}, Test samples: {len(test['samples'])}\")\n\n        if b > 0:\n            print(f\"test model on data block-{b} ...\", flush=True)\n            kms_score, dbs_score = test_on_block(model, cfg, blk, b)\n            kmeans_scores.append(kms_score)\n            dbscan_scores.append(dbs_score)\n\n            print(\"KMeans:\")\n            print_scores(kmeans_scores)\n            print(\"DBSCAN:\")\n            print_scores(dbscan_scores)\n\n        if b % 3 == 0:\n            gc.collect()\n            print(f\"train on data block-{b} ...\", flush=True)\n            model, ckpt = train_on_block(model, cfg, blk, b)\n            ckpt_list.append(ckpt)\n\n        if b == 0 and args.offline:\n            print(f\"close test on data block-{b} ...\", flush=True)\n            kms_score, dbs_score = test_on_block(model, args, blk, b)\n            kmeans_scores.append(kms_score)\n            dbscan_scores.append(dbs_score)\n\n            print(\"KMeans:\")\n            print_scores(kmeans_scores)\n            print(\"DBSCAN:\")\n            print_scores(dbscan_scores)\n\n    if args.offline:\n        ckpt_list_file = os.path.join(args.ckpt_path, 'best_off_model.txt')\n    else:\n        ckpt_list_file = os.path.join(args.ckpt_path, 'ckpt_list.txt')\n\n    with open(ckpt_list_file, 'w', encoding='utf8') as f:\n        ckpt_list = [str(p) for p in ckpt_list]\n        f.write(\"\\n\".join(ckpt_list))\n\n    return model\n\n\ndef train_on_block(model, args, blk, blk_id=0):\n    # reload plm in tuning mode\n    if blk_id > 0 and args.plm_tuning:\n        print(\"accumulate reload PLM parameters\", flush=True)\n        model.accumulate_reload_plm(args.device)\n    train, valid = blk['train'], blk['valid']\n    # fewer data for code test\n    ###\n    # train['samples'] = train['samples'][:500]\n    # valid['samples'] = valid['samples'][:200]\n\n    args.train_evt_num = len(train['ev_to_idx'])\n\n    train_gen, train_batch_num = data_generator(train['samples'], args.batch_size, True, True)\n    valid_gen, valid_batch_num = data_generator(valid['samples'], args.batch_size, False, True)\n\n    # create model, optimizer and learning rate scheduler\n    optimizer, lr_scheduler = initialize(model, args, train_batch_num)\n\n    # print model parameters\n    # summary(model, input_size=((args.batch_size, 50), (args.batch_size, 50)))\n\n    # Setup model trainer and evaluator\n    trainer = create_trainer(model, optimizer, lr_scheduler, args)\n    evaluator = create_evaluator(model, args)\n\n    @trainer.on(Events.EPOCH_STARTED)\n    def log_learning_rates(_):\n        for param_group in optimizer.param_groups:\n            print(\"{} lr: {:>1.2e}\".format(param_group.get('name', ''), param_group[\"lr\"]))\n\n    # Run model evaluation every epoch and show results\n    @trainer.on(Events.EPOCH_COMPLETED(every=1))\n    def evaluate_model():  # eng\n        print(\"Evaluate model on eval data ...\")\n        val_state = evaluator.run(valid_gen, epoch_length=valid_batch_num)\n\n        eval_metrics = [(m, val_state.metrics[m]) for m in ['loss', 'acc']]\n        eval_metrics = \", \".join([(\"%s: %.4f\" % (m, v)) for m, v in eval_metrics])\n\n        print(f\"Eval: {eval_metrics}\\n\", flush=True)\n\n    def score_function(_):\n        if args.early_stop_monitor == 'loss':\n            return - evaluator.state.metrics['loss']\n        elif args.early_stop_monitor in evaluator.state.metrics:\n            return evaluator.state.metrics[args.early_stop_monitor]\n        else:\n            raise Exception('unsupported metric %s' % args.early_stop_monitor)\n\n    if args.offline:\n        filename_prefix = f\"{args.model_name}-{'tuning' if args.plm_tuning else 'fixed'}-{args.dataset_name}-offline\"\n    else:\n        filename_prefix = f\"{args.model_name}-{'tuning' if args.plm_tuning else 'fixed'}-{args.dataset_name}-{blk_id}\"\n    ckpt_handler = ModelCheckpoint(args.ckpt_path, score_function=score_function,\n                                   filename_prefix=filename_prefix, n_saved=3,\n                                   create_dir=True, require_empty=False)\n\n    # if not tuning plm,\n    model_state = get_model_state(model, ['pair_cls', 'pfx_embedding'], args.plm_tuning)\n    ckpt = {'model': model_state, 'args': CkptWrapper(args)}\n    trainer.add_event_handler(Events.EPOCH_COMPLETED(every=1), ckpt_handler, ckpt)\n\n    hdl_early_stop = EarlyStopping(patience=args.early_stop_patience, score_function=score_function, trainer=trainer)\n    # Note: the handler is attached to an *Evaluator* (runs one epoch on validation dataset).\n    evaluator.add_event_handler(Events.COMPLETED, hdl_early_stop)\n\n    # start training loop\n    trainer.run(train_gen, max_epochs=args.max_epochs, epoch_length=train_batch_num)\n\n    # load best checkpoint\n    best_ckpt = ckpt_handler.last_checkpoint\n    print(f\"Load best model checkpoint from '{best_ckpt}'\")\n    ckpt = torch.load(best_ckpt)\n    model.load_state_dict(ckpt['model'], strict=False)\n    del ckpt\n    return model, best_ckpt\n\n\n# utils\n\ndef load_data_blocks(path_to_data, args, tokenizer):\n    print(f\"load data from '{path_to_data}'... \", end='')\n    dataset = np.load(path_to_data, allow_pickle=True)\n    print(\"\\tDone\")\n\n    path_to_blocks = []\n    print(f\"encode block samples, \")\n\n    for i, blk in enumerate(dataset):\n        print(f\"Message Block{i}\", flush=True)\n        train, valid, test = (blk[n] for n in ('train', 'valid', 'test'))\n        print(\n            f\"Train samples: {len(train['samples'])}, Valid samples: {len(valid['samples'])}, Test samples: {len(test['samples'])}\")\n\n        path = f\"{args.file_path}/cache/cache_long_tail/\"\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n        if args.offline:\n            # blk_path = os.path.join(path, f\"{args.model_name}-{args.dataset_name}-offline.npy\")\n            blk_path = os.path.join(path, f\"{args.model_name}-{args.dataset_name}.npy\")\n        else:\n            blk_path = os.path.join(path, f\"{args.model_name}-{args.dataset_name}-M{i + 1}.npy\")\n\n        if not os.path.exists(blk_path):\n            print(\"train dateset processing\", end=\" \")\n            train['samples'] = encode_samples(train['samples'], train['data'], tokenizer, args.pmt_feats)\n            print(\"done\")\n\n            print(\"valid dateset processing\", end=\" \")\n            valid['samples'] = encode_samples(valid['samples'], valid['data'], tokenizer, args.pmt_feats)\n            print(\"done\")\n\n            print(\"test dateset processing\", end=\" \")\n            test['samples'] = encode_samples(test['samples'], test['data'], tokenizer, args.pmt_feats)\n            print(\"done\")\n\n            torch.save(\n                {'train': train, 'valid': valid, 'test': test},\n                blk_path\n            )\n        if blk_path not in path_to_blocks:\n            path_to_blocks.append(blk_path)\n\n    del dataset\n    print(\"Done\")\n\n    path_to_blocks = ['../model/model_saved/rplmsed/cache/cache_long_tail/roberta-large-data.npy']\n    for blk_path in path_to_blocks:\n        print(f\"load block from '{blk_path}'... \\n\", end='')\n        loaded_blk = torch.load(blk_path)\n        # 检查加载的数据是否为字典\n        if isinstance(loaded_blk, dict):\n            print(f\"Loaded block type: {type(loaded_blk)}\")\n            print(f\"Loaded block keys: {loaded_blk.keys()}\")\n            yield loaded_blk\n        else:\n            print(f\"Error: Loaded block is not a dictionary, but {type(loaded_blk)}\")\n            yield None\n\n\nclass CkptWrapper:\n    def __init__(self, state: Any):\n        self.state = state\n\n    def state_dict(self):\n        return self.state\n\n\ndef get_model_state(model, params, plm_tuning):\n    if plm_tuning:\n        return model\n    else:\n        model_state = model.state_dict()\n        state = OrderedDict()\n        for k, v in model_state.items():\n            for p in params:\n                if k.startswith(p):\n                    state[k] = model_state[k]\n                    break\n\n        return CkptWrapper(state)\n\n\ndef width(text):\n    return sum([2 if '\\u4E00' <= c <= '\\u9FA5' else 1 for c in text])\n\n\ndef print_table(tab):\n    col_width = [max(width(x) for x in col) for col in zip(*tab)]\n    print(\"+-\" + \"-+-\".join(\"{:-^{}}\".format('-', col_width[i]) for i, x in enumerate(tab[0])) + \"-+\")\n    for line in tab:\n        print(\"| \" + \" | \".join(\"{:{}}\".format(x, col_width[i]) for i, x in enumerate(line)) + \" |\")\n    print(\"+-\" + \"-+-\".join(\"{:-^{}}\".format('-', col_width[i]) for i, x in enumerate(tab[0])) + \"-+\")\n\n\ndef data_generator(data, batch_size, shuffle=False, repeat=False):\n    batch_num = math.ceil(len(data) / batch_size)\n    return create_data_generator(data, batch_size, shuffle, repeat, batch_num), batch_num\n\n\ndef create_data_generator(data, batch_size, shuffle, repeat, batch_num):\n    while True:\n        if shuffle:\n            shuffled_idx = [i for i in range(len(data))]\n            random.shuffle(shuffled_idx)\n            data = [data[i] for i in shuffled_idx]\n\n        batch_id = 0\n        while batch_id < batch_num:\n            offset = batch_id * batch_size\n            batch_data = data[offset:offset + batch_size]\n            yield batch_data\n\n            batch_id = batch_id + 1\n        if repeat:\n            continue\n        else:\n            break\n\n\ndef pad_seq(seq, max_len, pad=0, pad_left=False):\n    \"\"\"\n    padding or truncate sequence to fixed length\n    :param seq: input sequence\n    :param max_len: max length\n    :param pad: padding token id\n    :param pad_left: pad on left\n    :return: padded sequence\n    \"\"\"\n    if max_len < len(seq):\n        seq = seq[:max_len]\n    elif max_len > len(seq):\n        padding = [pad] * (max_len - len(seq))\n        if pad_left:\n            seq = padding + seq\n        else:\n            seq = seq + padding\n    return seq\n\n\ndef run_kmeans(msg_feats, n_clust, msg_tags):\n    # defalut:10\n    k_means = KMeans(init=\"k-means++\", n_clusters=n_clust, n_init=40, random_state=0)\n    k_means.fit(msg_feats)\n\n    msg_pred = k_means.labels_\n    score_funcs = [\n        (\"NMI\", metrics.normalized_mutual_info_score),\n        (\"AMI\", metrics.adjusted_mutual_info_score),\n        (\"ARI\", metrics.adjusted_rand_score),\n    ]\n\n    scores = {m: fun(msg_tags, msg_pred) for m, fun in score_funcs}\n\n    return scores\n\n\ndef run_hdbscan(msg_feats, msg_tags):\n    hdb = HDBSCAN(min_cluster_size=8)\n    hdb.fit(msg_feats)\n\n    msg_pred = hdb.labels_\n    score_funcs = [\n        (\"NMI\", metrics.normalized_mutual_info_score),\n        (\"AMI\", metrics.adjusted_mutual_info_score),\n        (\"ARI\", metrics.adjusted_rand_score),\n    ]\n\n    scores = {m: fun(msg_tags, msg_pred) for m, fun in score_funcs}\n\n    return scores\n\n\ndef run_dbscan(msg_feats, msg_tags):\n    db = OPTICS(min_cluster_size=8, xi=0.01)\n    db.fit(msg_feats)\n\n    msg_pred = db.labels_\n    score_funcs = [\n        (\"NMI\", metrics.normalized_mutual_info_score),\n        (\"AMI\", metrics.adjusted_mutual_info_score),\n        (\"ARI\", metrics.adjusted_rand_score),\n    ]\n\n    scores = {m: fun(msg_tags, msg_pred) for m, fun in score_funcs}\n\n    return scores\n\n\ndef print_scores(scores):\n    line = [' ' * 4] + [f'   M{i:02d} ' for i in range(1, len(scores) + 1)]\n    print(\"\".join(line))\n\n    score_names = ['NMI', 'AMI', 'ARI']\n    for n in score_names:\n        line = [f'{n} '] + [f'  {s[n]:1.3f}' for s in scores]\n        print(\"\".join(line))\n    print('\\n', flush=True)\n\n\ndef encode_samples(samples, raw_data, tokenizer, pmt_idx):\n    data = []\n    for tag, ev_idx, (tw_a, tw_b), pmt_feat in samples:\n        tw_a_text = raw_data[tw_a].text\n        tw_b_text = raw_data[tw_b].text\n        tok = tokenizer(tw_a_text, tw_b_text, padding=True)\n\n        # 只保留需要的关联特征\n        # (entities, hashtags, user, words, time)\n        pmt_feat = [pmt_feat[f] for f in pmt_idx]\n\n        base = [2 * i for i in range(len(pmt_feat))]\n        pmt_ids = [b + f for f, b in zip(pmt_feat, base)]\n\n        if 'token_type_ids' not in tok:\n            types = [0, 0, 1, 1]\n            token_type_ids = tok.encodings[0].sequence_ids\n            j = 0\n            for i, t in enumerate(token_type_ids):\n                if t is None:\n                    token_type_ids[i] = types[j]\n                    j += 1\n        else:\n            token_type_ids = tok['token_type_ids']\n\n        data.append((tag, ev_idx, tw_a, tw_b, pmt_ids, tok['input_ids'], token_type_ids))\n\n    return data\n\n\ndef count_condition(data, key, threshold):\n    return sum(entry[key] > threshold for entry in data), sum(entry[key] <= threshold for entry in data)\n\n\ndef calculate_average_min_score(newscore, min_score, max_score):\n    for i, score in enumerate(newscore):\n        for key, value in score.items():\n            min_score[i][key] = min(min_score[i][key], value)\n            max_score[i][key] = max(max_score[i][key], value)\n\n    return min_score, max_score\n\n\nclass StructAttention(torch.nn.Module):\n    \"\"\"\n    The class is an implementation of the paper A Structured Self-Attentive Sentence Embedding\n    \"\"\"\n\n    def __init__(self, feat_dim, hid_dim, att_head_num=1):\n        \"\"\"\n        Initializes parameters suggested in paper\n        Args:\n            feat_dim:       {int} hidden dimension for lstm\n            hid_dim:        {int} hidden dimension for the dense layer\n            att_head_num:   {int} attention-hops or attention heads\n        Returns:\n            self\n        Raises:\n            Exception\n        \"\"\"\n        super(StructAttention, self).__init__()\n        self.W1 = torch.nn.Linear(feat_dim, hid_dim, bias=False)\n        nn.init.xavier_normal_(self.W1.weight)\n\n        self.W2 = torch.nn.Linear(hid_dim, att_head_num, bias=False)\n        nn.init.xavier_normal_(self.W2.weight)\n\n        self.att_head_num = att_head_num\n\n    def forward(self, inpt, mask=None):\n        \"\"\"\n        :param inpt: [len, bsz, dim]\n        :param mask: [len, bsz]\n        :return: [bsz, head_num, dim], [bsz, head_num, len]\n        \"\"\"\n        hid = torch.tanh(self.W1(inpt))\n        hid = self.W2(hid)\n\n        if mask is not None:\n            mask = mask.float().unsqueeze(-1).expand(-1, -1, self.att_head_num)\n            mask = (1. - mask) * 1e10\n            hid = hid - mask\n        att = torch.softmax(hid, dim=0).permute(1, 2, 0)\n\n        outp = att @ inpt.permute(1, 0, 2)\n\n        return outp, att\n\n\nclass PairPfxTuningEncoder(nn.Module):\n    def __init__(self, pmt_len,\n                 plm_path, plm_tuning=False, from_config=False,\n                 use_ctx_att=True, ctx_att_head_num=2):\n        super().__init__()\n        self.pfx_len = pmt_len\n        self.plm_path = plm_path\n\n        if from_config:\n            config = AutoConfig.from_pretrained(plm_path)\n            self.plm = AutoModel.from_config(config)\n        else:\n            self.plm = AutoModel.from_pretrained(plm_path)\n\n        if not plm_tuning:\n            for name, param in self.plm.named_parameters():\n                param.requires_grad = False\n                param.detach_()\n\n        self.plm_oupt_dim = self.plm.config.hidden_size\n\n        self.plm_emb_dim = self.plm.embeddings.word_embeddings.embedding_dim\n\n        self.pfx_embedding = nn.Embedding(self.pfx_len * 2, self.plm_emb_dim)\n        self.pfx_mask = torch.ones((1, self.pfx_len), dtype=torch.bool)\n\n        self.linear = nn.Linear(self.plm_oupt_dim, self.plm_oupt_dim // 2)\n\n        self.ctx_att = None\n        if use_ctx_att:\n            self.ctx_att = StructAttention(self.plm_oupt_dim // 2, self.plm_oupt_dim // 4,\n                                           att_head_num=ctx_att_head_num)\n        self.pair_cls = nn.Linear(2 * (self.plm_oupt_dim // 2), 1)\n\n    def feat_size(self):\n        return self.plm_oupt_dim // 2\n\n    def reload_plm(self, device):\n        self.plm = AutoModel.from_pretrained(self.plm_path).to(device)\n\n    # 0.4\n    def accumulate_reload_plm(self, device, accumulate_rate=0.4):\n        origin = AutoModel.from_pretrained(self.plm_path).to('cpu')\n        plm_params = self.plm.named_parameters()\n        origin_params = origin.named_parameters()\n        for ((tgt_name, tgt_param), (src_name, src_param)) in zip(plm_params, origin_params):\n            assert (tgt_name == src_name), f\"param name {tgt_name} and {src_name} does not match\"\n            tgt_param.data = (1. - accumulate_rate) * tgt_param.data + accumulate_rate * src_param.to(device).data\n\n    def fix_plm(self):\n        for name, param in self.plm.named_parameters():\n            param.requires_grad = False\n            param.detach_()\n\n    def forward(self, inputs, types, prompt, mask):\n        bsz, txt_len = mask.size()\n\n        pmt_msk = self.pfx_mask.to(inputs.device).expand(bsz, -1)\n        ext_msk = torch.cat([pmt_msk, mask], dim=-1)\n        # ext_msk =mask#\n\n        pmt_emb = self.pfx_embedding(prompt)\n        pmt_len = prompt.size(-1)\n        txt_emb = self.plm.embeddings(inputs)\n        embed = torch.cat([pmt_emb, txt_emb], dim=1)\n        # embed= txt_emb #\n        att_msk = ext_msk[:, None, None, :]\n        att_msk = (1.0 - att_msk.float()) * torch.finfo(torch.float).min\n        plm_oupt = self.plm.encoder(embed, att_msk, output_hidden_states=True)\n\n        hidden = plm_oupt['last_hidden_state']\n        # if self.ctx_att is not None:\n        hidden = torch.tanh(self.linear(hidden))\n\n        pmt_feat = hidden[:, :pmt_len, ...]\n        tok_feat = hidden[:, pmt_len:, ...]\n        # tok_feat = hidden#\n\n        left_msk = (1 - types) * mask\n        left_feat = tok_feat * left_msk.unsqueeze(-1)\n        left_msk = torch.cat([pmt_msk.int(), left_msk], dim=1)\n        left_feat = torch.cat([pmt_feat, left_feat], dim=1)\n        if self.ctx_att is None:\n            left_feat = left_feat.sum(dim=-2) / left_msk.sum(-1, keepdims=True)\n        else:\n            left_feat, left_att = self.ctx_att(left_feat.permute(1, 0, 2), mask=left_msk.permute(1, 0))\n            left_feat = torch.mean(left_feat, dim=1)\n\n        right_msk = types * mask\n        right_feat = tok_feat * right_msk.unsqueeze(-1)\n        if self.ctx_att is None:\n            right_feat = right_feat.sum(dim=-2) / right_msk.sum(-1, keepdims=True)\n        else:\n            right_feat, right_att = self.ctx_att(right_feat.permute(1, 0, 2), mask=right_msk.permute(1, 0))\n            right_feat = torch.mean(right_feat, dim=1)\n\n        cls_feat = torch.cat([left_feat, right_feat], dim=-1)\n\n        logit = self.pair_cls(cls_feat).squeeze(dim=-1)\n\n        return logit, left_feat\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/etgnn.py", "content": "import argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport dgl\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nfrom scipy import sparse\nimport spacy\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\nimport copy\nimport datetime\nimport torch.nn.functional as F\nfrom sklearn.cluster import KMeans\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n\n\nclass ETGNN:\n    r\"\"\"The ETGNN model for social event detection that uses uncertainty-aware contrastive learning\n    for event detection.\n\n    .. note::\n        This detector uses uncertainty-aware contrastive learning to identify events in social media data.\n        The model requires a dataset object with load_data() and get_dataset_language() methods.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() and get_dataset_language() methods.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/etgnn/'``.\n    epoch : int, optional\n        Number of training epochs. Default: ``50``.\n    batch_size : int, optional\n        Batch size for training. Default: ``128``.\n    neighbours_num : int, optional\n        Number of neighbors to sample. Default: ``80``.\n    GNN_h_dim : int, optional\n        Hidden dimension of GNN. Default: ``256``.\n    GNN_out_dim : int, optional\n        Output dimension of GNN. Default: ``256``.\n    E_h_dim : int, optional\n        Hidden dimension of encoder. Default: ``128``.\n    use_uncertainty : bool, optional\n        Whether to use uncertainty estimation. Default: ``True``.\n    use_cuda : bool, optional\n        Whether to use GPU acceleration. Default: ``True``.\n    gpuid : int, optional\n        GPU device ID to use. Default: ``0``.\n    mode : int, optional\n        Training mode. Default: ``0``.\n    mse : bool, optional\n        Whether to use MSE loss. Default: ``False``.\n    digamma : bool, optional\n        Whether to use digamma function. Default: ``True``.\n    log : bool, optional\n        Whether to use log transformation. Default: ``False``.\n    learning_rate : float, optional\n        Learning rate for optimizer. Default: ``1e-4``.\n    weight_decay : float, optional\n        Weight decay for optimizer. Default: ``1e-5``.\n    \"\"\"\n    def __init__(\n        self,\n        dataset,\n        file_path='../model/model_saved/etgnn/',\n        epoch=50,\n        batch_size=128,\n        neighbours_num=80,\n        GNN_h_dim=256,\n        GNN_out_dim=256,\n        E_h_dim=128,\n        use_cuda=True,\n        gpuid=0,\n        mode=0,\n        mse=False,\n        digamma=True,\n        log=False,\n        learning_rate=1e-4,\n        weight_decay=1e-5\n    ):\n        # 将参数赋值给 self\n        self.file_path = file_path\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.neighbours_num = neighbours_num\n        self.GNN_h_dim = GNN_h_dim\n        self.GNN_out_dim = GNN_out_dim\n        self.E_h_dim = E_h_dim\n        self.use_cuda = use_cuda\n        self.gpuid = gpuid\n        self.mode = mode\n        self.mse = mse\n        self.digamma = digamma\n        self.log = log\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n\n        self.save_path = None\n        self.test_indices = None\n        self.val_indices = None\n        self.train_indices = None\n        self.mask_path = None\n        self.labels = None\n        self.times = None\n        self.g_dict = None\n        self.views = None\n        self.features = None\n        self.dataset = dataset.load_data()\n        self.language = dataset.get_dataset_language()\n        \n\n    def preprocess(self):\n        preprocessor = Preprocessor(self)\n        preprocessor.construct_graph(self.dataset,self.language)\n\n    def fit(self):\n        args=self\n        parser = argparse.ArgumentParser()\n        print(\"Using CUDA:\", args.use_cuda)\n        if args.use_cuda:\n            torch.cuda.set_device(args.gpuid)\n\n        self.views = ['h', 'e', 'u']\n        self.g_dict, self.times, self.features, self.labels = get_dgl_data(self,self.views,self.language)\n        self.mask_path = f\"{args.file_path}{self.language}/\" + \"masks/\"\n        if not os.path.exists(self.mask_path):\n            os.mkdir(self.mask_path)\n        self.train_indices, self.val_indices, self.test_indices = ava_split_data(len(self.labels), self.labels,\n                                                                                 len(set(self.labels)))\n        torch.save(self.train_indices, self.mask_path + \"train_indices.pt\")\n        torch.save(self.val_indices, self.mask_path + \"val_indices.pt\")\n        torch.save(self.test_indices, self.mask_path + \"test_indices.pt\")\n\n        if args.mode == 0:\n            flag = ''\n            if args.use_uncertainty:\n                print(\"use_uncertainty\")\n                flag = \"evi\"\n            self.save_path = f\"{args.file_path}{self.language}/\" + flag + \"/\"\n            print(self.save_path)\n            os.makedirs(self.save_path, exist_ok=True)\n        else:\n            self.save_path = '../model/model_saved/etgnn/'\n\n\n        criterion = nn.CrossEntropyLoss()\n\n        self.model = UCLSED_model(self.features.shape[1], args.GNN_h_dim, args.GNN_out_dim, args.E_h_dim,\n                                  len(set(self.labels)), self.views)\n        self.model = train_model(self.model, self.g_dict, self.views, self.features, self.times, self.labels,\n                                 args.epoch, criterion, self.mask_path, self.save_path, args)\n\n    def detection(self):\n        args=self\n        self.model.eval()\n        self.val_indices = torch.load(self.mask_path + \"val_indices.pt\")\n        classes = len(set(self.labels))\n        self.labels = make_onehot(self.labels, classes)\n        device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n        if args.use_cuda:\n            self.model = self.model.cuda()\n            self.features = self.features.cuda()\n            self.times = self.times.cuda()\n            self.labels = self.labels.cuda()\n            self.train_indices = self.train_indices.cuda()\n            self.test_indices = self.test_indices.cuda()\n            self.val_indices = self.val_indices.cuda()\n            for v in self.views:\n                self.g_dict[v] = self.g_dict[v].to(device)\n                self.g_dict[v].ndata['features'] = self.features\n                self.g_dict[v].ndata['t'] = self.times\n\n        out, emb, nids = extract_results(self.g_dict, self.views, self.labels, self.model, args)\n        ori_labels = self.labels\n        # extract_labels = ori_labels[nids]\n        extract_labels = ori_labels[nids].cpu()\n        \n        comb_out = None\n        if args.use_uncertainty:\n            alpha = []\n            for out_v in out.values():\n                evi_v = relu_evidence(out_v)\n                alpha_v = evi_v + 1\n                alpha.append(alpha_v)\n            comb_out, comb_u = DS_Combin(alpha=alpha, classes=classes)\n\n        else:\n            for i, out_v in enumerate(out.values()):\n                if i == 0:\n                    comb_out = out_v\n                else:\n                    comb_out += out_v\n\n        _, val_pred = torch.max(comb_out[self.val_indices.cpu().numpy()], 1)\n        #val_labels = torch.IntTensor(extract_labels[self.val_indices.cpu().numpy()])\n        val_labels = torch.argmax(extract_labels[self.val_indices.cpu().numpy()], 1)\n        predictions = val_pred.cpu().numpy()\n        ground_truth = val_labels.cpu().numpy()\n\n        return ground_truth, predictions\n\n    def evaluate(self, ground_truth, predictions):\n        val_f1 = f1_score(ground_truth, predictions, average='macro')\n        val_acc = accuracy_score(ground_truth, predictions)\n\n        print(f\"Validation F1 Score: {val_f1}\")\n        print(f\"Validation Accuracy: {val_acc}\")\n\n        return val_f1, val_acc\n\n\nclass Preprocessor():\n    def __init__(self,  args):\n        super(Preprocessor, self).__init__()\n\n        self.args = args\n\n    def str2list(self, str_ele):\n        if str_ele == \"[]\":\n            value = []\n        else:\n            value = [e.replace('\\'', '').lstrip().replace(\":\", '') for e in str(str_ele)[1:-1].split(',') if\n                     len(e.replace('\\'', '').lstrip().replace(\":\", '')) > 0]\n        return value\n\n    def load_data(self, dataset):\n        ori_df = dataset\n\n        ori_df.drop_duplicates([\"tweet_id\"], keep='first', inplace=True)\n        event_id_num_dict = {}\n        select_index_list = []\n\n        for id in set(ori_df[\"event_id\"]):\n            num = len(ori_df.loc[ori_df[\"event_id\"] == id])\n            if int(num / 3) >= 25:\n                event_id_num_dict[id] = int(num / 3 + 50)\n                select_index_list += list(ori_df.loc[ori_df[\"event_id\"] == id].index)[0:int(num / 3 + 50)]\n        select_df = ori_df.loc[select_index_list]\n        select_df = select_df.reset_index(drop=True)\n        id_num = sorted(event_id_num_dict.items(), key=lambda x: x[1], reverse=True)\n\n        for (i, j) in id_num[0:100]:\n            print(j, end=\",\")\n        \n        event_ids = [item[0] for item in id_num]\n        sorted_id_dict = dict(zip(event_ids, range(len(set(ori_df[\"event_id\"])))))\n\n        sorted_df = select_df\n        sorted_df[\"event_id\"] = sorted_df[\"event_id\"].apply(lambda x: sorted_id_dict[x])\n\n        print(sorted_df.shape)\n        \n        # 修改这部分：使用列表来选择多个列\n        columns = [\n            'tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n            'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions'\n        ]\n        \n        # 检查列是否存在\n        existing_columns = [col for col in columns if col in sorted_df.columns]\n        if len(existing_columns) != len(columns):\n            missing_columns = set(columns) - set(existing_columns)\n            print(f\"Warning: Missing columns in DataFrame: {missing_columns}\")\n            print(f\"Available columns: {sorted_df.columns.tolist()}\")\n        \n        data_value = sorted_df[existing_columns].values\n        \n        event_df = pd.DataFrame(data=data_value, columns=existing_columns)\n        \n        # 确保所需的列存在后再处理\n        if 'hashtags' in event_df.columns:\n            event_df['hashtags'] = event_df['hashtags'].apply(lambda x: [\"h_\" + str(i) for i in x])\n        if 'entities' in event_df.columns:\n            event_df['entities'] = event_df['entities'].apply(lambda x: [\"e_\" + str(i) for i in x])\n        if 'user_mentions' in event_df.columns:\n            event_df['user_mentions'] = event_df['user_mentions'].apply(lambda x: [\"u_\" + str(i) for i in x])\n        \n        event_df = event_df.loc[event_df['event_id'] < 100]\n        event_df = event_df.reset_index(drop=True)\n\n        print(event_df.shape)\n        return event_df\n\n    def get_nlp(self, lang):\n        if lang == \"English\":\n            nlp =spacy.load('en_core_web_lg')\n        elif lang == \"French\":\n            nlp=spacy.load('fr_core_news_lg')\n        elif lang == \"Arabic\":\n            nlp = spacy.load('ar_core_news_lg')\n        return nlp\n\n    def construct_graph_base_eles(self, view_dict, df, path, lang):\n        os.makedirs(path, exist_ok=True)\n        nlp = self.get_nlp(lang)\n        df = df.drop_duplicates(subset=['tweet_id'])\n        df.reset_index()\n        df.drop_duplicates([\"tweet_id\"], keep='first', inplace=True)\n        print(\"generate text features---------\")\n        features = np.stack(df['filtered_words'].apply(lambda x: nlp(' '.join(x)).vector).values, axis=0)\n        print(features.shape)\n        np.save(path + \"features.npy\", features)\n        print(\"text features are saved in {}features.npy\".format(path))\n        \n        if 'timestamp' not in df.columns and 'created_at' in df.columns:\n            try:\n                # 如果 created_at 是字符串格式，先转换为 datetime\n                if df['created_at'].dtype == 'object':\n                    df['timestamp'] = pd.to_datetime(df['created_at'])\n                else:\n                    df['timestamp'] = df['created_at']\n                \n                # 转换为 UNIX 时间戳（秒）\n                df['timestamp'] = df['timestamp'].astype(np.int64) // 10**9\n                \n            except Exception as e:\n                print(f\"Error converting created_at to timestamp: {e}\")\n                print(\"Using default timestamps...\")\n                df['timestamp'] = np.arange(len(df))  # 使用序列号作为后备方案\n        elif 'timestamp' not in df.columns:\n            print(\"Warning: No timestamp or created_at column found, using sequential numbers\")\n            df['timestamp'] = np.arange(len(df))\n    \n        \n        np.save(path + \"time.npy\", df['timestamp'].values)\n        print(\"time features are saved in {}time.npy\".format(path))\n        df[\"event_id\"] = df[\"event_id\"].apply(lambda x: int(x))\n        np.save(path + \"label.npy\", df['event_id'].values)\n        print(\"labels are saved in {}label.npy\".format(path))\n\n        true_matrix = np.eye(df.shape[0])\n        for i in range(df.shape[0]):\n            label_i = df[\"event_id\"].values[i]\n            indices = df[df[\"event_id\"] == label_i].index\n            true_matrix[i, indices] = 1\n        # print(true_matrix)\n\n        print(\"construct graph---------------\")\n        G = nx.Graph()\n        for _, row in df.iterrows():\n            tid = str(row['tweet_id'])\n            G.add_node(tid)\n            G.nodes[tid]['tweet_id'] = True  # right-hand side value is irrelevant for the lookup\n            edges = []\n            for view in view_dict.values():\n                for ele in view:\n                    if len(row[ele]) > 0:\n                        ele_values = row[ele]\n                        G.add_nodes_from(ele_values)\n                        for each in ele_values:\n                            G.nodes[each][ele] = True\n                        edges += [(tid, each) for each in row[ele]]\n\n            G.add_edges_from(edges)\n\n        all_nodes = list(G.nodes)\n        matrix = nx.to_scipy_sparse_array(G)\n        tweet_nodes = list(nx.get_node_attributes(G, \"tweet_id\").keys())\n        # print(tweet_nodes)\n        print(len(tweet_nodes))\n        tweet_index = [all_nodes.index(t_node) for t_node in tweet_nodes]\n\n        for v, view in zip(view_dict.keys(), view_dict.values()):\n            s_tweet_tweet_matrix = sparse.csr_matrix(np.identity(len(tweet_nodes)))\n            for ele in view:\n                ele_nodes = list(nx.get_node_attributes(G, ele).keys())\n                ele_index = [all_nodes.index(e_node) for e_node in ele_nodes]\n                tweet_ele_matrix = matrix[tweet_index, :][:, ele_index]\n                s_ele_tweet_tweet_matrix = sparse.csr_matrix(tweet_ele_matrix @ tweet_ele_matrix.transpose())\n                s_tweet_tweet_matrix += s_ele_tweet_tweet_matrix\n            s_tweet_tweet_matrix = s_tweet_tweet_matrix.astype('bool')\n            sparse.save_npz(os.path.join(path, f\"s_tweet_tweet_matrix_{v}.npz\"), s_tweet_tweet_matrix)\n            print(f\"Sparse binary {v} commuting matrix is saved in {path}s_tweet_tweet_matrix_{v}.npz\")\n\n    def construct_graph(self, dataset, lang):\n        args=self\n        event_df = self.load_data(dataset)\n        view_dict = {\n            \"h\": [\"hashtags\", \"urls\"], \n            \"u\": [\"user_mentions\"],  # 修改这里：mention_user -> user_mentions\n            \"e\": [\"entities\"]\n        }\n        path = self.args.file_path + lang + '/'\n        self.construct_graph_base_eles(view_dict, event_df, path, lang)\n\n\ndef extract_results(g_dict, views, labels, model, args, train_indices=None):\n    with torch.no_grad():\n        model.eval()\n        out_list = []\n        emb_list = []\n        nids_list = []\n        all_indices = torch.LongTensor(range(0, labels.shape[0]))\n        if args.use_cuda:\n            all_indices = all_indices.cuda()\n        print(all_indices)\n        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n        dataloader = dgl.dataloading.NodeDataLoader(\n            g_dict[views[0]], all_indices, sampler,\n            batch_size=args.batch_size,\n            shuffle=False,\n            drop_last=False,\n        )\n\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n            extract_indices = blocks[-1].dstdata[dgl.NID].to(device)\n            blocks_dict = {}\n            blocks_dict[views[0]] = blocks\n            for v in views[1:]:\n                blocks_v = list(dgl.dataloading.NodeDataLoader(\n                    g_dict[v], extract_indices, sampler,\n                    batch_size=args.batch_size,\n                    shuffle=False,\n                    drop_last=False,\n                ))[0][2]\n                blocks_dict[v] = blocks_v\n            for v in views:\n                blocks_dict[v] = [b.to(device) for b in blocks_dict[v]]\n\n            # 将 blocks_dict 中的所有 blocks 移动到相同的设备\n            # blocks_dict = {v: [b.to(device) for b in blocks] for v, blocks in blocks_dict.items()}\n            # extract_indices = extract_indices.to(device)\n\n            out, emb = model(blocks_dict)\n            out_list.append(out)\n            emb_list.append(emb)\n            nids_list.append(extract_indices)\n\n    # assert batch_id==0\n    all_out = {}\n    all_emb = {}\n    for v in views:\n        all_out[v] = []\n        all_emb[v] = []\n        for out, emb in zip(out_list, emb_list):\n            all_out[v].append(out[v])\n            all_emb[v].append(emb[v])\n        if args.use_cuda:\n            all_out[v] = torch.cat(all_out[v]).cpu()\n            all_emb[v] = torch.cat(all_emb[v]).cpu()\n        else:\n            all_out[v] = torch.cat(all_out[v])\n            all_emb[v] = torch.cat(all_emb[v])\n\n    extract_nids = torch.cat(nids_list)\n    if args.use_cuda:\n        extract_nids = extract_nids.cpu()\n\n    return all_out, all_emb, extract_nids\n\n\ndef train_model(model, g_dict, views, features, times, labels, epoch, criterion, mask_path, save_path, args):\n    train_indices = torch.load(mask_path + \"train_indices.pt\")\n    val_indices = torch.load(mask_path + \"val_indices.pt\")\n    test_indices = torch.load(mask_path + \"test_indices.pt\")\n    classes = len(set(labels))\n    ori_labels = labels\n\n    labels = make_onehot(labels, classes)\n    device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n    if args.use_cuda:\n        model = model.cuda()\n        features = features.cuda()\n        times = times.cuda()\n        labels = labels.cuda()\n        train_indices = train_indices.cuda()\n        val_indices = val_indices.cuda()\n        test_indices = test_indices.cuda()\n\n    for v in views:\n        if args.use_cuda:\n            g_dict[v] = g_dict[v].to(device)\n        g_dict[v].ndata['features'] = features\n        g_dict[v].ndata['t'] = times\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n    if args.mode == 0:\n        message = \"----------begin training---------\\n\"\n        with open(save_path + \"log.txt\", 'w') as f:\n            f.write(message)\n\n        best_vali = 0\n        test_acc_in_best_e = 0\n        best_epoch = 0\n        test_acc_list = []\n        label_u = torch.FloatTensor(np.ones(classes))\n\n        for e in range(epoch):\n            print(f\"Epoch {e + 1}/{epoch}\")\n\n            _, GNN_out_fea, extract_nids = extract_results(g_dict, views, labels, model, args)\n\n            for v in GNN_out_fea:\n                GNN_out_fea[v] = GNN_out_fea[v].to(device)\n                # print(f'GNN_out_fea[{v}].device: {GNN_out_fea[v].device}')  # 确认设备\n\n            extract_labels = ori_labels[extract_nids]\n            label_center = {}\n            for v in views:\n                label_center[v] = []\n            for l in range(classes):\n                l_indices = torch.LongTensor(np.where(extract_labels == l)[0].reshape(-1)).to(device)\n                # print(l_indices.device)\n                for v in views:\n                    # print(f'GNN_out_fea[{v}].device:{GNN_out_fea[v].device}')\n                    # print(f'l_indices.device:{l_indices.device}')\n\n                    l_feas = GNN_out_fea[v][l_indices]\n                    l_cen = torch.mean(l_feas, dim=0)\n                    label_center[v].append(l_cen)\n\n            for v in views:\n                label_center[v] = torch.stack(label_center[v], dim=0)\n                label_center[v] = F.normalize(label_center[v], 2, 1)\n\n                if args.use_cuda:\n                    label_center[v] = label_center[v].cuda()\n                    label_u = label_u.cuda()\n\n            losses = []\n            total_loss = 0\n            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n            dataloader = dgl.dataloading.NodeDataLoader(\n                g_dict[views[0]], train_indices, sampler,\n                batch_size=args.batch_size,\n                shuffle=False,\n                drop_last=False,\n                device=device\n            )\n\n            print(f\"Dataloader initialized with {len(dataloader)} batches\")\n            for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n                print(f\"Processing batch {batch_id + 1}/{len(dataloader)}\")\n                batch_indices = blocks[-1].dstdata[dgl.NID]\n                if args.use_cuda:\n                    batch_indices = batch_indices.cuda()\n                blocks_dict = {}\n                blocks_dict[views[0]] = blocks\n                for v in views[1:]:\n                    blocks_v = list(dgl.dataloading.NodeDataLoader(\n                        g_dict[v], batch_indices, sampler,\n                        batch_size=args.batch_size,\n                        shuffle=False,\n                        drop_last=False,\n                    ))[0][2]\n                    blocks_dict[v] = blocks_v\n\n                for v in views:\n                    blocks_dict[v] = [b.to(device) for b in blocks_dict[v]]\n\n                batch_labels = labels[batch_indices]\n                batch_ori_labels = torch.LongTensor(ori_labels).to(device)[batch_indices]\n                model.train()\n                out, emb = model(blocks_dict)\n\n                view_contra_loss = 0\n                e_loss = 0\n                if args.use_uncertainty:\n                    alpha = []\n                    true_labels = torch.LongTensor(ori_labels).to(device)[batch_indices]\n                    for i, v in enumerate(views):\n                        emb[v] = F.normalize(emb[v], 2, 1)\n                        batch_center = label_center[v][batch_ori_labels]\n\n                        view_contra_loss += torch.mean(-torch.log(\n                            (torch.exp(torch.sum(torch.mul(emb[v], batch_center), dim=1)) - 0.1 * label_u[\n                                batch_ori_labels]) / (\n                                torch.sum(torch.exp(torch.mm(emb[v], label_center[v].T)),\n                                          dim=1))))  # *label_u[batch_ori_labels])\n\n                        alpha_v = relu_evidence(out[v]) + 1\n                        alpha.append(alpha_v)\n\n                    comb_alpha, comb_u = DS_Combin(alpha=alpha, classes=classes)\n\n                    e_loss = EUC_loss(comb_alpha, comb_u, true_labels, e)\n                    loss = e_loss + criterion(comb_alpha, batch_labels, true_labels, e, classes, 100,\n                                              device) + 2 * view_contra_loss\n\n                else:\n                    batch_labels = torch.argmax(batch_labels, 1)\n                    for i, v in enumerate(views):\n                        if i == 0:\n                            comb_out = out[v]\n                        else:\n                            comb_out += out[v]\n                        emb[v] = F.normalize(emb[v], 2, 1)\n                        batch_center = label_center[v][batch_ori_labels]\n                        view_contra_loss += torch.mean(-torch.log(\n                            (torch.exp(torch.sum(torch.mul(emb[v], batch_center), dim=1))) / (\n                                torch.sum(torch.exp(torch.mm(emb[v], label_center[v].T)), dim=1))))\n                    loss = criterion(comb_out, batch_labels)  # + view_contra_loss\n\n                com_loss = 0\n                for i in range(len(emb) - 1):\n                    for j in range(i + 1, len(emb)):\n                        com_loss += common_loss(emb[views[i]], emb[views[j]])\n                loss += 1 * com_loss\n                print(\"com_loss:\", 1 * com_loss)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                losses.append(loss.item())\n                total_loss += loss.item()\n                print(loss)\n                print(\"Batch loss:\", loss.item())\n\n            total_loss /= (batch_id + 1)\n            message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(e + 1, args.epoch, total_loss)\n            print(message)\n            with open(save_path + '/log.txt', 'a') as f:\n                f.write(message)\n                f.write(\"\\n\")\n            out, emb, nids = extract_results(g_dict, views, labels, model, args)\n            # nids = torch.cat(nids).cpu().numpy().astype(int)  # 确保 nids 是整数数组\n            extract_labels = ori_labels[nids]\n            if args.use_uncertainty:\n                alpha = []\n                for out_v in out.values():\n                    evi_v = relu_evidence(out_v)\n                    alpha_v = evi_v + 1\n                    alpha.append(alpha_v)\n                comb_out, comb_u = DS_Combin(alpha=alpha, classes=classes)\n                train_labels = extract_labels[train_indices.cpu().numpy()]\n\n                comb_u = comb_u.cuda()\n                train_u = comb_u[train_indices].cpu().numpy()\n                train_i_u = []\n                for i in range(classes):\n                    i_indices = np.where(train_labels == i)\n                    i_u = np.mean(train_u[i_indices])\n                    train_i_u.append(i_u)\n                label_u = torch.FloatTensor(train_i_u).cuda()\n                # print(\"label_u:\",label_u)\n\n            else:\n                for i, out_v in enumerate(out.values()):\n                    if i == 0:\n                        comb_out = out_v\n                    else:\n                        comb_out += out_v\n\n            _, val_pred = torch.max(comb_out[val_indices.cpu().numpy()], 1)\n            val_labels = torch.IntTensor(extract_labels[val_indices.cpu().numpy()])\n            val_f1 = f1_score(val_labels.cpu().numpy(), val_pred.cpu().numpy(), average='macro')\n            val_match = torch.reshape(torch.eq(val_pred, val_labels).float(), (-1, 1))\n            val_acc = torch.mean(val_match)\n\n            _, test_pred = torch.max(comb_out[test_indices.cpu().numpy()], 1)\n            test_labels = torch.IntTensor(extract_labels[test_indices.cpu().numpy()])\n            test_f1 = f1_score(test_labels.cpu().numpy(), test_pred.cpu().numpy(), average='macro')\n            test_match = torch.reshape(torch.eq(test_pred, test_labels).float(), (-1, 1))\n            test_acc = torch.mean(test_match)\n            # t = classification_report(test_labels.cpu().numpy(), test_pred.cpu().numpy(), target_names=[i for i in range(classes)])\n            message = \"val_acc: %.4f val_f1:%.4f  test_acc: %.4f test_f1:%.4f\" % (val_acc, val_f1, test_acc, test_f1)\n            print(message)\n            with open(save_path + '/log.txt', 'a') as f:\n                f.write(message)\n\n            test_acc_list.append(test_acc)\n\n            if val_acc > best_vali:\n                best_vali = val_acc\n                best_epoch = e + 1\n                test_acc_in_best_e = test_acc\n                p = save_path + 'best.pt'\n                torch.save(model.state_dict(), p)\n\n        np.save(save_path + \"testacc.npy\", np.array(test_acc_list))\n        message = \"best epoch:%d  test_acc:%.4f\" % (best_epoch, test_acc_in_best_e)\n        print(message)\n        with open(save_path + '/log.txt', 'a') as f:\n            f.write(message)\n\n    else:\n        model.load_state_dict(torch.load(save_path + '/best.pt'))\n        model.eval()\n        out, emb, nids = extract_results(g_dict, views, labels, model, args)\n        extract_labels = ori_labels[nids]\n        if args.use_uncertainty:\n            alpha = []\n            for v in ['h', 'u', 'e']:\n                evi_v = relu_evidence(out[v])\n                alpha_v = evi_v + 1\n                alpha.append(alpha_v)\n            comb_out, comb_u = DS_Combin(alpha=alpha, classes=classes)\n        else:\n            for i, v in enumerate(views):\n                if i == 0:\n                    comb_out = out[v]\n                else:\n                    comb_out += out[v]\n\n        _, test_pred = torch.max(comb_out[test_indices.cpu().numpy()], 1)\n        test_labels = torch.IntTensor(extract_labels[test_indices.cpu().numpy()])\n        if args.use_uncertainty:\n            test_u = comb_u[test_indices].cpu().numpy()\n            test_match = torch.reshape(torch.eq(test_pred.cpu().numpy(), test_labels.cpu().numpy()).float(), (-1, 1))\n            test_i_u = []\n            for i in range(classes):\n                i_indices = np.where(test_labels.cpu().numpy() == i)\n                i_u = np.mean(test_u[i_indices])\n                test_i_u.append(i_u)\n\n        test_f1 = f1_score(test_labels.cpu().numpy(), test_pred.cpu().numpy(), average='macro')\n        test_match = torch.reshape(torch.eq(test_pred.cpu().numpy(), test_labels.cpu().numpy()).float(), (-1, 1))\n        test_acc = torch.mean(test_match)\n        t = classification_report(test_labels.cpu().numpy(), test_pred.cpu().numpy())\n        message = \"test_acc: %.4f test_f1:%.4f\" % (test_acc, test_f1)\n        print(message)\n\n    return model\n\n\nclass Tem_Agg_Layer(nn.Module):\n    def __init__(self, in_dim, out_dim, use_residual):\n        super(Tem_Agg_Layer, self).__init__()\n        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n        self.temporal_fc = nn.Linear(out_dim, 1, bias=False)\n        self.reset_parameters()\n        self.use_residual = use_residual\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n\n    def edge_attention(self, edges):\n        deltas = edges.src['t'] - edges.dst['t']\n        deltas = deltas.cpu().detach().numpy()\n        weights = -abs(deltas)\n        return {'e': torch.tensor(weights).unsqueeze(1).to(edges.src['t'].device)}\n\n    def message_func(self, edges):\n        return {'z': edges.src['z'], 'e': edges.data['e']}\n\n    def reduce_func(self, nodes):\n        alpha = F.softmax(torch.exp(self.temporal_fc(nodes.mailbox['z']) * nodes.mailbox['e'] / 500), dim=1)\n        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n        return {'h': h}\n\n    def forward(self, blocks, layer_id):\n        device = blocks[layer_id].device  # 获取当前block的设备\n        h = blocks[layer_id].srcdata['features'].to(device)\n        z = self.fc(h)\n        blocks[layer_id].srcdata['z'] = z\n        z_dst = z[:blocks[layer_id].number_of_dst_nodes()]\n\n        blocks[layer_id].dstdata['z'] = z_dst\n        blocks[layer_id].apply_edges(self.edge_attention)\n\n        blocks[layer_id].update_all(self.message_func, self.reduce_func)\n\n        if self.use_residual:\n            return z_dst + blocks[layer_id].dstdata['h']\n        return blocks[layer_id].dstdata['h']\n\n\nclass GNN(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, use_residual=False):\n        super(GNN, self).__init__()\n        self.layer1 = Tem_Agg_Layer(in_dim, hidden_dim, use_residual)\n        self.layer2 = Tem_Agg_Layer(hidden_dim, out_dim, use_residual)\n\n    def forward(self, blocks):\n        device = blocks[0].device  # 获取第一个block的设备\n        self.layer1 = self.layer1.to(device)\n        self.layer2 = self.layer2.to(device)\n\n        h = self.layer1(blocks, 0)\n        h = F.elu(h)\n        blocks[1].srcdata['features'] = h.to(device)\n        h = self.layer2(blocks, 1)\n        return h\n\n    def edge_attention(self, edges):\n        device = edges.data['features'].device\n        return self.calculate_attention(edges).to(device)\n\n    def calculate_attention(self, edges):\n        # edge attention的计算逻辑\n        pass\n\n\nclass EDNN(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, use_dropout=True):\n        super(EDNN, self).__init__()\n        self.use_dropout = use_dropout\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        hidden = F.relu(self.fc1(x))\n        if self.use_dropout:\n            hidden = F.dropout(hidden, training=self.training)\n        out = self.fc2(hidden)\n        return out\n\n\nclass UCLSED_model(nn.Module):\n    def __init__(self, GNN_in_dim, GNN_h_dim, GNN_out_dim, E_h_dim, E_out_dim, views):\n        super(UCLSED_model, self).__init__()\n        self.views = views\n        self.GNN = GNN(GNN_in_dim, GNN_h_dim, GNN_out_dim)\n        self.EDNNs = nn.ModuleList([EDNN(GNN_out_dim, E_h_dim, E_out_dim) for v in self.views])\n\n    def forward(self, blocks_dict, is_EDNN_input=False, i=None, emb_v=None):\n        out = dict()\n        if not is_EDNN_input:\n            emb = dict()\n            for i, v in enumerate(self.views):\n                emb[v] = self.GNN(blocks_dict[v])\n                out[v] = self.EDNNs[i](emb[v])\n            return out, emb\n        else:\n            out = self.EDNNs[i](emb_v)\n            return out\n\n\n# loss\ndef common_loss(emb1, emb2):\n    emb1 = emb1 - torch.mean(emb1, dim=0, keepdim=True)\n    emb2 = emb2 - torch.mean(emb2, dim=0, keepdim=True)\n    emb1 = torch.nn.functional.normalize(emb1, p=2, dim=1)\n    emb2 = torch.nn.functional.normalize(emb2, p=2, dim=1)\n    cov1 = torch.matmul(emb1, emb1.t())\n    cov2 = torch.matmul(emb2, emb2.t())\n    cost = torch.mean((cov1 - cov2) ** 2)\n    return cost\n\n\ndef EUC_loss(alpha, u, true_labels, e):\n    _, pred_label = torch.max(alpha, 1)\n    true_indices = torch.where(pred_label == true_labels)\n    false_indices = torch.where(pred_label != true_labels)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n    p, _ = torch.max(alpha / S, 1)\n    a = -0.01 * torch.exp(-(e + 1) / 10 * torch.log(torch.FloatTensor([0.01]))).cuda()\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor((e + 1) / 10, dtype=torch.float32),\n    )\n    EUC_loss = -annealing_coef * torch.sum((p[true_indices] * (torch.log(1.000000001 - u[true_indices]).squeeze(\n        -1))))  # -(1-annealing_coef)*torch.sum(((1-p[false_indices])*(torch.log(u[false_indices]).squeeze(-1))))\n\n    return EUC_loss\n\n\ndef relu_evidence(y):\n    return F.relu(y)\n\n\ndef exp_evidence(y):\n    return torch.exp(torch.clamp(y, -10, 10))\n\n\ndef softplus_evidence(y):\n    return F.softplus(y)\n\n\ndef kl_divergence(alpha, num_classes, device):\n    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)\n    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n    first_term = (\n            torch.lgamma(sum_alpha)\n            - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n            + torch.lgamma(ones).sum(dim=1, keepdim=True)\n            - torch.lgamma(ones.sum(dim=1, keepdim=True))\n    )\n    second_term = (\n        (alpha - ones)\n        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n        .sum(dim=1, keepdim=True)\n    )\n    kl = first_term + second_term\n    return kl\n\n\ndef kl_pred_divergence(alpha, y, num_classes, device):\n    # max_alpha, _ = torch.max(alpha, 1)\n    # ones = alpha*(1-y) + (max_alpha+1) * y\n    ones = y + 0.01 * torch.ones([1, num_classes], dtype=torch.float32, device=device)\n    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n    first_term = (\n            torch.lgamma(sum_alpha)\n            - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n            + torch.lgamma(ones).sum(dim=1, keepdim=True)\n            - torch.lgamma(ones.sum(dim=1, keepdim=True))\n    )\n    second_term = (\n        (alpha - ones)\n        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n        .sum(dim=1, keepdim=True)\n    )\n    kl = first_term + second_term\n    return kl\n\n\ndef loglikelihood_loss(y, alpha, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n    loglikelihood_err = torch.sum((y - (alpha / S)) ** 2, dim=1, keepdim=True)\n    loglikelihood_var = torch.sum(\n        alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True\n    )\n    loglikelihood = loglikelihood_err + loglikelihood_var\n    return loglikelihood\n\n\ndef mse_loss(y, alpha, epoch_num, num_classes, annealing_step, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    loglikelihood = loglikelihood_loss(y, alpha, device)\n\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n    )\n\n    kl_alpha = (alpha - 1) * (1 - y) + 1\n    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n    return loglikelihood + kl_div\n\n\ndef edl_loss(func, y, true_labels, alpha, epoch_num, num_classes, annealing_step, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n\n    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)\n\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor((epoch_num + 1) / 10, dtype=torch.float32),\n    )\n\n    _, pred_label = torch.max(alpha, 1)\n    true_indices = torch.where(pred_label == true_labels)\n    false_indices = torch.where(pred_label != true_labels)\n    kl_alpha = (alpha - 1) * (1 - y) + 1\n    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n    print(\"kl_div:\", 1 * torch.mean(kl_div))\n    print(\"A:\", 20 * torch.mean(A))\n\n    return 20 * A + 1 * kl_div\n\n\ndef edl_mse_loss(alpha, target, true_labels, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n    loss = torch.mean(\n        mse_loss(target, alpha, true_labels, epoch_num, num_classes, annealing_step, device=device)\n    )\n    return loss\n\n\ndef edl_log_loss(alpha, target, true_labels, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n    loss = torch.mean(edl_loss(\n        torch.log, target, alpha, true_labels, epoch_num, num_classes, annealing_step, device\n    )\n    )\n    return loss\n\n\ndef edl_digamma_loss(alpha, target, true_labels, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n\n    loss = torch.mean(edl_loss(\n        torch.digamma, target, true_labels, alpha, epoch_num, num_classes, annealing_step, device\n\n    ))\n    return loss\n\n\n# utils\ndef make_onehot(input, classes):\n    input = torch.LongTensor(input).unsqueeze(1)\n    result = torch.zeros(len(input), classes).long()\n    result.scatter_(dim=1, index=input.long(), src=torch.ones(len(input), classes).long())\n    return result\n\n\ndef relu_evidence(y):\n    return F.relu(y)\n\n\ndef exp_evidence(y):\n    return torch.exp(torch.clamp(y, -10, 10))\n\n\ndef softplus_evidence(y):\n    return F.softplus(y)\n\n\ndef DS_Combin(alpha, classes):\n    \"\"\"\n    :param alpha: All Dirichlet distribution parameters.\n    :return: Combined Dirichlet distribution parameters.\n    \"\"\"\n\n    def DS_Combin_two(alpha1, alpha2, classes):\n        \"\"\"\n        :param alpha1: Dirichlet distribution parameters of view 1\n        :param alpha2: Dirichlet distribution parameters of view 2\n        :return: Combined Dirichlet distribution parameters\n        \"\"\"\n        alpha = dict()\n        alpha[0], alpha[1] = alpha1, alpha2\n        b, S, E, u = dict(), dict(), dict(), dict()\n        for v in range(2):\n            S[v] = torch.sum(alpha[v], dim=1, keepdim=True)\n            E[v] = alpha[v] - 1\n            b[v] = E[v] / (S[v].expand(E[v].shape))\n            u[v] = classes / S[v]\n\n        # b^0 @ b^(0+1)\n        bb = torch.bmm(b[0].view(-1, classes, 1), b[1].view(-1, 1, classes))\n        # b^0 * u^1\n        uv1_expand = u[1].expand(b[0].shape)\n        bu = torch.mul(b[0], uv1_expand)\n        # b^1 * u^0\n        uv_expand = u[0].expand(b[0].shape)\n        ub = torch.mul(b[1], uv_expand)\n        # calculate C\n        bb_sum = torch.sum(bb, dim=(1, 2), out=None)\n        bb_diag = torch.diagonal(bb, dim1=-2, dim2=-1).sum(-1)\n        C = bb_sum - bb_diag\n\n        # calculate b^a\n        b_a = (torch.mul(b[0], b[1]) + bu + ub) / ((1 - C).view(-1, 1).expand(b[0].shape))\n        # calculate u^a\n        u_a = torch.mul(u[0], u[1]) / ((1 - C).view(-1, 1).expand(u[0].shape))\n\n        # calculate new S\n        S_a = classes / u_a\n        # calculate new e_k\n        e_a = torch.mul(b_a, S_a.expand(b_a.shape))\n        alpha_a = e_a + 1\n        return alpha_a, u_a\n\n    if len(alpha) == 1:\n        S = torch.sum(alpha[0], dim=1, keepdim=True)\n        u = classes / S\n        return alpha[0], u\n    for v in range(len(alpha) - 1):\n        if v == 0:\n            alpha_a, u_a = DS_Combin_two(alpha[0], alpha[1], classes)\n        else:\n            alpha_a, u_a = DS_Combin_two(alpha_a, alpha[v + 1], classes)\n    return alpha_a, u_a\n\n\ndef graph_statistics(G, save_path):\n    message = '\\nGraph statistics:\\n'\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    ave_degree = (num_edges / 2) // num_nodes\n    in_degrees = G.in_degrees()\n    isolated_nodes = torch.zeros([in_degrees.size()[0]], dtype=torch.long)\n    isolated_nodes = (in_degrees == isolated_nodes)\n    torch.save(isolated_nodes, save_path + '/isolated_nodes.pt')\n    num_isolated_nodes = torch.sum(isolated_nodes).item()\n    message += 'We have ' + str(num_nodes) + ' nodes.\\n'\n    message += 'We have ' + str(num_edges / 2) + ' in-edges.\\n'\n    message += 'Average degree: ' + str(ave_degree) + '\\n'\n    message += 'Number of isolated nodes: ' + str(num_isolated_nodes) + '\\n'\n    print(message)\n    with open(save_path + \"/graph_statistics.txt\", \"w\") as f:\n        f.write(message)\n    return num_isolated_nodes\n\n\ndef get_dgl_data(args,views,language):\n    g_dict = {}\n    path = args.file_path + language + '/'\n    features = torch.FloatTensor(np.load(path + \"features.npy\"))\n    times = np.load(path + \"time.npy\")\n    times = torch.FloatTensor(((times - times.min()).astype('timedelta64[D]') / np.timedelta64(1, 'D')))\n    labels = np.load(path + \"label.npy\")\n    for v in views:\n        if v == \"h\":\n            matrix = sparse.load_npz(path + \"s_tweet_tweet_matrix_{}.npz\".format(v))\n            # matrix = np.load(path + \"matrix_{}.npy\".format(v+noise))\n        else:\n            matrix = sparse.load_npz(path + \"s_tweet_tweet_matrix_{}.npz\".format(v))\n        g = dgl.DGLGraph(matrix, readonly=True)\n        save_path_v = path + v\n        if not os.path.exists(save_path_v):\n            os.mkdir(save_path_v)\n        num_isolated_nodes = graph_statistics(g, save_path_v)\n        g.set_n_initializer(dgl.init.zero_initializer)\n        # g.readonly(readonly_state=True)\n        # g.ndata['features'] = features\n        # # g.ndata['labels'] = labels\n        # g.ndata['times'] = times\n        g_dict[v] = g\n    return g_dict, times, features, labels\n\n\ndef split_data(length, train_p, val_p, test_p):\n    indices = torch.randperm(length)\n    val_samples = int(length * val_p)\n    val_indices = indices[:val_samples]\n    test_samples = val_samples + int(length * test_p)\n    test_indeces = indices[val_samples:test_samples]\n    train_indices = indices[test_samples:]\n    return train_indices, val_indices, test_indeces\n\n\ndef ava_split_data(length, labels, classes):\n    indices = torch.randperm(length)\n    labels = torch.LongTensor(labels[indices])\n\n    train_indices = []\n    test_indices = []\n    val_indices = []\n\n    for l in range(classes):\n        l_indices = torch.LongTensor(np.where(labels.numpy() == l)[0].reshape(-1))\n        val_indices.append(l_indices[:20].reshape(-1, 1))\n        test_indices.append(l_indices[20:50].reshape(-1, 1))\n        train_indices.append(l_indices[50:].reshape(-1, 1))\n\n    val_indices = indices[torch.cat(val_indices, dim=0).reshape(-1)]\n    test_indices = indices[torch.cat(test_indices, dim=0).reshape(-1)]\n    train_indices = indices[torch.cat(train_indices, dim=0).reshape(-1)]\n    print(train_indices.shape, val_indices.shape, test_indices.shape)\n    print(train_indices)\n    return train_indices, val_indices, test_indices\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/hcrc.py", "content": "import gc\nimport random\nimport numpy as np\nimport os\nimport time\nfrom datetime import datetime\nfrom collections import deque\nfrom sklearn import metrics\nfrom scipy.sparse import csr_matrix\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch import optim\nfrom torch_geometric.nn import global_add_pool, global_mean_pool, global_max_pool, GlobalAttention, Set2Set\nimport pickle\nimport copy\nimport argparse\nimport networkx as nx\nimport scipy.sparse as sp\nfrom torch_geometric import loader\nimport spacy\nimport pandas as pd\nimport networkx as nx\nfrom torch_geometric.data import Data\nfrom torch_geometric import loader\nfrom torch_geometric.nn import GCNConv\nfrom torch.distributions import Categorical, MultivariateNormal\nfrom sklearn.metrics import silhouette_score,calinski_harabasz_score\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader,Event2012\n\ndef currentTime():\n    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\n\nclass HCRC:\n    r\"\"\"The HCRC model for social event detection that uses hierarchical clustering \n    and reinforcement learning for adaptive event detection.\n\n    .. note::\n        This detector uses hierarchical clustering and reinforcement learning to adaptively\n        detect events in social media data. The model requires a dataset object with a\n        load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/hcrc/'``.\n    result_path : str, optional\n        Path to save results file. Default: ``'../model/model_saved/hcrc/res.txt'``.\n    task : str, optional\n        Task type, e.g. 'DRL' for deep reinforcement learning. Default: ``'DRL'``.\n    layers : str, optional\n        Hidden layer dimensions as string. Default: ``'[256]'``.\n    N_pred_hid : int, optional\n        Node prediction hidden dimension. Default: ``64``.\n    G_pred_hid : int, optional\n        Graph prediction hidden dimension. Default: ``16``.\n    eval_freq : float, optional\n        Evaluation frequency. Default: ``5``.\n    mad : float, optional\n        Moving average decay rate. Default: ``0.9``.\n    Glr : float, optional\n        Learning rate for graph model. Default: ``0.0000006``.\n    Nlr : float, optional\n        Learning rate for node model. Default: ``0.00001``.\n    Ges : int, optional\n        Graph model early stopping patience. Default: ``50``.\n    Nes : int, optional\n        Node model early stopping patience. Default: ``2000``.\n    Gepochs : int, optional\n        Number of graph model training epochs. Default: ``105``.\n    Nepochs : int, optional\n        Number of node model training epochs. Default: ``100``.\n    device : int, optional\n        GPU device ID to use. Default: ``0``.\n    \"\"\"\n\n    \n    def __init__(self,\n                 dataset,\n                 # 文件路径相关参数\n                 file_path: str = '../model/model_saved/hcrc/',\n                 result_path: str = '../model/model_saved/hcrc/res.txt',\n\n                 # 任务相关参数\n                 task: str = 'DRL',\n\n                 # 模型结构相关参数\n                 layers: str = '[256]',\n                 N_pred_hid: int = 64,\n                 G_pred_hid: int = 16,\n\n                 # 训练和评估相关参数\n                 eval_freq: float = 5,\n                 mad: float = 0.9,\n                 Glr: float = 0.0000006,\n                 Nlr: float = 0.00001,\n                 Ges: int = 50,\n                 Nes: int = 2000,\n                 Gepochs: int = 105,\n                 Nepochs: int = 100,\n\n                 # 设备相关参数\n                 device: int = 0):\n        self.dataset = dataset.load_data()\n        # 文件路径相关参数\n        self.file_path = file_path\n        self.result_path = result_path\n\n        # 任务相关参数\n        self.task = task\n\n        # 模型结构相关参数\n        self.layers = layers\n        self.N_pred_hid = N_pred_hid\n        self.G_pred_hid = G_pred_hid\n\n        # 训练和评估相关参数\n        self.eval_freq = eval_freq\n        self.mad = mad\n        self.Glr = Glr\n        self.Nlr = Nlr\n        self.Ges = Ges\n        self.Nes = Nes\n        self.Gepochs = Gepochs\n        self.Nepochs = Nepochs\n\n        # 设备相关参数\n        self.device = device\n\n    def fit(self):\n        pass\n\n    def detection(self):\n        args=self\n        for i in range(22):\n            print(\"************Message Block \"+str(i)+\" start! ************\")\n            #Node-level learning\n            embedder_N = Node_ModelTrainer(self,i)\n            Node_emb,label = embedder_N.get_embedding()\n            #Graph-level learning\n            embedder_G = Graph_ModelTrainer(self,i)\n            Graph_emb,label = embedder_G.get_embedding()\n            #combining vectors\n            if i==0:\n                all_embeddings = np.concatenate((Graph_emb,Node_emb),axis=1)\n                all_label = label\n            else:\n                temp = np.concatenate((Graph_emb,Node_emb),axis=1)\n                all_embeddings = np.concatenate((all_embeddings,temp),axis=0)\n                all_label = all_label+label\n            all_embeddings = torch.tensor(all_embeddings)\n            all_embeddings = F.normalize(all_embeddings, dim=-1, p=2).detach().cpu().numpy()\n                \n            if i == 0:\n                pred_y = evaluate_fun(all_embeddings,label,i,None,args.result_path,args.task)\n                all_pred_y = pred_y\n            else:\n                pred_y = evaluate_fun(all_embeddings,label,i,all_pred_y,args.result_path,args.task)\n                all_pred_y = all_pred_y + pred_y\n            print(\"************Message Block \"+str(i)+\" end! ************\\n\\n\")\n\n        predictions = all_pred_y\n        ground_truths = all_label\n\n        return predictions, ground_truths\n\n    def evaluate(self, predictions, ground_truths):\n        print(\"************Evaluation start! ************\")\n        ars = metrics.adjusted_rand_score(ground_truths, predictions)\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        \n        print(f\"Model Adjusted Rand Index (ARI): {ars}\")\n        print(f\"Model Adjusted Mutual Information (AMI): {ami}\")\n        print(f\"Model Normalized Mutual Information (NMI): {nmi}\")\n        return ars, ami, nmi\n\nclass EMA:  #Exponential Moving Average\n    def __init__(self, beta, epochs):\n        super().__init__()\n        self.beta = beta\n        self.step = 0\n        self.total_steps = epochs\n\n    def update_average(self, old, new):\n        if old is None:\n            return new\n        return old * self.beta + (1 - self.beta) * new\n\ndef get_task(strs):\n    tasks = [\"DRL\",\"random\",\"semi-supervised\",\"traditional\"]\n    if len(strs) == 1:\n        return \"DRL\"\n    if (\"--task\" in strs) and len(strs) == 2:\n        return \"DRL\"\n    if (\"--task\" not in strs) or len(strs)!=3:\n        return False\n    elif strs[-1] not in tasks:\n        return False\n    else:\n        return strs[-1]\n\ndef init_weights(m):  #Model parameter initialization\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n        \ndef sim(z1, z2):\n    z1 = F.normalize(z1)\n    z2 = F.normalize(z2)\n    return torch.mm(z1, z2.t())\n\ndef semi_loss(z1, z2):\n    f = lambda x: torch.exp(x / 0.05)\n    refl_sim = f(sim(z1, z1))\n    between_sim = f(sim(z1, z2))\n\n    return -torch.log(between_sim.diag() / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))\n\ndef get_loss(h1, h2):\n    l1 = semi_loss(h1, h2)\n    l2 = semi_loss(h2, h1)\n\n    ret = (l1 + l2) * 0.5\n    ret = ret.mean()\n\n    return ret\n\ndef update_moving_average(ema_updater, ma_model, current_model):\n    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n        old_weight, up_weight = ma_params.data, current_params.data\n        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n\ndef set_requires_grad(model, val):\n    #set require_grad\n    for p in model.parameters():\n        p.requires_grad = val\n\ndef enumerateConfig(args):\n    args_names = []\n    args_vals = []\n    for arg in vars(args):\n        args_names.append(arg)\n        args_vals.append(getattr(args, arg))\n\n    return args_names, args_vals\n\ndef config2string(args):\n    args_names, args_vals = enumerateConfig(args)\n    st = ''\n    for name, val in zip(args_names, args_vals):\n        if val == False:\n            continue\n        if name not in ['device','root','epochs','isAnneal','dropout','warmup_step','clus_num_iters']:\n            st_ = \"{}_{}_\".format(name, val)\n            st += st_\n\n    return st[:-1]\n\ndef printConfig(args): \n    args_names, args_vals = enumerateConfig(args)\n    print(args_names)\n    print(args_vals)\n\n\nclass embedder:\n    def __init__(self, args):\n        self.args = args\n        self.hidden_layers = eval(args.layers)\n\nclass Encoder(nn.Module):\n\n    def __init__(self, layer_config):\n        super().__init__()\n        self.stacked_gnn = nn.ModuleList(\n            [GCNConv(layer_config[i - 1], layer_config[i]) for i in range(1, len(layer_config))])\n        self.stacked_bns = nn.ModuleList(\n            [nn.BatchNorm1d(layer_config[i], momentum=0.01) for i in range(1, len(layer_config))])\n        self.stacked_prelus = nn.ModuleList([nn.PReLU() for _ in range(1, len(layer_config))])\n\n    def forward(self, x, edge_index):\n        for i, gnn in enumerate(self.stacked_gnn):\n            x = gnn(x, edge_index, edge_weight=None)\n            x = self.stacked_bns[i](x)\n            x = self.stacked_prelus[i](x)\n\n        return x\n\nM =[20254,28976,30467,32302,34312,36146,37422,42700,44260,45623,46719,\n    47951,51188,53160,56116,58665,59575,62251,64138,65537,66430,68840]\n\ndef DRL_cluster(all_embeddings,block_num,pred_label):\n    para = 0.1\n    if block_num == 0:\n        print(\"Evaluating initial message block...\")\n        start_time = time.time()\n        sp = SinglePass(0.87, all_embeddings, 0, pred_label, M[0], None, para, 0, sim=False)\n        end_time = time.time()\n        run_time = end_time - start_time\n        print(\"Done! \" + \"It takes \"+str(int(run_time))+\" seconds.\\n\")\n    else:\n        print(\"Using DRL-Single-Pass to learn threshold...\")\n        global_step = 0\n        agent = PPO([5], 1, continuous=True)\n        sp_sim = SinglePass(0.6, all_embeddings, 1, pred_label, M[block_num] - M[block_num - 1], agent, para, M[block_num-1]-2000, sim=True)\n        \n        global_step = sp_sim.global_step\n        sp = SinglePass(0.6, all_embeddings, 1, pred_label, M[block_num] - M[block_num - 1], agent, para, M[block_num-1]-2000, sim=False)\n    \n    return sp.cluster_result,sp.sim_threshold\n    \ndef random_cluster(all_embeddings,block_num,pred_label):\n    threshold = random.uniform(0.6,0.8)\n    if block_num == 0:\n        print(\"Evaluating initial message block...\")\n        start_time = time.time()\n        sp = SinglePass(0.87, all_embeddings, 0, pred_label, M[0], None, 0, 0, sim=False)\n        end_time = time.time()\n        run_time = end_time - start_time\n        print(\"Done! \" + \"It takes \"+str(int(run_time))+\" seconds.\\n\")\n        threshold = 0.87\n    else:\n        print(\"Evaluating message block...\")\n        start_time = time.time()\n        sp = SinglePass(threshold, all_embeddings, 2, pred_label, M[block_num] - M[block_num - 1], None, 0, 0, sim=False)\n        end_time = time.time()\n        run_time = end_time - start_time\n        print(\"Done! \" + \"It takes \"+str(int(run_time))+\" seconds.\\n\")\n    return sp.cluster_result,threshold\n    \ndef semi_cluster(all_embeddings,label,block_num,pred_label):\n    if block_num == 0:\n        print(\"Evaluating initial message block...\")\n        start_time = time.time()\n        sp = SinglePass(0.87, all_embeddings, 0, pred_label, M[0], None, 0, 0, sim=False)\n        end_time = time.time()\n        run_time = end_time - start_time\n        print(\"Done! \" + \"It takes \"+str(int(run_time))+\" seconds.\\n\")\n        threshold = 0.87\n    else:\n        print(\"Evaluating message block...\")\n        start_time = time.time()\n        embeddings = all_embeddings.tolist()\n        size = M[block_num] - M[block_num - 1]\n        embeddings = embeddings[0:len(embeddings)-int(size*0.9)]\n        pre_label = pred_label[0:len(embeddings)]\n        \n        size = len(embeddings) - M[block_num - 1]\n        embeddings = np.array(embeddings)\n        thresholds = [0.6,0.65,0.7,0.75,0.8]\n        s1s = []\n        for t in thresholds:\n            sp = SinglePass(t, embeddings, 2, pre_label, size, None, 0, 0, sim=False)\n            true_label = label[0:len(sp.cluster_result)]\n            s1 = metrics.normalized_mutual_info_score(true_label, sp.cluster_result, average_method='arithmetic')\n            s1s.append(s1)\n        index = s1s.index(max(s1s))\n        sp = SinglePass(thresholds[index], all_embeddings, 2, pred_label, M[block_num] - M[block_num - 1], None, 0, 0, sim=False)\n        end_time = time.time()\n        run_time = end_time - start_time\n        print(\"Done! \" + \"It takes \"+str(int(run_time))+\" seconds.\\n\")\n        threshold = thresholds[index]\n    return sp.cluster_result,threshold\n        \ndef NMI_cluster(all_embeddings,label,block_num,pred_label):\n    if block_num == 0:\n        print(\"Evaluating initial message block...\")\n        start_time = time.time()\n        sp = SinglePass(0.87, all_embeddings, 0, pred_label, M[0], None, 0, 0, sim=False)\n        end_time = time.time()\n        run_time = end_time - start_time\n        print(\"Done! \" + \"It takes \"+str(int(run_time))+\" seconds.\\n\")\n        threshold = 0.87\n    else:\n        print(\"Evaluating message block...\")\n        start_time = time.time()\n        thresholds = [0.6,0.65,0.7,0.75,0.8]\n        s1s = []\n        for t in thresholds:\n            sp = SinglePass(t, all_embeddings, 2, pred_label, M[block_num] - M[block_num - 1], None, 0, 0, sim=False)\n            s1 = metrics.normalized_mutual_info_score(label, sp.cluster_result, average_method='arithmetic')\n            s1s.append(s1)\n        index = s1s.index(max(s1s))\n        sp = SinglePass(thresholds[index], all_embeddings, 2, pred_label, M[block_num] - M[block_num - 1], None, 0, 0, sim=False)\n        end_time = time.time()\n        run_time = end_time - start_time\n        print(\"Done! \" + \"It takes \"+str(int(run_time))+\" seconds.\\n\")\n        threshold = thresholds[index]\n    return sp.cluster_result,threshold \n        \ndef evaluate_fun(all_embeddings,label,block_num,pred_label,result_path,task):\n    if task == \"DRL\":\n        y_pred,threshold = DRL_cluster(all_embeddings,block_num,pred_label)\n    elif task == \"random\":\n        y_pred,threshold = random_cluster(all_embeddings,block_num,pred_label)\n    elif task == \"semi-supervised\":\n        y_pred,threshold = semi_cluster(all_embeddings,label,block_num,pred_label)\n    elif task == \"traditional\":\n        y_pred,threshold = NMI_cluster(all_embeddings,label,block_num,pred_label)\n    \n    #NMI\n    s1 = metrics.normalized_mutual_info_score(label, y_pred, average_method='arithmetic')\n    #AMI\n    s2 = metrics.adjusted_mutual_info_score(label, y_pred, average_method='arithmetic')\n    #ARI\n    s3 = metrics.adjusted_rand_score(label, y_pred)\n    \n    print('** Theta:{:.2f} **\\n'.format(threshold))\n    print('** NMI: {:.2f} **\\n'.format(s1))\n    print('** AMI: {:.2f} **\\n'.format(s2))\n    print('** ARI: {:.2f} **\\n'.format(s3))\n    result = '\\nmessage_block_'+str(block_num)+'\\nthreshold: {:.2f} '.format(threshold)+'\\n** NMI: {:.2f} **\\n'.format(s1) + '** AMI: {:.2f} **\\n'.format(s2) + '** ARI: {:.2f} **\\n'.format(s3)\n\n    if not os.path.exists(result_path) :\n        pass\n    else:\n        with open(result_path,encoding='utf-8') as file:\n            content=file.read()\n        result = content.rstrip() + result\n    file = open(result_path, mode='w')\n    file.write(result)\n    file.close()\n    return y_pred\n\nclass SinglePass:\n    def __init__(self, sim_threshold, data, flag, label, size, agent, para, sim_init, sim=False, global_step=0):\n        self.device = torch.device('cuda:0')\n        self.text_vec = None  #\n        self.topic_serial = None\n        self.topic_cnt = 0\n        self.sim_threshold = sim_threshold\n        \n        self.done_data = data[0:data.shape[0] - size]\n        self.new_data = data[data.shape[0] - size:]\n        self.done_label = label\n        \n        if flag == 0 or flag == 2:\n            self.cluster_result = self.run_cluster(flag, size)\n        else:\n            self.agent = agent\n            self.scheme = [\"state\", \"action\", \"reward\", \"done\", \"log_prob\"]\n            self.global_step = global_step\n            self.sim = sim\n            if self.sim:\n                start_time = time.time()\n                self.cluster_result = self.run_cluster_sim(flag, size, para, sim_init, sim, data)  \n                end_time = time.time()\n                self.time = end_time - start_time\n                print(\"Creating Environment Done! \" + \"It takes \"+str(int(self.time))+\" seconds.\")\n            else:\n                start_time = time.time()\n                self.pseudo_labels = self.run_cluster_init(0.6, size)\n                if flag == 1:\n                    self.text_vec = self.done_data\n                    self.topic_serial = copy.deepcopy(self.done_label)\n                    self.topic_cnt = max(self.topic_serial)\n                state = self.get_state(sim, sim_init, data)\n                action, action_log_prob = self.agent.select_action(state)\n                # action projection\n                sim_threshold = torch.clamp(action, -1, 1).detach()\n                sim_threshold += 7\n                sim_threshold /=10\n                self.sim_threshold = sim_threshold.item()\n                end_time = time.time()\n                self.time = end_time - start_time\n                print(\"Getting Threshold Done! \" + \"It takes \"+str(int(self.time))+\" seconds. \")\n                print(\"Threshold is \"+str(self.sim_threshold)+\".\\n\")\n                print(\"Evaluating message block...\")\n                start_time = time.time()\n                self.cluster_result = self.run_cluster(flag, size)  # clustering\n                end_time = time.time()\n                self.time = end_time - start_time\n                print(\"Done! \" + \"It takes \"+str(int(self.time))+\" seconds.\\n\")\n\n    def clustering(self, sen_vec):\n        if self.topic_cnt == 0:\n            self.text_vec = sen_vec\n            self.topic_cnt += 1\n            self.topic_serial = [self.topic_cnt]\n        else:\n            sim_vec = np.dot(sen_vec, self.text_vec.T)\n            max_value = np.max(sim_vec)\n\n            topic_ser = self.topic_serial[np.argmax(sim_vec)]\n            self.text_vec = np.vstack([self.text_vec, sen_vec])\n\n            if max_value >= self.sim_threshold:\n                self.topic_serial.append(topic_ser)\n            else:\n                self.topic_cnt += 1\n                self.topic_serial.append(self.topic_cnt)\n    \n    def clustering_init(self, t, sen_vec):\n        if self.topic_cnt_init == 0:\n            self.text_vec_init = sen_vec\n            self.topic_cnt_init += 1\n            self.topic_serial_init = [self.topic_cnt_init]\n        else:\n            sim_vec = np.dot(sen_vec, self.text_vec_init.T)\n            max_value = np.max(sim_vec)\n\n            topic_ser = self.topic_serial_init[np.argmax(sim_vec)]\n            self.text_vec_init = np.vstack([self.text_vec_init, sen_vec])\n\n            if max_value >= t:\n                self.topic_serial_init.append(topic_ser)\n            else:\n                self.topic_cnt_init += 1\n                self.topic_serial_init.append(self.topic_cnt_init)\n    \n    def run_cluster_init(self, t, size):\n        self.text_vec_init = []\n        self.topic_serial_init = []\n        self.topic_cnt_init = 0\n        for vec in self.new_data:\n            self.clustering_init(t,vec)\n        return self.topic_serial_init\n    \n    def run_cluster_sim(self, flag, size, para, sim_init, sim, data):\n        self.text_vec = []\n        self.topic_serial = []\n        self.topic_cnt = 0\n        if flag == 1:\n            self.text_vec = self.done_data\n            self.topic_serial = copy.deepcopy(self.done_label)\n            self.topic_cnt = max(self.topic_serial)\n        for i, vec in enumerate(self.new_data):\n            self.global_step += 1\n            if i > 200:\n                break\n            if i > self.new_data.shape[0] * para:\n                break\n            state = self.get_state(sim, sim_init, data)\n            \n            action, action_log_prob = self.agent.select_action(state)\n            self.sim_threshold = action.item()\n            self.clustering(vec)\n            \n            reward = self.get_reward(sim_init, data)\n            done = False\n            transition = make_transition(self.scheme, state, action, reward, done, action_log_prob)\n            self.agent.add_buffer(transition)\n            if self.global_step % 200==0:\n                self.agent.learn()\n\n        return self.topic_serial[len(self.topic_serial) - size:]\n\n    def run_cluster(self, flag, size):\n        self.text_vec = []\n        self.topic_serial = []\n        self.topic_cnt = 0\n        if flag == 1 or flag == 2:\n            self.text_vec = self.done_data\n            self.topic_serial = copy.deepcopy(self.done_label)\n            self.topic_cnt = max(self.topic_serial)\n        for i, vec in enumerate(self.new_data):\n            self.clustering(vec)\n        return self.topic_serial[len(self.topic_serial) - size:]\n\n    def get_center(self,label,data):\n        centers = []\n        indexs_per_cluster = []\n        label_u = list(set(label))\n        for i in range(len(label_u)):\n            indexs = [False] * data.shape[0]\n            tmp_indexs_text = []\n            for j in range(len(indexs)):\n                if label[j] == label_u[i]:\n                    indexs[j] = True\n                    tmp_indexs_text.append(j)\n            center = np.mean(data[indexs], 0).tolist()\n            centers.append(center)\n            indexs_per_cluster.append(tmp_indexs_text)\n        return centers,indexs_per_cluster\n\n    def get_info_cluster(self,text_vec,indexs_per_cluster):  # Get detailed clustering results\n        res = []\n        for i in range(len(indexs_per_cluster)):\n            tmp_vec = []\n            for j in range(len(indexs_per_cluster[i])):\n                tmp_vec.append(text_vec[indexs_per_cluster[i][j]])\n            tmp_vec = np.array(tmp_vec)\n            res.append(tmp_vec)\n        return res\n\n    def get_state(self, sim, sim_init, data):  # get state of RL\n        state = []\n        if sim:\n            data = data[sim_init:len(self.topic_serial)]\n            topic_serial = self.topic_serial[sim_init:]\n        else:\n            data = self.new_data\n            topic_serial = self.pseudo_labels\n        centers,indexs_per_cluster = self.get_center(topic_serial, data)\n        \n        centers = np.array(centers)\n        neighbor_dists = np.dot(centers, centers.T)\n        \n        neighbor_dists = np.nan_to_num(neighbor_dists, 0.0001)\n        # the minimum neighbor distance\n        state.append(neighbor_dists.min())\n        # the average separation distance\n        state.append((neighbor_dists.mean() * max(topic_serial) - 1) / max(topic_serial))\n        info_of_cluster = self.get_info_cluster(data,indexs_per_cluster)\n\n        coh_dists = 0\n        for cluster in info_of_cluster:\n            if cluster.shape[0] == 1:\n                continue\n            else:\n                sums = cluster.shape[0] * (cluster.shape[0] - 1) / 2\n            tmp_vec = np.array(cluster)\n            cohdist = np.dot(tmp_vec, tmp_vec.T)\n            if cohdist.max() > coh_dists:\n                coh_dists = cohdist.max()\n#         Dunn index\n        state.append(neighbor_dists.min()/coh_dists)\n    \n        #Sum of intra-group error squares\n        SSE = 0\n        SSEE = 0\n        for i in range(len(indexs_per_cluster)):\n            sumtmp = 0\n            for j in range(len(indexs_per_cluster[i])):\n                tmp = np.dot(data[indexs_per_cluster[i][j]].T,centers[i])\n                SSE = SSE + (tmp)**2\n                sumtmp = sumtmp + (tmp)**2\n            SSEE = SSEE + sumtmp/len(indexs_per_cluster[i])\n#         state.append(SSE)\n\n        # Sum of squared errors between groups\n        SSR = 0\n        SSRR = 0\n        for i in range(len(centers)):\n            SSR = SSR + np.dot(centers[i].T,centers.mean(axis=0))\n            SSRR = SSRR + np.dot(centers[i].T,centers.mean(axis=0))**2\n        SSRR = SSRR / max(topic_serial)\n#         state.append(SSR)\n        #the average cohesion distance\n        coh_dists = 0\n        for cluster in info_of_cluster:\n            if cluster.shape[0] == 1:\n                continue\n            else:\n                sums = cluster.shape[0] * (cluster.shape[0] - 1) / 2\n            tmp_vec = np.array(cluster)\n            cohdist = np.dot(tmp_vec, tmp_vec.T)\n            cohdist = np.maximum(cohdist, -cohdist)\n            coh_dists = coh_dists + (cohdist.sum() - cluster.shape[0]) / (2 * sums + 0.0001)\n        state.append(coh_dists / max(topic_serial))\n\n        state.append(silhouette_score(data, topic_serial, metric='euclidean'))\n        return np.array(state)\n\n    def get_reward(self, sim_init, data):  # get reward of RL\n\n        data = data[sim_init:len(self.topic_serial)]\n        topic_serial = self.topic_serial[sim_init:]\n        \n        return calinski_harabasz_score(data, topic_serial)\n\nclass Node_ModelTrainer(embedder):\n\n    def __init__(self, args,block_num):\n        embedder.__init__(self, args)\n        self._args = args\n        self.block_num = block_num\n        self._init()\n\n    def _init(self):\n        block_num = self.block_num\n        args = self._args\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)\n        self._device = f'cuda:{args.device}' if torch.cuda.is_available() else \"cpu\"\n        torch.cuda.set_device(self._device)\n        \n        \n        if block_num == 0 or block_num == 1:\n            args.Nes = 200\n        else:\n            args.Nes = 2000\n        self.true_label = []\n        #load data\n        self._loader,self.true_label = get_Node_Dataset(args,block_num)\n\n        layers = [302] + self.hidden_layers\n        self._model = NodeLevel(layers, args)\n\n        self._optimizer = optim.AdamW(params=self._model.parameters(), lr=args.Nlr, weight_decay=1e-5)\n        self.train()\n        self.all_embeddings = F.normalize(self.all_embeddings, dim=-1, p=2).detach().cpu().numpy()\n    \n    def get_embedding(self):\n        return self.all_embeddings,self.true_label\n    \n    def train(self):\n\n        loss_t = 1e10\n        cnt_wait = 0\n        # Start Model Training\n        print(\"----Node-Level Training Start! ----\\n\")\n        for epoch in range(self._args.Nepochs):\n            losses = []\n            embs = []\n            for batch in self._loader:\n\n                emb,loss = self._model(batch)\n                self._optimizer.zero_grad()\n                loss.backward()\n                self._optimizer.step()\n                self._model.update_moving_average()\n                losses.append(loss.item())\n                embs = embs +emb\n\n            st = '[{}][Epoch {}/{}] Loss: {:.4f}'.format(currentTime(), \n                                                         epoch, self._args.Nepochs, np.mean(np.array(losses)))\n            print(st)\n            #Early Stopping Criterion\n            if  np.mean(np.array(losses)) < loss_t:\n                loss_t = np.mean(np.array(losses))\n            else:\n                cnt_wait = cnt_wait + 1\n            if cnt_wait > self._args.Nes:\n                print(\"Early Stopping Criterion\")\n                break\n\n        self.all_embeddings  = torch.tensor(embs)\n        print(\"\\n----Node-Level Training Done! ----\\n\")\n\nclass NodeLevel(nn.Module):\n    def __init__(self, layer_config, args):\n        super().__init__()\n        self._device = f'cuda:{args.device}' if torch.cuda.is_available() else \"cpu\"\n        #encoder\n        self.student_encoder = Encoder(layer_config=layer_config)\n        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n        self.student_encoder = self.student_encoder.to(self._device)\n        self.teacher_encoder = self.teacher_encoder.to(self._device)\n        set_requires_grad(self.teacher_encoder, False)\n        self.teacher_ema_updater = EMA(args.mad, args.Nepochs)\n\n        rep_dim = layer_config[-1]\n\n        #projection head\n        self.student_projector = nn.Sequential(nn.Linear(rep_dim, args.N_pred_hid), nn.BatchNorm1d(args.N_pred_hid),\n                                               nn.PReLU(), nn.Linear(args.N_pred_hid, rep_dim))\n     \n        self.student_projector = self.student_projector.to(self._device)\n\n        self.student_projector.apply(init_weights)\n        self.teacher_projector = copy.deepcopy(self.student_projector)\n        set_requires_grad(self.teacher_projector, False)\n        \n    def reset_moving_average(self):\n        del self.teacher_encoder\n        self.teacher_encoder = None\n\n    def update_moving_average(self):\n        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n        update_moving_average(self.teacher_ema_updater, self.teacher_projector, self.student_projector)\n\n    def forward(self, batch):\n\n        student = self.student_encoder(batch.x1.to(torch.float32).to(self._device),batch.edge_index1.to(self._device))\n\n        h1 = self.student_projector(student)\n        \n        with torch.no_grad(): #stop gradient\n            teacher = self.teacher_encoder(batch.x2.to(torch.float32).to(self._device),\n                                           batch.edge_index2.to(self._device))\n        with torch.no_grad(): #stop gradient\n            h2 = self.teacher_projector(teacher)\n\n        emb = self.student_encoder(batch.x.to(torch.float32).to(self._device),batch.edge_index.to(self._device))\n   \n        emb = emb.detach().cpu().numpy().tolist()\n\n        loss = get_loss(h1,h2)              \n          \n        return emb,loss\n\nclass Graph_ModelTrainer(embedder):\n\n    def __init__(self, args,block_num):\n        self.block_num = block_num\n        embedder.__init__(self, args)\n        self._args = args\n        self._init()\n    \n    def _init(self):\n        args = self._args\n        block_num = self.block_num\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)\n        self._device = f'cuda:{args.device}' if torch.cuda.is_available() else \"cpu\"\n        torch.cuda.set_device(self._device)\n        if torch.cuda.is_available():\n            print(\"using cuda\")\n\n        layers = [300] + self.hidden_layers\n        #load data\n        self._model = GraphLevel(layers, args).to(self._device)\n        self._model.to(self._device)\n        self._optimizer = optim.AdamW(params=self._model.parameters(), lr=args.Glr, weight_decay=1e-5)\n\n        self._loader,self.true_label = get_Graph_Dataset(args,block_num)\n        self.train()\n        self.all_embeddings = F.normalize(self.all_embeddings, dim=-1, p=2).detach().cpu().numpy()\n\n\n    def get_embedding(self):\n        return self.all_embeddings,self.true_label\n            \n    def train(self):\n        h_loss = 1e10\n        cnt_wait = 0\n        # Start Model Training\n        print(\"----Graph-Level Training Start! ----\\n\")\n        for epoch in range(self._args.Gepochs):\n            losses = []\n            embs = []\n            for batch in self._loader:\n                batch = batch.to(self._device)\n                emb,loss = self._model(batch)\n                self._optimizer.zero_grad() \n                loss.backward()\n                self._optimizer.step()\n                self._model.update_moving_average()\n                losses.append(loss.item())\n                embs = embs + emb\n            st = '[{}][Epoch {}/{}] Loss: {:.4f}'.format(currentTime(), \n                                                     epoch,self._args.Gepochs, np.mean(np.array(losses)))\n            print(st)\n\n            #Early Stopping Criterion\n            if np.mean(np.array(losses)) < h_loss:\n                h_loss = np.mean(np.array(losses)) \n            elif np.mean(np.array(losses)) > h_loss:\n                cnt_wait = cnt_wait + 1\n            if cnt_wait > 5:\n                break\n        self.all_embeddings = torch.tensor(embs)\n        print(\"\\n----Graph-Level Training Done! ----\")\n        \nclass GraphLevel(nn.Module):\n    def __init__(self, layer_config, args):\n\n        self.args = args\n        super().__init__()\n        self._device = f'cuda:{args.device}' if torch.cuda.is_available() else \"cpu\"\n        #encoder\n        self.student_encoder = Encoder(layer_config)\n        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n        self.student_encoder.to(self._device)\n        self.teacher_encoder.to(self._device)\n        set_requires_grad(self.teacher_encoder, False)\n        self.teacher_ema_updater = EMA(args.mad, args.Gepochs)\n\n        rep_dim = layer_config[-1]\n        #projection head\n        self.student_projector = nn.Sequential(nn.Linear(rep_dim, args.G_pred_hid), nn.BatchNorm1d(args.G_pred_hid),\n                                               nn.PReLU(), nn.Linear(args.G_pred_hid, rep_dim))\n        self.student_projector.to(self._device)\n        self.student_projector.apply(init_weights)\n        self.teacher_projector = copy.deepcopy(self.student_projector)\n        set_requires_grad(self.teacher_projector, False)\n        #pooling\n        self.pool = GlobalAttention(gate_nn=nn.Sequential(\n                nn.Linear(rep_dim, rep_dim), nn.BatchNorm1d(rep_dim), nn.ReLU(), nn.Linear(rep_dim, 1))) \n  \n    def reset_moving_average(self):\n        del self.teacher_encoder\n        self.teacher_encoder = None\n\n    def update_moving_average(self):\n        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n        update_moving_average(self.teacher_ema_updater, self.teacher_projector, self.student_projector)\n\n    def forward(self,batch):\n\n        student = self.student_encoder(batch.x1.to(self._device),batch.edge_index1.to(self._device))\n\n        h1 = self.pool(student,batch.batch)\n        h1 = self.student_projector(h1)\n\n        with torch.no_grad(): #stop gradient\n            teacher = self.teacher_encoder(batch.x2.to(self._device),batch.edge_index2.to(self._device))\n        h2 = self.pool(teacher,batch.batch)\n        with torch.no_grad(): #stop gradient\n            h2 = self.teacher_projector(h2)\n\n        emb = self.student_encoder(batch.x.to(self._device),batch.edge_index.to(self._device))\n        emb = self.pool(emb,batch.batch)\n        emb = emb.detach().cpu().numpy().tolist()\n        loss = get_loss(h1,h2)\n        res_emb = emb\n\n        return res_emb,loss\n\ndef make_transition(trans, *items):\n    transition = {}\n    for key, item in zip(trans, items):\n        if isinstance(item, list):\n            item = torch.stack(item)\n            transition[key] = item\n        elif isinstance(item, np.ndarray):\n            item = torch.from_numpy(item)\n            transition[key] = item\n        elif isinstance(item, torch.Tensor):\n            transition[key] = item\n        else:\n            transition[key] = torch.Tensor([item])\n\n    return transition\n\ndef make_batch(state, action, old_log_prob, advantage, old_value, learn_size, batch_size, use_cuda):\n    batch = []\n    total_indices = torch.randperm(learn_size)\n    for i in range(learn_size // batch_size):\n        indices = total_indices[batch_size * i: batch_size * (i + 1)]\n        mini_state = torch.Tensor([])\n        mini_action = torch.Tensor([])\n        mini_old_log_prob = torch.Tensor([])\n        mini_advantage = torch.Tensor([])\n        mini_old_value = torch.Tensor([])\n        if use_cuda:\n            mini_state = mini_state.cuda()\n            mini_action = mini_action.cuda()\n            mini_old_log_prob = mini_old_log_prob.cuda()\n            mini_advantage = mini_advantage.cuda()\n            mini_old_value = mini_old_value.cuda()\n        for ind in indices:\n            mini_state = torch.cat((mini_state, state[ind].unsqueeze(0)), dim=0)\n            mini_action = torch.cat((mini_action, action[ind].unsqueeze(0)), dim=0)\n            mini_old_log_prob = torch.cat((mini_old_log_prob, old_log_prob[ind].unsqueeze(0)), dim=0)\n            mini_advantage = torch.cat((mini_advantage, advantage[ind].unsqueeze(0)), dim=0)\n            mini_old_value = torch.cat((mini_old_value, old_value[ind].unsqueeze(0)), dim=0)\n        batch.append([mini_state, mini_action, mini_old_log_prob, mini_advantage, mini_old_value])\n    return batch\n\ndef calculate_nature_cnn_out_dim(height, weight):\n    size_h = np.floor((height - 8) / 4) + 1\n    size_h = np.floor((size_h - 4) / 2) + 1\n    size_h = np.floor((size_h - 3) / 1) + 1\n    size_w = np.floor((weight - 8) / 4) + 1\n    size_w = np.floor((size_w - 4) / 2) + 1\n    size_w = np.floor((size_w - 3) / 1) + 1\n    return size_h, size_w\n\nclass DQN_Config:\n    def __init__(self, input_type, input_size=None):\n        self.max_buffer = 100000\n        self.update_freq = 200\n        self.use_cuda = True\n        self.trans = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n        self.lr = 0.001\n        self.tau = 0.005\n        self.gamma = 0.99\n        self.batch_size = 128\n        self.max_grad_norm = 1\n        self.epsilon_init = 1\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.05\n        self.q_layer = [256, 256]\n        if input_type == \"vector\":\n            self.encoder = \"mlp\"\n            self.encoder_layer = [512, 256]\n            self.feature_dim = 256\n        elif input_type == \"image\":\n            self.encoder = \"cnn\"\n            self.encoder_layer = [[input_size[0], 32, 8, 4],\n                                  [32, 64, 4, 2],\n                                  [64, 64, 3, 1]]\n            size_h, size_w = calculate_nature_cnn_out_dim(input_size[1], input_size[2])\n            self.feature_dim = [int(64 * size_h * size_w), 256]\n\nclass DQN:\n    def __init__(self, state_dim, action_dim, input_type=\"vector\", args=None):\n        if args is None:\n            self.args = DQN_Config(input_type, state_dim)\n        self.action_dim = action_dim\n        self.buffer = BaseBuffer(self.args.trans, self.args.max_buffer)\n        self.policy_net = QNet(state_dim[0], action_dim, self.args.q_layer, self.args.encoder, self.args.encoder_layer,\n                               self.args.feature_dim)\n        self.target_net = QNet(state_dim[0], action_dim, self.args.q_layer, self.args.encoder, self.args.encoder_layer,\n                               self.args.feature_dim)\n        if self.args.use_cuda:\n            self.policy_net = self.policy_net.cuda()\n            self.target_net = self.target_net.cuda()\n        self.update_network()\n        self.policy_net.eval()\n        self.target_net.eval()\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.args.lr)\n\n        self.epsilon = self.args.epsilon_init\n\n    def select_action(self, state, epsilon=None):\n        if epsilon is None:\n            epsilon = self.epsilon\n        if random.random() > epsilon:\n            state = torch.Tensor(state)\n            if self.args.use_cuda:\n                state = state.cuda()\n            q_value = self.policy_net(state)\n            return torch.argmax(q_value).cpu().unsqueeze(0).detach()\n        else:\n            return torch.Tensor([random.choice(np.arange(self.action_dim))]).type(torch.int64).detach()\n\n    def add_buffer(self, transition):\n        self.buffer.add(transition)\n\n    def epsilon_decay(self):\n        self.epsilon = max(self.epsilon * self.args.epsilon_decay, self.args.epsilon_min)\n\n    def update_network(self):\n        self.target_net.load_state_dict(self.policy_net.state_dict())\n\n    def save_model(self, model_path):\n        torch.save(self.policy_net.state_dict(), model_path)\n\n    def learn(self, step):\n        self.policy_net.train()\n        data = self.buffer.sample(self.args.batch_size)\n        states = torch.stack(data[\"state\"])  # [batch_size, state_dim]\n        actions = torch.stack(data[\"action\"])  # [batch_size, 1]\n        rewards = torch.stack(data[\"reward\"])  # [batch_size, 1]\n        next_states = torch.stack(data[\"next_state\"])  # [batch_size, state_dim]\n        dones = torch.stack(data[\"done\"])  # [batch_size, 1]\n        # print(\"shape check\", states.shape, actions.shape, rewards.shape, next_states.shape, dones.shape)\n        if self.args.use_cuda:\n            states = states.cuda()\n            actions = actions.cuda()\n            rewards = rewards.cuda()\n            next_states = next_states.cuda()\n            dones = dones.cuda()\n        actions = actions.type(torch.int64)\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n\n        # get q-values for all actions in current states\n        predicted_q = self.policy_net(states)  # [batch_size, action_dim]\n        # select q-values for chosen actions\n        predicted_q_actions = torch.gather(predicted_q, -1, actions)  # [batch_size, 1]\n        # compute q-values for all actions in next states\n        predicted_next_q = self.target_net(next_states)  # [batch_size, action_dim]\n        # compute V*(next_states) using predicted next q-values\n        next_state_values, indexes = torch.max(predicted_next_q, dim=-1)  # [batch_size]\n        next_state_values = next_state_values.unsqueeze(-1)  # [batch_size, 1]\n        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n        target_q_actions = rewards + self.args.gamma * next_state_values * (1 - dones)  # [batch_size, 1]\n        loss = nn.SmoothL1Loss()(predicted_q_actions, target_q_actions.detach())\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.policy_net.eval()\n        self.epsilon_decay()\n\n        if step % self.args.update_freq == 0:\n            print(\"update target network\")\n            self.update_network()\n        return loss.item()\n\nclass PPO_Config:\n    def __init__(self, input_type, input_size=None):\n        self.max_buffer = 2048\n        self.trainable_std = False\n        self.use_cuda = True\n        self.trans = [\"state\", \"action\", \"reward\", \"done\", \"log_prob\"]\n        self.lr = 0.0003\n        self.gamma = 0.99\n        self.lambda_ = 0.95\n\n        self.train_epoch = 80\n        self.clip_ratio = 0.2\n        self.critic_coef = 0.5\n        self.entropy_coef = 0.01\n        self.max_grad_norm = 0.5\n\n        self.action_std_init = 0.6\n        self.action_std_decay_rate = 0.05\n        self.action_std_min = 0.1\n        self.action_std_update_freq = 100\n\n        self.actor_layer = [32, 32]\n        self.critic_layer = [32, 32]\n        if input_type == \"vector\":\n            self.encoder = \"mlp\"\n            self.encoder_layer = [64, 64]\n            self.feature_dim = 32\n        elif input_type == \"image\":\n            self.encoder = \"cnn\"\n            self.encoder_layer = [[input_size[0], 32, 8, 4],\n                                  [32, 64, 4, 2],\n                                  [64, 64, 3, 1]]\n            size_h, size_w = calculate_nature_cnn_out_dim(input_size[1], input_size[2])\n            self.feature_dim = [int(64 * size_h * size_w), 256]\n\nclass PPO:\n    def __init__(self, state_dim, action_dim, continuous=True, input_type=\"vector\", args=None):\n        if args is None:\n            self.args = PPO_Config(input_type, state_dim)\n        self.buffer = BaseBuffer(self.args.trans, self.args.max_buffer)\n        self.model = ActorCritic(state_dim[0], action_dim, self.args.actor_layer, self.args.critic_layer,\n                                 self.args.encoder, self.args.encoder_layer, self.args.feature_dim, continuous,\n                                 self.args.action_std_init)\n        if self.args.use_cuda:\n            self.model = self.model.cuda()\n        self.model.eval()\n        self.optimizer = optim.Adam(self.model.parameters(), lr=self.args.lr)\n\n    def select_action(self, state):\n        state = torch.Tensor(state).float()\n        if self.args.use_cuda:\n            state = state.cuda()\n        action, action_log_prob = self.model.act(state)\n        return action.detach().cpu(), action_log_prob.detach().cpu()\n\n    def add_buffer(self, transition):\n        self.buffer.add(transition)\n\n    def save_model(self, model_path):\n        torch.save(self.model.state_dict(), model_path)\n\n    def learn(self):\n        self.model.train()\n        data, size = self.buffer.get_data_buffer()\n        states = data[\"state\"]\n        actions = data[\"action\"]\n        rewards = data[\"reward\"]\n        dones = data[\"done\"]\n        old_log_probs = data[\"log_prob\"]\n\n        # Monte Carlo estimate of returns\n        returns = []\n        discounted_reward = 0\n        for reward, done in zip(reversed(rewards), reversed(dones)):\n            if done:\n                discounted_reward = 0\n            discounted_reward = reward + (self.args.gamma * discounted_reward)\n            returns.insert(0, discounted_reward)\n        # Normalizing the rewards\n        returns = torch.tensor(returns, dtype=torch.float32)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n\n        # trans into tensor and send to cpu/gpu\n        states = torch.stack(states).float()\n        actions = torch.stack(actions)\n        old_log_probs = torch.stack(old_log_probs)\n        if self.args.use_cuda:\n            states = states.cuda()  # [batch_size, state_dim]\n            actions = actions.cuda()  # [batch_size]\n            old_log_probs = old_log_probs.cuda()  # [batch_size]\n            returns = returns.cuda()  # [batch_size]\n\n        loss_list = []\n        for e in range(self.args.train_epoch):\n            # Evaluating old actions and values\n            log_probs, values, dist_entropy = self.model.evaluate_AC(states, actions)\n            # print(\"shape check\", log_probs.shape, values.shape, dist_entropy.shape)\n            values = values.squeeze(-1)\n\n            # Finding the ratio (pi_theta / pi_theta__old)\n            ratios = torch.exp(log_probs - old_log_probs.detach())\n\n            # Finding Surrogate Loss\n            advantages = returns - values.detach()\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.args.clip_ratio, 1 + self.args.clip_ratio) * advantages\n\n            # final loss of clipped objective PPO\n            actor_loss = -torch.min(surr1, surr2)\n            critic_loss = nn.MSELoss()(values, returns)\n            entropy_bonus = -dist_entropy\n            loss = actor_loss + 0.5 * critic_loss + 0.01 * entropy_bonus\n            loss = loss.mean()\n\n            # take gradient step\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n            loss_list.append(loss.item())\n        self.model.eval()\n        self.buffer.clear()\n        return np.mean(loss_list)\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, actor_layer, critic_layer, encoder=None, encoder_layer=None,\n                 feature_dim=None, continuous=False, std_init=0.6):\n        super(ActorCritic, self).__init__()\n        self.continuous = continuous\n        if continuous:\n            self.action_var = torch.full((action_dim,), std_init * std_init)\n\n        self.encoder = encoder\n        if encoder is None:\n            input_dim = state_dim\n        elif encoder == \"mlp\":\n            self.encoder = MLPEncoder(state_dim, encoder_layer, feature_dim)\n            input_dim = self.encoder.get_dim()\n        elif encoder == \"cnn\":\n            self.encoder = CNNEncoder(encoder_layer, feature_dim)\n            input_dim = self.encoder.get_dim()\n        else:\n            raise NotImplementedError\n\n        if self.continuous:\n            layers = [nn.Linear(input_dim, actor_layer[0]),\n                      nn.ReLU(inplace=True)]\n            for i in range(len(actor_layer) - 1):\n                layers.append(nn.Linear(actor_layer[i], actor_layer[i + 1]))\n                layers.append(nn.ReLU(inplace=True))\n            layers.append(nn.Linear(actor_layer[-1], action_dim))\n            self.actor = nn.Sequential(*layers)\n\n        else:\n            layers = [nn.Linear(input_dim, actor_layer[0]),\n                      nn.ReLU(inplace=True)]\n            for i in range(len(actor_layer) - 1):\n                layers.append(nn.Linear(actor_layer[i], actor_layer[i + 1]))\n                layers.append(nn.ReLU(inplace=True))\n            layers.append(nn.Linear(actor_layer[-1], action_dim))\n            layers.append(nn.Softmax(dim=-1))\n            self.actor = nn.Sequential(*layers)\n\n        layers = [nn.Linear(input_dim, critic_layer[0]),\n                  nn.ReLU(inplace=True)]\n        for i in range(len(critic_layer) - 1):\n            layers.append(nn.Linear(critic_layer[i], critic_layer[i + 1]))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Linear(critic_layer[-1], 1))\n        self.critic = nn.Sequential(*layers)\n        self.__network_init()\n\n    def forward(self):\n        raise NotImplementedError\n\n    def __network_init(self):\n        for layer in self.modules():\n            if isinstance(layer, nn.Linear):\n                nn.init.orthogonal_(layer.weight)\n                if layer.bias is not None:\n                    layer.bias.data.zero_()\n\n    def act(self, state):\n        if self.encoder is not None:\n            state = self.encoder(state)\n        if self.continuous:\n            mu = self.actor(state).cpu()\n            cov_mat = torch.diag(self.action_var)\n            dist = MultivariateNormal(mu, cov_mat)\n        else:\n            action_prob = self.actor(state)\n            dist = Categorical(action_prob)\n        action = dist.sample()\n        action_log_prob = dist.log_prob(action)\n        return action.detach(), action_log_prob.detach()\n\n    def evaluate_AC(self, state, action):\n        if self.encoder is not None:\n            state = self.encoder(state)\n        if self.continuous:\n            mu = self.actor(state).cpu()\n            action_var = self.action_var.expand_as(mu)\n            cov_mat = torch.diag_embed(action_var)\n            dist = MultivariateNormal(mu, cov_mat)\n        else:\n            action_prob = self.actor(state)\n            dist = Categorical(action_prob)\n        action_log_prob = dist.log_prob(action.cpu())\n        dist_entropy = dist.entropy()\n        value = self.critic(state)\n        return action_log_prob.cuda(), value, dist_entropy.cuda()\n\nclass MLPEncoder(nn.Module):\n    def __init__(self, state_dim, layer_dim: list, feature_dim: int):\n        super(MLPEncoder, self).__init__()\n        layers = [nn.Linear(state_dim, layer_dim[0]),\n                  # nn.BatchNorm1d(layer_dim[0]),\n                  nn.ReLU(inplace=True)]\n        for i in range(len(layer_dim) - 1):\n            layers.append(nn.Linear(layer_dim[i], layer_dim[i + 1]))\n            # layers.append(nn.BatchNorm1d(layer_dim[i + 1]))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Linear(layer_dim[-1], feature_dim, bias=False))\n        self.encoder = nn.Sequential(*layers)\n        self.out_dim = feature_dim\n\n    def forward(self, x):\n        return self.encoder(x)\n\n    def get_dim(self):\n        return self.out_dim\n\nclass CNNEncoder(nn.Module):\n    def __init__(self, layer_dim: list, feature_dim: list):\n        super(CNNEncoder, self).__init__()\n        layers = []\n        for layer in layer_dim:\n            layers.append(nn.Conv2d(layer[0], layer[1], layer[2], layer[3]))\n            # layers.append(nn.BatchNorm2d(layer[1]))\n            layers.append(nn.ReLU(inplace=True))\n        self.encoder = nn.Sequential(*layers)\n        self.projector = nn.Linear(feature_dim[0], feature_dim[1], bias=False)\n        self.out_dim = feature_dim[1]\n\n    def forward(self, x):\n        f = self.encoder(x)\n        f = f.reshape(f.shape[0], -1)\n        f = self.projector(f)\n        return f\n\n    def get_dim(self):\n        return self.out_dim\n\nclass QNet(nn.Module):\n    def __init__(self, state_dim, action_dim, q_layer, encoder=None, encoder_layer=None, feature_dim=None):\n        super(QNet, self).__init__()\n        self.encoder = encoder\n        if encoder is None:\n            input_dim = state_dim\n        elif encoder == \"mlp\":\n            self.encoder = MLPEncoder(state_dim, encoder_layer, feature_dim)\n            input_dim = self.encoder.get_dim()\n        elif encoder == \"cnn\":\n            self.encoder = CNNEncoder(encoder_layer, feature_dim)\n            input_dim = self.encoder.get_dim()\n        else:\n            raise NotImplementedError\n\n        layers = [nn.Linear(input_dim, q_layer[0]),\n                  nn.ReLU(inplace=True)]\n        for i in range(len(q_layer)-1):\n            layers.append(nn.Linear(q_layer[i], q_layer[i + 1]))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Linear(q_layer[-1], action_dim))\n        self.q_net = nn.Sequential(*layers)\n        self.__network_init()\n\n    def __network_init(self):\n        for layer in self.modules():\n            if isinstance(layer, nn.Linear):\n                nn.init.orthogonal_(layer.weight)\n                if layer.bias is not None:\n                    layer.bias.data.zero_()\n\n    def forward(self, state):\n        if self.encoder is not None:\n            state = self.encoder(state)\n        q = self.q_net(state)\n        return q\n\nclass BaseBuffer:\n    def __init__(self, trans, max_len):\n        self.trans = trans\n        self.max_len = max_len\n        self.data = {}\n        for key in trans:\n            self.data[key] = deque(maxlen=self.max_len)\n        self.total_idx = 0\n\n    def get_len(self):\n        return self.total_idx\n\n    def clear(self):\n        \"\"\"\n        clear the buffer\n        :return:\n        \"\"\"\n        self.data = {}\n        for key in self.trans:\n            self.data[key] = []\n        self.total_idx = 0\n\n    def add(self, transition):\n        \"\"\"\n        add a transition in buffer\n        :return:\n        \"\"\"\n        for key in transition:\n            self.data[key].append(transition[key])\n        self.total_idx += 1\n\n    def get_data_buffer(self):\n        data_size = len(self.data[\"state\"])\n        data = {}\n        for key in self.data:\n            data[key] = list(self.data[key])\n        return data, data_size\n\n    def sample(self, size):\n        data_size = len(self.data[\"state\"])\n        size = min(data_size, size)\n        indices = torch.randperm(data_size)[:size]\n        data = {}\n        for key in self.trans:\n            data[key] = []\n            for idx, ind in enumerate(indices):\n                data[key].append(self.data[key][ind])\n        return data\n\n\ndef unique(lists):  \n    #delete duplicate attribute values\n    lists = list(map(lambda x: x.lower(), lists ))\n    if lists[0]=='':\n        res = []\n    else: \n        res = [lists[0]]\n    for i in range(len(lists)):\n        if i==0 or (lists[i] in lists[0:i]) or lists[i]=='':\n            continue\n        else:\n            res.append(lists[i])\n    return res\n\ndef construct_graph_from_df(df, G=None):\n    # construct graph according to df\n    if G is None:\n        G = nx.Graph()\n    for _, row in df.iterrows():\n        tid = 't_' + str(row['tweet_id'])\n        G.add_node(tid)\n\n        user_ids = row['user_mentions']\n        user_ids.append(row['user_id'])\n        user_ids = ['u_' + str(each) for each in user_ids]\n        G.add_nodes_from(user_ids)\n\n        words = row['filtered_words']\n        words = [('w_' + each).lower() for each in words]\n        G.add_nodes_from(words)\n\n        hashtags = row['hashtags']\n        hashtags = [('h_' + each).lower() for each in hashtags]\n        G.add_nodes_from(hashtags)\n\n        edges = []\n        #Connect the message node with each related user node, word node, etc\n        edges += [(tid, each) for each in user_ids] \n        edges += [(tid, each) for each in words]\n        edges += [(tid, each) for each in hashtags]\n        G.add_edges_from(edges)\n    return G\n\ndef construct_graph(data,feature,index):\n#Build graph for a single tweet \n    G = nx.Graph()\n    X = []\n\n    tweet = data[\"text\"].values\n    X.append(feature[index].tolist())\n    index = index+1\n    tweet_id = data[\"tweet_id\"].values\n    G.add_node(tweet_id[0])\n\n    user_loc = data[\"user_loc\"].values\n\n    f_w = data[\"filtered_words\"].tolist()\n    edges = []\n\n    h_t = data[\"hashtags\"].tolist()\n    h_t = h_t[0]\n    n = [user_loc[0]] + f_w[0] + h_t \n    n = unique(n)\n    \n    if len(n)!=0:\n        for each in n:\n            X.append(feature[index].tolist())\n            index = index+1\n        G.add_nodes_from(n)\n        edges +=[(tweet_id[0], each) for each in n]\n    G.add_edges_from(edges)\n    return G,X\n\ndef normalize_adj(adj):\n    # Symmetrically normalize adjacency matrix\n    adj = sp.coo_matrix(adj) \n    rowsum = np.array(adj.sum(1)) \n    d_inv_sqrt = np.power(rowsum, -0.5).flatten() \n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0. \n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt) \n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo() \n\ndef aug_edge(adj): #  edge perturbation\n    adj = np.array(adj)\n    aug_adj1 = np.array([[i for i in j] for j in adj])\n    aug_adj2 = np.array([[i for i in j] for j in adj])\n    p = np.random.randint(0,len(adj)-1)\n    aug_adj1[p][0] = 0\n    aug_adj1[0][p] = 0\n    t = np.random.randint(1,len(adj)-1)\n    aug_adj1[t][p] = 1\n    aug_adj1[p][t] = 1\n    \n    p = np.random.randint(0,len(adj)-1)\n    aug_adj2[p][0] = 0\n    aug_adj2[0][p] = 0\n    t = np.random.randint(1,len(adj)-1)\n    aug_adj2[t][p] = 1\n    aug_adj2[p][t] = 1\n        \n    return aug_adj1,aug_adj2\n\ndef get_edge_index(adj):  #Get edge set according to adjacency matrix\n\n    edge_index1 = []\n    edge_index2 = []\n    for i in range(len(adj)):\n        for j in range(len(adj)):\n            if adj[i][j]==1 and i<j:\n                edge_index1.append(i)\n                edge_index2.append(j)\n\n    edge_index = [edge_index1] + [edge_index2]\n    \n    return edge_index\n\ndef get_data(message_num,start,tweet_sum,save_path):\n\n    os.makedirs(save_path, exist_ok=True)\n    \n    \n    dataset = Event2012().load_data()\n    df = dataset.sort_values(by='created_at').reset_index()\n    ini_df = df[start:tweet_sum]\n\n    G = construct_graph_from_df(ini_df)\n\n    d_features = documents_to_features(df)\n    print(\"Document features generated.\")\n    t_features = df_to_t_features(df)\n    print(\"Time features generated.\")\n    combined_features = np.concatenate((d_features, t_features), axis=1)\n    print(\"Concatenated document features and time features.\")\n    np.save(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy', combined_features)\n    print(\"Initial features saved.\")\n\n    combined_features = np.load(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered.npy')\n    A = nx.adjacency_matrix(G).todense().tolist()\n    \n    X = []\n    nodes = list(G.nodes)\n    \n    tweet=[]\n    j = 0\n\n    for i in range(len(nodes)):\n        t=nodes[i][0:2]\n        e=nodes[i][2:]\n        if t==\"t_\":\n            tweet.append(i)\n            index=list(ini_df[\"tweet_id\"]).index(int(e))\n            X.append(list(combined_features[index]))\n            j=j+1\n    X = torch.tensor(X)\n    adj = np.array([[0]*len(tweet)]*len(tweet))\n\n    for i in range(len(tweet)):\n        for j in range(len(A)):\n            if A[tweet[i]][j]==1:\n                for s in range(len(tweet)):\n                    if A[j][tweet[s]]==1 and s!=i:\n                        adj[i][s] = 1\n    edge_index = get_edge_index(adj)\n\n    edge_index1 = copy.deepcopy(edge_index)\n    edge_index2 = copy.deepcopy(edge_index)\n    true_y = torch.tensor(list(ini_df['event_id']))\n\n    drop_percent = 0.2\n    i = 0\n    while 1:\n        if i >= len(G.edges)*drop_percent:\n            break\n        m1 = random.randint(0, len(edge_index1[0])-1)\n        m2 = random.randint(0, len(edge_index2[0])-1)\n        if m1==m2:\n            continue\n        else:\n            del edge_index1[0][m1]\n            del edge_index1[1][m1]\n            del edge_index2[0][m2]\n            del edge_index2[1][m2]\n    \n        i = i + 1\n    edge_index = torch.tensor(edge_index)\n    edge_index1 = torch.tensor(edge_index1)\n    edge_index2 = torch.tensor(edge_index2)\n\n    dict_graph = {}\n\n    dict_graph['x'] = X\n    dict_graph['x1'] = X\n    dict_graph['x2'] = X\n    dict_graph['edge_index'] = edge_index\n    dict_graph['edge_index1'] = edge_index1\n    dict_graph['edge_index2'] = edge_index2\n    dict_graph['y'] = true_y\n    return dict_graph\n\ndef getData(args,M_num):  #construct an entire graph within a block\n#     print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))\n    M =[20254,28976,30467,32302,34312,36146,37422,42700,44260,45623,46719,\n        47951,51188,53160,56116,58665,59575,62251,64138,65537,66430,68840]  \n\n    if M_num == 0:\n        num = 0\n        size = 500\n    elif M_num == 1:\n        num = M[M_num-1]\n        size = 500\n    elif M[M_num]-M[M_num-1]>2000:\n        num = M[M_num-1]\n        size = 1000\n    else:\n        num = M[M_num-1]\n        size = M[M_num]-M[M_num-1]\n    data = []\n    i = M_num\n    j = 0\n\n    while 1:\n        if (num+size)>=M[i]:\n            tmp = get_data(i, num ,M[i], args.file_path)\n            data.append(tmp)\n            break\n        else:\n            tmp = get_data(i, num, num+size, args.file_path)\n            data.append(tmp)\n            j = j + 1\n            print(\"***************Block \"+str(j)+\" is done.****************\")\n            num = num+size\n    \n\n    save_data(data, args.file_path, M_num)\n    return data\n\ndef save_data(data, save_path, M_num):\n    os.makedirs(save_path, exist_ok=True)\n    file_path = os.path.join(save_path, f'data_{M_num}.pkl')\n    with open(file_path, 'wb') as f:\n        pickle.dump(data, f)\n    print(f\"Data saved at {file_path}\")\n\n\ndef get_Graph_Dataset(args,message_number):\n    print(\"\\nBuilding graph-level social network...\")\n    start_time = time.time() \n    #load data for graph-level contrastive learning\n    dataset = []\n    label = []\n    file_name = args.file_path + 'GCL-data/message_block_'+str(message_number)+'.npy'\n    data = np.load(file_name,allow_pickle=True)\n\n    for dict_data in data:\n        data = Data(x=dict_data['X'],x1=dict_data['x1'],x2=dict_data['x2'],\n                    edge_index=dict_data['edge_index'],edge_index1=dict_data['edge_index1'],\n                    edge_index2=dict_data['edge_index2'])\n        dataset.append(data)\n        label.append(dict_data['label'])\n    if message_number == 0 :\n        dataset = loader.DataLoader(dataset,batch_size=4096)\n    else:\n        dataset = loader.DataLoader(dataset,batch_size=len(dataset))\n    end_time = time.time()\n    run_time = end_time - start_time\n    print(\"Done! It takes \"+str(int(run_time))+\" seconds.\\n\")\n    return dataset,label\n\ndef get_Node_Dataset(args,message_number):\n    #load data for node-level contrastive learning\n    print(\"\\nBuilding node-level social network...\")\n    start_time = time.time() \n    #datas = getData(message_number)\n    file_path = os.path.join(args.file_path, f'data_{message_number}.pkl')\n\n    if os.path.exists(file_path):\n        with open(file_path, 'rb') as f:\n                datas = pickle.load(f)\n        print(\"Data loaded successfully.\")\n        # 现在你可以使用 data 进行进一步操作\n    else:\n        print(f\"No data file found at {file_path}\")\n        datas = getData(message_number)\n    \n    dataset = []\n    labels = []\n    \n    for data in datas:\n        dict_data = data\n\n        dict_data['x'] = torch.tensor(np.array(dict_data['x']))\n        dict_data['x1'] = torch.tensor(np.array(dict_data['x1']))\n        dict_data['x2'] = torch.tensor(np.array(dict_data['x2']))\n        dict_data['edge_index'] = torch.tensor(np.array(dict_data['edge_index']))\n        dict_data['edge_index1'] = torch.tensor(np.array(dict_data['edge_index1']))\n        dict_data['edge_index2'] = torch.tensor(np.array(dict_data['edge_index2']))\n        data = Data(x=dict_data['x'],x1=dict_data['x1'],x2=dict_data['x2'],\n                        edge_index=dict_data['edge_index'],edge_index1=dict_data['edge_index1'],\n                        edge_index2=dict_data['edge_index2'])\n\n        label = dict_data['y']\n        if len(labels)==0:\n            labels = label\n        else:\n            labels = torch.cat([labels,label])\n    \n        dataset.append(data)\n    end_time = time.time()\n    run_time = end_time - start_time\n    print(\"Done! It takes \"+str(int(run_time))+\" seconds.\\n\")\n    return dataset,np.array(labels).tolist()\n\n# Calculate the embeddings of all the documents in the dataframe, \n# the embedding of each document is an average of the pre-trained embeddings of all the words in it\ndef documents_to_features(df):\n    nlp = spacy.load(\"en_core_web_lg\")\n    features = df.filtered_words.apply(lambda x: nlp(' '.join(x)).vector).values\n    return np.stack(features, axis=0)\n\n# encode one times-tamp\n# t_str: a string of format '2012-10-11 07:19:34'\ndef extract_time_feature(t_str):\n    t = datetime.fromisoformat(str(t_str))\n    OLE_TIME_ZERO = datetime(1899, 12, 30)\n    delta = t - OLE_TIME_ZERO\n    return [(float(delta.days) / 100000.), (float(delta.seconds) / 86400)]  # 86,400 seconds in day\n\n# encode the times-tamps of all the messages in the dataframe\ndef df_to_t_features(df):\n    t_features = np.asarray([extract_time_feature(t_str) for t_str in df['created_at']])\n    return t_features\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/bert.py", "content": "import os\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertTokenizer, BertModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport torch\nimport logging\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n# Setup logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nclass BERT:\n    r\"\"\"The BERT model for social event detection that uses BERT embeddings to \n    detect events in social media data.\n\n    .. note::\n        This detector uses BERT embeddings to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    model_name : str, optional\n        Path to pretrained BERT model or name from HuggingFace.\n        If path doesn't exist, defaults to 'bert-base-uncased'.\n        Default: ``'../model/model_needed/bert-base-uncased'``.\n    max_length : int, optional\n        Maximum sequence length for BERT tokenizer.\n        Longer sequences will be truncated.\n        Default: ``128``.\n    df : pandas.DataFrame, optional\n        Preprocessed dataframe. If None, will be created during preprocessing.\n        Default: ``None``.\n    train_df : pandas.DataFrame, optional\n        Training data split. If None, will be created during model fitting.\n        Default: ``None``.\n    test_df : pandas.DataFrame, optional\n        Test data split. If None, will be created during model fitting.\n        Default: ``None``.\n    \"\"\"\n    def __init__(self,\n                 dataset,\n                 model_name='../model/model_needed/bert-base-uncased',\n                 max_length=128,\n                 df=None,\n                 train_df=None,\n                 test_df=None ):\n        self.dataset = dataset.load_data()\n        if os.path.exists(model_name):\n            self.model_name = model_name\n        else:\n            self.model_name = 'bert-base-uncased'\n        self.max_length = max_length\n        self.df = df\n        self.train_df = train_df\n        self.test_df = test_df\n        # self.device = torch.device(\"cpu\")\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n        self.model = BertModel.from_pretrained(self.model_name).to(self.device)\n\n    def preprocess(self):\n        \"\"\"\n        Data preprocessing: tokenization, stop words removal, etc.\n        \"\"\"\n        df = self.dataset\n        df['processed_text'] = df['filtered_words'].apply(\n            lambda x: ' '.join([str(word).lower() for word in x]) if isinstance(x, list) else '')\n        self.df = df\n        return df\n\n    def get_bert_embeddings(self, text):\n        \"\"\"\n        Get BERT embeddings for a given text.\n        \"\"\"\n        inputs = self.tokenizer(text, return_tensors='pt', max_length=self.max_length, truncation=True,\n                                padding='max_length')\n        inputs = {key: val.to(self.device) for key, val in inputs.items()}\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n        last_hidden_states = outputs.last_hidden_state\n        mean_embedding = torch.mean(last_hidden_states, dim=1).squeeze().cpu().numpy()\n        return mean_embedding\n        \n    def fit(self):\n        pass\n\n    def detection(self):\n        \"\"\"\n        Detect events by comparing BERT embeddings.\n        \"\"\"\n        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=42)\n        self.train_df = train_df\n        self.test_df = test_df\n\n        logging.info(\"Calculating BERT embeddings for the training set...\")\n        train_df['bert_embedding'] = train_df['processed_text'].apply(self.get_bert_embeddings)\n        logging.info(\"BERT embeddings calculated for the training set.\")\n\n        logging.info(\"Calculating BERT embeddings for the test set...\")\n        test_df['bert_embedding'] = test_df['processed_text'].apply(self.get_bert_embeddings)\n        logging.info(\"BERT embeddings calculated for the test set.\")\n\n        train_embeddings = np.stack(self.train_df['bert_embedding'].values)\n        test_embeddings = np.stack(self.test_df['bert_embedding'].values)\n\n        predictions = []\n        for test_emb in test_embeddings:\n            distances = np.linalg.norm(train_embeddings - test_emb, axis=1)\n            closest_idx = np.argmin(distances)\n            predictions.append(self.train_df.iloc[closest_idx]['event_id'])\n\n        ground_truths = self.test_df['event_id'].tolist()\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the BERT-based model.\n        \"\"\"\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        return ari, ami, nmi\n\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/uclsed.py", "content": "import argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport dgl\nimport networkx as nx\nimport pandas as pd\nimport numpy as np\nfrom scipy import sparse\nimport spacy\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score\nimport copy\nimport datetime\nimport torch.nn.functional as F\nfrom sklearn.cluster import KMeans\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\n\n\nclass UCLSED:\n\n    r\"\"\"The UCLSED model for social event detection that uses uncertainty-aware contrastive learning\n    for event detection.\n\n    Args:\n        * dataset: The dataset object containing social media data.\n            The dataset should provide methods:\n            - load_data(): Returns the raw data\n            - get_dataset_language(): Returns the language of the dataset\n        * file_path (str, optional): Path to save model files. (default: '../model/model_saved/uclsed/')\n        * epoch (int, optional): Number of training epochs. (default: 50)\n        * batch_size (int, optional): Batch size for training. (default: 128)\n        * neighbours_num (int, optional): Number of neighbors to sample. (default: 80)\n        * GNN_h_dim (int, optional): Hidden dimension of GNN. (default: 256)\n        * GNN_out_dim (int, optional): Output dimension of GNN. (default: 256)\n        * E_h_dim (int, optional): Hidden dimension of encoder. (default: 128)\n        * use_uncertainty (bool, optional): Whether to use uncertainty estimation. (default: True)\n        * use_cuda (bool, optional): Whether to use GPU acceleration. (default: True)\n        * gpuid (int, optional): GPU device ID to use. (default: 0)\n        * mode (int, optional): Training mode. (default: 0)\n        * mse (bool, optional): Whether to use MSE loss. (default: False)\n        * digamma (bool, optional): Whether to use digamma function. (default: True)\n        * log (bool, optional): Whether to use log transformation. (default: False)\n        * learning_rate (float, optional): Learning rate for optimizer. (default: 1e-4)\n        * weight_decay (float, optional): Weight decay for optimizer. (default: 1e-5)\n    \"\"\"\n    def __init__(\n        self,\n        dataset,\n        file_path='../model/model_saved/uclsed/',\n        epoch=50,\n        batch_size=128,\n        neighbours_num=80,\n        GNN_h_dim=256,\n        GNN_out_dim=256,\n        E_h_dim=128,\n        use_uncertainty=True,\n        use_cuda=True,\n        gpuid=0,\n        mode=0,\n        mse=False,\n        digamma=True,\n        log=False,\n        learning_rate=1e-4,\n        weight_decay=1e-5\n    ):\n        # 将参数赋值给 self\n        self.file_path = file_path\n        self.epoch = epoch\n        self.batch_size = batch_size\n        self.neighbours_num = neighbours_num\n        self.GNN_h_dim = GNN_h_dim\n        self.GNN_out_dim = GNN_out_dim\n        self.E_h_dim = E_h_dim\n        self.use_uncertainty = use_uncertainty\n        self.use_cuda = use_cuda\n        self.gpuid = gpuid\n        self.mode = mode\n        self.mse = mse\n        self.digamma = digamma\n        self.log = log\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n\n        self.save_path = None\n        self.test_indices = None\n        self.val_indices = None\n        self.train_indices = None\n        self.mask_path = None\n        self.labels = None\n        self.times = None\n        self.g_dict = None\n        self.views = None\n        self.features = None\n        self.dataset = dataset.load_data()\n        self.language = dataset.get_dataset_language()\n        \n\n    def preprocess(self):\n        preprocessor = Preprocessor(self)\n        preprocessor.construct_graph(self.dataset,self.language)\n\n    def fit(self):\n        args=self\n        parser = argparse.ArgumentParser()\n        print(\"Using CUDA:\", args.use_cuda)\n        if args.use_cuda:\n            torch.cuda.set_device(args.gpuid)\n\n        self.views = ['h', 'e', 'u']\n        self.g_dict, self.times, self.features, self.labels = get_dgl_data(self,self.views,self.language)\n        self.mask_path = f\"{args.file_path}{self.language}/\" + \"masks/\"\n        if not os.path.exists(self.mask_path):\n            os.mkdir(self.mask_path)\n        self.train_indices, self.val_indices, self.test_indices = ava_split_data(len(self.labels), self.labels,\n                                                                                 len(set(self.labels)))\n        torch.save(self.train_indices, self.mask_path + \"train_indices.pt\")\n        torch.save(self.val_indices, self.mask_path + \"val_indices.pt\")\n        torch.save(self.test_indices, self.mask_path + \"test_indices.pt\")\n\n        if args.mode == 0:\n            flag = ''\n            if args.use_uncertainty:\n                print(\"use_uncertainty\")\n                flag = \"evi\"\n            self.save_path = f\"{args.file_path}{self.language}/\" + flag + \"/\"\n            print(self.save_path)\n            os.makedirs(self.save_path, exist_ok=True)\n        else:\n            self.save_path = '../model/model_saved/uclsed/'\n\n        if args.use_uncertainty:\n            if args.digamma:\n                criterion = edl_digamma_loss\n            elif args.log:\n                criterion = edl_log_loss\n            elif args.mse:\n                criterion = edl_mse_loss\n            else:\n                parser.error(\"--uncertainty requires --mse, --log or --digamma.\")\n        else:\n            criterion = nn.CrossEntropyLoss()\n\n        self.model = UCLSED_model(self.features.shape[1], args.GNN_h_dim, args.GNN_out_dim, args.E_h_dim,\n                                  len(set(self.labels)), self.views)\n        self.model = train_model(self.model, self.g_dict, self.views, self.features, self.times, self.labels,\n                                 args.epoch, criterion, self.mask_path, self.save_path, args)\n\n    def detection(self):\n        args=self\n        self.model.eval()\n        self.val_indices = torch.load(self.mask_path + \"val_indices.pt\")\n        classes = len(set(self.labels))\n        self.labels = make_onehot(self.labels, classes)\n        device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n        if args.use_cuda:\n            self.model = self.model.cuda()\n            self.features = self.features.cuda()\n            self.times = self.times.cuda()\n            self.labels = self.labels.cuda()\n            self.train_indices = self.train_indices.cuda()\n            self.test_indices = self.test_indices.cuda()\n            self.val_indices = self.val_indices.cuda()\n            for v in self.views:\n                self.g_dict[v] = self.g_dict[v].to(device)\n                self.g_dict[v].ndata['features'] = self.features\n                self.g_dict[v].ndata['t'] = self.times\n\n        out, emb, nids = extract_results(self.g_dict, self.views, self.labels, self.model, args)\n        ori_labels = self.labels\n        # extract_labels = ori_labels[nids]\n        extract_labels = ori_labels[nids].cpu()\n        \n        comb_out = None\n        if args.use_uncertainty:\n            alpha = []\n            for out_v in out.values():\n                evi_v = relu_evidence(out_v)\n                alpha_v = evi_v + 1\n                alpha.append(alpha_v)\n            comb_out, comb_u = DS_Combin(alpha=alpha, classes=classes)\n\n        else:\n            for i, out_v in enumerate(out.values()):\n                if i == 0:\n                    comb_out = out_v\n                else:\n                    comb_out += out_v\n\n        _, val_pred = torch.max(comb_out[self.val_indices.cpu().numpy()], 1)\n        #val_labels = torch.IntTensor(extract_labels[self.val_indices.cpu().numpy()])\n        val_labels = torch.argmax(extract_labels[self.val_indices.cpu().numpy()], 1)\n        predictions = val_pred.cpu().numpy()\n        ground_truth = val_labels.cpu().numpy()\n\n        return ground_truth, predictions\n\n    def evaluate(self, ground_truth, predictions):\n        val_f1 = f1_score(ground_truth, predictions, average='macro')\n        val_acc = accuracy_score(ground_truth, predictions)\n\n        print(f\"Validation F1 Score: {val_f1}\")\n        print(f\"Validation Accuracy: {val_acc}\")\n\n        return val_f1, val_acc\n\n\nclass Preprocessor():\n    def __init__(self,  args):\n        super(Preprocessor, self).__init__()\n\n        self.args = args\n\n    def str2list(self, str_ele):\n        if str_ele == \"[]\":\n            value = []\n        else:\n            value = [e.replace('\\'', '').lstrip().replace(\":\", '') for e in str(str_ele)[1:-1].split(',') if\n                     len(e.replace('\\'', '').lstrip().replace(\":\", '')) > 0]\n        return value\n\n    def load_data(self, dataset):\n        ori_df = dataset\n\n        ori_df.drop_duplicates([\"tweet_id\"], keep='first', inplace=True)\n        event_id_num_dict = {}\n        select_index_list = []\n\n        for id in set(ori_df[\"event_id\"]):\n            num = len(ori_df.loc[ori_df[\"event_id\"] == id])\n            if int(num / 3) >= 25:\n                event_id_num_dict[id] = int(num / 3 + 50)\n                select_index_list += list(ori_df.loc[ori_df[\"event_id\"] == id].index)[0:int(num / 3 + 50)]\n        select_df = ori_df.loc[select_index_list]\n        select_df = select_df.reset_index(drop=True)\n        id_num = sorted(event_id_num_dict.items(), key=lambda x: x[1], reverse=True)\n\n        for (i, j) in id_num[0:100]:\n            print(j, end=\",\")\n        \n        event_ids = [item[0] for item in id_num]\n        sorted_id_dict = dict(zip(event_ids, range(len(set(ori_df[\"event_id\"])))))\n\n        sorted_df = select_df\n        sorted_df[\"event_id\"] = sorted_df[\"event_id\"].apply(lambda x: sorted_id_dict[x])\n\n        print(sorted_df.shape)\n        \n        # 修改这部分：使用列表来选择多个列\n        columns = [\n            'tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n            'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions'\n        ]\n        \n        # 检查列是否存在\n        existing_columns = [col for col in columns if col in sorted_df.columns]\n        if len(existing_columns) != len(columns):\n            missing_columns = set(columns) - set(existing_columns)\n            print(f\"Warning: Missing columns in DataFrame: {missing_columns}\")\n            print(f\"Available columns: {sorted_df.columns.tolist()}\")\n        \n        data_value = sorted_df[existing_columns].values\n        \n        event_df = pd.DataFrame(data=data_value, columns=existing_columns)\n        \n        # 确保所需的列存在后再处理\n        if 'hashtags' in event_df.columns:\n            event_df['hashtags'] = event_df['hashtags'].apply(lambda x: [\"h_\" + str(i) for i in x])\n        if 'entities' in event_df.columns:\n            event_df['entities'] = event_df['entities'].apply(lambda x: [\"e_\" + str(i) for i in x])\n        if 'user_mentions' in event_df.columns:\n            event_df['user_mentions'] = event_df['user_mentions'].apply(lambda x: [\"u_\" + str(i) for i in x])\n        \n        event_df = event_df.loc[event_df['event_id'] < 100]\n        event_df = event_df.reset_index(drop=True)\n\n        print(event_df.shape)\n        return event_df\n\n    def get_nlp(self, lang):\n        if lang == \"English\":\n            nlp =spacy.load('en_core_web_lg')\n        elif lang == \"French\":\n            nlp=spacy.load('fr_core_news_lg')\n        elif lang == \"Arabic\":\n            nlp = spacy.load('ar_core_news_lg')\n        return nlp\n\n    def construct_graph_base_eles(self, view_dict, df, path, lang):\n        os.makedirs(path, exist_ok=True)\n        nlp = self.get_nlp(lang)\n        df = df.drop_duplicates(subset=['tweet_id'])\n        df.reset_index()\n        df.drop_duplicates([\"tweet_id\"], keep='first', inplace=True)\n        print(\"generate text features---------\")\n        features = np.stack(df['filtered_words'].apply(lambda x: nlp(' '.join(x)).vector).values, axis=0)\n        print(features.shape)\n        np.save(path + \"features.npy\", features)\n        print(\"text features are saved in {}features.npy\".format(path))\n        \n        if 'timestamp' not in df.columns and 'created_at' in df.columns:\n            try:\n                # 如果 created_at 是字符串格式，先转换为 datetime\n                if df['created_at'].dtype == 'object':\n                    df['timestamp'] = pd.to_datetime(df['created_at'])\n                else:\n                    df['timestamp'] = df['created_at']\n                \n                # 转换为 UNIX 时间戳（秒）\n                df['timestamp'] = df['timestamp'].astype(np.int64) // 10**9\n                \n            except Exception as e:\n                print(f\"Error converting created_at to timestamp: {e}\")\n                print(\"Using default timestamps...\")\n                df['timestamp'] = np.arange(len(df))  # 使用序列号作为后备方案\n        elif 'timestamp' not in df.columns:\n            print(\"Warning: No timestamp or created_at column found, using sequential numbers\")\n            df['timestamp'] = np.arange(len(df))\n    \n        \n        np.save(path + \"time.npy\", df['timestamp'].values)\n        print(\"time features are saved in {}time.npy\".format(path))\n        df[\"event_id\"] = df[\"event_id\"].apply(lambda x: int(x))\n        np.save(path + \"label.npy\", df['event_id'].values)\n        print(\"labels are saved in {}label.npy\".format(path))\n\n        true_matrix = np.eye(df.shape[0])\n        for i in range(df.shape[0]):\n            label_i = df[\"event_id\"].values[i]\n            indices = df[df[\"event_id\"] == label_i].index\n            true_matrix[i, indices] = 1\n        # print(true_matrix)\n\n        print(\"construct graph---------------\")\n        G = nx.Graph()\n        for _, row in df.iterrows():\n            tid = str(row['tweet_id'])\n            G.add_node(tid)\n            G.nodes[tid]['tweet_id'] = True  # right-hand side value is irrelevant for the lookup\n            edges = []\n            for view in view_dict.values():\n                for ele in view:\n                    if len(row[ele]) > 0:\n                        ele_values = row[ele]\n                        G.add_nodes_from(ele_values)\n                        for each in ele_values:\n                            G.nodes[each][ele] = True\n                        edges += [(tid, each) for each in row[ele]]\n\n            G.add_edges_from(edges)\n\n        all_nodes = list(G.nodes)\n        matrix = nx.to_scipy_sparse_array(G)\n        tweet_nodes = list(nx.get_node_attributes(G, \"tweet_id\").keys())\n        # print(tweet_nodes)\n        print(len(tweet_nodes))\n        tweet_index = [all_nodes.index(t_node) for t_node in tweet_nodes]\n\n        for v, view in zip(view_dict.keys(), view_dict.values()):\n            s_tweet_tweet_matrix = sparse.csr_matrix(np.identity(len(tweet_nodes)))\n            for ele in view:\n                ele_nodes = list(nx.get_node_attributes(G, ele).keys())\n                ele_index = [all_nodes.index(e_node) for e_node in ele_nodes]\n                tweet_ele_matrix = matrix[tweet_index, :][:, ele_index]\n                s_ele_tweet_tweet_matrix = sparse.csr_matrix(tweet_ele_matrix @ tweet_ele_matrix.transpose())\n                s_tweet_tweet_matrix += s_ele_tweet_tweet_matrix\n            s_tweet_tweet_matrix = s_tweet_tweet_matrix.astype('bool')\n            sparse.save_npz(os.path.join(path, f\"s_tweet_tweet_matrix_{v}.npz\"), s_tweet_tweet_matrix)\n            print(f\"Sparse binary {v} commuting matrix is saved in {path}s_tweet_tweet_matrix_{v}.npz\")\n\n    def construct_graph(self, dataset, lang):\n        args=self\n        event_df = self.load_data(dataset)\n        view_dict = {\n            \"h\": [\"hashtags\", \"urls\"], \n            \"u\": [\"user_mentions\"],  # 修改这里：mention_user -> user_mentions\n            \"e\": [\"entities\"]\n        }\n        path = self.args.file_path + lang + '/'\n        self.construct_graph_base_eles(view_dict, event_df, path, lang)\n\n\ndef extract_results(g_dict, views, labels, model, args, train_indices=None):\n    with torch.no_grad():\n        model.eval()\n        out_list = []\n        emb_list = []\n        nids_list = []\n        all_indices = torch.LongTensor(range(0, labels.shape[0]))\n        if args.use_cuda:\n            all_indices = all_indices.cuda()\n        print(all_indices)\n        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n        dataloader = dgl.dataloading.NodeDataLoader(\n            g_dict[views[0]], all_indices, sampler,\n            batch_size=args.batch_size,\n            shuffle=False,\n            drop_last=False,\n        )\n\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n            extract_indices = blocks[-1].dstdata[dgl.NID].to(device)\n            blocks_dict = {}\n            blocks_dict[views[0]] = blocks\n            for v in views[1:]:\n                blocks_v = list(dgl.dataloading.NodeDataLoader(\n                    g_dict[v], extract_indices, sampler,\n                    batch_size=args.batch_size,\n                    shuffle=False,\n                    drop_last=False,\n                ))[0][2]\n                blocks_dict[v] = blocks_v\n            for v in views:\n                blocks_dict[v] = [b.to(device) for b in blocks_dict[v]]\n\n            # 将 blocks_dict 中的所有 blocks 移动到相同的设备\n            # blocks_dict = {v: [b.to(device) for b in blocks] for v, blocks in blocks_dict.items()}\n            # extract_indices = extract_indices.to(device)\n\n            out, emb = model(blocks_dict)\n            out_list.append(out)\n            emb_list.append(emb)\n            nids_list.append(extract_indices)\n\n    # assert batch_id==0\n    all_out = {}\n    all_emb = {}\n    for v in views:\n        all_out[v] = []\n        all_emb[v] = []\n        for out, emb in zip(out_list, emb_list):\n            all_out[v].append(out[v])\n            all_emb[v].append(emb[v])\n        if args.use_cuda:\n            all_out[v] = torch.cat(all_out[v]).cpu()\n            all_emb[v] = torch.cat(all_emb[v]).cpu()\n        else:\n            all_out[v] = torch.cat(all_out[v])\n            all_emb[v] = torch.cat(all_emb[v])\n\n    extract_nids = torch.cat(nids_list)\n    if args.use_cuda:\n        extract_nids = extract_nids.cpu()\n\n    return all_out, all_emb, extract_nids\n\n\ndef train_model(model, g_dict, views, features, times, labels, epoch, criterion, mask_path, save_path, args):\n    train_indices = torch.load(mask_path + \"train_indices.pt\")\n    val_indices = torch.load(mask_path + \"val_indices.pt\")\n    test_indices = torch.load(mask_path + \"test_indices.pt\")\n    classes = len(set(labels))\n    ori_labels = labels\n\n    labels = make_onehot(labels, classes)\n    device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n    if args.use_cuda:\n        model = model.cuda()\n        features = features.cuda()\n        times = times.cuda()\n        labels = labels.cuda()\n        train_indices = train_indices.cuda()\n        val_indices = val_indices.cuda()\n        test_indices = test_indices.cuda()\n\n    for v in views:\n        if args.use_cuda:\n            g_dict[v] = g_dict[v].to(device)\n        g_dict[v].ndata['features'] = features\n        g_dict[v].ndata['t'] = times\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\n    if args.mode == 0:\n        message = \"----------begin training---------\\n\"\n        with open(save_path + \"log.txt\", 'w') as f:\n            f.write(message)\n\n        best_vali = 0\n        test_acc_in_best_e = 0\n        best_epoch = 0\n        test_acc_list = []\n        label_u = torch.FloatTensor(np.ones(classes))\n\n        for e in range(epoch):\n            print(f\"Epoch {e + 1}/{epoch}\")\n\n            _, GNN_out_fea, extract_nids = extract_results(g_dict, views, labels, model, args)\n\n            for v in GNN_out_fea:\n                GNN_out_fea[v] = GNN_out_fea[v].to(device)\n                # print(f'GNN_out_fea[{v}].device: {GNN_out_fea[v].device}')  # 确认设备\n\n            extract_labels = ori_labels[extract_nids]\n            label_center = {}\n            for v in views:\n                label_center[v] = []\n            for l in range(classes):\n                l_indices = torch.LongTensor(np.where(extract_labels == l)[0].reshape(-1)).to(device)\n                # print(l_indices.device)\n                for v in views:\n                    # print(f'GNN_out_fea[{v}].device:{GNN_out_fea[v].device}')\n                    # print(f'l_indices.device:{l_indices.device}')\n\n                    l_feas = GNN_out_fea[v][l_indices]\n                    l_cen = torch.mean(l_feas, dim=0)\n                    label_center[v].append(l_cen)\n\n            for v in views:\n                label_center[v] = torch.stack(label_center[v], dim=0)\n                label_center[v] = F.normalize(label_center[v], 2, 1)\n\n                if args.use_cuda:\n                    label_center[v] = label_center[v].cuda()\n                    label_u = label_u.cuda()\n\n            losses = []\n            total_loss = 0\n            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n            dataloader = dgl.dataloading.NodeDataLoader(\n                g_dict[views[0]], train_indices, sampler,\n                batch_size=args.batch_size,\n                shuffle=False,\n                drop_last=False,\n                device=device\n            )\n\n            print(f\"Dataloader initialized with {len(dataloader)} batches\")\n            for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n                print(f\"Processing batch {batch_id + 1}/{len(dataloader)}\")\n                batch_indices = blocks[-1].dstdata[dgl.NID]\n                if args.use_cuda:\n                    batch_indices = batch_indices.cuda()\n                blocks_dict = {}\n                blocks_dict[views[0]] = blocks\n                for v in views[1:]:\n                    blocks_v = list(dgl.dataloading.NodeDataLoader(\n                        g_dict[v], batch_indices, sampler,\n                        batch_size=args.batch_size,\n                        shuffle=False,\n                        drop_last=False,\n                    ))[0][2]\n                    blocks_dict[v] = blocks_v\n\n                for v in views:\n                    blocks_dict[v] = [b.to(device) for b in blocks_dict[v]]\n\n                batch_labels = labels[batch_indices]\n                batch_ori_labels = torch.LongTensor(ori_labels).to(device)[batch_indices]\n                model.train()\n                out, emb = model(blocks_dict)\n\n                view_contra_loss = 0\n                e_loss = 0\n                if args.use_uncertainty:\n                    alpha = []\n                    true_labels = torch.LongTensor(ori_labels).to(device)[batch_indices]\n                    for i, v in enumerate(views):\n                        emb[v] = F.normalize(emb[v], 2, 1)\n                        batch_center = label_center[v][batch_ori_labels]\n\n                        view_contra_loss += torch.mean(-torch.log(\n                            (torch.exp(torch.sum(torch.mul(emb[v], batch_center), dim=1)) - 0.1 * label_u[\n                                batch_ori_labels]) / (\n                                torch.sum(torch.exp(torch.mm(emb[v], label_center[v].T)),\n                                          dim=1))))  # *label_u[batch_ori_labels])\n\n                        alpha_v = relu_evidence(out[v]) + 1\n                        alpha.append(alpha_v)\n\n                    comb_alpha, comb_u = DS_Combin(alpha=alpha, classes=classes)\n\n                    e_loss = EUC_loss(comb_alpha, comb_u, true_labels, e)\n                    loss = e_loss + criterion(comb_alpha, batch_labels, true_labels, e, classes, 100,\n                                              device) + 2 * view_contra_loss\n\n                else:\n                    batch_labels = torch.argmax(batch_labels, 1)\n                    for i, v in enumerate(views):\n                        if i == 0:\n                            comb_out = out[v]\n                        else:\n                            comb_out += out[v]\n                        emb[v] = F.normalize(emb[v], 2, 1)\n                        batch_center = label_center[v][batch_ori_labels]\n                        view_contra_loss += torch.mean(-torch.log(\n                            (torch.exp(torch.sum(torch.mul(emb[v], batch_center), dim=1))) / (\n                                torch.sum(torch.exp(torch.mm(emb[v], label_center[v].T)), dim=1))))\n                    loss = criterion(comb_out, batch_labels)  # + view_contra_loss\n\n                com_loss = 0\n                for i in range(len(emb) - 1):\n                    for j in range(i + 1, len(emb)):\n                        com_loss += common_loss(emb[views[i]], emb[views[j]])\n                loss += 1 * com_loss\n                print(\"com_loss:\", 1 * com_loss)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                losses.append(loss.item())\n                total_loss += loss.item()\n                print(loss)\n                print(\"Batch loss:\", loss.item())\n\n            total_loss /= (batch_id + 1)\n            message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(e + 1, args.epoch, total_loss)\n            print(message)\n            with open(save_path + '/log.txt', 'a') as f:\n                f.write(message)\n                f.write(\"\\n\")\n            out, emb, nids = extract_results(g_dict, views, labels, model, args)\n            # nids = torch.cat(nids).cpu().numpy().astype(int)  # 确保 nids 是整数数组\n            extract_labels = ori_labels[nids]\n            if args.use_uncertainty:\n                alpha = []\n                for out_v in out.values():\n                    evi_v = relu_evidence(out_v)\n                    alpha_v = evi_v + 1\n                    alpha.append(alpha_v)\n                comb_out, comb_u = DS_Combin(alpha=alpha, classes=classes)\n                train_labels = extract_labels[train_indices.cpu().numpy()]\n\n                comb_u = comb_u.cuda()\n                train_u = comb_u[train_indices].cpu().numpy()\n                train_i_u = []\n                for i in range(classes):\n                    i_indices = np.where(train_labels == i)\n                    i_u = np.mean(train_u[i_indices])\n                    train_i_u.append(i_u)\n                label_u = torch.FloatTensor(train_i_u).cuda()\n                # print(\"label_u:\",label_u)\n\n            else:\n                for i, out_v in enumerate(out.values()):\n                    if i == 0:\n                        comb_out = out_v\n                    else:\n                        comb_out += out_v\n\n            _, val_pred = torch.max(comb_out[val_indices.cpu().numpy()], 1)\n            val_labels = torch.IntTensor(extract_labels[val_indices.cpu().numpy()])\n            val_f1 = f1_score(val_labels.cpu().numpy(), val_pred.cpu().numpy(), average='macro')\n            val_match = torch.reshape(torch.eq(val_pred, val_labels).float(), (-1, 1))\n            val_acc = torch.mean(val_match)\n\n            _, test_pred = torch.max(comb_out[test_indices.cpu().numpy()], 1)\n            test_labels = torch.IntTensor(extract_labels[test_indices.cpu().numpy()])\n            test_f1 = f1_score(test_labels.cpu().numpy(), test_pred.cpu().numpy(), average='macro')\n            test_match = torch.reshape(torch.eq(test_pred, test_labels).float(), (-1, 1))\n            test_acc = torch.mean(test_match)\n            # t = classification_report(test_labels.cpu().numpy(), test_pred.cpu().numpy(), target_names=[i for i in range(classes)])\n            message = \"val_acc: %.4f val_f1:%.4f  test_acc: %.4f test_f1:%.4f\" % (val_acc, val_f1, test_acc, test_f1)\n            print(message)\n            with open(save_path + '/log.txt', 'a') as f:\n                f.write(message)\n\n            test_acc_list.append(test_acc)\n\n            if val_acc > best_vali:\n                best_vali = val_acc\n                best_epoch = e + 1\n                test_acc_in_best_e = test_acc\n                p = save_path + 'best.pt'\n                torch.save(model.state_dict(), p)\n\n        np.save(save_path + \"testacc.npy\", np.array(test_acc_list))\n        message = \"best epoch:%d  test_acc:%.4f\" % (best_epoch, test_acc_in_best_e)\n        print(message)\n        with open(save_path + '/log.txt', 'a') as f:\n            f.write(message)\n\n    else:\n        model.load_state_dict(torch.load(save_path + '/best.pt'))\n        model.eval()\n        out, emb, nids = extract_results(g_dict, views, labels, model, args)\n        extract_labels = ori_labels[nids]\n        if args.use_uncertainty:\n            alpha = []\n            for v in ['h', 'u', 'e']:\n                evi_v = relu_evidence(out[v])\n                alpha_v = evi_v + 1\n                alpha.append(alpha_v)\n            comb_out, comb_u = DS_Combin(alpha=alpha, classes=classes)\n        else:\n            for i, v in enumerate(views):\n                if i == 0:\n                    comb_out = out[v]\n                else:\n                    comb_out += out[v]\n\n        _, test_pred = torch.max(comb_out[test_indices.cpu().numpy()], 1)\n        test_labels = torch.IntTensor(extract_labels[test_indices.cpu().numpy()])\n        if args.use_uncertainty:\n            test_u = comb_u[test_indices].cpu().numpy()\n            test_match = torch.reshape(torch.eq(test_pred.cpu().numpy(), test_labels.cpu().numpy()).float(), (-1, 1))\n            test_i_u = []\n            for i in range(classes):\n                i_indices = np.where(test_labels.cpu().numpy() == i)\n                i_u = np.mean(test_u[i_indices])\n                test_i_u.append(i_u)\n\n        test_f1 = f1_score(test_labels.cpu().numpy(), test_pred.cpu().numpy(), average='macro')\n        test_match = torch.reshape(torch.eq(test_pred.cpu().numpy(), test_labels.cpu().numpy()).float(), (-1, 1))\n        test_acc = torch.mean(test_match)\n        t = classification_report(test_labels.cpu().numpy(), test_pred.cpu().numpy())\n        message = \"test_acc: %.4f test_f1:%.4f\" % (test_acc, test_f1)\n        print(message)\n\n    return model\n\n\nclass Tem_Agg_Layer(nn.Module):\n    def __init__(self, in_dim, out_dim, use_residual):\n        super(Tem_Agg_Layer, self).__init__()\n        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n        self.temporal_fc = nn.Linear(out_dim, 1, bias=False)\n        self.reset_parameters()\n        self.use_residual = use_residual\n\n    def reset_parameters(self):\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n\n    def edge_attention(self, edges):\n        deltas = edges.src['t'] - edges.dst['t']\n        deltas = deltas.cpu().detach().numpy()\n        weights = -abs(deltas)\n        return {'e': torch.tensor(weights).unsqueeze(1).to(edges.src['t'].device)}\n\n    def message_func(self, edges):\n        return {'z': edges.src['z'], 'e': edges.data['e']}\n\n    def reduce_func(self, nodes):\n        alpha = F.softmax(torch.exp(self.temporal_fc(nodes.mailbox['z']) * nodes.mailbox['e'] / 500), dim=1)\n        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n        return {'h': h}\n\n    def forward(self, blocks, layer_id):\n        device = blocks[layer_id].device  # 获取当前block的设备\n        h = blocks[layer_id].srcdata['features'].to(device)\n        z = self.fc(h)\n        blocks[layer_id].srcdata['z'] = z\n        z_dst = z[:blocks[layer_id].number_of_dst_nodes()]\n\n        blocks[layer_id].dstdata['z'] = z_dst\n        blocks[layer_id].apply_edges(self.edge_attention)\n\n        blocks[layer_id].update_all(self.message_func, self.reduce_func)\n\n        if self.use_residual:\n            return z_dst + blocks[layer_id].dstdata['h']\n        return blocks[layer_id].dstdata['h']\n\n\nclass GNN(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, use_residual=False):\n        super(GNN, self).__init__()\n        self.layer1 = Tem_Agg_Layer(in_dim, hidden_dim, use_residual)\n        self.layer2 = Tem_Agg_Layer(hidden_dim, out_dim, use_residual)\n\n    def forward(self, blocks):\n        device = blocks[0].device  # 获取第一个block的设备\n        self.layer1 = self.layer1.to(device)\n        self.layer2 = self.layer2.to(device)\n\n        h = self.layer1(blocks, 0)\n        h = F.elu(h)\n        blocks[1].srcdata['features'] = h.to(device)\n        h = self.layer2(blocks, 1)\n        return h\n\n    def edge_attention(self, edges):\n        device = edges.data['features'].device\n        return self.calculate_attention(edges).to(device)\n\n    def calculate_attention(self, edges):\n        # edge attention的计算逻辑\n        pass\n\n\nclass EDNN(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, use_dropout=True):\n        super(EDNN, self).__init__()\n        self.use_dropout = use_dropout\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        hidden = F.relu(self.fc1(x))\n        if self.use_dropout:\n            hidden = F.dropout(hidden, training=self.training)\n        out = self.fc2(hidden)\n        return out\n\n\nclass UCLSED_model(nn.Module):\n    def __init__(self, GNN_in_dim, GNN_h_dim, GNN_out_dim, E_h_dim, E_out_dim, views):\n        super(UCLSED_model, self).__init__()\n        self.views = views\n        self.GNN = GNN(GNN_in_dim, GNN_h_dim, GNN_out_dim)\n        self.EDNNs = nn.ModuleList([EDNN(GNN_out_dim, E_h_dim, E_out_dim) for v in self.views])\n\n    def forward(self, blocks_dict, is_EDNN_input=False, i=None, emb_v=None):\n        out = dict()\n        if not is_EDNN_input:\n            emb = dict()\n            for i, v in enumerate(self.views):\n                emb[v] = self.GNN(blocks_dict[v])\n                out[v] = self.EDNNs[i](emb[v])\n            return out, emb\n        else:\n            out = self.EDNNs[i](emb_v)\n            return out\n\n\n# loss\ndef common_loss(emb1, emb2):\n    emb1 = emb1 - torch.mean(emb1, dim=0, keepdim=True)\n    emb2 = emb2 - torch.mean(emb2, dim=0, keepdim=True)\n    emb1 = torch.nn.functional.normalize(emb1, p=2, dim=1)\n    emb2 = torch.nn.functional.normalize(emb2, p=2, dim=1)\n    cov1 = torch.matmul(emb1, emb1.t())\n    cov2 = torch.matmul(emb2, emb2.t())\n    cost = torch.mean((cov1 - cov2) ** 2)\n    return cost\n\n\ndef EUC_loss(alpha, u, true_labels, e):\n    _, pred_label = torch.max(alpha, 1)\n    true_indices = torch.where(pred_label == true_labels)\n    false_indices = torch.where(pred_label != true_labels)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n    p, _ = torch.max(alpha / S, 1)\n    a = -0.01 * torch.exp(-(e + 1) / 10 * torch.log(torch.FloatTensor([0.01]))).cuda()\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor((e + 1) / 10, dtype=torch.float32),\n    )\n    EUC_loss = -annealing_coef * torch.sum((p[true_indices] * (torch.log(1.000000001 - u[true_indices]).squeeze(\n        -1))))  # -(1-annealing_coef)*torch.sum(((1-p[false_indices])*(torch.log(u[false_indices]).squeeze(-1))))\n\n    return EUC_loss\n\n\ndef relu_evidence(y):\n    return F.relu(y)\n\n\ndef exp_evidence(y):\n    return torch.exp(torch.clamp(y, -10, 10))\n\n\ndef softplus_evidence(y):\n    return F.softplus(y)\n\n\ndef kl_divergence(alpha, num_classes, device):\n    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)\n    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n    first_term = (\n            torch.lgamma(sum_alpha)\n            - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n            + torch.lgamma(ones).sum(dim=1, keepdim=True)\n            - torch.lgamma(ones.sum(dim=1, keepdim=True))\n    )\n    second_term = (\n        (alpha - ones)\n        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n        .sum(dim=1, keepdim=True)\n    )\n    kl = first_term + second_term\n    return kl\n\n\ndef kl_pred_divergence(alpha, y, num_classes, device):\n    # max_alpha, _ = torch.max(alpha, 1)\n    # ones = alpha*(1-y) + (max_alpha+1) * y\n    ones = y + 0.01 * torch.ones([1, num_classes], dtype=torch.float32, device=device)\n    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n    first_term = (\n            torch.lgamma(sum_alpha)\n            - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n            + torch.lgamma(ones).sum(dim=1, keepdim=True)\n            - torch.lgamma(ones.sum(dim=1, keepdim=True))\n    )\n    second_term = (\n        (alpha - ones)\n        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n        .sum(dim=1, keepdim=True)\n    )\n    kl = first_term + second_term\n    return kl\n\n\ndef loglikelihood_loss(y, alpha, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n    loglikelihood_err = torch.sum((y - (alpha / S)) ** 2, dim=1, keepdim=True)\n    loglikelihood_var = torch.sum(\n        alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True\n    )\n    loglikelihood = loglikelihood_err + loglikelihood_var\n    return loglikelihood\n\n\ndef mse_loss(y, alpha, epoch_num, num_classes, annealing_step, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    loglikelihood = loglikelihood_loss(y, alpha, device)\n\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n    )\n\n    kl_alpha = (alpha - 1) * (1 - y) + 1\n    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n    return loglikelihood + kl_div\n\n\ndef edl_loss(func, y, true_labels, alpha, epoch_num, num_classes, annealing_step, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n\n    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)\n\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor((epoch_num + 1) / 10, dtype=torch.float32),\n    )\n\n    _, pred_label = torch.max(alpha, 1)\n    true_indices = torch.where(pred_label == true_labels)\n    false_indices = torch.where(pred_label != true_labels)\n    kl_alpha = (alpha - 1) * (1 - y) + 1\n    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n    print(\"kl_div:\", 1 * torch.mean(kl_div))\n    print(\"A:\", 20 * torch.mean(A))\n\n    return 20 * A + 1 * kl_div\n\n\ndef edl_mse_loss(alpha, target, true_labels, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n    loss = torch.mean(\n        mse_loss(target, alpha, true_labels, epoch_num, num_classes, annealing_step, device=device)\n    )\n    return loss\n\n\ndef edl_log_loss(alpha, target, true_labels, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n    loss = torch.mean(edl_loss(\n        torch.log, target, alpha, true_labels, epoch_num, num_classes, annealing_step, device\n    )\n    )\n    return loss\n\n\ndef edl_digamma_loss(alpha, target, true_labels, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n\n    loss = torch.mean(edl_loss(\n        torch.digamma, target, true_labels, alpha, epoch_num, num_classes, annealing_step, device\n\n    ))\n    return loss\n\n\n# utils\ndef make_onehot(input, classes):\n    input = torch.LongTensor(input).unsqueeze(1)\n    result = torch.zeros(len(input), classes).long()\n    result.scatter_(dim=1, index=input.long(), src=torch.ones(len(input), classes).long())\n    return result\n\n\ndef relu_evidence(y):\n    return F.relu(y)\n\n\ndef exp_evidence(y):\n    return torch.exp(torch.clamp(y, -10, 10))\n\n\ndef softplus_evidence(y):\n    return F.softplus(y)\n\n\ndef DS_Combin(alpha, classes):\n    \"\"\"\n    :param alpha: All Dirichlet distribution parameters.\n    :return: Combined Dirichlet distribution parameters.\n    \"\"\"\n\n    def DS_Combin_two(alpha1, alpha2, classes):\n        \"\"\"\n        :param alpha1: Dirichlet distribution parameters of view 1\n        :param alpha2: Dirichlet distribution parameters of view 2\n        :return: Combined Dirichlet distribution parameters\n        \"\"\"\n        alpha = dict()\n        alpha[0], alpha[1] = alpha1, alpha2\n        b, S, E, u = dict(), dict(), dict(), dict()\n        for v in range(2):\n            S[v] = torch.sum(alpha[v], dim=1, keepdim=True)\n            E[v] = alpha[v] - 1\n            b[v] = E[v] / (S[v].expand(E[v].shape))\n            u[v] = classes / S[v]\n\n        # b^0 @ b^(0+1)\n        bb = torch.bmm(b[0].view(-1, classes, 1), b[1].view(-1, 1, classes))\n        # b^0 * u^1\n        uv1_expand = u[1].expand(b[0].shape)\n        bu = torch.mul(b[0], uv1_expand)\n        # b^1 * u^0\n        uv_expand = u[0].expand(b[0].shape)\n        ub = torch.mul(b[1], uv_expand)\n        # calculate C\n        bb_sum = torch.sum(bb, dim=(1, 2), out=None)\n        bb_diag = torch.diagonal(bb, dim1=-2, dim2=-1).sum(-1)\n        C = bb_sum - bb_diag\n\n        # calculate b^a\n        b_a = (torch.mul(b[0], b[1]) + bu + ub) / ((1 - C).view(-1, 1).expand(b[0].shape))\n        # calculate u^a\n        u_a = torch.mul(u[0], u[1]) / ((1 - C).view(-1, 1).expand(u[0].shape))\n\n        # calculate new S\n        S_a = classes / u_a\n        # calculate new e_k\n        e_a = torch.mul(b_a, S_a.expand(b_a.shape))\n        alpha_a = e_a + 1\n        return alpha_a, u_a\n\n    if len(alpha) == 1:\n        S = torch.sum(alpha[0], dim=1, keepdim=True)\n        u = classes / S\n        return alpha[0], u\n    for v in range(len(alpha) - 1):\n        if v == 0:\n            alpha_a, u_a = DS_Combin_two(alpha[0], alpha[1], classes)\n        else:\n            alpha_a, u_a = DS_Combin_two(alpha_a, alpha[v + 1], classes)\n    return alpha_a, u_a\n\n\ndef graph_statistics(G, save_path):\n    message = '\\nGraph statistics:\\n'\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    ave_degree = (num_edges / 2) // num_nodes\n    in_degrees = G.in_degrees()\n    isolated_nodes = torch.zeros([in_degrees.size()[0]], dtype=torch.long)\n    isolated_nodes = (in_degrees == isolated_nodes)\n    torch.save(isolated_nodes, save_path + '/isolated_nodes.pt')\n    num_isolated_nodes = torch.sum(isolated_nodes).item()\n    message += 'We have ' + str(num_nodes) + ' nodes.\\n'\n    message += 'We have ' + str(num_edges / 2) + ' in-edges.\\n'\n    message += 'Average degree: ' + str(ave_degree) + '\\n'\n    message += 'Number of isolated nodes: ' + str(num_isolated_nodes) + '\\n'\n    print(message)\n    with open(save_path + \"/graph_statistics.txt\", \"w\") as f:\n        f.write(message)\n    return num_isolated_nodes\n\n\ndef get_dgl_data(args,views,language):\n    g_dict = {}\n    path = args.file_path + language + '/'\n    features = torch.FloatTensor(np.load(path + \"features.npy\"))\n    times = np.load(path + \"time.npy\")\n    times = torch.FloatTensor(((times - times.min()).astype('timedelta64[D]') / np.timedelta64(1, 'D')))\n    labels = np.load(path + \"label.npy\")\n    for v in views:\n        if v == \"h\":\n            matrix = sparse.load_npz(path + \"s_tweet_tweet_matrix_{}.npz\".format(v))\n            # matrix = np.load(path + \"matrix_{}.npy\".format(v+noise))\n        else:\n            matrix = sparse.load_npz(path + \"s_tweet_tweet_matrix_{}.npz\".format(v))\n        g = dgl.DGLGraph(matrix, readonly=True)\n        save_path_v = path + v\n        if not os.path.exists(save_path_v):\n            os.mkdir(save_path_v)\n        num_isolated_nodes = graph_statistics(g, save_path_v)\n        g.set_n_initializer(dgl.init.zero_initializer)\n        # g.readonly(readonly_state=True)\n        # g.ndata['features'] = features\n        # # g.ndata['labels'] = labels\n        # g.ndata['times'] = times\n        g_dict[v] = g\n    return g_dict, times, features, labels\n\n\ndef split_data(length, train_p, val_p, test_p):\n    indices = torch.randperm(length)\n    val_samples = int(length * val_p)\n    val_indices = indices[:val_samples]\n    test_samples = val_samples + int(length * test_p)\n    test_indeces = indices[val_samples:test_samples]\n    train_indices = indices[test_samples:]\n    return train_indices, val_indices, test_indeces\n\n\ndef ava_split_data(length, labels, classes):\n    indices = torch.randperm(length)\n    labels = torch.LongTensor(labels[indices])\n\n    train_indices = []\n    test_indices = []\n    val_indices = []\n\n    for l in range(classes):\n        l_indices = torch.LongTensor(np.where(labels.numpy() == l)[0].reshape(-1))\n        val_indices.append(l_indices[:20].reshape(-1, 1))\n        test_indices.append(l_indices[20:50].reshape(-1, 1))\n        train_indices.append(l_indices[50:].reshape(-1, 1))\n\n    val_indices = indices[torch.cat(val_indices, dim=0).reshape(-1)]\n    test_indices = indices[torch.cat(test_indices, dim=0).reshape(-1)]\n    train_indices = indices[torch.cat(train_indices, dim=0).reshape(-1)]\n    print(train_indices.shape, val_indices.shape, test_indices.shape)\n    print(train_indices)\n    return train_indices, val_indices, test_indices\n\n"}
{"type": "source_file", "path": "SocialED/utils/dataprocess.py", "content": "# -*- coding: utf-8 -*-\n\"\"\"Data processing utilities for multilingual social media data.\"\"\"\n\nimport numpy as np\nimport torch\nimport os\n\nfrom datetime import datetime\nfrom collections import Counter\nimport requests\nfrom statistics import mean\n\ndef construct_graph(df, G=None):\n    \"\"\"Construct a graph from a DataFrame containing social media data.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing social media data with columns:\n        tweet_id, user_mentions, user_id, entities, sampled_words\n    G : networkx.Graph, optional (default=None)\n        Existing graph to add nodes/edges to. If None, creates new graph.\n        \n    Returns\n    -------\n    G : networkx.Graph\n        Graph with nodes for tweets, users, entities and words, and edges between them.\n    \"\"\"\n    import networkx as nx\n    \n    if G is None:\n        G = nx.Graph()\n        \n    for _, row in df.iterrows():\n        # Add tweet node\n        tid = 't_' + str(row['tweet_id'])\n        G.add_node(tid)\n        G.nodes[tid]['tweet_id'] = True\n\n        # Add user nodes\n        user_ids = row['user_mentions']\n        user_ids.append(row['user_id']) \n        user_ids = ['u_' + str(each) for each in user_ids]\n        G.add_nodes_from(user_ids)\n        for each in user_ids:\n            G.nodes[each]['user_id'] = True\n\n        # Add entity nodes\n        entities = row['entities']\n        G.add_nodes_from(entities)\n        for each in entities:\n            G.nodes[each]['entity'] = True\n\n        # Add word nodes\n        words = ['w_' + each for each in row['sampled_words']]\n        G.add_nodes_from(words)\n        for each in words:\n            G.nodes[each]['word'] = True\n\n        # Add edges between tweet and other nodes\n        edges = []\n        edges += [(tid, each) for each in user_ids]\n        edges += [(tid, each) for each in entities] \n        edges += [(tid, each) for each in words]\n        G.add_edges_from(edges)\n\n    return G\n\ndef load_data(name, cache_dir=None):\n    \"\"\"\n    Data loading function that downloads .npy files from SocialED_datasets repository.\n\n    Parameters\n    ----------\n    name : str\n        The name of the dataset.\n    cache_dir : str, optional\n        The directory for dataset caching.\n        Default: ``None``.\n\n    Returns\n    -------\n    data : numpy.ndarray\n        The loaded dataset.\n\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = os.path.join(os.path.expanduser('~'), '.socialed/data')\n    file_path = os.path.join(cache_dir, name + '.npy')\n\n    if os.path.exists(file_path):\n        data = np.load(file_path, allow_pickle=True)\n    else:\n        url = \"https://github.com/ChenBeici/SocialED_datasets/raw/main/npy_data/\" + name + \".npy\"\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        r = requests.get(url, stream=True)\n        if r.status_code != 200:\n            raise RuntimeError(\"Failed downloading url %s\" % url)\n        with open(file_path, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=1024):\n                if chunk:  # filter out keep-alive new chunks\n                    f.write(chunk)\n        data = np.load(file_path, allow_pickle=True)\n    return data\n\n\ndef graph_statistics(G, save_path):\n    \"\"\"\n    Calculate and save basic statistics of a graph.\n\n    Parameters\n    ----------\n    G : networkx.Graph\n        The input graph to analyze.\n    save_path : str\n        Directory path to save the statistics.\n\n    Returns\n    -------\n    num_isolated_nodes : int\n        Number of isolated nodes in the graph.\n    \"\"\"\n    message = '\\nGraph statistics:\\n'\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    ave_degree = (num_edges / 2) // num_nodes\n    in_degrees = G.in_degrees()\n    isolated_nodes = torch.zeros([in_degrees.size()[0]], dtype=torch.long)\n    isolated_nodes = (in_degrees == isolated_nodes)\n    torch.save(isolated_nodes, save_path + '/isolated_nodes.pt')\n    num_isolated_nodes = torch.sum(isolated_nodes).item()\n\n    message += 'We have ' + str(num_nodes) + ' nodes.\\n'\n    message += 'We have ' + str(num_edges / 2) + ' in-edges.\\n'\n    message += 'Average degree: ' + str(ave_degree) + '\\n'\n    message += 'Number of isolated nodes: ' + str(num_isolated_nodes) + '\\n'\n    print(message)\n    with open(save_path + \"/graph_statistics.txt\", \"a\") as f:\n        f.write(message)\n\n    return num_isolated_nodes\n\n\ndef extract_time_feature(t_str):\n    \"\"\"\n    Extract time features from timestamp string.\n\n    Parameters\n    ----------\n    t_str : str\n        Timestamp string in ISO format.\n\n    Returns\n    -------\n    list\n        List containing two normalized time features: [days, seconds].\n    \"\"\"\n    t = datetime.fromisoformat(str(t_str))\n    OLE_TIME_ZERO = datetime(1899, 12, 30)\n    delta = t - OLE_TIME_ZERO\n    return [(float(delta.days) / 100000.), (float(delta.seconds) / 86400)]  # 86,400 seconds in day\n\ndef get_word2id_emb(wordpath,embpath):\n    \"\"\"\n    Load word-to-id mapping and embeddings from files.\n\n    Parameters\n    ----------\n    wordpath : str\n        Path to file containing words.\n    embpath : str\n        Path to file containing embeddings.\n\n    Returns\n    -------\n    tuple\n        (word2id dictionary, embeddings array).\n    \"\"\"\n    word2id = {}\n    with open(wordpath, 'r') as f:\n        for i, w in enumerate(list(f.readlines()[0].split())):\n            word2id[w] = i\n    embeddings = np.load(embpath)\n    return word2id,embeddings\n\n\ndef df_to_t_features(df):\n    \"\"\"\n    Convert DataFrame timestamps to time features.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame with 'created_at' column containing timestamps.\n\n    Returns\n    -------\n    numpy.ndarray\n        Array of time features for each timestamp.\n    \"\"\"\n    t_features = np.asarray([extract_time_feature(t_str) for t_str in df['created_at']])\n    return t_features\n\n\ndef check_class_sizes(ground_truths, predictions):\n    \"\"\"\n    Check sizes of predicted classes against ground truth classes.\n\n    Parameters\n    ----------\n    ground_truths : array-like\n        Ground truth class labels.\n    predictions : array-like\n        Predicted class labels.\n\n    Returns\n    -------\n    list\n        List of predicted class labels that are larger than average ground truth class size.\n    \"\"\"\n    count_true_labels = list(Counter(ground_truths).values())  \n    ave_true_size = mean(count_true_labels)\n    distinct_predictions = list(Counter(predictions).keys()) \n    count_predictions = list(Counter(predictions).values()) \n    large_classes = [distinct_predictions[i] for i, count in enumerate(count_predictions) if count > ave_true_size]\n    return large_classes\n\n"}
{"type": "source_file", "path": "docs/examples/example.py", "content": "\"\"\"\nThis example shows how to plot a simple line chart using matplotlib with SocialED data.\n\nSection 1: Data Preparation\n---------------------------\n\nWe start by creating a simple dataset.\n\"\"\"\n\nimport matplotlib.pyplot as plt\n\n# 数据准备\nx = [1, 2, 3, 4]\ny = [10, 15, 13, 17]\n\n# 绘制折线图\nplt.plot(x, y)\nplt.title(\"Simple Line Chart\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()  # 这个命令会生成并显示图表\n"}
{"type": "source_file", "path": "SocialED/utils/__init__.py", "content": "from .utility import *\nfrom .dataprocess import *\n\n\n"}
{"type": "source_file", "path": "docs/_build/html/_downloads/65d3db1d68ebe4f35385b0d61dd17b19/example.py", "content": "\"\"\"\nThis example shows how to plot a simple line chart using matplotlib with SocialED data.\n\nSection 1: Data Preparation\n---------------------------\n\nWe start by creating a simple dataset.\n\"\"\"\n\nimport matplotlib.pyplot as plt\n\n# 数据准备\nx = [1, 2, 3, 4]\ny = [10, 15, 13, 17]\n\n# 绘制折线图\nplt.plot(x, y)\nplt.title(\"Simple Line Chart\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()  # 这个命令会生成并显示图表\n"}
{"type": "source_file", "path": "SocialED/version.py", "content": "\"\"\"\n``SocialED`` is a python library for social event detection\n\"\"\"\n\n\n__version__ = '1.1.5'\n"}
{"type": "source_file", "path": "SocialED/utils/utility.py", "content": "# -*- coding: utf-8 -*-\n\"\"\"A set of utility functions to support social event detection tasks.\"\"\"\n\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport warnings\nfrom itertools import combinations\nfrom datetime import datetime\nimport numpy as np\n\n\ndef construct_graph(df, G=None):\n    \"\"\"Construct a graph from a DataFrame containing social media data.\n    \n    Parameters\n    ----------\n    df : pandas.DataFrame\n        DataFrame containing social media data with columns:\n        tweet_id, user_mentions, user_id, entities, sampled_words\n    G : networkx.Graph, optional (default=None)\n        Existing graph to add nodes/edges to. If None, creates new graph.\n        \n    Returns\n    -------\n    G : networkx.Graph\n        Graph with nodes for tweets, users, entities and words, and edges between them.\n    \"\"\"\n    import networkx as nx\n    \n    if G is None:\n        G = nx.Graph()\n        \n    for _, row in df.iterrows():\n        # Add tweet node\n        tid = 't_' + str(row['tweet_id'])\n        G.add_node(tid)\n        G.nodes[tid]['tweet_id'] = True\n\n        # Add user nodes\n        user_ids = row['user_mentions']\n        user_ids.append(row['user_id']) \n        user_ids = ['u_' + str(each) for each in user_ids]\n        G.add_nodes_from(user_ids)\n        for each in user_ids:\n            G.nodes[each]['user_id'] = True\n\n        # Add entity nodes\n        entities = row['entities']\n        G.add_nodes_from(entities)\n        for each in entities:\n            G.nodes[each]['entity'] = True\n\n        # Add word nodes\n        words = ['w_' + each for each in row['sampled_words']]\n        G.add_nodes_from(words)\n        for each in words:\n            G.nodes[each]['word'] = True\n\n        # Add edges between tweet and other nodes\n        edges = []\n        edges += [(tid, each) for each in user_ids]\n        edges += [(tid, each) for each in entities] \n        edges += [(tid, each) for each in words]\n        G.add_edges_from(edges)\n\n    return G\n\n\n\ndef tokenize_text(text, max_length=512):\n    \"\"\"Tokenize text for social event detection tasks.\n    \n    Parameters\n    ----------\n    text : str\n        The input text to tokenize.\n    max_length : int, optional (default=512)\n        Maximum length of tokenized sequence.\n        \n    Returns\n    -------\n    tokens : list\n        List of tokenized words/subwords.\n    \"\"\"\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    \n    # Basic tokenization by splitting on whitespace\n    tokens = text.lower().split()\n    \n    # Truncate if exceeds max length\n    if len(tokens) > max_length:\n        tokens = tokens[:max_length]\n        \n    return tokens\n\ndef pprint(params, offset=0, printer=repr):\n    \"\"\"Pretty print the dictionary 'params'.\n    \n    Parameters\n    ----------\n    params : dict\n        The dictionary to pretty print\n    offset : int, optional (default=0)\n        The offset at the beginning of each line\n    printer : callable, optional (default=repr)\n        The function to convert entries to strings\n        \n    Returns\n    -------\n    str\n        Pretty printed string representation\n    \"\"\"\n\n    params_list = list()\n    this_line_length = offset\n    line_sep = ',\\n' + (1 + offset) * ' '\n    for i, (k, v) in enumerate(sorted(params.items())):\n        if type(v) is float:\n            # use str for representing floating point numbers\n            # this way we get consistent representation across\n            # architectures and versions.\n            this_repr = '%s=%s' % (k, str(v))\n        else:\n            # use repr of the rest\n            this_repr = '%s=%s' % (k, printer(v))\n        if len(this_repr) > 500:\n            this_repr = this_repr[:300] + '...' + this_repr[-100:]\n        if i > 0:\n            if this_line_length + len(this_repr) >= 75 or '\\n' in this_repr:\n                params_list.append(line_sep)\n                this_line_length = len(line_sep)\n            else:\n                params_list.append(', ')\n                this_line_length += 2\n        params_list.append(this_repr)\n        this_line_length += len(this_repr)\n\n    lines = ''.join(params_list)\n    # Strip trailing space to avoid nightmare in doctests\n    lines = '\\n'.join(l.rstrip(' ') for l in lines.split('\\n'))\n    return lines\n\ndef validate_device(gpu_id):\n    \"\"\"Validate the input GPU ID is valid on the given environment.\n    If no GPU is presented, return 'cpu'.\n\n    Parameters\n    ----------\n    gpu_id : int\n        GPU ID to check.\n\n    Returns\n    -------\n    device : str\n        Valid device, e.g., 'cuda:0' or 'cpu'.\n    \"\"\"\n\n    # cast to int for checking\n    gpu_id = int(gpu_id)\n\n    # if it is cpu\n    if gpu_id == -1:\n        return 'cpu'\n\n    # if gpu is available\n    if torch.cuda.is_available():\n        # check if gpu id is between 0 and the total number of GPUs\n        check_parameter(gpu_id, 0, torch.cuda.device_count(),\n                        param_name='gpu id', include_left=True,\n                        include_right=False)\n        device = 'cuda:{}'.format(gpu_id)\n    else:\n        if gpu_id != 'cpu':\n            warnings.warn('The cuda is not available. Set to cpu.')\n        device = 'cpu'\n\n    return device\n\ndef check_parameter(value, lower, upper, param_name, include_left=True, include_right=True):\n    \"\"\"Check if a parameter value is within specified bounds.\n\n    Parameters\n    ----------\n    value : int or float\n        The parameter value to check\n    lower : int or float \n        Lower bound\n    upper : int or float\n        Upper bound\n    param_name : str\n        Name of the parameter for error messages\n    include_left : bool, optional (default=True)\n        Whether to include lower bound in valid range\n    include_right : bool, optional (default=True)\n        Whether to include upper bound in valid range\n\n    Returns\n    -------\n    bool\n        True if parameter is valid, raises ValueError otherwise\n    \"\"\"\n\n    if include_left:\n        if value < lower:\n            raise ValueError(f\"{param_name} must be greater than or equal to {lower}\")\n    if include_right:\n        if value > upper:\n            raise ValueError(f\"{param_name} must be less than or equal to {upper}\")\n    return True\n\ndef currentTime():\n    \"\"\"Get current time as formatted string.\n    \n    Returns\n    -------\n    str\n        Current time in format 'YYYY-MM-DD HH:MM:SS'\n    \"\"\"\n    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n    \ndef sim(z1, z2):\n    \"\"\"Compute cosine similarity between two sets of vectors.\n    \n    Parameters\n    ----------\n    z1 : torch.Tensor\n        First set of vectors\n    z2 : torch.Tensor\n        Second set of vectors\n        \n    Returns\n    -------\n    torch.Tensor\n        Similarity matrix\n    \"\"\"\n    z1 = F.normalize(z1)\n    z2 = F.normalize(z2)\n    return torch.mm(z1, z2.t())\n\ndef pairwise_sample(embeddings, labels=None, model=None):\n    if model == None:\n        labels = labels.cpu().data.numpy()\n        indices = np.arange(0,len(labels),1)\n        pairs = np.array(list(combinations(indices, 2)))\n        pair_labels = (labels[pairs[:,0]]==labels[pairs[:,1]])\n\n        pair_matrix = np.eye(len(labels))\n        ind = np.where(pair_labels)\n        pair_matrix[pairs[ind[0],0],pairs[ind[0],1]] = 1\n        pair_matrix[pairs[ind[0],1], pairs[ind[0],0]] = 1\n\n        return torch.LongTensor(pairs), torch.LongTensor(pair_labels.astype(int)),torch.LongTensor(pair_matrix)\n\n    else:\n        pair_matrix = model(embeddings)\n        return pair_matrix\n\ndef SBERT_embed(s_list, language):\n    '''\n    Use Sentence-BERT to embed sentences.\n    s_list: a list of sentences/ tokens to be embedded.\n    language: the language of the sentences ('English', 'French', 'Arabic').\n    output: the embeddings of the sentences/ tokens.\n    '''\n    # Model paths or names for each language\n    model_map = {\n        'English': '../model/model_needed/all-MiniLM-L6-v2',\n        'French': '../model/model_needed/distiluse-base-multilingual-cased-v1',\n        'Arabic': '../model/model_needed/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Default model for Hugging Face\n    hf_model_map = {\n        'English': 'sentence-transformers/all-MiniLM-L6-v2',\n        'French': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n        'Arabic': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Print language and model being used\n    print(f\"Embedding sentences in language: {language}\")\n    \n    # Determine model path\n    model_path = model_map.get(language)\n    if not model_path:\n        raise ValueError(f\"Unsupported language: {language}. Supported languages are: {', '.join(model_map.keys())}\")\n\n    print(f\"Using model: {model_path}\")\n\n    # Load the model, downloading if necessary\n    try:\n        model = SentenceTransformer(model_path)\n        print(f\"Successfully loaded model from local path: {model_path}\")\n    except Exception as e:\n        print(f\"Model {model_path} not found locally. Attempting to download from Hugging Face...\")\n        model = SentenceTransformer(hf_model_map[language])\n        print(f\"Model downloaded from Hugging Face: {hf_model_map[language]}\")\n\n    # Compute embeddings\n    embeddings = model.encode(s_list, convert_to_tensor=True, normalize_embeddings=True)\n    print(f\"Computed embeddings for {len(s_list)} sentences/tokens.\")\n    \n    return embeddings.cpu()\n\ndef DS_Combin(alpha, classes):\n    \"\"\"\n    :param alpha: All Dirichlet distribution parameters.\n    :return: Combined Dirichlet distribution parameters.\n    \"\"\"\n\n    def DS_Combin_two(alpha1, alpha2, classes):\n        \"\"\"\n        :param alpha1: Dirichlet distribution parameters of view 1\n        :param alpha2: Dirichlet distribution parameters of view 2\n        :return: Combined Dirichlet distribution parameters\n        \"\"\"\n        alpha = dict()\n        alpha[0], alpha[1] = alpha1, alpha2\n        b, S, E, u = dict(), dict(), dict(), dict()\n        for v in range(2):\n            S[v] = torch.sum(alpha[v], dim=1, keepdim=True)\n            E[v] = alpha[v] - 1\n            b[v] = E[v] / (S[v].expand(E[v].shape))\n            u[v] = classes / S[v]\n\n        # b^0 @ b^(0+1)\n        bb = torch.bmm(b[0].view(-1, classes, 1), b[1].view(-1, 1, classes))\n        # b^0 * u^1\n        uv1_expand = u[1].expand(b[0].shape)\n        bu = torch.mul(b[0], uv1_expand)\n        # b^1 * u^0\n        uv_expand = u[0].expand(b[0].shape)\n        ub = torch.mul(b[1], uv_expand)\n        # calculate C\n        bb_sum = torch.sum(bb, dim=(1, 2), out=None)\n        bb_diag = torch.diagonal(bb, dim1=-2, dim2=-1).sum(-1)\n        C = bb_sum - bb_diag\n\n        # calculate b^a\n        b_a = (torch.mul(b[0], b[1]) + bu + ub) / ((1 - C).view(-1, 1).expand(b[0].shape))\n        # calculate u^a\n        u_a = torch.mul(u[0], u[1]) / ((1 - C).view(-1, 1).expand(u[0].shape))\n\n        # calculate new S\n        S_a = classes / u_a\n        # calculate new e_k\n        e_a = torch.mul(b_a, S_a.expand(b_a.shape))\n        alpha_a = e_a + 1\n        return alpha_a, u_a\n\n    if len(alpha)==1:\n        S = torch.sum(alpha[0], dim=1, keepdim=True)\n        u = classes / S\n        return alpha[0],u\n    for v in range(len(alpha) - 1):\n        if v == 0:\n            alpha_a,u_a = DS_Combin_two(alpha[0], alpha[1], classes)\n        else:\n            alpha_a,u_a = DS_Combin_two(alpha_a, alpha[v + 1], classes)\n    return alpha_a,u_a\n\ndef graph_statistics(G, save_path):\n    message = '\\nGraph statistics:\\n'\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    ave_degree = (num_edges / 2) // num_nodes\n    in_degrees = G.in_degrees()\n    isolated_nodes = torch.zeros([in_degrees.size()[0]], dtype=torch.long)\n    isolated_nodes = (in_degrees == isolated_nodes)\n    torch.save(isolated_nodes, save_path + '/isolated_nodes.pt')\n    num_isolated_nodes = torch.sum(isolated_nodes).item()\n    message += 'We have ' + str(num_nodes) + ' nodes.\\n'\n    message += 'We have ' + str(num_edges / 2) + ' in-edges.\\n'\n    message += 'Average degree: ' + str(ave_degree) + '\\n'\n    message += 'Number of isolated nodes: ' + str(num_isolated_nodes) + '\\n'\n    print(message)\n    with open(save_path + \"/graph_statistics.txt\", \"w\") as f:\n        f.write(message)\n    return num_isolated_nodes\n\n\n"}
{"type": "source_file", "path": "SocialED/model/__init__.py", "content": ""}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\nwith open('requirements.txt') as f:\n    requirements = f.read().splitlines()\n\nsetup(\n    name='SocialED',\n    version='1.1.5',\n    packages=find_packages(),\n    author='beici',\n    author_email='zhangkun23@buaa.edu.cn',\n    description='A Python Library for Social Event Detection',\n    install_requires=requirements,\n    include_package_data=True,\n    long_description=open('README.md', encoding='utf-8').read(),\n    long_description_content_type='text/markdown',\n    url='https://github.com/RingBDStack/SocialED',\n    classifiers=[\n        'Programming Language :: Python :: 3.8',\n        'License :: OSI Approved :: MIT License',\n        'Operating System :: OS Independent',\n    ],\n    python_requires='>=3.8',\n)\n"}
{"type": "source_file", "path": "SocialED/detector/glove.py", "content": "import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport logging\nimport datetime\nimport pickle\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import DatasetLoader\nfrom huggingface_hub import hf_hub_download\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n\nclass GloVe:\n    r\"\"\"The GloVe model for social event detection that uses GloVe word embeddings\n    to detect events in social media data.\n\n    .. note::\n        This detector uses word embeddings to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    num_clusters : int, optional\n        Number of clusters for KMeans clustering. Default: ``50``.\n    random_state : int, optional\n        Random seed for reproducibility. Default: ``1``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/GloVe/'``.\n    model : str, optional\n        Path to pre-trained GloVe word vectors file. Default: ``'../model/model_needed/glove.6B.100d.txt'``.\n    \"\"\"\n    def __init__(self, dataset, num_clusters=50, random_state=1, file_path='../model/model_saved/GloVe/',\n                 model='../model/model_needed/glove.6B.100d.txt'):\n        self.dataset = dataset.load_data()\n        self.num_clusters = num_clusters\n        self.random_state = random_state\n        self.model_path = os.path.join(file_path, 'kmeans_model')\n        self.df = None\n        self.train_df = None\n        self.test_df = None\n        self.model = model\n        self.embeddings_index = self.load_glove_vectors()\n\n    def load_glove_vectors(self):\n        \"\"\"\n        Load GloVe pre-trained word vectors.\n        \"\"\"\n        embeddings_index = {}\n\n        with open(self.model, 'r', encoding='utf8') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                coefs = np.asarray(values[1:], dtype='float32')\n                embeddings_index[word] = coefs\n        return embeddings_index\n\n    def preprocess(self):\n        \"\"\"\n        Data preprocessing: tokenization, stop words removal, etc.\n        \"\"\"\n        df = self.dataset[['filtered_words', 'event_id']].copy()\n        df['processed_text'] = df['filtered_words'].apply(\n            lambda x: [str(word).lower() for word in x] if isinstance(x, list) else [])\n        self.df = df\n        return df\n\n    def text_to_glove_vector(self, text, embedding_dim=100):\n        \"\"\"\n        Convert text to GloVe vector representation.\n        \"\"\"\n        words = text\n        embedding = np.zeros(embedding_dim)\n        valid_words = 0\n        for word in words:\n            if word in self.embeddings_index:\n                embedding += self.embeddings_index[word]\n                valid_words += 1\n        if valid_words > 0:\n            embedding /= valid_words\n        return embedding\n\n    def create_vectors(self, df, text_column):\n        \"\"\"\n        Create GloVe vectors for each document.\n        \"\"\"\n        texts = df[text_column].tolist()\n        vectors = np.array([self.text_to_glove_vector(text) for text in texts])\n        return vectors\n\n    def load_model(self):\n        \"\"\"\n        Load the KMeans model from a file.\n        \"\"\"\n        logging.info(f\"Loading KMeans model from {self.model_path}...\")\n        kmeans_model = KMeans(n_clusters=self.num_clusters, random_state=self.random_state)\n        kmeans_model = kmeans_model.fit(self.train_vectors)  # 重新训练模型\n        logging.info(\"KMeans model loaded successfully.\")\n\n        self.kmeans_model = kmeans_model\n        return kmeans_model\n\n    def fit(self):\n        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n\n        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=self.random_state)\n        self.train_df = train_df\n        self.test_df = test_df\n        self.train_vectors = self.create_vectors(train_df, 'processed_text')\n\n        logging.info(\"Training KMeans model...\")\n        kmeans_model = KMeans(n_clusters=self.num_clusters, random_state=self.random_state)\n        kmeans_model.fit(self.train_vectors)\n        logging.info(\"KMeans model trained successfully.\")\n\n        # Save the trained model to a file\n        with open(self.model_path, 'wb') as f:\n            pickle.dump(kmeans_model, f)\n        logging.info(f\"KMeans model saved to {self.model_path}\")\n\n    def detection(self):\n        \"\"\"\n        Assign clusters to each document.\n        \"\"\"\n        self.load_model()  # Ensure the model is loaded before making detections\n        self.test_vectors = self.create_vectors(self.test_df, 'processed_text')\n        labels = self.kmeans_model.predict(self.test_vectors)\n\n        # Get the ground truth labels and predicted labels\n        ground_truths = self.test_df['event_id'].tolist()\n        predicted_labels = labels.tolist()\n        return ground_truths, predicted_labels\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n"}
{"type": "source_file", "path": "docs/conf.py", "content": "# Configuration file for the Sphinx documentation builder.\n\nimport os\nimport sys\nfrom os.path import dirname, abspath\n\n# 将 SocialED 项目的根目录添加到 sys.path\nsys.path.insert(0, abspath('../'))\nroot_dir = dirname(dirname(abspath(__file__)))\n\nclass MockSpacy:\n    def load(self, *args, **kwargs):\n        return self\n    \n    def __call__(self, *args, **kwargs):\n        return self\n\n    def __getattr__(self, name):\n        return self\n\nsys.modules['spacy'] = MockSpacy()\nsys.modules['en_core_web_lg'] = MockSpacy()\n\n\n# -- Project information -----------------------------------------------------\n\nproject = 'SocialED'\ncopyright = '2024 beici'\nauthor = 'beici'\n\n# 如果您有版本信息，可以使用以下代码获取版本号\n#version_path = os.path.join(root_dir, 'SocialED', 'version.py')\n#exec(open(version_path).read())\nversion = '1.1.5'\nrelease = version\n\n# -- General configuration ---------------------------------------------------\n\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinxcontrib.bibtex'\n]\n\nbibtex_bibfiles = ['zreferences.bib']\n\nautodoc_member_order = 'bysource'\n\nautodoc_mock_imports = [\n    # 'en_core_web_lg',\n    # 'fr_core_news_lg',\n    # 'dgl',\n    # 'dgl.function',\n    # 'dgl.dataloading',\n    # 'spacy',\n    # 'torch',\n    # 'torch.nn',\n    # 'torch.cuda',\n    # 'transformers',\n    # 'transformers.modeling_bert',\n    # 'transformers.tokenization_bert',\n    # 'numpy',\n    # 'pandas',\n    # 'scikit-learn'\n    'numpy',\n    'torch',\n    'pandas',\n    'sklearn',\n    'scipy',\n    'networkx',\n    'spacy',\n    'thinc',\n    'gensim',\n    'transformers',\n    'nltk',\n    'tensorflow',\n    'keras',\n    'matplotlib',\n    'seaborn',\n    'tqdm',\n    'cupy',\n    'dgl',\n    'torch_geometric',\n    'torch_scatter',\n    'torch_sparse',\n    'torch_cluster',\n    'torch_spline_conv',\n    'tokenizers',\n    'sentence_transformers',\n    'allennlp',\n    'overrides',\n    'faiss',\n    'numba',\n    'cudf',\n    'cugraph',\n    'cucim',\n    'en_core_web_lg',\n    'ignite',\n    'ignite.distributed',\n    'ignite.distributed.auto',\n    'ignite.distributed.utils',\n    'ignite.distributed.comp_models',\n    'ignite.engine',\n    'ignite.metrics',\n    'packaging',\n    'packaging.version',\n    'hdbscan',\n]\n\ntemplates_path = ['_templates']\nsource_suffix = '.rst'\nmaster_doc = 'index'\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']\n\n# -- Options for HTML output -------------------------------------------------\nhtml_theme = \"furo\"\nhtml_favicon = 'socialed.ico'\nhtml_static_path = ['_static']\n\n# -- Options for HTMLHelp output ---------------------------------------------\nhtmlhelp_basename = 'socialEDdoc'\n\n# -- Options for LaTeX output ------------------------------------------------\nlatex_documents = [\n    (master_doc, 'socialED.tex', 'SocialED Documentation',\n     'beici', 'manual'),\n]\n\n# -- Options for manual page output ------------------------------------------\nman_pages = [\n    (master_doc, 'socialED', 'SocialED Documentation',\n     [author], 1)\n]\n\n# -- Options for Texinfo output ----------------------------------------------\ntexinfo_documents = [\n    (master_doc, 'socialED', 'SocialED Documentation',\n     author, 'SocialED', 'A Python library for social event detection.',\n     'Miscellaneous'),\n]\n\n# -- Extension configuration -------------------------------------------------\n# from sphinx_gallery.sorting import FileNameSortKey\n\nhtml_static_path = []\n\n\n\n# sphinx_gallery_conf = {\n#     'examples_dirs': 'examples/',   # Path to your example scripts\n#     'gallery_dirs': 'tutorials/',\n#     'within_subsection_order': FileNameSortKey,\n#     'filename_pattern': '.py',\n#     'download_all_examples': False,\n# }\n\nnapoleon_google_docstring = True\nnapoleon_numpy_docstring = True\nnapoleon_include_init_with_doc = True\nnapoleon_include_private_with_doc = True\n\n# -- Options for intersphinx extension ---------------------------------------\nintersphinx_mapping = {\n    \"python\": (\"https://docs.python.org/3\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n    \"scipy\": (\"https://docs.scipy.org/doc/scipy/\", None),\n    \"sklearn\": (\"https://scikit-learn.org/stable/\", None),\n    \"networkx\": (\"https://networkx.org/documentation/stable/\", None),\n    'torch': (\"https://pytorch.org/docs/master\", None),\n    'torch_geometric': (\"https://pytorch-geometric.readthedocs.io/en/latest\", None),\n}\n\n\n# 添加类型提示支持\nset_type_checking_flag = True\nalways_document_param_types = True\n\n"}
{"type": "source_file", "path": "SocialED/detector/adpsemevent.py", "content": "import networkx as nx\nfrom itertools import combinations, chain\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn import metrics\nfrom sklearn.cluster import SpectralClustering\nimport sys\nfrom datetime import datetime\nimport math\nimport pickle\nimport pandas as pd\nimport os\nfrom os.path import exists\nimport time\nimport multiprocessing\nimport torch\nfrom matplotlib import pyplot as plt\nfrom networkx.algorithms import cuts\nfrom sentence_transformers import SentenceTransformer\nimport re\nfrom sklearn.model_selection import train_test_split\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\n\nclass ADPSEMEvent:\n    \"\"\"ADPSEMEvent class for event detection.\n    \n    This class implements adaptive semantic event detection.\n    \n    Args:\n        dataset: Input dataset\n        ...\n    \"\"\"  # 修复缩进问题\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.language = dataset.get_dataset_language()\n        self.dataset_name = dataset.get_dataset_name()\n        self.save_path = \"../model/model_saved/adpsemevent/\"+self.dataset_name+\"/\"\n\n    def preprocess(self):\n        preprocessor = Preprocessor(self.dataset)\n        preprocessor.preprocess()\n\n    def detection(self):\n        ground_truths, predictions = run_hier_2D_SE_mini_closed_set(self.save_path, n=300, e_a=True, e_s=True)\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\nclass Preprocessor:\n    def __init__(self, dataset, mode='close'):\n        \"\"\"Initialize preprocessor\n        Args:\n            dataset: Dataset calss (e.g. Event2012, Event2018, etc.)\n            language: Language of the dataset (default 'English')\n            mode: 'open' or 'close' (default 'close') - determines preprocessing mode\n        \"\"\"\n        self.dataset = dataset\n        self.language = dataset.get_dataset_language()\n        self.dataset_name = dataset.get_dataset_name()\n        self.mode = mode\n        self.columns = ['tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n                       'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions']\n\n    def get_closed_set_test_df(self, df):\n        \"\"\"Get closed set test dataframe\"\"\"\n        save_path = f'../model/model_saved/adpsemevent/{self.dataset_name}/closed_set/'\n        if not exists(save_path):\n            os.makedirs(save_path)\n        \n        test_set_df_np_path = save_path + 'test_set.npy'\n        if not exists(test_set_df_np_path):\n            # Use 2012-style processing for all datasets\n            test_mask = torch.load(f'../model/model_saved/adpsemevent/{self.dataset_name}/masks/test_mask.pt').cpu().detach().numpy()\n            test_mask = list(np.where(test_mask==True)[0])\n            test_df = df.iloc[test_mask]\n            \n            test_df_np = test_df.to_numpy()\n            np.save(test_set_df_np_path, test_df_np)\n        return\n\n    def get_closed_set_messages_embeddings(self):\n        \"\"\"Get SBERT embeddings for closed set messages\"\"\"\n        save_path = f'../model/model_saved/adpsemevent/{self.dataset_name}/closed_set/'\n        \n        SBERT_embedding_path = f'{save_path}/SBERT_embeddings.pkl'\n        if not exists(SBERT_embedding_path):\n            test_set_df_np_path = save_path + 'test_set.npy'\n            test_df_np = np.load(test_set_df_np_path, allow_pickle=True)\n            \n            test_df = pd.DataFrame(data=test_df_np, columns=self.columns)\n            print(\"Dataframe loaded.\")\n\n            processed_text = [preprocess_sentence(s) for s in test_df['text'].values]\n            print('message text contents preprocessed.')\n\n            embeddings = SBERT_embed(processed_text, language=self.language)\n\n            with open(SBERT_embedding_path, 'wb') as fp:\n                pickle.dump(embeddings, fp)\n            print('SBERT embeddings stored.')\n        return\n\n    def get_open_set_messages_embeddings(self):\n        \"\"\"Get SBERT embeddings for open set messages\"\"\"\n        save_path = f'../model/model_saved/adpsemevent/{self.dataset_name}/open_set/'\n        num_blocks = 21  # Use 2012-style processing for all datasets\n        \n        for i in range(num_blocks):\n            block = i + 1\n            print('\\n\\n====================================================')\n            print('block: ', block)\n\n            SBERT_embedding_path = f'{save_path}{block}/SBERT_embeddings.pkl'\n\n            if not exists(SBERT_embedding_path):\n                df_np = np.load(f'{save_path}{block}/{block}.npy', allow_pickle=True)\n                \n                df = pd.DataFrame(data=df_np, columns=self.columns + ['original_index', 'date'])\n                print(\"Dataframe loaded.\")\n\n                df['processed_text'] = [preprocess_sentence(s) for s in df['text']]\n                print('message text contents preprocessed.')\n\n                embeddings = SBERT_embed(df['processed_text'].tolist(), language=self.language)\n\n                with open(SBERT_embedding_path, 'wb') as fp:\n                    pickle.dump(embeddings, fp)\n                print('SBERT embeddings stored.')\n        return\n\n    def split_open_set(self, df, root_path):\n        \"\"\"Split data into open set blocks\"\"\"\n        if not exists(root_path):\n            os.makedirs(root_path)\n        \n        df = df.sort_values(by='created_at').reset_index()\n        df['date'] = [d.date() for d in df['created_at']]\n\n        distinct_dates = df.date.unique()\n\n        # First week -> block 0\n        folder = root_path + '0/'\n        if not exists(folder):\n            os.mkdir(folder)\n            \n        df_np_path = folder + '0.npy'\n        if not exists(df_np_path):\n            ini_df = df.loc[df['date'].isin(distinct_dates[:7])]\n            ini_df_np = ini_df.to_numpy()\n            np.save(df_np_path, ini_df_np)\n\n        # Following dates -> block 1, 2, ...\n        end = len(distinct_dates) - 1  # Use 2012-style processing\n        for i in range(7, end):\n            folder = root_path + str(i - 6) + '/'\n            if not exists(folder):\n                os.mkdir(folder)\n            \n            df_np_path = folder + str(i - 6) + '.npy'\n            if not exists(df_np_path):\n                incr_df = df.loc[df['date'] == distinct_dates[i]]\n                incr_df_np = incr_df.to_numpy()\n                np.save(df_np_path, incr_df_np)\n        return\n\n    def preprocess(self):\n        \"\"\"Main preprocessing function\"\"\"\n        # Load raw data using 2012-style processing\n        df_np = self.dataset.load_data()\n        \n        print(\"Loaded data.\")\n        df = pd.DataFrame(data=df_np, columns=self.columns)\n        print(\"Data converted to dataframe.\")\n\n        \n        if self.mode == 'open':\n            # Open-set setting\n            root_path = f'../model/model_saved/adpsemevent/{self.dataset_name}/open_set/'\n            self.split_open_set(df, root_path)\n            self.get_open_set_messages_embeddings()\n        else:\n            # Close-set setting\n            # Create masks directory and generate train/val/test splits\n            save_dir = os.path.join(f'../model/model_saved/adpsemevent/{self.dataset_name}', 'masks')\n            os.makedirs(save_dir, exist_ok=True)\n            \n            # Split and save masks\n            self.split_and_save_masks(df, save_dir)\n            print(\"Generated and saved train/val/test masks.\")\n\n            self.get_closed_set_test_df(df)\n            self.get_closed_set_messages_embeddings()\n        \n        return\n\n    def split_and_save_masks(self, df, save_dir, train_size=0.7, val_size=0.1, test_size=0.2, random_seed=42):\n        \"\"\"\n        Splits the DataFrame into training, validation, and test sets, and saves the indices (masks) as .pt files.\n        \n        Parameters:\n        - df (pd.DataFrame): The DataFrame to be split\n        - save_dir (str): Directory to save the masks\n        - train_size (float): Proportion for training (default 0.7)\n        - val_size (float): Proportion for validation (default 0.1) \n        - test_size (float): Proportion for testing (default 0.2)\n        - random_seed (int): Random seed for reproducibility\n        \"\"\"\n        if train_size + val_size + test_size != 1.0:\n            raise ValueError(\"train_size + val_size + test_size must equal 1.0\")\n\n        if df.empty:\n            raise ValueError(\"The input DataFrame is empty.\")\n\n        print(f\"Total samples in DataFrame: {len(df)}\")\n        \n        # Set random seed\n        torch.manual_seed(random_seed)\n\n        # Split into train and temp\n        train_data, temp_data = train_test_split(df, train_size=train_size, random_state=random_seed)\n        \n        # Split temp into val and test\n        val_data, test_data = train_test_split(temp_data, \n                                             train_size=val_size/(val_size + test_size),\n                                             random_state=random_seed)\n\n        # Create boolean masks\n        full_train_mask = torch.zeros(len(df), dtype=torch.bool)\n        full_val_mask = torch.zeros(len(df), dtype=torch.bool)\n        full_test_mask = torch.zeros(len(df), dtype=torch.bool)\n\n        # Set indices\n        full_train_mask[train_data.index] = True\n        full_val_mask[val_data.index] = True  \n        full_test_mask[test_data.index] = True\n\n        print(f\"Training samples: {full_train_mask.sum()}\")\n        print(f\"Validation samples: {full_val_mask.sum()}\")\n        print(f\"Test samples: {full_test_mask.sum()}\")\n\n        # Save masks\n        mask_paths = {\n            'train_mask.pt': full_train_mask,\n            'val_mask.pt': full_val_mask, \n            'test_mask.pt': full_test_mask\n        }\n\n        for filename, mask in mask_paths.items():\n            mask_path = os.path.join(save_dir, filename)\n            if not os.path.exists(mask_path):\n                try:\n                    torch.save(mask, mask_path)\n                    print(f\"Saved {filename}\")\n                except Exception as e:\n                    print(f\"Error saving {filename}: {e}\")\n            else:\n                print(f\"{filename} already exists\")\n\n            # Verify saved file\n            if os.path.exists(mask_path):\n                saved_mask = torch.load(mask_path)\n                if saved_mask.numel() == 0:\n                    print(f\"Warning: {filename} is empty\")\n                else:\n                    print(f\"Verified {filename} with {saved_mask.numel()} elements\")\n\n        print(\"Mask generation completed\")\n\ndef get_stable_point(path, if_updata, epsilon):\n    stable_point_path = path + f'stable_point_{epsilon}.pkl'\n    if not exists(stable_point_path) or if_updata == True:\n        embeddings_path = path + 'SBERT_embeddings.pkl'\n        with open(embeddings_path, 'rb') as f:\n            embeddings = pickle.load(f)\n        first_stable_point, global_stable_point, Sensitivity = search_stable_points(embeddings, epsilon, path)\n        stable_points = {'first': first_stable_point, 'global': global_stable_point}\n        with open(stable_point_path, 'wb') as fp:\n            pickle.dump(stable_points, fp)\n        print('stable points stored.')\n\n    with open(stable_point_path, 'rb') as f:\n        stable_points = pickle.load(f)\n    print('stable points loaded.')\n    return stable_points, Sensitivity\n\ndef run_hier_2D_SE_mini_open_set(save_path, n=400, e_a=True, e_s=True, test_with_one_block=True, epsilon=0.2):\n    if test_with_one_block:\n        blocks = [16]\n    else:\n        blocks = [i+1 for i in range(20) if i+1>=1]\n        \n    for block in blocks:\n        print('\\n\\n====================================================')\n        print('block: ', block)\n        print(datetime.now().strftime(\"%H:%M:%S\"))\n\n        folder = f'{save_path}{block}/'\n        \n        # Load message embeddings\n        embeddings_path = folder + 'SBERT_embeddings.pkl'\n        with open(embeddings_path, 'rb') as f:\n            embeddings = pickle.load(f)\n        \n        # Load and process dataframe\n        df_np = np.load(f'{folder}{block}.npy', allow_pickle=True)\n        columns = ['tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n                  'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions']\n        df = pd.DataFrame(data=df_np, columns=columns)\n        \n        all_node_features = [list(set([str(u)] + \\\n            [str(each) for each in um] + \\\n            [h.lower() for h in hs] + \\\n            e)) \\\n            for u, um, hs, e in \\\n            zip(df['user_id'], df['user_mentions'], df['hashtags'], df['entities'])]\n        \n        start_time = time.time()\n        stable_points, Sensitivity = get_stable_point(folder, if_updata=True, epsilon=epsilon)\n        if e_a == False: # only rely on e_s (semantic-similarity-based edges)\n            default_num_neighbors = stable_points['global']\n        else:\n            default_num_neighbors = stable_points['first']\n        if default_num_neighbors == 0:\n            default_num_neighbors = math.ceil((len(embeddings)/1000)*10)\n            \n        global_edges = get_global_edges(all_node_features, epsilon, folder, default_num_neighbors, e_a=e_a, e_s=e_s)\n        \n        corr_matrix = np.load(f\"{folder}corr_matrix_{epsilon}.npy\")\n        weighted_global_edges = [(edge[0], edge[1], corr_matrix[edge[0]-1, edge[1]-1]) for edge in global_edges \\\n            if corr_matrix[edge[0]-1, edge[1]-1] > 0]\n            \n        division = hier_2D_SE_mini(weighted_global_edges, len(embeddings), n=n)\n        print(datetime.now().strftime(\"%H:%M:%S\"))\n\n        prediction = decode(division)\n        \n        labels_true = df['event_id'].tolist()\n        n_clusters = len(list(set(labels_true)))\n        print('n_clusters gt: ', n_clusters)\n\n        nmi, ami, ari = evaluate_labels(labels_true, prediction)\n        print('n_clusters pred: ', len(division))\n        print('nmi: ', nmi)\n        print('ami: ', ami)\n        print('ari: ', ari)\n        \n        with open(f\"open_set_{epsilon}.txt\", 'a') as f:\n            f.write(\"block:\" + str(block) + '\\n')\n            f.write(\"division:\"+str(division)+ '\\n')\n            f.write('Runtime: ' + str(time.time() - start_time) + \" Seconds\" + '\\n')\n            f.write('n_clusters gt: '+ str(len(list(set(labels_true))))+ '\\n')\n            f.write('n_clusters pred: ' + str(len(division)) + '\\n')\n            f.write('epsilon: ' + str(epsilon) + '\\n')\n            f.write('n: ' + str(n) + '\\n')\n            f.write('Sensitivity: ' + str(Sensitivity) + '\\n')\n            f.write('nmi: ' + str(nmi) + '\\n')\n            f.write('ami: ' + str(ami) + '\\n')\n            f.write('ari: ' + str(ari) + '\\n' + '\\n')\n            \n    return\n\ndef run_hier_2D_SE_mini_closed_set(save_path, n=300, e_a=True, e_s=True, epsilon=None):\n    \n    save_path = save_path + 'closed_set/'\n    # Load test set dataframe\n    test_set_df_np_path = save_path + 'test_set.npy'\n    test_df_np = np.load(test_set_df_np_path, allow_pickle=True)\n    columns = ['tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n              'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions']\n    test_df = pd.DataFrame(data=test_df_np, columns=columns)\n    print(\"Dataframe loaded.\")\n    \n    all_node_features = [[str(u)] + \\\n                        [str(each) for each in (um if isinstance(um, (list, tuple)) else [])] + \\\n                        [str(h).lower() if isinstance(h, str) else str(h) for h in (hs if isinstance(hs, (list, tuple)) else [])] + \\\n                        [str(e) for e in (e if isinstance(e, (list, tuple)) else [])] \\\n                        for u, um, hs, e in \\\n                        zip(test_df['user_id'], test_df['user_mentions'], test_df['hashtags'], test_df['entities'])]\n\n\n    # Load embeddings\n    with open(f'{save_path}/SBERT_embeddings.pkl', 'rb') as f:\n        embeddings = pickle.load(f)\n\n    start_time = time.time()\n    stable_points, Sensitivity = get_stable_point(save_path, if_updata=True, epsilon=epsilon)\n    default_num_neighbors = stable_points['first']\n\n    global_edges = get_global_edges(all_node_features, epsilon, save_path, default_num_neighbors, e_a=e_a, e_s=e_s)\n    corr_matrix = np.load(f\"{save_path}corr_matrix_{epsilon}.npy\")\n    weighted_global_edges = [(edge[0], edge[1], corr_matrix[edge[0]-1, edge[1]-1]) for edge in global_edges \\\n        if corr_matrix[edge[0]-1, edge[1]-1] > 0]\n\n    division = hier_2D_SE_mini(weighted_global_edges, len(embeddings), n=n)\n    prediction = decode(division)\n\n    labels_true = test_df['event_id'].tolist()\n    n_clusters = len(list(set(labels_true)))\n    print('n_clusters gt: ', n_clusters)\n\n    print('n_clusters pred: ', len(division))\n    \n        \n    return labels_true, prediction\n\ndef create_process_open_set(epsilon):\n    target = run_hier_2D_SE_mini_open_set\n    kwargs = {\n        \"n\": 100,\n        \"e_a\": True,\n        \"e_s\": True,\n        \"test_with_one_block\": True,\n        \"epsilon\": epsilon\n    }\n       \n    p = multiprocessing.Process(target=target, kwargs=kwargs)\n    p.start()\n    return p\n\ndef create_process_closed_set(epsilon):\n    target = run_hier_2D_SE_mini_closed_set\n    n = 300\n\n    kwargs = {\n        \"n\": n,\n        \"e_a\": True,\n        \"e_s\": True,\n        \"epsilon\": epsilon\n    }\n    \n    p = multiprocessing.Process(target=target, kwargs=kwargs)\n    p.start()\n    return p\n\ndef run_processes(epsilons, dataset_name, mode='close'):\n    if mode == 'open':\n        processes = [create_process_open_set(dataset_name, epsilon) for epsilon in epsilons]\n    else:\n        processes = [create_process_closed_set(dataset_name, epsilon) for epsilon in epsilons]\n    for process in processes:\n        process.join()\n    print(\"All processes have completed their tasks.\")\n\ndef make_symmetric(matrix):\n    return np.triu(matrix) + np.triu(matrix, 1).T\n\ndef search_stable_points(embeddings, epsilon, path, max_num_neighbors = 200):\n    print(\"size_of_embeddings\",len(embeddings))\n    corr_matrix = np.corrcoef(embeddings)  \n    np.fill_diagonal(corr_matrix, 0)\n\n    print(\"epsilon=\",epsilon)\n    s = -1\n    if epsilon != None:\n        max_ = np.max(corr_matrix)\n        min_ = np.min(corr_matrix)\n        print(\"Local Sensitivity:\",(max_- min_))\n        # delta = 10e-6  \n        delta = 1 / len(embeddings)**2  \n        beta = epsilon / (2 * np.log(2/delta))\n        S = np.exp(-beta) * (max_- min_) * 2\n        print(\"Smooth Sensitivity:\", S)\n        if S < 2:\n            s = S\n        else:\n            s = 2\n\n        print(\"Sensitivity=\",s)\n        corr_matrix = [[i+np.random.laplace(loc=0, scale=s/epsilon) for i in corr_matrix_] for corr_matrix_ in corr_matrix]\n        corr_matrix = np.array(corr_matrix)\n        corr_matrix = make_symmetric(corr_matrix)\n\n    np.fill_diagonal(corr_matrix, 0)\n    print(f\"{path}\"+f'corr_matrix_{epsilon}.npy')\n    np.save(f\"{path}\"+f'corr_matrix_{epsilon}.npy', corr_matrix)\n    corr_matrix_sorted_indices = np.argsort(corr_matrix)\n    \n    all_1dSEs = []\n    seg = None\n    for i in range(max_num_neighbors):\n        dst_ids = corr_matrix_sorted_indices[:, -(i+1)]\n        knn_edges = [(s+1, d+1, corr_matrix[s, d]) \\\n            for s, d in enumerate(dst_ids) if corr_matrix[s, d] > 0] # (s+1, d+1): +1 as node indexing starts from 1 instead of 0\n        if i == 0:\n            g = nx.Graph()\n            g.add_weighted_edges_from(knn_edges)\n            seg = SE(g)\n            all_1dSEs.append(seg.calc_1dSE())\n        else:\n            all_1dSEs.append(seg.update_1dSE(all_1dSEs[-1], knn_edges))\n    \n    #print('all_1dSEs: ', all_1dSEs)\n    stable_indices = []\n    for i in range(1, len(all_1dSEs) - 1):\n        if all_1dSEs[i] < all_1dSEs[i - 1] and all_1dSEs[i] < all_1dSEs[i + 1]:\n            stable_indices.append(i)\n    if len(stable_indices) == 0:\n        print('No stable points found after checking k = 1 to ', max_num_neighbors)\n        return 0, 0, s\n    else:\n        stable_SEs = [all_1dSEs[index] for index in stable_indices]\n        index = stable_indices[stable_SEs.index(min(stable_SEs))]\n        print('stable_indices: ', stable_indices)\n        print('stable_SEs: ', stable_SEs)\n        print('First stable point: k = ', stable_indices[0]+1, ', corresponding 1dSE: ', stable_SEs[0]) # n_neighbors should be index + 1\n        print('Global stable point within the searching range: k = ', index + 1, \\\n            ', corresponding 1dSE: ', all_1dSEs[index]) # n_neighbors should be index + 1\n\n    return stable_indices[0]+1, index + 1, s # first stable point, global stable point\n\ndef get_graph_edges(attributes):\n    attr_nodes_dict = {}\n    for i, l in enumerate(attributes):\n        for attr in l:\n            if attr not in attr_nodes_dict:\n                attr_nodes_dict[attr] = [i+1] # node indexing starts from 1\n            else:\n                attr_nodes_dict[attr].append(i+1)\n\n    for attr in attr_nodes_dict.keys():\n        attr_nodes_dict[attr].sort()\n\n    graph_edges = []\n    for l in attr_nodes_dict.values():\n        graph_edges += list(combinations(l, 2))\n    return list(set(graph_edges))\n\ndef get_knn_edges(epsilon, path, default_num_neighbors):\n    # corr_matrix = np.corrcoef(embeddings)\n    # np.fill_diagonal(corr_matrix, 0)\n    corr_matrix = np.load(f\"{path}\"+f'corr_matrix_{epsilon}.npy')\n    corr_matrix_sorted_indices = np.argsort(corr_matrix)\n    knn_edges = []\n    for i in range(default_num_neighbors):\n        dst_ids = corr_matrix_sorted_indices[:, -(i+1)]\n        knn_edges += [(s+1, d+1) if s < d else (d+1, s+1) \\\n            for s, d in enumerate(dst_ids) if corr_matrix[s, d] > 0] # (s+1, d+1): +1 as node indexing starts from 1 instead of 0\n    return list(set(knn_edges))\n\ndef get_global_edges(attributes, epsilon, folder, default_num_neighbors, e_a = True, e_s = True):\n    graph_edges, knn_edges = [], []\n    if e_a == True:\n        graph_edges = get_graph_edges(attributes)\n    if e_s == True:\n        knn_edges = get_knn_edges(epsilon, folder, default_num_neighbors)\n    return list(set(knn_edges + graph_edges))\n\ndef get_subgraphs_edges(clusters, graph_splits, weighted_global_edges):\n    \"\"\"Get subgraph edges.\n    \n    Args:\n        clusters: a list containing the current clusters, each cluster is a list of nodes of the original graph\n        graph_splits: a list of (start_index, end_index) pairs, each (start_index, end_index) pair indicates a subset of clusters, \n            which will serve as the nodes of a new subgraph\n        weighted_global_edges: a list of (start node, end node, edge weight) tuples, each tuple is an edge in the original graph\n\n    Returns:\n        all_subgraphs_edges: a list containing the edges of all subgraphs\n    \"\"\"  # 修复缩进和块引用问题\n    all_subgraphs_edges = []\n    for split in graph_splits:\n        subgraph_clusters = clusters[split[0]:split[1]]\n        subgraph_nodes = list(chain(*subgraph_clusters))\n        subgraph_edges = [edge for edge in weighted_global_edges if edge[0] in subgraph_nodes and edge[1] in subgraph_nodes]\n        all_subgraphs_edges.append(subgraph_edges)\n    return all_subgraphs_edges\n\ndef get_best_egde(adj_matrix_, subgraphs_, all_subgraphs):\n    adj_matrix = adj_matrix_.copy()\n    \n    mask_nodes = list(set(all_subgraphs+subgraphs_))  \n    if len(mask_nodes) >0:\n        adj_matrix[mask_nodes, :] = 0\n        adj_matrix[:, mask_nodes] = 0\n\n    flat_index = np.argmax(adj_matrix)\n    egde = np.unravel_index(flat_index, adj_matrix.shape)\n    weight = adj_matrix[egde]\n    if weight > 0:\n        return list(egde), weight\n    else:\n        print(\"There is no egdes in current G\")\n        return -1, -1\n\ndef get_best_node(adj_matrix_, subgraphs_, all_subgraphs):\n    adj_matrix = adj_matrix_.copy()\n\n    mask_nodes = list(set(all_subgraphs+subgraphs_))  \n    nodes_to_modify = np.array(mask_nodes)\n    adj_matrix[np.ix_(nodes_to_modify, nodes_to_modify)] = 0\n\n    distance = adj_matrix[subgraphs_].sum(axis=0)\n    distance_sort_arg = np.argsort(distance)[::-1]\n    distance_sort = np.sort(distance)[::-1]\n    avg = np.mean(distance[distance>0])\n    indices = distance_sort[distance_sort>avg]\n\n    if len(indices) > 0:\n        return distance_sort_arg[:len(indices)].tolist(), distance_sort[:len(indices)].tolist()\n    else:\n        print(\"There are no edges connected to the current subgraph\")\n        return -1, -1\n\ndef get_subgraphs(adj_matrix, division, n, k_max):\n    merged_rows_matrix = np.vstack([ adj_matrix[np.array(ls_)-1].sum(axis=0).tolist() for ls_ in division ])\n    final_sum = np.array([ merged_rows_matrix[:, np.array(ls_)-1].sum(axis=1).tolist() for ls_ in division ] )\n    np.fill_diagonal(final_sum, 0)\n    G = nx.from_numpy_array(final_sum)\n    \n    subgraphs = []\n    all_subgraphs = [] \n    for k in range(k_max):\n        subgraphs_ = []\n        if len(final_sum) - len(all_subgraphs)<= n: \n            G.remove_nodes_from(all_subgraphs)\n            subgraphs_ = list(G.nodes)\n            subgraphs.append(subgraphs_)\n            print(len(subgraphs_), subgraphs_)\n            break\n\n        max_edge_or_node, max_weight = get_best_egde(final_sum, subgraphs_, all_subgraphs)\n        subgraphs_.extend(max_edge_or_node)\n        all_subgraphs.extend(max_edge_or_node)\n        while True:\n            if len(subgraphs_) >= n:\n                break\n            node_, weight_ = get_best_node(final_sum, subgraphs_, all_subgraphs)\n            if node_ == -1:\n                max_edge_or_node, max_weight = get_best_egde(final_sum, subgraphs_, all_subgraphs)\n                subgraphs_.extend(max_edge_or_node)\n                all_subgraphs.extend(max_edge_or_node)\n                continue\n            else:\n                if len(subgraphs_) + len(node_) > n:\n                    index_ = n - len(subgraphs_)\n                    subgraphs_.extend(node_[:index_])\n                    all_subgraphs.extend(node_[:index_])\n                else:\n                    subgraphs_.extend(node_)\n                    all_subgraphs.extend(node_)\n        subgraphs.append(subgraphs_)\n        # print(len(subgraphs_), subgraphs_)\n\n    # subgraphs = [[element + 1 for element in row] for row in subgraphs]\n\n    new_division = []\n    for subgraphs_index in subgraphs:\n        new_division_ = []\n        for index in subgraphs_index:\n            new_division_.append(division[index])\n        new_division.append(new_division_)\n        \n    return new_division\n\ndef hier_2D_SE_mini(weighted_global_edges, n_messages, n = 100):\n    '''\n    hierarchical 2D SE minimization\n    '''\n    ite = 0\n    # initially, each node (message) is in its own cluster\n    # node encoding starts from 1\n\n    G = nx.Graph()\n    G.add_weighted_edges_from(weighted_global_edges)\n    adj_matrix = nx.to_numpy_array(G)\n\n    clusters = [[i] for i in list(G.nodes)]\n    while True:\n        ite += 1\n        print('\\n=========Iteration ', str(ite), '=========')\n        n_clusters = len(clusters)\n        graph_splits = [(s, min(s+n, n_clusters)) for s in range(0, n_clusters, n)] # [s, e)\n        # all_subgraphs_edges = get_subgraphs_edges(clusters, graph_splits, weighted_global_edges)\n\n        if 1:\n            subgraphs = get_subgraphs(adj_matrix, clusters, n, len(graph_splits))\n\n            all_subgraphs_edges = []\n            for subgraph_nodes in subgraphs:\n                subgraph_nodes = [str(item) for sublist in subgraph_nodes for item in sublist]\n                subgraph_edges = [(int(edge[0]),int(edge[1]),edge[2]) for edge in weighted_global_edges \n                                  if str(edge[0]) in subgraph_nodes and str(edge[1]) in subgraph_nodes]\n                all_subgraphs_edges.append(subgraph_edges)\n\n        else:\n            all_subgraphs_edges = get_subgraphs_edges(clusters, graph_splits, weighted_global_edges)\n\n\n        last_clusters = clusters\n        print(f\"the number of clusters: {len(last_clusters)}\")\n        clusters = []\n        for i, subgraph_edges in enumerate(all_subgraphs_edges):\n            print('\\tSubgraph ', str(i+1))\n\n            g = nx.Graph()\n            g.add_weighted_edges_from(subgraph_edges)\n            seg = SE(g)\n            if 1:\n                seg.division = {j: cluster for j, cluster in enumerate(subgraphs[i]) }\n                # print({j: cluster for j, cluster in enumerate(subgraphs[i]) })\n            else:\n                seg.division = {j: cluster for j, cluster in enumerate(last_clusters[graph_splits[i][0]:graph_splits[i][1]])}\n                # print(seg.division)\n            seg.add_isolates()\n            for k in seg.division.keys():\n                for node in seg.division[k]:\n                    seg.graph.nodes[node]['comm'] = k\n            seg.update_struc_data()\n            seg.update_struc_data_2d()\n            seg.update_division_MinSE()\n\n            print(f\"size of subgraph{str(i+1)}: {len(subgraphs[i])} to {len(list(seg.division.values()))}\")\n\n            clusters += list(seg.division.values())\n\n        if len(graph_splits) == 1:\n            break\n        if clusters == last_clusters:\n            n *= 2\n    return clusters\n\nclass SE:\n    def __init__(self, graph: nx.Graph):\n        self.graph = graph.copy()\n        self.vol = self.get_vol()\n        self.division = {}  # {comm1: [node11, node12, ...], comm2: [node21, node22, ...], ...}\n        self.struc_data = {}  # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }\n        self.struc_data_2d = {} # {comm1: {comm2: [vol_after_merge, cut_after_merge, comm_node_SE_after_merge, leaf_nodes_SE_after_merge], comm3: [], ...}, ...}\n\n    def get_vol(self):\n        '''\n        get the volume of the graph\n        '''\n        return cuts.volume(self.graph, self.graph.nodes, weight = 'weight')\n\n    def calc_1dSE(self):\n        '''\n        get the 1D SE of the graph\n        '''\n        SE = 0\n        for n in self.graph.nodes:\n            d = cuts.volume(self.graph, [n], weight = 'weight')\n            SE += - (d / self.vol) * math.log2(d / self.vol)\n        return SE\n\n    def update_1dSE(self, original_1dSE, new_edges):\n        '''\n        get the updated 1D SE after new edges are inserted into the graph\n        '''\n    \n        affected_nodes = []\n        for edge in new_edges:\n            affected_nodes += [edge[0], edge[1]]\n        affected_nodes = set(affected_nodes)\n\n        original_vol = self.vol\n        original_degree_dict = {node:0 for node in affected_nodes}\n        for node in affected_nodes.intersection(set(self.graph.nodes)):\n            original_degree_dict[node] = self.graph.degree(node, weight = 'weight')\n\n        # insert new edges into the graph\n        self.graph.add_weighted_edges_from(new_edges)\n\n        self.vol = self.get_vol()\n        updated_vol = self.vol\n        updated_degree_dict = {}\n        for node in affected_nodes:\n            updated_degree_dict[node] = self.graph.degree(node, weight = 'weight')\n        \n        updated_1dSE = (original_vol / updated_vol) * (original_1dSE - math.log2(original_vol / updated_vol))\n        for node in affected_nodes:\n            d_original = original_degree_dict[node]\n            d_updated = updated_degree_dict[node]\n            if d_original != d_updated:\n                if d_original != 0:\n                    updated_1dSE += (d_original / updated_vol) * math.log2(d_original / updated_vol)\n                updated_1dSE -= (d_updated / updated_vol) * math.log2(d_updated / updated_vol)\n\n        return updated_1dSE\n\n    def get_cut(self, comm):\n        '''\n        get the sum of the degrees of the cut edges of community comm\n        '''\n        return cuts.cut_size(self.graph, comm, weight = 'weight')\n\n    def get_volume(self, comm):\n        '''\n        get the volume of community comm\n        '''\n        return cuts.volume(self.graph, comm, weight = 'weight')\n\n    def calc_2dSE(self):\n        '''\n        get the 2D SE of the graph\n        '''\n        SE = 0\n        for comm in self.division.values():\n            g = self.get_cut(comm)\n            v = self.get_volume(comm)\n            SE += - (g / self.vol) * math.log2(v / self.vol)\n            for node in comm:\n                d = self.graph.degree(node, weight = 'weight')\n                SE += - (d / self.vol) * math.log2(d / v)\n        return SE\n\n    def show_division(self):\n        print(self.division)\n        return self.division\n\n    def show_struc_data(self):\n        print(self.struc_data)\n    \n    def show_struc_data_2d(self):\n        print(self.struc_data_2d)\n        return self.struc_data_2d\n        \n    def print_graph(self):\n        fig, ax = plt.subplots()\n        nx.draw(self.graph, ax=ax, with_labels=True)\n        plt.show()\n    \n    def update_struc_data(self):\n        '''\n        calculate the volume, cut, communitiy mode SE, and leaf nodes SE of each cummunity, \n        then store them into self.struc_data\n        '''\n        self.struc_data = {} # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }\n        for vname in self.division.keys():\n            comm = self.division[vname]\n            volume = self.get_volume(comm)\n            cut = self.get_cut(comm)\n            if volume == 0:\n                vSE = 0\n            else:\n                vSE = - (cut / self.vol) * math.log2(volume / self.vol)\n            vnodeSE = 0\n            for node in comm:\n                d = self.graph.degree(node, weight = 'weight')\n                if d != 0:\n                    vnodeSE -= (d / self.vol) * math.log2(d / volume)\n            self.struc_data[vname] = [volume, cut, vSE, vnodeSE]\n\n    def update_struc_data_2d(self):\n        '''\n        calculate the volume, cut, communitiy mode SE, and leaf nodes SE after merging each pair of cummunities, \n        then store them into self.struc_data_2d\n        '''\n        self.struc_data_2d = {} # {(comm1, comm2): [vol_after_merge, cut_after_merge, comm_node_SE_after_merge, leaf_nodes_SE_after_merge], (comm1, comm3): [], ...}\n        comm_num = len(self.division)\n        for i in range(comm_num):\n            for j in range(i + 1, comm_num):\n                v1 = list(self.division.keys())[i]\n                v2 = list(self.division.keys())[j]\n                if v1 < v2:\n                    k = (v1, v2)\n                else:\n                    k = (v2, v1)\n\n                comm_merged = self.division[v1] + self.division[v2]\n                gm = self.get_cut(comm_merged)\n                vm = self.struc_data[v1][0] + self.struc_data[v2][0]\n                if self.struc_data[v1][0] == 0 or self.struc_data[v2][0] == 0:\n                    vmSE = self.struc_data[v1][2] + self.struc_data[v2][2]\n                    vmnodeSE = self.struc_data[v1][3] + self.struc_data[v2][3]\n                else:\n                    vmSE = - (gm / self.vol) * math.log2(vm / self.vol)\n                    vmnodeSE = self.struc_data[v1][3] - (self.struc_data[v1][0]/ self.vol) * math.log2(self.struc_data[v1][0] / vm) + \\\n                            self.struc_data[v2][3] - (self.struc_data[v2][0]/ self.vol) * math.log2(self.struc_data[v2][0] / vm)\n                self.struc_data_2d[k] = [vm, gm, vmSE, vmnodeSE]\n\n    def init_division(self):\n        '''\n        initialize self.division such that each node assigned to its own community\n        '''\n        self.division = {}\n        for node in self.graph.nodes:\n            new_comm = node\n            self.division[new_comm] = [node]\n            self.graph.nodes[node]['comm'] = new_comm\n\n    def add_isolates(self):\n        '''\n        add any isolated nodes into graph\n        '''\n        all_nodes = list(chain(*list(self.division.values())))\n        all_nodes.sort()\n        edge_nodes = list(self.graph.nodes)\n        edge_nodes.sort()\n        if all_nodes != edge_nodes:\n            for node in set(all_nodes)-set(edge_nodes):\n                self.graph.add_node(node)\n\n    def update_division_MinSE(self):\n        '''\n        greedily update the encoding tree to minimize 2D SE\n        '''\n        def Mg_operator(v1, v2):\n            '''\n            MERGE operator. It calculates the delta SE caused by mergeing communities v1 and v2, \n            without actually merging them, i.e., the encoding tree won't be changed\n            '''\n            v1SE = self.struc_data[v1][2] \n            v1nodeSE = self.struc_data[v1][3]\n\n            v2SE = self.struc_data[v2][2]\n            v2nodeSE = self.struc_data[v2][3]\n\n            if v1 < v2:\n                k = (v1, v2)\n            else:\n                k = (v2, v1)\n            vm, gm, vmSE, vmnodeSE = self.struc_data_2d[k]\n            delta_SE = vmSE + vmnodeSE - (v1SE + v1nodeSE + v2SE + v2nodeSE)\n            return delta_SE\n\n        # continue merging any two communities that can cause the largest decrease in SE, \n        # until the SE can't be further reduced\n        while True: \n            comm_num = len(self.division)\n            delta_SE = 99999\n            vm1 = None\n            vm2 = None\n            for i in range(comm_num):\n                for j in range(i + 1, comm_num):\n                    v1 = list(self.division.keys())[i]\n                    v2 = list(self.division.keys())[j]\n                    new_delta_SE = Mg_operator(v1, v2)\n                    if new_delta_SE < delta_SE:\n                        delta_SE = new_delta_SE\n                        vm1 = v1\n                        vm2 = v2\n\n            if delta_SE < 0:\n                # Merge v2 into v1, and update the encoding tree accordingly\n                for node in self.division[vm2]:\n                    self.graph.nodes[node]['comm'] = vm1\n                self.division[vm1] += self.division[vm2]\n                self.division.pop(vm2)\n\n                volume = self.struc_data[vm1][0] + self.struc_data[vm2][0]\n                cut = self.get_cut(self.division[vm1])\n                vmSE = - (cut / self.vol) * math.log2(volume / self.vol)\n                vmnodeSE = self.struc_data[vm1][3] - (self.struc_data[vm1][0]/ self.vol) * math.log2(self.struc_data[vm1][0] / volume) + \\\n                        self.struc_data[vm2][3] - (self.struc_data[vm2][0]/ self.vol) * math.log2(self.struc_data[vm2][0] / volume)\n                self.struc_data[vm1] = [volume, cut, vmSE, vmnodeSE]\n                self.struc_data.pop(vm2)\n\n                struc_data_2d_new = {}\n                for k in self.struc_data_2d.keys():\n                    if k[0] == vm2 or k[1] == vm2:\n                        continue\n                    elif k[0] == vm1 or k[1] == vm1:\n                        v1 = k[0]\n                        v2 = k[1]\n                        comm_merged = self.division[v1] + self.division[v2]\n                        gm = self.get_cut(comm_merged)\n                        vm = self.struc_data[v1][0] + self.struc_data[v2][0]\n                        if self.struc_data[v1][0] == 0 or self.struc_data[v2][0] == 0:\n                            vmSE = self.struc_data[v1][2] + self.struc_data[v2][2]\n                            vmnodeSE = self.struc_data[v1][3] + self.struc_data[v2][3]\n                        else:\n                            vmSE = - (gm / self.vol) * math.log2(vm / self.vol)\n                            vmnodeSE = self.struc_data[v1][3] - (self.struc_data[v1][0]/ self.vol) * math.log2(self.struc_data[v1][0] / vm) + \\\n                                    self.struc_data[v2][3] - (self.struc_data[v2][0]/ self.vol) * math.log2(self.struc_data[v2][0] / vm)\n                        struc_data_2d_new[k] = [vm, gm, vmSE, vmnodeSE]\n                    else:\n                        struc_data_2d_new[k] = self.struc_data_2d[k]\n                self.struc_data_2d = struc_data_2d_new\n            else:\n                break\n\ndef vanilla_2D_SE_mini(weighted_edges):\n    '''\n    vanilla (greedy) 2D SE minimization\n    '''\n    g = nx.Graph()\n    g.add_weighted_edges_from(weighted_edges)\n    \n    seg = SE(g)\n    seg.init_division()\n    #seg.show_division()\n    SE1D = seg.calc_1dSE()\n\n    seg.update_struc_data()\n    #seg.show_struc_data()\n    seg.update_struc_data_2d()\n    #seg.show_struc_data_2d()\n    initial_SE2D = seg.calc_2dSE()\n\n    seg.update_division_MinSE()\n    communities = seg.division\n    minimized_SE2D = seg.calc_2dSE()\n\n    return SE1D, initial_SE2D, minimized_SE2D, communities\n\ndef test_vanilla_2D_SE_mini():\n    weighted_edges = [(1, 2, 2), (1, 3, 4)]\n\n    g = nx.Graph()\n    g.add_weighted_edges_from(weighted_edges)\n    A = nx.adjacency_matrix(g).todense()\n    print('adjacency matrix: \\n', A)\n    print('g.nodes: ', g.nodes)\n    print('g.edges: ', g.edges)\n    print('degrees of nodes: ', list(g.degree(g.nodes, weight = 'weight')))\n\n    SE1D, initial_SE2D, minimized_SE2D, communities = vanilla_2D_SE_mini(weighted_edges)\n    print('\\n1D SE of the graph: ', SE1D)\n    print('initial 2D SE of the graph: ', initial_SE2D)\n    print('the minimum 2D SE of the graph: ', minimized_SE2D)\n    print('communities detected: ', communities)\n    return\n\ndef replaceAtUser(text):\n    \"\"\" Replaces \"@user\" with \"\" \"\"\"\n    text = re.sub('@[^\\s]+|RT @[^\\s]+','',text)\n    return text\n\ndef removeUnicode(text):\n    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n    return text\n\ndef replaceURL(text):\n    \"\"\" Replaces url address with \"url\" \"\"\"\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\ndef replaceMultiExclamationMark(text):\n    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n    text = re.sub(r\"(\\!)\\1+\", '!', text)\n    return text\n\ndef replaceMultiQuestionMark(text):\n    \"\"\" Replaces repetitions of question marks \"\"\"\n    text = re.sub(r\"(\\?)\\1+\", '?', text)\n    return text\n\ndef removeEmoticons(text):\n    \"\"\" Removes emoticons from text \"\"\"\n    text = re.sub(':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:', '', text)\n    return text\n\ndef removeNewLines(text):\n    text = re.sub('\\n', '', text)\n    return text\n\ndef preprocess_sentence(s):\n    return removeNewLines(replaceAtUser(removeEmoticons(replaceMultiQuestionMark(replaceMultiExclamationMark(removeUnicode(replaceURL(s)))))))\n\ndef preprocess_french_sentence(s):\n    return removeNewLines(replaceAtUser(removeEmoticons(replaceMultiQuestionMark(replaceMultiExclamationMark(replaceURL(s))))))\n\ndef SBERT_embed(s_list, language):\n    '''\n    Use Sentence-BERT to embed sentences.\n    s_list: a list of sentences/ tokens to be embedded.\n    language: the language of the sentences ('English', 'French', 'Arabic').\n    output: the embeddings of the sentences/ tokens.\n    '''\n    # Model paths or names for each language\n    model_map = {\n        'English': '../model/model_needed/all-MiniLM-L6-v2',\n        'French': '../model/model_needed/distiluse-base-multilingual-cased-v1',\n        'Arabic': '../model/model_needed/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Default model for Hugging Face\n    hf_model_map = {\n        'English': 'sentence-transformers/all-MiniLM-L6-v2',\n        'French': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n        'Arabic': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Print language and model being used\n    print(f\"Embedding sentences in language: {language}\")\n    \n    # Determine model path\n    model_path = model_map.get(language)\n    if not model_path:\n        raise ValueError(f\"Unsupported language: {language}. Supported languages are: {', '.join(model_map.keys())}\")\n\n    print(f\"Using model: {model_path}\")\n\n    # Load the model, downloading if necessary\n    try:\n        model = SentenceTransformer(model_path)\n        print(f\"Successfully loaded model from local path: {model_path}\")\n    except Exception as e:\n        print(f\"Model {model_path} not found locally. Attempting to download from Hugging Face...\")\n        model = SentenceTransformer(hf_model_map[language])\n        print(f\"Model downloaded from Hugging Face: {hf_model_map[language]}\")\n\n    # Compute embeddings\n    embeddings = model.encode(s_list, convert_to_tensor=True, normalize_embeddings=True)\n    print(f\"Computed embeddings for {len(s_list)} sentences/tokens.\")\n    \n    return embeddings.cpu()\n\ndef evaluate_labels(labels_true, labels_pred):\n    nmi = metrics.normalized_mutual_info_score(labels_true, labels_pred)\n    ami = metrics.adjusted_mutual_info_score(labels_true, labels_pred)\n    ari = metrics.adjusted_rand_score(labels_true, labels_pred)\n    return nmi, ami, ari\n\ndef decode(division):\n    if type(division) is dict:\n        prediction_dict = {m: event for event, messages in division.items() for m in messages}\n    elif type(division) is list:\n        prediction_dict = {m: event for event, messages in enumerate(division) for m in messages}\n    prediction_dict_sorted = dict(sorted(prediction_dict.items()))\n    return list(prediction_dict_sorted.values())\n"}
{"type": "source_file", "path": "SocialED/detector/qsgnn.py", "content": "import argparse\nimport json\nimport numpy as np\nimport os\nfrom time import strftime, localtime, time\nimport torch\nfrom scipy import sparse\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom datetime import datetime\nimport spacy\nimport networkx as nx\nimport dgl\nfrom dgl.data.utils import save_graphs, load_graphs\nimport pickle\nfrom collections import Counter\nimport torch.optim as optim\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom itertools import combinations\nimport re\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import Event2012,Arabic_Twitter,Event2018\n\n\n\n\n\nclass QSGNN:\n    r\"\"\"The QSGNN model for social event detection that uses a query-based streaming graph neural network\n    for event detection.\n\n    .. note::\n        This detector uses graph neural networks with query-based streaming to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    finetune_epochs : int, optional\n        Number of fine-tuning epochs. Default: ``1``.\n    n_epochs : int, optional\n        Number of training epochs. Default: ``5``.\n    oldnum : int, optional\n        Number of old classes. Default: ``20``.\n    novelnum : int, optional\n        Number of novel classes. Default: ``20``.\n    n_infer_epochs : int, optional\n        Number of inference epochs. Default: ``0``.\n    window_size : int, optional\n        Size of sliding window. Default: ``3``.\n    patience : int, optional\n        Early stopping patience. Default: ``5``.\n    margin : float, optional\n        Margin for triplet loss. Default: ``3.0``.\n    a : float, optional\n        Scaling factor. Default: ``8.0``.\n    lr : float, optional\n        Learning rate for optimizer. Default: ``1e-3``.\n    batch_size : int, optional\n        Batch size for training. Default: ``1000``.\n    n_neighbors : int, optional\n        Number of neighbors to sample. Default: ``1200``.\n    word_embedding_dim : int, optional\n        Word embedding dimension. Default: ``300``.\n    hidden_dim : int, optional\n        Hidden layer dimension. Default: ``16``.\n    out_dim : int, optional\n        Output dimension. Default: ``64``.\n    num_heads : int, optional\n        Number of attention heads. Default: ``4``.\n    use_residual : bool, optional\n        Whether to use residual connections. Default: ``True``.\n    validation_percent : float, optional\n        Percentage of data for validation. Default: ``0.1``.\n    test_percent : float, optional\n        Percentage of data for testing. Default: ``0.2``.\n    use_hardest_neg : bool, optional\n        Whether to use hardest negative mining. Default: ``True``.\n    metrics : str, optional\n        Evaluation metric to use. Default: ``'nmi'``.\n    use_cuda : bool, optional\n        Whether to use GPU acceleration. Default: ``True``.\n    add_ort : bool, optional\n        Whether to add orthogonal regularization. Default: ``True``.\n    gpuid : int, optional\n        GPU device ID to use. Default: ``0``.\n    mask_path : str, optional\n        Path to mask file. Default: ``None``.\n    log_interval : int, optional\n        Number of steps between logging. Default: ``10``.\n    is_incremental : bool, optional\n        Whether to use incremental learning. Default: ``True``.\n    data_path : str, optional\n        Path to save model data. Default: ``'../model/model_saved/qsgnn/English'``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/qsgnn'``.\n    add_pair : bool, optional\n        Whether to add pair-wise constraints. Default: ``False``.\n    initial_lang : str, optional\n        Initial language for processing. Default: ``'English'``.\n    is_static : bool, optional\n        Whether to use static graph. Default: ``False``.\n    graph_lang : str, optional\n        Language for graph construction. Default: ``'English'``.\n    days : int, optional\n        Number of days for temporal window. Default: ``2``.\n    \"\"\"\n    \n    def __init__(\n        self,\n        dataset,\n        finetune_epochs=1,\n        n_epochs=5,\n        oldnum=20,\n        novelnum=20,\n        n_infer_epochs=0,\n        window_size=3,\n        patience=5,\n        margin=3.0,\n        a=8.0,\n        lr=1e-3,\n        batch_size=1000,\n        n_neighbors=1200,\n        word_embedding_dim=300,\n        hidden_dim=16,\n        out_dim=64,\n        num_heads=4,\n        use_residual=True,\n        validation_percent=0.1,\n        test_percent=0.2,\n        use_hardest_neg=True,\n        metrics='nmi',\n        use_cuda=True,\n        add_ort=True,\n        gpuid=0,\n        mask_path=None,\n        log_interval=10,\n        is_incremental=True,\n        data_path='../model/model_saved/qsgnn/English',\n        file_path='../model/model_saved/qsgnn',\n        add_pair=False,\n        initial_lang='English',\n        is_static=False,\n        graph_lang='English',\n        days=2\n    ):\n        # 将参数赋值给 self\n        self.finetune_epochs = finetune_epochs\n        self.n_epochs = n_epochs\n        self.oldnum = oldnum\n        self.novelnum = novelnum\n        self.n_infer_epochs = n_infer_epochs\n        self.window_size = window_size\n        self.patience = patience\n        self.margin = margin\n        self.a = a\n        self.lr = lr\n        self.batch_size = batch_size\n        self.n_neighbors = n_neighbors\n        self.word_embedding_dim = word_embedding_dim\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n        self.num_heads = num_heads\n        self.use_residual = use_residual\n        self.validation_percent = validation_percent\n        self.test_percent = test_percent\n        self.use_hardest_neg = use_hardest_neg\n        self.metrics = metrics\n        self.use_cuda = use_cuda\n        self.add_ort = add_ort\n        self.gpuid = gpuid\n        self.mask_path = mask_path\n        self.log_interval = log_interval\n        self.is_incremental = is_incremental\n        self.data_path = data_path\n        self.file_path = file_path\n        self.add_pair = add_pair\n        self.initial_lang = initial_lang\n        self.is_static = is_static\n        self.graph_lang = graph_lang\n        self.days = days\n        \n        self.dataset = dataset.load_data()\n        if self.use_cuda:\n            torch.cuda.set_device(self.gpuid)\n\n        self.data_split = None\n        \n\n    def preprocess(self):\n        args=self\n        preprocessor = Preprocessor(self.dataset)\n        preprocessor.generate_initial_features(self.dataset)\n        preprocessor.construct_graph()\n\n        self.embedding_save_path = self.data_path + '/embeddings'\n        os.makedirs(self.embedding_save_path, exist_ok=True)\n        # with open(self.embedding_save_path + '/args.txt', 'w') as f:\n        #     json.dump(self.__dict__, f, indent=2)\n        self.data_split = np.load(self.data_path + '/data_split.npy')\n\n    def fit(self):\n        args=self\n        if self.use_hardest_neg:\n            loss_fn = OnlineTripletLoss(self.margin, HardestNegativeTripletSelector(self.margin))\n        else:\n            loss_fn = OnlineTripletLoss(self.margin, RandomNegativeTripletSelector(self.margin))\n        metrics = [AverageNonzeroTripletsMetric()]\n\n        if self.add_pair:\n            self.model = GAT(302, self.hidden_dim, self.out_dim, self.num_heads, self.use_residual)\n            best_model_path = self.embedding_save_path + '/block_0/models/best.pt'\n            label_center_emb = torch.load(self.embedding_save_path + '/block_0/models/center.pth')\n            self.model.load_state_dict(torch.load(best_model_path))\n\n            if self.use_cuda:\n                self.model.cuda()\n\n            if self.is_incremental:\n                kmeans_scores = []\n                for i in range(1, self.data_split.shape[0]):\n                    print(\"incremental setting\")\n                    print(\"enter i \", str(i))\n                    _, score = continue_train(i, self.data_split, metrics, self.embedding_save_path, loss_fn,\n                                              self.model, label_center_emb, args)\n                    kmeans_scores.append(score)\n                    print(\"KMeans:\")\n                    print_scores(kmeans_scores)\n                print(self.finetune_epochs, self.oldnum, self.novelnum, self.a,\n                      self.batch_size, end=\"\\n\\n\")\n        else:\n            self.model = initial_train(0, self, self.data_split, metrics, self.embedding_save_path, loss_fn, None)\n        print(\"fit:\", type(self.model))\n\n        torch.save(self.model.state_dict(), self.embedding_save_path + '/final_model.pth')\n\n    def detection(self):\n        data = SocialDataset(self.data_path, 0)\n        features = torch.FloatTensor(data.features)\n        labels = torch.LongTensor(data.labels)\n        in_feats = features.shape[1]  # feature dimension\n\n        g = dgl.DGLGraph(data.matrix, readonly=True)\n        g.set_n_initializer(dgl.init.zero_initializer)\n        g.readonly(readonly_state=True)\n\n        predictions = []\n        ground_truths = []\n        self.detection_path = self.file_path + '/detection_split/'\n        os.makedirs(self.detection_path, exist_ok=True)\n\n        self.model = GAT(in_feats, self.hidden_dim, self.out_dim, self.num_heads, self.use_residual)\n        best_model_path = self.embedding_save_path + '/block_0/models/best.pt'\n        self.model.load_state_dict(torch.load(best_model_path))\n\n        train_indices, validation_indices, test_indices = generateMasks(len(labels), self.data_split, 0,\n                                                                        self.validation_percent,\n                                                                        self.test_percent,\n                                                                        self.detection_path)\n\n        device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n        if args.use_cuda:\n            self.model.cuda()  # 转移模型到cuda\n            print(\"Model moved to CUDA.\")\n            g = g.to(device)\n            features, labels = features.cuda(), labels.cuda()\n            test_indices = test_indices.cuda()\n\n        g.ndata['features'] = features\n        g.ndata['labels'] = labels\n        print(\"detection:\", type(self.model))\n\n        '''\n        print(\"detection:\", type(self.model))\n        self.model = GAT(302, self.hidden_dim, self.out_dim, self.num_heads, self.use_residual)\n        self.model.load_state_dict(torch.load(self.embedding_save_path + '/final_model.pth'))\n        print(\"detection2:\", type(self.model))\n        '''\n\n        extract_features, extract_labels = extract_embeddings(g, self.model, len(labels), self)\n\n        test_indices = torch.load(self.detection_path + '/test_indices.pt')\n\n        labels_true = extract_labels[test_indices]\n        # Extract features\n        X = extract_features[test_indices, :]\n        assert labels_true.shape[0] == X.shape[0]\n        n_test_tweets = X.shape[0]\n\n        # Get the total number of classes\n        n_classes = len(set(list(labels_true)))\n\n        # kmeans clustering\n        kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n        predictions = kmeans.labels_\n        ground_truths = labels_true\n\n        return predictions, ground_truths\n\n    def evaluate(self, predictions, ground_truths):\n        ars = metrics.adjusted_rand_score(ground_truths, predictions)\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n\n        print(f\"Model Adjusted Rand Index (ARI): {ars}\")\n        print(f\"Model Adjusted Mutual Information (AMI): {ami}\")\n        print(f\"Model Normalized Mutual Information (NMI): {nmi}\")\n        return ars, ami, nmi\n\n\nclass Preprocessor(QSGNN):\n    def __init__(self,dataser):\n        super().__init__(dataset=dataset)\n\n    def generate_initial_features(self,dataset):\n        args=self\n        save_path = args.file_path + '/features/'\n        os.makedirs(save_path, exist_ok=True)\n\n        df = dataset\n        print(type(df))\n        print(\"Loaded {} data  shape {}\".format(args.initial_lang, df.shape))\n        print(df.head(10))\n\n        t_features = self.df_to_t_features(df)\n        print(\"Time features generated.\")\n        d_features = self.documents_to_features(df, args.initial_lang)\n        print(\"Original document features generated\")\n\n        combined_features = np.concatenate((d_features, t_features), axis=1)\n        print(\"Concatenated document features and time features.\")\n        np.save(save_path + 'features_69612_0709_spacy_lg_zero_multiclasses_filtered_{}.npy'.format(args.initial_lang),\n                combined_features)\n\n    def documents_to_features(self, df, initial_lang):\n        if initial_lang == \"French\":\n            nlp = spacy.load('fr_core_news_lg')\n        elif initial_lang == \"Arabic\":\n            nlp = spacy.load('spacy.arabic.model')\n            nlp.tokenizer = Arabic_preprocessor(nlp.tokenizer)\n        elif initial_lang == \"English\":\n            nlp = spacy.load('en_core_web_lg')\n        else:\n            print(\"not have that language!\")\n            return None\n\n        features = df.filtered_words.apply(lambda x: nlp(' '.join(x)).vector if len(x) != 0 else nlp(' ').vector).values\n        print(features)\n        return np.stack(features, axis=0)\n\n    def extract_time_feature(self, t_str):\n        t = datetime.fromisoformat(str(t_str))\n        OLE_TIME_ZERO = datetime(1899, 12, 30)\n        delta = t - OLE_TIME_ZERO\n        return [(float(delta.days) / 100000.), (float(delta.seconds) / 86400)]  # 86,400 seconds in day\n\n    def df_to_t_features(self, df):\n        t_features = np.asarray([self.extract_time_feature(t_str) for t_str in df['created_at']])\n        return t_features\n\n    def construct_graph(self):\n        args=self\n        if args.is_static:\n            save_path = \"../model/model_saved/qsgnn/hash_static-{}-{}/\".format(str(args.days), args.graph_lang)\n        else:\n            save_path = \"../model/model_saved/qsgnn/{}/\".format(args.graph_lang)\n\n        if not os.path.exists(save_path):\n            os.mkdir(save_path)\n\n        if args.graph_lang == \"French\":\n             df = Event2018().load_data()\n        elif args.graph_lang == \"Arabic\":\n            df = Arabic_Twitter().load_data()\n            name2id = {}\n            for id, name in enumerate(df['event_id'].unique()):\n                name2id[name] = id\n            print(name2id)\n            df['event_id'] = df['event_id'].apply(lambda x: name2id[x])\n            df.drop_duplicates(['tweet_id'], inplace=True, keep='first')\n\n        elif args.graph_lang == \"English\":\n            df = Event2012().load_data()\n\n        print(\"{} Data converted to dataframe.\".format(args.graph_lang))\n        df = df.sort_values(by='created_at').reset_index()\n\n        df['created_at'] = pd.to_datetime(df['created_at'])\n        df['date'] = [d.date() for d in df['created_at']]\n\n        f = np.load(args.file_path + '/features/features_69612_0709_spacy_lg_zero_multiclasses_filtered_{}.npy'.format(\n            args.graph_lang))\n\n        message, data_split, all_graph_mins = self.construct_incremental_dataset(args, df, save_path, f, False)\n        with open(save_path + \"node_edge_statistics.txt\", \"w\") as text_file:\n            text_file.write(message)\n        np.save(save_path + 'data_split.npy', np.asarray(data_split))\n        print(\"Data split: \", data_split)\n        np.save(save_path + 'all_graph_mins.npy', np.asarray(all_graph_mins))\n        print(\"Time spent on heterogeneous -> homogeneous graph conversions: \", all_graph_mins)\n\n    def construct_graph_from_df(self, df, G=None):\n        if G is None:\n            G = nx.Graph()\n        for _, row in df.iterrows():\n            tid = 't_' + str(row['tweet_id'])\n            G.add_node(tid)\n            G.nodes[tid]['tweet_id'] = True  # right-hand side value is irrelevant for the lookup\n\n            user_ids = row['user_mentions']\n            user_ids.append(row['user_id'])\n            user_ids = ['u_' + str(each) for each in user_ids]\n            G.add_nodes_from(user_ids)\n            for each in user_ids:\n                G.nodes[each]['user_id'] = True\n\n            entities = row['entities']\n            G.add_nodes_from(entities)\n            for each in entities:\n                G.nodes[each]['entity'] = True\n\n            hashtags = row['hashtags']\n            G.add_nodes_from(hashtags)\n            for each in hashtags:\n                G.nodes[each]['hashtag'] = True\n\n            edges = []\n            edges += [(tid, each) for each in user_ids]\n            edges += [(tid, each) for each in entities]\n            edges += [(tid, each) for each in hashtags]\n            G.add_edges_from(edges)\n\n        return G\n\n    def networkx_to_dgl_graph(self, G, save_path=None):\n        message = ''\n        print('Start converting heterogeneous networkx graph to homogeneous dgl graph.')\n        message += 'Start converting heterogeneous networkx graph to homogeneous dgl graph.\\n'\n        all_start = time()\n\n        print('\\tGetting a list of all nodes ...')\n        message += '\\tGetting a list of all nodes ...\\n'\n        start = time()\n        all_nodes = list(G.nodes)\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\tGetting adjacency matrix ...')\n        message += '\\tGetting adjacency matrix ...\\n'\n        start = time()\n        A = nx.to_numpy_array(G)  # Returns the graph adjacency matrix as a NumPy matrix.\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\tGetting lists of nodes of various types ...')\n        message += '\\tGetting lists of nodes of various types ...\\n'\n        start = time()\n        tid_nodes = list(nx.get_node_attributes(G, 'tweet_id').keys())\n        userid_nodes = list(nx.get_node_attributes(G, 'user_id').keys())\n        hash_nodes = list(nx.get_node_attributes(G, 'hashtag').keys())\n        entity_nodes = list(nx.get_node_attributes(G, 'entity').keys())\n        del G\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\tConverting node lists to index lists ...')\n        message += '\\tConverting node lists to index lists ...\\n'\n        start = time()\n        indices_tid = [all_nodes.index(x) for x in tid_nodes]\n        indices_userid = [all_nodes.index(x) for x in userid_nodes]\n        indices_hashtag = [all_nodes.index(x) for x in hash_nodes]\n        indices_entity = [all_nodes.index(x) for x in entity_nodes]\n        del tid_nodes\n        del userid_nodes\n        del hash_nodes\n        del entity_nodes\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\tStart constructing tweet-user-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-user matrix ...')\n        message += '\\tStart constructing tweet-user-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-user matrix ...\\n'\n        start = time()\n        w_tid_userid = A[np.ix_(indices_tid, indices_userid)]\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_userid = sparse.csr_matrix(w_tid_userid)\n        del w_tid_userid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_userid_tid = s_w_tid_userid.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-user * user-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-user * user-tweet ...\\n'\n        start = time()\n        s_m_tid_userid_tid = s_w_tid_userid * s_w_userid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_userid_tid.npz\", s_m_tid_userid_tid)\n            print(\"Sparse binary userid commuting matrix saved.\")\n            del s_m_tid_userid_tid\n        del s_w_tid_userid\n        del s_w_userid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\tStart constructing tweet-ent-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-ent matrix ...')\n        message += '\\tStart constructing tweet-ent-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-ent matrix ...\\n'\n        start = time()\n        w_tid_entity = A[np.ix_(indices_tid, indices_entity)]\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_entity = sparse.csr_matrix(w_tid_entity)\n        del w_tid_entity\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_entity_tid = s_w_tid_entity.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-ent * ent-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-ent * ent-tweet ...\\n'\n        start = time()\n        s_m_tid_entity_tid = s_w_tid_entity * s_w_entity_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_entity_tid.npz\", s_m_tid_entity_tid)\n            print(\"Sparse binary entity commuting matrix saved.\")\n            del s_m_tid_entity_tid\n        del s_w_tid_entity\n        del s_w_entity_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\tStart constructing tweet-word-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-word matrix ...')\n        message += '\\tStart constructing tweet-word-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-word matrix ...\\n'\n        start = time()\n        w_tid_word = A[np.ix_(indices_tid, indices_hashtag)]\n        del A\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_word = sparse.csr_matrix(w_tid_word)\n        del w_tid_word\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_word_tid = s_w_tid_word.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-word * word-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-word * word-tweet ...\\n'\n        start = time()\n        s_m_tid_word_tid = s_w_tid_word * s_w_word_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_word_tid.npz\", s_m_tid_word_tid)\n            print(\"Sparse binary word commuting matrix saved.\")\n            del s_m_tid_word_tid\n        del s_w_tid_word\n        del s_w_word_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n\n        print('\\tComputing tweet-tweet adjacency matrix ...')\n        message += '\\tComputing tweet-tweet adjacency matrix ...\\n'\n        start = time()\n        if save_path is not None:\n            s_m_tid_userid_tid = sparse.load_npz(save_path + \"s_m_tid_userid_tid.npz\")\n            print(\"Sparse binary userid commuting matrix loaded.\")\n            s_m_tid_entity_tid = sparse.load_npz(save_path + \"s_m_tid_entity_tid.npz\")\n            print(\"Sparse binary entity commuting matrix loaded.\")\n            s_m_tid_word_tid = sparse.load_npz(save_path + \"s_m_tid_word_tid.npz\")\n            print(\"Sparse binary word commuting matrix loaded.\")\n\n        s_A_tid_tid = s_m_tid_userid_tid + s_m_tid_entity_tid\n        del s_m_tid_userid_tid\n        del s_m_tid_entity_tid\n        s_bool_A_tid_tid = (s_A_tid_tid + s_m_tid_word_tid).astype('bool')\n        del s_m_tid_word_tid\n        del s_A_tid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: ' + str(mins) + ' mins\\n'\n        all_mins = (time() - all_start) / 60\n        print('\\tOver all time elapsed: ', all_mins, ' mins\\n')\n        message += '\\tOver all time elapsed: ' + str(all_mins) + ' mins\\n'\n\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_bool_A_tid_tid.npz\", s_bool_A_tid_tid)\n            print(\"Sparse binary adjacency matrix saved.\")\n            s_bool_A_tid_tid = sparse.load_npz(save_path + \"s_bool_A_tid_tid.npz\")\n            print(\"Sparse binary adjacency matrix loaded.\")\n\n        G = dgl.DGLGraph(s_bool_A_tid_tid)\n        print('We have %d nodes.' % G.number_of_nodes())\n        print('We have %d edges.' % G.number_of_edges())\n        message += 'We have ' + str(G.number_of_nodes()) + ' nodes. We have ' + str(G.number_of_edges()) + ' edges.\\n'\n\n        return all_mins, message\n\n    def construct_incremental_dataset(self, args, df, save_path, features, test=False):\n        data_split = []\n        all_graph_mins = []\n        message = \"\"\n        distinct_dates = df.date.unique()\n        print(\"Number of distinct dates: \", len(distinct_dates))\n        message += \"Number of distinct dates: \" + str(len(distinct_dates)) + \"\\n\"\n        print(\"Start constructing initial graph ...\")\n        message += \"\\nStart constructing initial graph ...\\n\"\n\n        if args.is_static:\n            ini_df = df.loc[df['date'].isin(distinct_dates[:args.days])]\n        else:\n            ini_df = df.loc[df['date'].isin(distinct_dates[:7])]\n\n        path = save_path + '0/'\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n        y = ini_df['event_id'].values\n        y = [int(each) for each in y]\n        np.save(path + 'labels.npy', np.asarray(y))\n\n        G = self.construct_graph_from_df(ini_df)\n        grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n        message += graph_message\n        print(\"Initial graph saved\")\n        message += \"Initial graph saved\\n\"\n        data_split.append(ini_df.shape[0])\n        all_graph_mins.append(grap_mins)\n        y = ini_df['event_id'].values\n        y = [int(each) for each in y]\n        np.save(path + 'labels.npy', np.asarray(y))\n        np.save(path + 'df.npy', ini_df)\n        print(\"Labels saved.\")\n        message += \"Labels saved.\\n\"\n        indices = ini_df['index'].values.tolist()\n        x = features[indices, :]\n        np.save(path + 'features.npy', x)\n        print(\"Features saved.\")\n        message += \"Features saved.\\n\\n\"\n\n        if not args.is_static:\n            inidays = 7\n            j = 6\n            for i in range(inidays, len(distinct_dates)):\n                print(\"Start constructing graph \", str(i - j), \" ...\")\n                message += \"\\nStart constructing graph \" + str(i - j) + \" ...\\n\"\n                incr_df = df.loc[df['date'] == distinct_dates[i]]\n                path = save_path + str(i - j) + '/'\n                if not os.path.exists(path):\n                    os.mkdir(path)\n                np.save(path + \"/\" + \"dataframe.npy\", incr_df)\n\n                G = self.construct_graph_from_df(incr_df)\n                grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n                message += graph_message\n                print(\"Graph \", str(i - j), \" saved\")\n                message += \"Graph \" + str(i - j) + \" saved\\n\"\n                data_split.append(incr_df.shape[0])\n                all_graph_mins.append(grap_mins)\n                y = [int(each) for each in incr_df['event_id'].values]\n                np.save(path + 'labels.npy', y)\n                print(\"Labels saved.\")\n                message += \"Labels saved.\\n\"\n                indices = incr_df['index'].values.tolist()\n                x = features[indices, :]\n                np.save(path + 'features.npy', x)\n                np.save(path + 'df.npy', incr_df)\n                print(\"Features saved.\")\n                message += \"Features saved.\\n\"\n        return message, data_split, all_graph_mins\n\n\nclass SocialDataset(Dataset):\n    def __init__(self, path, index):\n        self.features = np.load(path + '/' + str(index) + '/features.npy')\n        temp = np.load(path + '/' + str(index) + '/labels.npy', allow_pickle=True)\n        self.labels = np.asarray([int(each) for each in temp])\n        self.matrix = self.load_adj_matrix(path, index)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n    def load_adj_matrix(self, path, index):\n        s_bool_A_tid_tid = sparse.load_npz(path + '/' + str(index) + '/s_bool_A_tid_tid.npz')\n        print(\"Sparse binary adjacency matrix loaded.\")\n        return s_bool_A_tid_tid\n\n    # Used by remove_obsolete mode 1\n    def remove_obsolete_nodes(self, indices_to_remove=None):  # indices_to_remove: list\n        # torch.range(0, (self.labels.shape[0] - 1), dtype=torch.long)\n        if indices_to_remove is not None:\n            all_indices = np.arange(0, self.labels.shape[0]).tolist()\n            indices_to_keep = list(set(all_indices) - set(indices_to_remove))\n            self.features = self.features[indices_to_keep, :]\n            self.labels = self.labels[indices_to_keep]\n            self.matrix = self.matrix[indices_to_keep, :]\n            self.matrix = self.matrix[:, indices_to_keep]\n\n\nclass Arabic_preprocessor:\n    def __init__(self, tokenizer, **cfg):\n        self.tokenizer = tokenizer\n\n    def clean_text(self, text):\n        search = [\"أ\", \"إ\", \"آ\", \"ة\", \"_\", \"-\", \"/\", \".\", \"،\", \" و \", \" يا \", '\"', \"ـ\", \"'\", \"ى\", \"\\\\\", '\\n', '\\t',\n                  '&quot;', '?', '؟', '!']\n        replace = [\"ا\", \"ا\", \"ا\", \"ه\", \" \", \" \", \"\", \"\", \"\", \" و\", \" يا\", \"\", \"\", \"\", \"ي\", \"\", ' ', ' ', ' ', ' ? ',\n                   ' ؟ ', ' ! ']\n\n        # remove tashkeel\n        p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n        text = re.sub(p_tashkeel, \"\", text)\n\n        # remove longation\n        p_longation = re.compile(r'(.)\\1+')\n        subst = r\"\\1\\1\"\n        text = re.sub(p_longation, subst, text)\n\n        text = text.replace('وو', 'و')\n        text = text.replace('يي', 'ي')\n        text = text.replace('اا', 'ا')\n\n        for i in range(len(search)):\n            text = text.replace(search[i], replace[i])\n\n        # trim    \n        text = text.strip()\n\n        return text\n\n    def __call__(self, text):\n        preprocessed = self.clean_text(text)\n        return self.tokenizer(preprocessed)\n\nclass EDNN(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, use_dropout=False):\n        super(EDNN, self).__init__()\n        self.use_dropout = use_dropout\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, out_dim)\n\n    def forward(self, x):\n        hidden = F.relu(self.fc1(x))\n        if self.use_dropout:\n            hidden = F.dropout(hidden, training=self.training)\n        out = self.fc2(hidden)\n        return out\n\n\nclass simNN(nn.Module):\n    def __init__(self, in_dim, use_dropout=False):\n        super(simNN, self).__init__()\n        self.fc = nn.Linear(in_dim, in_dim)\n\n    def forward(self, x):\n        hidden = self.fc(x)\n        out = torch.mm(hidden, x.t())\n        return out\n\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, use_residual=False):\n        super(GATLayer, self).__init__()\n        # equation (1) reference: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html\n        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n        # equation (2)\n        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n        self.use_residual = use_residual\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        \"\"\"Reinitialize learnable parameters.\"\"\"\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n\n    def edge_attention(self, edges):\n        # edge UDF for equation (2)\n        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n        a = self.attn_fc(z2)\n        return {'e': F.leaky_relu(a)}\n\n    def message_func(self, edges):\n        # message UDF for equation (3) & (4)\n        return {'z': edges.src['z'], 'e': edges.data['e']}\n\n    def reduce_func(self, nodes):\n        # reduce UDF for equation (3) & (4)\n        # equation (3)\n        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n        # equation (4)\n        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n        return {'h': h}\n\n    def forward(self, blocks, layer_id):\n        h = blocks[layer_id].srcdata['features']\n        z = self.fc(h)\n        blocks[layer_id].srcdata['z'] = z\n        z_dst = z[:blocks[layer_id].number_of_dst_nodes()]\n\n        blocks[layer_id].dstdata['z'] = z_dst\n        blocks[layer_id].apply_edges(self.edge_attention)\n        # equation (3) & (4)\n        blocks[layer_id].update_all(  # block_id – The block to run the computation.\n            self.message_func,  # Message function on the edges.\n            self.reduce_func)  # Reduce function on the node.\n\n        # nf.layers[layer_id].data.pop('z')\n        # nf.layers[layer_id + 1].data.pop('z')\n\n        if self.use_residual:\n            return z_dst + blocks[layer_id].dstdata['h']  # residual connection\n        return blocks[layer_id].dstdata['h']\n\n\nclass MultiHeadGATLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, num_heads, merge='cat', use_residual=False):\n        super(MultiHeadGATLayer, self).__init__()\n        self.heads = nn.ModuleList()\n        for i in range(num_heads):\n            self.heads.append(GATLayer(in_dim, out_dim, use_residual))\n        self.merge = merge\n\n    def forward(self, blocks, layer_id):\n        head_outs = [attn_head(blocks, layer_id) for attn_head in self.heads]\n        if self.merge == 'cat':\n            # concat on the output feature dimension (dim=1)\n            return torch.cat(head_outs, dim=1)\n        else:\n            # merge using average\n            return torch.mean(torch.stack(head_outs))\n\n\nclass GAT(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, use_residual=False):\n        super(GAT, self).__init__()\n        self.layer1 = MultiHeadGATLayer(in_dim, hidden_dim, num_heads, 'cat', use_residual)\n        # Be aware that the input dimension is hidden_dim*num_heads since\n        # multiple head outputs are concatenated together. Also, only\n        # one attention head in the output layer.\n        self.layer2 = MultiHeadGATLayer(hidden_dim * num_heads, out_dim, 1, 'cat', use_residual)\n\n    def forward(self, blocks):\n        h = self.layer1(blocks, 0)\n        h = F.elu(h)\n        # print(h.shape)\n        blocks[1].srcdata['features'] = h\n        h = self.layer2(blocks, 1)\n        # h = F.normalize(h, p=2, dim=1)\n        return h\n\n\ndef graph_statistics(G, save_path):\n    message = '\\nGraph statistics:\\n'\n\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    ave_degree = (num_edges / 2) // num_nodes\n    in_degrees = G.in_degrees()\n    isolated_nodes = torch.zeros([in_degrees.size()[0]], dtype=torch.long)\n    isolated_nodes = (in_degrees == isolated_nodes)\n    torch.save(isolated_nodes, save_path + '/isolated_nodes.pt')\n    num_isolated_nodes = torch.sum(isolated_nodes).item()\n\n    message += 'We have ' + str(num_nodes) + ' nodes.\\n'\n    message += 'We have ' + str(num_edges / 2) + ' in-edges.\\n'\n    message += 'Average degree: ' + str(ave_degree) + '\\n'\n    message += 'Number of isolated nodes: ' + str(num_isolated_nodes) + '\\n'\n    print(message)\n    with open(save_path + \"/graph_statistics.txt\", \"a\") as f:\n        f.write(message)\n\n    return num_isolated_nodes\n\n\ndef generateMasks(length, data_split, i, validation_percent=0.2, test_percent=0.2, save_path=None):\n    # verify total number of nodes\n    print(length, data_split[i])\n    assert length == data_split[i]\n    if i == 0:\n        # randomly suffle the graph indices\n        train_indices = torch.randperm(length)\n        # get total number of validation indices\n        n_validation_samples = int(length * validation_percent)\n        # sample n_validation_samples validation indices and use the rest as training indices\n        validation_indices = train_indices[:n_validation_samples]\n        n_test_samples = n_validation_samples + int(length * test_percent)\n        test_indices = train_indices[n_validation_samples:n_test_samples]\n        train_indices = train_indices[n_test_samples:]\n\n        if save_path is not None:\n            torch.save(validation_indices, save_path + '/validation_indices.pt')\n            torch.save(train_indices, save_path + '/train_indices.pt')\n            torch.save(test_indices, save_path + '/test_indices.pt')\n            validation_indices = torch.load(save_path + '/validation_indices.pt')\n            train_indices = torch.load(save_path + '/train_indices.pt')\n            test_indices = torch.load(save_path + '/test_indices.pt')\n        return train_indices, validation_indices, test_indices\n    # If is in inference(prediction) epochs, generate test indices\n    else:\n        test_indices = torch.range(0, (data_split[i] - 1), dtype=torch.long)\n        if save_path is not None:\n            torch.save(test_indices, save_path + '/test_indices.pt')\n            test_indices = torch.load(save_path + '/test_indices.pt')\n        return test_indices\n\n\ndef getdata(embedding_save_path, data_split, i, args):\n    save_path_i = embedding_save_path + '/block_' + str(i)\n    if not os.path.isdir(save_path_i):\n        os.mkdir(save_path_i)\n    # load data\n    data = SocialDataset(args.data_path, i)\n    features = torch.FloatTensor(data.features)\n    labels = torch.LongTensor(data.labels)\n    in_feats = features.shape[1]  # feature dimension\n\n    g = dgl.DGLGraph(data.matrix,\n                     readonly=True)\n    num_isolated_nodes = graph_statistics(g, save_path_i)\n    g.set_n_initializer(dgl.init.zero_initializer)\n    g.readonly(readonly_state=True)\n\n    mask_path = save_path_i + '/masks'\n    if not os.path.isdir(mask_path):\n        os.mkdir(mask_path)\n\n    if i == 0:\n        train_indices, validation_indices, test_indices = generateMasks(len(labels), data_split, i,\n                                                                        args.validation_percent,\n                                                                        args.test_percent,\n                                                                        mask_path)\n    else:\n        test_indices = generateMasks(len(labels), data_split, i, args.validation_percent,\n                                     args.test_percent,\n                                     mask_path)\n    device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n    if args.use_cuda:\n        g = g.to(device)\n        features, labels = features.cuda(), labels.cuda()\n        test_indices = test_indices.cuda()\n        if i == 0:\n            train_indices, validation_indices = train_indices.cuda(), validation_indices.cuda()\n\n    g.ndata['features'] = features\n    g.ndata['labels'] = labels\n\n    if i == 0:\n        return save_path_i, in_feats, num_isolated_nodes, g, labels, train_indices, validation_indices, test_indices\n    else:\n        return save_path_i, in_feats, num_isolated_nodes, g, labels, test_indices\n\n\ndef intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\n\ndef run_kmeans(extract_features, extract_labels, indices, args, isoPath=None):\n    # Extract the features and labels of the test tweets\n    indices = indices.cpu().detach().numpy()\n\n    if isoPath is not None:\n        # Remove isolated points\n        temp = torch.load(isoPath)\n        temp = temp.cpu().detach().numpy()\n        non_isolated_index = list(np.where(temp != 1)[0])\n        indices = intersection(indices, non_isolated_index)\n\n    # Extract labels\n    labels_true = extract_labels[indices]\n    # Extract features\n    X = extract_features[indices, :]\n    assert labels_true.shape[0] == X.shape[0]\n    n_test_tweets = X.shape[0]\n\n    # Get the total number of classes\n    n_classes = len(set(list(labels_true)))\n\n    # kmeans clustering\n    kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n    labels = kmeans.labels_\n    nmi = metrics.normalized_mutual_info_score(labels_true, labels)\n    ari = metrics.adjusted_rand_score(labels_true, labels)\n    ami = metrics.adjusted_mutual_info_score(labels_true, labels, average_method='arithmetic')\n    print(\"nmi:\", nmi, 'ami:', ami, 'ari:', ari)\n    value = nmi\n    global NMI\n    NMI = nmi\n    global AMI\n    AMI = ami\n    global ARI\n    ARI = ari\n\n    if args.metrics == 'ari':\n        print('use ari')\n        value = ari\n    if args.metrics == 'ami':\n        print('use ami')\n        value = ami\n    # Return number  of test tweets, number of classes covered by the test tweets, and kMeans cluatering NMI\n    return (n_test_tweets, n_classes, value)\n\n\ndef evaluate(extract_features, extract_labels, indices, epoch, num_isolated_nodes, save_path, args, is_validation=True):\n    message = ''\n    message += '\\nEpoch '\n    message += str(epoch + 1)\n    message += '\\n'\n\n    # with isolated nodes\n    n_tweets, n_classes, value = run_kmeans(extract_features, extract_labels, indices, args)\n    if is_validation:\n        mode = 'validation'\n    else:\n        mode = 'test'\n    message += '\\tNumber of ' + mode + ' tweets: '\n    message += str(n_tweets)\n    message += '\\n\\tNumber of classes covered by ' + mode + ' tweets: '\n    message += str(n_classes)\n    message += '\\n\\t' + mode + ' '\n    message += args.metrics + ': '\n    message += str(value)\n    if num_isolated_nodes != 0:\n        # without isolated nodes\n        message += '\\n\\tWithout isolated nodes:'\n        n_tweets, n_classes, value = run_kmeans(extract_features, extract_labels, indices, args,\n                                                save_path + '/isolated_nodes.pt')\n        message += '\\tNumber of ' + mode + ' tweets: '\n        message += str(n_tweets)\n        message += '\\n\\tNumber of classes covered by ' + mode + ' tweets: '\n        message += str(n_classes)\n        message += '\\n\\t' + mode + ' value: '\n        message += str(value)\n    message += '\\n'\n    global NMI\n    global AMI\n    global ARI\n    with open(save_path + '/evaluate.txt', 'a') as f:\n        f.write(message)\n        f.write('\\n')\n        f.write(\"NMI \" + str(NMI) + \" AMI \" + str(AMI) + ' ARI ' + str(ARI))\n    print(message)\n\n    all_value_save_path = \"/\".join(save_path.split('/')[0:-1])\n    print(all_value_save_path)\n\n    with open(all_value_save_path + '/evaluate.txt', 'a') as f:\n        f.write(\"block \" + save_path.split('/')[-1])\n        f.write(message)\n        f.write('\\n')\n        f.write(\"NMI \" + str(NMI) + \" AMI \" + str(AMI) + ' ARI ' + str(ARI) + '\\n')\n\n    return value, NMI, AMI, ARI\n\n\n# 调整batch_size之后的函数\ndef extract_embeddings(g, model, num_all_samples, args):\n    with torch.no_grad():\n        model.eval()\n        indices = torch.LongTensor(np.arange(0, num_all_samples, 1))\n        if args.use_cuda:\n            indices = indices.cuda()\n        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n        batch_size = min(args.batch_size, num_all_samples)  # 使用较小的批量大小\n\n        dataloader = dgl.dataloading.NodeDataLoader(\n            g, block_sampler=sampler,\n            batch_size=batch_size,\n            nids=indices,\n            shuffle=False,\n            drop_last=False,\n        )\n\n        all_features = []\n        all_labels = []\n\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n            blocks = [b.to(device) for b in blocks]\n\n            with torch.cuda.amp.autocast():  # 使用混合精度\n                extract_labels = blocks[-1].dstdata['labels']\n                extract_features = model(blocks)\n\n            all_features.append(extract_features.data.cpu().numpy())\n            all_labels.append(extract_labels.data.cpu().numpy())\n\n            # 清理缓存\n            del extract_features, extract_labels, blocks\n            torch.cuda.empty_cache()\n\n        extract_features = np.concatenate(all_features, axis=0)\n        extract_labels = np.concatenate(all_labels, axis=0)\n\n    return extract_features, extract_labels\n\n\ndef initial_train(i, args, data_split, metrics, embedding_save_path, loss_fn, model=None):\n    print(\"Starting initial_train function.\")\n    save_path_i, in_feats, num_isolated_nodes, g, labels, train_indices, validation_indices, test_indices = getdata(\n        embedding_save_path, data_split, i, args)\n\n    print(\"Data loaded.\")\n\n    if model is None:  # Construct the initial model\n        model = GAT(in_feats, args.hidden_dim, args.out_dim, args.num_heads, args.use_residual)\n        print(\"Model constructed.\")\n    if args.use_cuda:\n        model.cuda()\n        print(\"Model moved to CUDA.\")\n\n    # Optimizer\n    # optimizer = optim.Adam([{\"params\":model.parameters()},lr=args.lr, weight_decay=1e-4)\n    optimizer = optim.Adam([{\"params\": model.parameters(), \"lr\": args.lr, \"weight_decay\": 1e-4}])\n    print(\"Optimizer initialized.\")\n\n    # Start training\n    message = \"\\n------------ Start initial training ------------\\n\"\n    print(message)\n    with open(save_path_i + '/log.txt', 'a') as f:\n        f.write(message)\n    # record the highest validation nmi ever got for early stopping\n    best_vali_value = 1e-9\n    best_epoch = 0\n    wait = 0\n    # record validation nmi of all epochs before early stop\n    all_vali_value = []\n    # record the time spent in seconds on each batch of all training/maintaining epochs\n    seconds_train_batches = []\n    # record the time spent in mins on each epoch\n    mins_train_epochs = []\n    for epoch in range(args.n_epochs):\n        start_epoch = time()\n        losses = []\n        total_loss = 0\n        for metric in metrics:\n            metric.reset()\n\n        extract_features, extract_labels = extract_embeddings(g, model, len(labels), args)\n        label_center = {}\n        for l in set(extract_labels):\n            l_indices = np.where(extract_labels == l)[0]\n            l_feas = extract_features[l_indices]\n            l_cen = np.mean(l_feas, 0)\n            label_center[l] = l_cen\n\n        print(f\"Epoch {epoch + 1}/{args.n_epochs} - Features and labels extracted.\")\n\n        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n        # dataloader = dgl.dataloading.NodeDataLoader(\n        #     g, train_indices, sampler,\n        #     batch_size=args.batch_size,\n        #     shuffle=True,\n        #     drop_last=False,\n        #     )\n        dataloader = dgl.dataloading.NodeDataLoader(\n            g, block_sampler=sampler,\n            batch_size=args.batch_size,\n            nids=train_indices,\n            shuffle=False,\n            drop_last=False,\n        )\n\n        print(f\"Epoch {epoch + 1}/{args.n_epochs} - DataLoader initialized.\")\n\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n            blocks = [b.to(device) for b in blocks]\n            batch_labels = blocks[-1].dstdata['labels']\n\n            start_batch = time()\n            model.train()\n            # forward\n            pred = model(blocks)  # Representations of the sampled nodes (in the last layer of the NodeFlow).\n\n            print(f\"Epoch {epoch + 1}/{args.n_epochs}, Batch {batch_id + 1} - Forward pass done.\")\n\n            # 计算到中心点的距离\n            dis = torch.empty([0, 1]).cuda()\n            for l in set(batch_labels.cpu().data.numpy()):\n                label_indices = torch.where(batch_labels == l)\n                l_center = torch.FloatTensor(label_center[l]).cuda()\n                dis_l = (pred[label_indices] - l_center).pow(2).sum(1).unsqueeze(-1)\n                dis = torch.cat([dis, dis_l], 0)\n\n            if args.add_pair:\n                pairs, pair_labels, pair_matrix = pairwise_sample(pred, batch_labels)\n                if args.use_cuda:\n                    pairs = pairs.to(device)\n                    pair_matrix = pair_matrix.to(device)\n                    pair_labels = pair_labels.to(device)\n\n                pos_indices = torch.where(pair_labels > 0)\n                neg_indices = torch.where(pair_labels == 0)\n                neg_ind = torch.randint(0, neg_indices[0].shape[0], [5 * pos_indices[0].shape[0]]).to(device)\n                neg_dis = (pred[pairs[neg_indices[0][neg_ind], 0]] - pred[pairs[neg_indices[0][neg_ind], 1]]).pow(\n                    2).sum(1).unsqueeze(-1)\n                pos_dis = (pred[pairs[pos_indices[0], 0]] - pred[pairs[pos_indices[0], 1]]).pow(2).sum(1).unsqueeze(-1)\n                pos_dis = torch.cat([pos_dis] * 5, 0)\n                pairs_indices = torch.where(torch.clamp(pos_dis + args.a - neg_dis, min=0.0) > 0)\n                loss = torch.mean(torch.clamp(pos_dis + args.a - neg_dis, min=0.0)[pairs_indices[0]])\n\n                label_center_emb = torch.FloatTensor(np.array(list(label_center.values()))).cuda()\n                pred = F.normalize(pred, 2, 1)\n                pair_out = torch.mm(pred, pred.t())\n                if args.add_ort:\n                    pair_loss = (pair_matrix - pair_out).pow(2).mean()\n                    print(\"pair loss:\", loss, \"pair orthogonal loss:  \", 100 * pair_loss)\n                    loss += 100 * pair_loss\n            else:\n                # 使用 triplet loss 作为默认的损失函数\n                loss_outputs = loss_fn(pred, batch_labels)\n                loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n\n            losses.append(loss.item())\n            total_loss += loss.item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            print(f\"Epoch {epoch + 1}/{args.n_epochs}, Batch {batch_id + 1} - Backward pass and optimization done.\")\n\n            batch_seconds_spent = time() - start_batch\n            seconds_train_batches.append(batch_seconds_spent)\n            # end one batch\n\n        total_loss /= (batch_id + 1)\n        message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch + 1, args.n_epochs, total_loss)\n        for metric in metrics:\n            message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n        mins_spent = (time() - start_epoch) / 60\n        message += '\\nThis epoch took {:.2f} mins'.format(mins_spent)\n        message += '\\n'\n        print(message)\n        with open(save_path_i + '/log.txt', 'a') as f:\n            f.write(message)\n        mins_train_epochs.append(mins_spent)\n\n        extract_features, extract_labels = extract_embeddings(g, model, len(labels), args)\n        np.save(save_path_i + '/features_' + str(epoch) + '.npy', extract_features)\n        np.save(save_path_i + '/labels_' + str(epoch) + '.npy', extract_labels)\n\n        validation_value, _, _, _ = evaluate(extract_features, extract_labels, validation_indices, epoch,\n                                             num_isolated_nodes,\n                                             save_path_i, args, True)\n        all_vali_value.append(validation_value)\n\n        print(f\"Epoch {epoch + 1}/{args.n_epochs} - Validation done. Value: {validation_value}\")\n\n        # Early stop\n        if validation_value > best_vali_value:\n            best_vali_value = validation_value\n            best_epoch = epoch\n            wait = 0\n            # Save model\n            model_path = save_path_i + '/models'\n            if (epoch == 0) and (not os.path.isdir(model_path)):\n                os.mkdir(model_path)\n            p = model_path + '/best.pt'\n            torch.save(model.state_dict(), p)\n            print(f\"Epoch {epoch + 1}/{args.n_epochs} - Best model saved.\")\n\n        else:\n            wait += 1\n        if wait == args.patience:\n            print('Saved all_mins_spent')\n            print('Early stopping at epoch ', str(epoch))\n            print('Best model was at epoch ', str(best_epoch))\n            break\n        # end one epoch\n\n    # Save all validation nmi\n    np.save(save_path_i + '/all_vali_value.npy', np.asarray(all_vali_value))\n    # Save time spent on epochs\n    np.save(save_path_i + '/mins_train_epochs.npy', np.asarray(mins_train_epochs))\n    print('Saved mins_train_epochs.')\n    # Save time spent on batches\n    np.save(save_path_i + '/seconds_train_batches.npy', np.asarray(seconds_train_batches))\n    print('Saved seconds_train_batches.')\n    # Load the best model of the current block\n    best_model_path = save_path_i + '/models/best.pt'\n    model.load_state_dict(torch.load(best_model_path))\n    print(\"Best model loaded.\")\n\n    extract_features, extract_labels = extract_embeddings(g, model, len(labels), args)\n    label_center = {}\n    for l in set(extract_labels):\n        l_indices = np.where(extract_labels == l)[0]\n        l_feas = extract_features[l_indices]\n        l_cen = np.mean(l_feas, 0)\n        label_center[l] = l_cen\n    label_center_emb = torch.FloatTensor(np.array(list(label_center.values()))).cuda()\n    torch.save(label_center_emb, save_path_i + '/models/center.pth')\n    print(\"Label center embeddings saved.\")\n\n    if args.add_pair:\n        return model, label_center_emb\n    else:\n        return model\n\n\ndef continue_train(i, data_split, metrics, embedding_save_path, loss_fn, model, label_center_emb, args):\n    save_path_i, in_feats, num_isolated_nodes, g, labels, test_indices = getdata(\n        embedding_save_path, data_split, i, args)\n\n    if i % 1 != 0:\n        extract_features, extract_labels = extract_embeddings(g, model, len(labels), args)\n        # save_embeddings(extract_nids, extract_features, extract_labels, extract_train_tags, save_path_i, epoch)\n        test_value = evaluate(extract_features, extract_labels, test_indices, 0, num_isolated_nodes,\n                              save_path_i, args, True)\n        return model\n\n    else:\n        extract_features, extract_labels = extract_embeddings(g, model, len(labels), args)\n\n        _, nmi, ami, ari = evaluate(extract_features, extract_labels, test_indices, -1, num_isolated_nodes,\n                                    save_path_i, args, True)\n        score = {\"NMI\": nmi, \"AMI\": ami, \"ARI\": ari}\n        # Optimizer\n        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n\n        # Start fine tuning\n        if i < 21:\n            message = \"\\n------------ Start fine tuning ------------\\n\"\n            print(message)\n            with open(save_path_i + '/log.txt', 'a') as f:\n                f.write(message)\n\n            # record the time spent in seconds on each batch of all training/maintaining epochs\n            seconds_train_batches = []\n            # record the time spent in mins on each epoch\n            mins_train_epochs = []\n            for epoch in range(args.finetune_epochs):\n                start_epoch = time()\n                losses = []\n                total_loss = 0\n                for metric in metrics:\n                    metric.reset()\n\n                sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n                # dataloader = dgl.dataloading.NodeDataLoader(\n                #     g, test_indices, sampler,\n                #     batch_size=args.batch_size,\n                #     shuffle=True,\n                #     drop_last=False,\n                #     )\n                dataloader = dgl.dataloading.NodeDataLoader(\n                    g, block_sampler=sampler,\n                    batch_size=args.batch_size,\n                    nids=test_indices,\n                    shuffle=False,\n                    drop_last=False,\n                )\n\n                for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n                    device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n                    blocks = [b.to(device) for b in blocks]\n                    batch_labels = blocks[-1].dstdata['labels']\n\n                    start_batch = time()\n                    model.train()\n                    label_center_emb.to(device)\n\n                    # forward\n                    pred = model(blocks)  # Representations of the sampled nodes (in the last layer of the NodeFlow).\n                    pred = F.normalize(pred, 2, 1)\n                    rela_center_vec = torch.mm(pred, label_center_emb.t())\n                    rela_center_vec = F.normalize(rela_center_vec, 2, 1)\n                    entropy = torch.mul(torch.log(rela_center_vec), rela_center_vec)\n                    entropy = torch.sum(entropy, dim=1)\n                    value, old_indices = torch.topk(entropy.reshape(-1), int(entropy.shape[0] / 2), largest=True)\n                    value, novel_indices = torch.topk(entropy.reshape(-1), int(entropy.shape[0] / 2), largest=False)\n                    print(old_indices.shape, novel_indices.shape)\n                    pair_matrix = torch.mm(rela_center_vec, rela_center_vec.t())\n\n                    pairs, pair_labels, _ = pairwise_sample(F.normalize(pred, 2, 1), batch_labels)\n\n                    if args.use_cuda:\n                        pairs.cuda()\n                        pair_labels.cuda()\n                        pair_matrix.cuda()\n                        # initial_pair_matrix.cuda()\n                        model.cuda()\n\n                    neg_values, novel_neg_ind = torch.topk(pair_matrix[novel_indices],\n                                                           min(args.novelnum, pair_matrix[old_indices].size(0)), 1,\n                                                           largest=False)\n                    pos_values, novel_pos_ind = torch.topk(pair_matrix[novel_indices],\n                                                           min(args.novelnum, pair_matrix[old_indices].size(0)), 1,\n                                                           largest=True)\n                    neg_values, old_neg_ind = torch.topk(pair_matrix[old_indices],\n                                                         min(args.oldnum, pair_matrix[old_indices].size(0)), 1,\n                                                         largest=False)\n                    pos_values, old_pos_ind = torch.topk(pair_matrix[old_indices],\n                                                         min(args.oldnum, pair_matrix[old_indices].size(0)), 1,\n                                                         largest=True)\n\n                    old_row = torch.LongTensor(\n                        [[i] * min(args.oldnum, pair_matrix[old_indices].size(0)) for i in old_indices])\n                    old_row = old_row.reshape(-1).cuda()\n                    novel_row = torch.LongTensor(\n                        [[i] * min(args.novelnum, pair_matrix[old_indices].size(0)) for i in novel_indices])\n                    novel_row = novel_row.reshape(-1).cuda()\n                    row = torch.cat([old_row, novel_row])\n                    neg_ind = torch.cat([old_neg_ind.reshape(-1), novel_neg_ind.reshape(-1)])\n                    pos_ind = torch.cat([old_pos_ind.reshape(-1), novel_pos_ind.reshape(-1)])\n                    neg_distances = (pred[row] - pred[neg_ind]).pow(2).sum(1).unsqueeze(-1)\n                    pos_distances = (pred[row] - pred[pos_ind]).pow(2).sum(1).unsqueeze(-1)\n\n                    loss = torch.mean(torch.clamp(pos_distances + args.a - neg_distances, min=0.0))\n\n                    losses.append(loss.item())\n                    total_loss += loss.item()\n\n                    optimizer.zero_grad()\n                    loss.backward()\n                    optimizer.step()\n\n                    batch_seconds_spent = time() - start_batch\n                    seconds_train_batches.append(batch_seconds_spent)\n                    # end one batch\n\n                total_loss /= (batch_id + 1)\n                message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch + 1, args.finetune_epochs, total_loss)\n                mins_spent = (time() - start_epoch) / 60\n                message += '\\nThis epoch took {:.2f} mins'.format(mins_spent)\n                message += '\\n'\n                print(message)\n                with open(save_path_i + '/log.txt', 'a') as f:\n                    f.write(message)\n                mins_train_epochs.append(mins_spent)\n\n                extract_features, extract_labels = extract_embeddings(g, model, len(labels), args)\n                # save_embeddings(extract_nids, extract_features, extract_labels, extract_train_tags, save_path_i, epoch)\n                test_value, _, _, _ = evaluate(extract_features, extract_labels, test_indices, epoch,\n                                               num_isolated_nodes,\n                                               save_path_i, args, True)\n\n            # Save model\n            model_path = save_path_i + '/models'\n            if not os.path.isdir(model_path):\n                os.mkdir(model_path)\n            p = model_path + '/finetune.pt'\n            torch.save(model.state_dict(), p)\n            print('finetune model saved after epoch ', str(epoch))\n\n            # Save time spent on epochs\n            np.save(save_path_i + '/mins_train_epochs.npy', np.asarray(mins_train_epochs))\n            print('Saved mins_train_epochs.')\n            # Save time spent on batches\n            np.save(save_path_i + '/seconds_train_batches.npy', np.asarray(seconds_train_batches))\n            print('Saved seconds_train_batches.')\n\n        return model, score\n\n\nclass OnlineTripletLoss(nn.Module):\n    \"\"\"\n    Online Triplets loss\n    Takes a batch of embeddings and corresponding labels.\n    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n    triplets\n    \"\"\"\n\n    def __init__(self, margin, triplet_selector):\n        super(OnlineTripletLoss, self).__init__()\n        self.margin = margin\n        self.triplet_selector = triplet_selector\n\n    def forward(self, embeddings, target):\n        triplets = self.triplet_selector.get_triplets(embeddings, target)\n\n        if embeddings.is_cuda:\n            triplets = triplets.cuda()\n\n        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(ap_distances - an_distances + self.margin)\n\n        return losses.mean(), len(triplets)\n\n\ndef pdist(vectors):\n    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n        dim=1).view(-1, 1)\n    return distance_matrix\n\n\nclass TripletSelector:\n    \"\"\"\n    Implementation should return indices of anchors, positive and negative samples\n    return np array of shape [N_triplets x 3]\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def get_triplets(self, embeddings, labels):\n        raise NotImplementedError\n\n\nclass FunctionNegativeTripletSelector(TripletSelector):\n    \"\"\"\n    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n    Margin should match the margin used in triplet loss.\n    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n    and return a negative index for that pair\n    \"\"\"\n\n    def __init__(self, margin, negative_selection_fn, cpu=True):\n        super(FunctionNegativeTripletSelector, self).__init__()\n        self.cpu = cpu\n        self.margin = margin\n        self.negative_selection_fn = negative_selection_fn\n\n    def get_triplets(self, embeddings, labels):\n        if self.cpu:\n            embeddings = embeddings.cpu()\n        distance_matrix = pdist(embeddings)\n        distance_matrix = distance_matrix.cpu()\n\n        labels = labels.cpu().data.numpy()\n        triplets = []\n\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            negative_indices = np.where(np.logical_not(label_mask))[0]\n            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n            anchor_positives = np.array(anchor_positives)\n\n            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n                loss_values = ap_distance - distance_matrix[\n                    torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n                loss_values = loss_values.data.cpu().numpy()\n                hard_negative = self.negative_selection_fn(loss_values)\n                if hard_negative is not None:\n                    hard_negative = negative_indices[hard_negative]\n                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n\n        if len(triplets) == 0:\n            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n\n        triplets = np.array(triplets)\n\n        return torch.LongTensor(triplets)\n\n\ndef print_scores(scores):\n    line = [' ' * 4] + [f'   M{i:02d} ' for i in range(1, len(scores) + 1)]\n    print(\"\".join(line))\n\n    score_names = ['NMI', 'AMI', 'ARI']\n    for n in score_names:\n        line = [f'{n} '] + [f'  {s[n]:1.3f}' for s in scores]\n        print(\"\".join(line))\n    print('\\n', flush=True)\n\ndef random_hard_negative(loss_values):\n    hard_negatives = np.where(loss_values > 0)[0]\n    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n\n\ndef hardest_negative(loss_values):\n    hard_negative = np.argmax(loss_values)\n    return hard_negative if loss_values[hard_negative] > 0 else None\n\n\ndef HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                              negative_selection_fn=hardest_negative,\n                                                                                              cpu=cpu)\n\n\ndef RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                             negative_selection_fn=random_hard_negative,\n                                                                                             cpu=cpu)\n\n\ndef relu_evidence(y):\n    return F.relu(y)\n\n\ndef exp_evidence(y):\n    return torch.exp(torch.clamp(y, -10, 10))\n\n\ndef softplus_evidence(y):\n    return F.softplus(y)\n\n\ndef kl_divergence(alpha, num_classes, device):\n    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)\n    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n    first_term = (\n            torch.lgamma(sum_alpha)\n            - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n            + torch.lgamma(ones).sum(dim=1, keepdim=True)\n            - torch.lgamma(ones.sum(dim=1, keepdim=True))\n    )\n    second_term = (\n        (alpha - ones)\n        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n        .sum(dim=1, keepdim=True)\n    )\n    kl = first_term + second_term\n    return kl\n\n\ndef loglikelihood_loss(y, alpha, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n    loglikelihood_err = torch.sum((y - (alpha / S)) ** 2, dim=1, keepdim=True)\n    loglikelihood_var = torch.sum(\n        alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True\n    )\n    loglikelihood = loglikelihood_err + loglikelihood_var\n    return loglikelihood\n\n\ndef mse_loss(y, alpha, epoch_num, num_classes, annealing_step, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    loglikelihood = loglikelihood_loss(y, alpha, device)\n\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n    )\n\n    kl_alpha = (alpha - 1) * (1 - y) + 1\n    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n    return loglikelihood + kl_div\n\n\ndef edl_loss(func, y, alpha, epoch_num, num_classes, annealing_step, device):\n    y = y.to(device)\n    alpha = alpha.to(device)\n    S = torch.sum(alpha, dim=1, keepdim=True)\n\n    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)\n\n    annealing_coef = torch.min(\n        torch.tensor(1.0, dtype=torch.float32),\n        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n    )\n\n    kl_alpha = (alpha - 1) * (1 - y) + 1\n    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n    return A + kl_div\n\n\ndef edl_mse_loss(alpha, target, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n    loss = torch.mean(\n        mse_loss(target, alpha, epoch_num, num_classes, annealing_step, device)\n    )\n    return loss\n\n\ndef edl_log_loss(alpha, target, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n    loss = torch.mean(\n        edl_loss(\n            torch.log, target, alpha, epoch_num, num_classes, annealing_step, device\n        )\n    )\n    return loss\n\n\ndef edl_digamma_loss(\n        alpha, target, epoch_num, num_classes, annealing_step, device):\n    # evidence = relu_evidence(output)\n    # alpha = evidence + 1\n    loss = torch.mean(\n        edl_loss(\n            torch.digamma, target, alpha, epoch_num, num_classes, annealing_step, device\n        )\n    )\n    return loss\n\n\ndef pairwise_sample(embeddings, labels=None, model=None):\n    if model == None:  # labels is not None:\n        labels = labels.cpu().data.numpy()\n        indices = np.arange(0, len(labels), 1)\n        pairs = np.array(list(combinations(indices, 2)))\n        pair_labels = (labels[pairs[:, 0]] == labels[pairs[:, 1]])\n\n        pair_matrix = np.eye(len(labels))\n        ind = np.where(pair_labels)\n        pair_matrix[pairs[ind[0], 0], pairs[ind[0], 1]] = 1\n        pair_matrix[pairs[ind[0], 1], pairs[ind[0], 0]] = 1\n\n        return torch.LongTensor(pairs), torch.LongTensor(pair_labels.astype(int)), torch.LongTensor(pair_matrix)\n\n    else:\n        pair_matrix = model(embeddings)\n        return pair_matrix\n    # torch.LongTensor(pair_labels.astype(int))\n\n\nclass Metric:\n    def __init__(self):\n        pass\n\n    def __call__(self, outputs, target, loss):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def value(self):\n        raise NotImplementedError\n\n    def name(self):\n        raise NotImplementedError\n\n\nclass AccumulatedAccuracyMetric(Metric):\n    \"\"\"\n    Works with classification model\n    \"\"\"\n\n    def __init__(self):\n        self.correct = 0\n        self.total = 0\n\n    def __call__(self, outputs, target, loss):\n        pred = outputs[0].data.max(1, keepdim=True)[1]\n        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()\n        self.total += target[0].size(0)\n        return self.value()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n\n    def value(self):\n        return 100 * float(self.correct) / self.total\n\n    def name(self):\n        return 'Accuracy'\n\n\nclass AverageNonzeroTripletsMetric(Metric):\n    '''\n    Counts average number of nonzero triplets found in minibatches\n    '''\n\n    def __init__(self):\n        self.values = []\n\n    def __call__(self, outputs, target, loss):\n        self.values.append(loss[1])\n        return self.value()\n\n    def reset(self):\n        self.values = []\n\n    def value(self):\n        return np.mean(self.values)\n\n    def name(self):\n        return 'Average nonzero triplets'\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/lda.py", "content": "import argparse\nimport os\nimport logging\nimport datetime\nimport pandas as pd\nimport numpy as np\nfrom gensim.models.ldamodel import LdaModel\nfrom gensim import corpora\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import Event2012\n\nclass LDA:\n    r\"\"\"The LDA model for social event detection that uses Latent Dirichlet Allocation\n    for topic modeling and event detection.\n\n    .. note::\n        This detector uses topic modeling to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    See :cite:`blei2003latent` for details.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    num_topics : int, optional\n        Number of topics to extract. Default: ``50``.\n    passes : int, optional\n        Number of passes through corpus during training. Default: ``20``.\n    iterations : int, optional\n        Maximum number of iterations through corpus. Default: ``50``.\n    alpha : str or float, optional\n        Prior document-topic distribution. Default: ``'symmetric'``.\n    eta : float, optional\n        Prior topic-word distribution. Default: ``None``.\n    random_state : int, optional\n        Random seed for reproducibility. Default: ``1``.\n    eval_every : int, optional\n        Log perplexity evaluation frequency. Default: ``10``.\n    chunksize : int, optional\n        Number of documents per training chunk. Default: ``2000``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/LDA/'``.\n    \"\"\"\n\n    def __init__(self,\n                 dataset,\n                 num_topics=50,\n                 passes=20,\n                 iterations=50,\n                 alpha='symmetric',\n                 eta=None,\n                 random_state=1,\n                 eval_every=10,\n                 chunksize=2000,\n                 file_path='../model/model_saved/LDA/'):\n        self.dataset = dataset.load_data()\n        self.num_topics = num_topics\n        self.passes = passes\n        self.iterations = iterations\n        self.alpha = alpha\n        self.eta = eta\n        self.random_state = random_state\n        self.eval_every = eval_every\n        self.chunksize = chunksize\n        self.df = None\n        self.train_df = None\n        self.test_df = None\n        self.file_path = file_path\n        self.model_path = os.path.join(file_path, 'lda_model')\n\n    def preprocess(self):\n        \"\"\"\n        Data preprocessing: tokenization, stop words removal, etc.\n        \"\"\"\n        df = self.dataset[['filtered_words', 'event_id']].copy()\n        df['processed_text'] = df['filtered_words'].apply(\n            lambda x: [str(word).lower() for word in x] if isinstance(x, list) else [])\n        self.df = df\n        return df\n\n    def create_corpus(self, df, text_column):\n        \"\"\"\n        Create corpus and dictionary required for LDA model.\n        \"\"\"\n        texts = df[text_column].tolist()\n        dictionary = corpora.Dictionary(texts)\n        corpus = [dictionary.doc2bow(text) for text in texts]\n        return corpus, dictionary\n\n    def load_model(self):\n        \"\"\"\n        Load the LDA model from a file.\n        \"\"\"\n        logging.info(f\"Loading LDA model from {self.model_path}...\")\n        lda_model = LdaModel.load(self.model_path)\n        logging.info(\"LDA model loaded successfully.\")\n\n        self.lda_model = lda_model\n        return lda_model\n\n    def display_topics(self, num_words=10):\n        \"\"\"\n        Display topics generated by the LDA model.\n        \"\"\"\n        topics = self.lda_model.show_topics(num_words=num_words, formatted=False)\n        for i, topic in topics:\n            print(f\"Topic {i}: {[word for word, _ in topic]}\")\n\n    def fit(self):\n        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n\n        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=self.random_state)\n        self.train_df = train_df\n        self.test_df = test_df\n        train_corpus, train_dictionary = self.create_corpus(train_df, 'processed_text')\n\n        logging.info(\"Training LDA model...\")\n        lda_model = LdaModel(corpus=train_corpus, id2word=train_dictionary, num_topics=self.num_topics,\n                             passes=self.passes,\n                             iterations=self.iterations, alpha=self.alpha, eta=self.eta, random_state=self.random_state,\n                             eval_every=self.eval_every, chunksize=self.chunksize)\n        logging.info(\"LDA model trained successfully.\")\n\n        # Save the trained model to a file\n        lda_model.save(self.model_path)\n        logging.info(f\"LDA model saved to {self.model_path}\")\n\n    def detection(self):\n        \"\"\"\n        Assign topics to each document and save unique ground truths and predictions to a CSV file.\n        \"\"\"\n        self.load_model()  # Ensure the model is loaded before making detections\n        corpus, _ = self.create_corpus(self.test_df, 'processed_text')\n        topics = [self.lda_model.get_document_topics(bow) for bow in corpus]\n\n        # Get the ground truth labels and predicted labels\n        ground_truths = self.test_df['event_id'].tolist()\n        predictions = [max(topic, key=lambda x: x[1])[0] for topic in topics]\n\n        # Convert to sets to remove duplicates\n        unique_ground_truths = list(set(ground_truths))\n        unique_predictions = list(set(predictions))\n\n        # Pad the shorter list with None to make them the same length\n        max_len = max(len(unique_ground_truths), len(unique_predictions))\n        unique_ground_truths.extend([None] * (max_len - len(unique_ground_truths)))\n        unique_predictions.extend([None] * (max_len - len(unique_predictions)))\n\n        # Combine into a dataframe\n        data = {\n            'Unique Ground Truths': unique_ground_truths,\n            'Unique Predictions': unique_predictions\n        }\n        df = pd.DataFrame(data)\n\n        # Save to a CSV file\n        output_file = os.path.join(self.file_path, \"unique_ground_truths_predictions.csv\")\n        df.to_csv(output_file, index=False)\n        print(f\"Unique ground truths and predictions have been saved to {output_file}\")\n\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n        # Get the current date and time\n        current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        # Save results to a file in append mode\n        with open(self.model_path + \"_evaluation.txt\", \"a\") as f:\n            f.write(f\"Date and Time: {current_datetime}\\n\")\n            f.write(f\"Normalized Mutual Information (NMI): {nmi}\\n\")\n            f.write(f\"Adjusted Mutual Information (AMI): {ami}\\n\")\n            f.write(f\"Adjusted Rand Index (ARI): {ari}\\n\")\n            f.write(\"\\n\")  # Add a newline for better readability\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/clkd.py", "content": "from time import localtime, strftime, time\nimport torch.optim as optim\nimport torch.nn as nn\nimport json\nimport argparse\nimport torch\nimport dgl\nimport numpy as np\nimport os\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nimport torch.nn.functional as F\nfrom itertools import combinations\nimport gensim\nimport re\nimport spacy\nimport pandas as pd\nfrom datetime import datetime\nimport networkx as nx\nfrom scipy import sparse\nfrom dgl.data.utils import save_graphs, load_graphs\nimport pickle\nfrom collections import Counter\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import Event2012,Event2018,Arabic_Twitter\nfrom torch.utils.data import Dataset\n\n\nclass CLKD:\n    r\"\"\"The CLKD (Contrastive Learning with Knowledge Distillation) model for social event detection.\n\n    .. note::\n        This detector uses contrastive learning and knowledge distillation to identify events in social media data.\n        The model requires a dataset object with a load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    n_epochs : int, optional\n        Number of training epochs. Default: ``1``.\n    n_infer_epochs : int, optional \n        Number of inference epochs. Default: ``0``.\n    window_size : int, optional\n        Size of sliding window for incremental learning. Default: ``3``.\n    patience : int, optional\n        Number of epochs to wait before early stopping. Default: ``5``.\n    margin : float, optional\n        Margin for triplet loss. Default: ``3.0``.\n    lr : float, optional\n        Learning rate. Default: ``1e-3``.\n    batch_size : int, optional\n        Mini-batch size. Default: ``2000``.\n    n_neighbors : int, optional\n        Number of neighbors for graph construction. Default: ``800``.\n    word_embedding_dim : int, optional\n        Dimension of word embeddings. Default: ``300``.\n    hidden_dim : int, optional\n        Hidden layer dimension. Default: ``8``.\n    out_dim : int, optional\n        Output dimension. Default: ``32``.\n    num_heads : int, optional\n        Number of attention heads. Default: ``4``.\n    use_residual : bool, optional\n        Whether to use residual connections. Default: ``True``.\n    validation_percent : float, optional\n        Percentage of data for validation. Default: ``0.1``.\n    test_percent : float, optional\n        Percentage of data for testing. Default: ``0.2``.\n    use_hardest_neg : bool, optional\n        Whether to use hardest negative mining. Default: ``False``.\n    metrics : str, optional\n        Evaluation metric to use. Default: ``'ami'``.\n    use_cuda : bool, optional\n        Whether to use GPU acceleration. Default: ``False``.\n    gpuid : int, optional\n        ID of GPU to use. Default: ``0``.\n    mask_path : str, optional\n        Path to attention mask file. Default: ``None``.\n    log_interval : int, optional\n        Number of steps between logging. Default: ``10``.\n    is_incremental : bool, optional\n        Whether to use incremental learning. Default: ``False``.\n    mutual : bool, optional\n        Whether to use mutual learning. Default: ``False``.\n    mode : int, optional\n        Training mode. Default: ``0``.\n    add_mapping : bool, optional\n        Whether to add mapping layer. Default: ``False``.\n    data_path : str, optional\n        Path to data directory. Default: ``'../model/model_saved/clkd/English'``.\n    file_path : str, optional\n        Path to save files. Default: ``'../model/model_saved/clkd'``.\n    Tmodel_path : str, optional\n        Path to teacher model. Default: ``'../model/model_saved/clkd/English/Tmodel/'``.\n    lang : str, optional\n        Language of the data. Default: ``'French'``.\n    Tealang : str, optional\n        Language of teacher model. Default: ``'English'``.\n    t : float, optional\n        Temperature parameter. Default: ``1``.\n    data_path1 : str, optional\n        Path to first language data. Default: ``'../model/model_saved/clkd/English'``.\n    data_path2 : str, optional\n        Path to second language data. Default: ``'../model/model_saved/clkd/French'``.\n    lang1 : str, optional\n        First language. Default: ``'English'``.\n    lang2 : str, optional\n        Second language. Default: ``'French'``.\n    e : float, optional\n        Epsilon parameter. Default: ``0``.\n    mt : float, optional\n        Momentum parameter. Default: ``0.5``.\n    rd : float, optional\n        Random drop rate. Default: ``0.1``.\n    is_static : bool, optional\n        Whether to use static embeddings. Default: ``False``.\n    graph_lang : str, optional\n        Language for graph construction. Default: ``'English'``.\n    tgtlang : str, optional\n        Target language. Default: ``'French'``.\n    days : int, optional\n        Number of days for temporal window. Default: ``7``.\n    initial_lang : str, optional\n        Initial language. Default: ``'French'``.\n    TransLinear : bool, optional\n        Whether to use linear transformation. Default: ``True``.\n    tgt : str, optional\n        Target language code. Default: ``'English'``.\n    embpath : str, optional\n        Path to embedding file. Default: ``'../model/model_saved/clkd/dictrans/fr-en-for.npy'``.\n    wordpath : str, optional\n        Path to word dictionary. Default: ``'../model/model_saved/clkd/dictrans/wordsFrench.txt'``.\n    \"\"\"\n    def __init__(self, \n                 dataset,\n                 n_epochs=1,\n                 n_infer_epochs=0,\n                 window_size=3,\n                 patience=5,\n                 margin=3.0,\n                 lr=1e-3,\n                 batch_size=2000,\n                 n_neighbors=800,\n                 word_embedding_dim=300,\n                 hidden_dim=8,\n                 out_dim=32,\n                 num_heads=4,\n                 use_residual=True,\n                 validation_percent=0.1,\n                 test_percent=0.2,\n                 use_hardest_neg=False,\n                 metrics='ami',\n                 use_cuda=False,\n                 gpuid=0,\n                 mask_path=None,\n                 log_interval=10,\n                 is_incremental=False,\n                 mutual=False,\n                 mode=0,\n                 add_mapping=False,\n                 data_path='../model/model_saved/clkd/English',\n                 file_path='../model/model_saved/clkd',\n                 Tmodel_path='../model/model_saved/clkd/English/Tmodel/',\n                 lang='French',\n                 Tealang='English',\n                 t=1,\n                 data_path1='../model/model_saved/clkd/English',\n                 data_path2='../model/model_saved/clkd/French',\n                 lang1='English',\n                 lang2='French',\n                 e=0,\n                 mt=0.5,\n                 rd=0.1,\n                 is_static=False,\n                 graph_lang='English',\n                 tgtlang='French',\n                 days=7,\n                 initial_lang='French',\n                 TransLinear=True,\n                 tgt='English',\n                 embpath='../model/model_saved/clkd/dictrans/fr-en-for.npy',\n                 wordpath='../model/model_saved/clkd/dictrans/wordsFrench.txt'):\n        self.embedding_save_path1 = None\n        self.embedding_save_path2 = None\n        self.embedding_save_path = None\n        self.data_split1 = None\n        self.data_split2 = None\n        self.data_split = None\n        self.dataset = dataset\n        \n        # Store all parameters as attributes\n        self.n_epochs = n_epochs\n        self.n_infer_epochs = n_infer_epochs\n        self.window_size = window_size\n        self.patience = patience\n        self.margin = margin\n        self.lr = lr\n        self.batch_size = batch_size\n        self.n_neighbors = n_neighbors\n        self.word_embedding_dim = word_embedding_dim\n        self.hidden_dim = hidden_dim\n        self.out_dim = out_dim\n        self.num_heads = num_heads\n        self.use_residual = use_residual\n        self.validation_percent = validation_percent\n        self.test_percent = test_percent\n        self.use_hardest_neg = use_hardest_neg\n        self.metrics = metrics\n        self.use_cuda = use_cuda\n        self.gpuid = gpuid\n        self.mask_path = mask_path\n        self.log_interval = log_interval\n        self.is_incremental = is_incremental\n        self.mutual = mutual\n        self.mode = mode\n        self.add_mapping = add_mapping\n        self.data_path = data_path\n        self.file_path = file_path\n        self.Tmodel_path = Tmodel_path\n        self.lang = lang\n        self.Tealang = Tealang\n        self.t = t\n        self.data_path1 = data_path1\n        self.data_path2 = data_path2\n        self.lang1 = lang1\n        self.lang2 = lang2\n        self.e = e\n        self.mt = mt\n        self.rd = rd\n        self.is_static = is_static\n        self.graph_lang = graph_lang\n        self.tgtlang = tgtlang\n        self.days = days\n        self.initial_lang = initial_lang\n        self.TransLinear = TransLinear\n        self.tgt = tgt\n        self.embpath = embpath\n        self.wordpath = wordpath\n\n    def preprocess(self):\n        preprocessor = Preprocessor(self.args)\n        preprocessor.generate_initial_features()\n        preprocessor.construct_graph()\n\n        use_cuda = self.args.use_cuda and torch.cuda.is_available()\n        if use_cuda:\n            torch.cuda.set_device(self.args.gpuid)\n            self.device = torch.device(\"cuda:{}\".format(self.args.gpuid))\n        else:\n            self.device = torch.device('cpu')\n\n        if self.args.mutual:\n            print(\"args.mutual is true\")\n            path1 = os.path.join(self.args.data_path1, f\"{self.args.mode}mode\")\n            path2 = os.path.join(self.args.data_path2, f\"{self.args.mode}mode\")\n            os.makedirs(path1, exist_ok=True)\n            os.makedirs(path2, exist_ok=True)\n\n            timestamp = strftime(\"%m%d%H%M%S\", localtime())\n            self.embedding_save_path1 = os.path.join(path1,\n                                                     f'embeddings_{timestamp}-{self.args.mode}-{self.args.lang2}')\n            self.embedding_save_path2 = os.path.join(path2,\n                                                     f'embeddings_{timestamp}-{self.args.mode}-{self.args.lang1}')\n\n            if not self.args.add_mapping and (self.args.mode in [0, 1, 2]):\n                self.embedding_save_path1 += \"-nomap\"\n                self.embedding_save_path2 += \"-nomap\"\n            else:\n                self.embedding_save_path1 += \"-map\"\n                self.embedding_save_path2 += \"-map\"\n\n            os.makedirs(self.embedding_save_path1, exist_ok=True)\n            os.makedirs(self.embedding_save_path2, exist_ok=True)\n\n            print(\"embedding_save_path1 and embedding_save_path2: \", self.embedding_save_path1,\n                  self.embedding_save_path2)\n            with open(os.path.join(self.embedding_save_path1, 'args.txt'), 'w') as f:\n                json.dump(self.args.__dict__, f, indent=2)\n            with open(os.path.join(self.embedding_save_path2, 'args.txt'), 'w') as f:\n                json.dump(self.args.__dict__, f, indent=2)\n\n            self.data_split1 = np.load(os.path.join(self.args.data_path1, 'data_split.npy'))\n            self.data_split2 = np.load(os.path.join(self.args.data_path2, 'data_split.npy'))\n            print(\"data_split1:\", self.data_split1, 'data_split2:', self.data_split2)\n        else:\n            embedding_dir = os.path.join(self.args.data_path, f'{self.args.mode}mode')\n            os.makedirs(embedding_dir, exist_ok=True)\n\n            timestamp = strftime(\"%m%d%H%M%S\", localtime())\n            self.embedding_save_path = os.path.join(embedding_dir,\n                                                    f'embeddings_{timestamp}-{self.args.mode}-{self.args.Tealang}')\n\n            if not self.args.add_mapping and (self.args.mode in [0, 1, 2]):\n                self.embedding_save_path += \"-nomap\"\n            else:\n                self.embedding_save_path += \"-map\"\n\n            os.makedirs(self.embedding_save_path, exist_ok=True)\n\n            print(\"embedding_save_path: \", self.embedding_save_path)\n            with open(os.path.join(self.embedding_save_path, 'args.txt'), 'w') as f:\n                json.dump(self.args.__dict__, f, indent=2)\n\n            self.data_split = np.load(os.path.join(self.args.data_path, 'data_split.npy'))\n\n    def fit(self):\n        # 初始化损失函数和度量指标\n        if self.args.use_hardest_neg:\n            loss_fn = OnlineTripletLoss(self.args.margin, HardestNegativeTripletSelector(self.args.margin))\n        else:\n            loss_fn = OnlineTripletLoss(self.args.margin, RandomNegativeTripletSelector(self.args.margin))\n\n        metrics = [AverageNonzeroTripletsMetric()]\n        self.train_i = 0\n\n        if self.args.mutual:\n            self.model1, self.model2 = mutual_train(self.embedding_save_path1, self.embedding_save_path2,\n                                                    self.data_split1, self.data_split2, self.train_i, 0,\n                                                    loss_fn, metrics, self.device)\n        else:\n            self.model = initial_maintain(self.train_i, 0, self.data_split, metrics, self.embedding_save_path, loss_fn,\n                                          None)\n\n    def detection(self):\n        if self.args.use_hardest_neg:\n            loss_fn = OnlineTripletLoss(self.args.margin, HardestNegativeTripletSelector(self.args.margin))\n        else:\n            loss_fn = OnlineTripletLoss(self.args.margin, RandomNegativeTripletSelector(self.args.margin))\n\n        metrics = [AverageNonzeroTripletsMetric()]\n\n        if self.args.mutual:\n            self.model1, self.model2 = mutual_infer(self.embedding_save_path1, self.embedding_save_path2,\n                                                    self.data_split1, self.data_split2,\n                                                    self.train_i, 0, loss_fn, metrics, self.model1, self.model2,\n                                                    self.device)\n            if self.args.is_incremental:\n                for i in range(1, min(self.data_split1.shape[0], self.data_split2.shape[0])):\n                    print(\"enter i \", str(i))\n                    self.model1, self.model2 = mutual_infer(self.embedding_save_path1, self.embedding_save_path2,\n                                                            self.data_split1, self.data_split2,\n                                                            self.train_i, i, loss_fn, metrics, self.model1, self.model2,\n                                                            self.device)\n                    if i % self.args.window_size == 0:\n                        self.train_i = i\n                        self.model1, self.model2 = mutual_train(self.embedding_save_path1, self.embedding_save_path2,\n                                                                self.data_split1, self.data_split2, self.train_i, i,\n                                                                loss_fn, metrics, self.device)\n\n            data = SocialDataset(self.args.data_path, 0)\n            g = dgl.DGLGraph(data.matrix)\n            labels = torch.LongTensor(data.labels)\n\n            self.mutual_detection_path1 = self.args.file_path + '/mutual_detection_split1/'\n            os.makedirs(self.mutual_detection_path1, exist_ok=True)\n\n            train_indices, validation_indices, test_indices = generateMasks(len(labels), self.data_split,\n                                                                               self.train_i, 0,\n                                                                               0.1, 0.2, self.mutual_detection_path)\n            g.ndata['h'] = torch.tensor(data.features)\n\n            _, extract_features, extract_labels = mutual_extract_embeddings(g, self.model1, self.model2, self.args.lang1,\n                                                                self.args.lang2, len(labels), labels, self.args,\n                                                                self.device)\n\n            predictions = []\n            ground_truths = []\n            # Extract labels\n            test_indices = torch.load(self.mutual_detection_path1 + '/test_indices.pt')\n            labels_true = extract_labels[test_indices]\n            # Extract features\n            X = extract_features[test_indices, :]\n            assert labels_true.shape[0] == X.shape[0]\n            # Get the total number of classes\n            n_classes = len(set(list(labels_true)))\n            # kmeans clustering\n            kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n            predictions = kmeans.labels_\n            ground_truths = labels_true\n\n\n            return predictions, ground_truths\n\n        else:\n            self.model = initial_maintain(self.train_i, 0, self.data_split, metrics, self.embedding_save_path, loss_fn,\n                                          self.model)\n            if self.args.is_incremental:\n                for i in range(1, self.data_split.shape[0]):\n                    print(\"incremental setting\")\n                    print(\"enter i \", str(i))\n                    self.model = infer(self.train_i, i, self.data_split, metrics, self.embedding_save_path, loss_fn,\n                                       self.model)\n                    if i % self.args.window_size == 0:\n                        self.model = initial_maintain(self.train_i, i, self.data_split, metrics,\n                                                      self.embedding_save_path, loss_fn, self.model)\n\n            data = SocialDataset(self.args.data_path, 0)\n            g = dgl.DGLGraph(data.matrix)\n            labels = torch.LongTensor(data.labels)\n\n            predictions = []\n            ground_truths = []\n            self.detection_path = self.args.file_path + '/detection_split/'\n            os.makedirs(self.detection_path, exist_ok=True)\n\n            train_indices, validation_indices, test_indices = generateMasks(len(labels), self.data_split, self.train_i,\n                                                                            0,\n                                                                            0.1, 0.2, self.detection_path)\n\n            g.ndata['h'] = torch.tensor(data.features)  # Assuming data.features contains the feature data\n\n            _, extract_features, extract_labels = extract_embeddings(g, self.model, len(labels), labels, self.args,\n                                                                     self.device)\n\n            # Extract labels\n            test_indices = torch.load(self.detection_path + '/test_indices.pt')\n\n            labels_true = extract_labels[test_indices]\n            # Extract features\n            X = extract_features[test_indices, :]\n            assert labels_true.shape[0] == X.shape[0]\n            n_test_tweets = X.shape[0]\n\n            # Get the total number of classes\n            n_classes = len(set(list(labels_true)))\n\n            # kmeans clustering\n            kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n            predictions = kmeans.labels_\n            ground_truths = labels_true\n\n            return predictions, ground_truths\n\n\n    def evaluate(self, predictions, ground_truths):\n        ars = metrics.adjusted_rand_score(ground_truths, predictions)\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        return ars, ami, nmi\n\nclass Preprocessor:\n    def __init__(self, args):\n        self.device = None\n        self.args = args\n\n    def generate_initial_features(self):\n        # self.args = self.self.args\n        save_path = self.args.file_path + '/features/'\n        os.makedirs(save_path, exist_ok=True)\n        #print(self.args.initial_lang) wasd\n        if self.args.initial_lang == \"French\":\n            df = Event2018().load_data()\n        elif self.args.initial_lang == \"Arabic\":\n            df = Arabic_Twitter().load_data()\n        elif self.args.initial_lang == \"English\":\n            df = Event2012().load_data()\n        else:\n            raise NotImplementedError(\"Language not supported\")\n\n        df = df[['event_id', 'words', 'filtered_words', 'created_at']].copy()\n        print(\"Loaded {} data, shape {}\".format(self.args.initial_lang, df.shape))\n        print(df.head(10))\n\n        t_features = self.df_to_t_features(df)\n        print(\"Time features generated.\")\n        d_features = self.documents_to_features(df, self.args.initial_lang)\n        print(\"Original document features generated\")\n\n        combined_features = np.concatenate((d_features, t_features), axis=1)\n        print(\"Concatenated document features and time features.\")\n        np.save(os.path.join(save_path, 'features_69612_0709_spacy_lg_zero_multiclasses_filtered_{}.npy'.format(\n            self.args.initial_lang)),\n                combined_features)\n\n        if self.args.TransLinear:\n            dl_features = self.getlinear_transform_features(d_features, self.args.initial_lang, self.args.tgt)\n            lcombined_features = np.concatenate((dl_features, t_features), axis=1)\n            print(\"Linear transformed features generated\")\n            np.save(os.path.join(save_path, 'features_69612_0709_spacy_lg_zero_multiclasses_filtered_{}_{}.npy'.format(\n                self.args.initial_lang, self.args.tgt)),\n                    lcombined_features)\n\n\n    def documents_to_features(self, df, initial_lang):\n        if initial_lang == \"French\":\n            nlp = spacy.load(\"fr_core_news_lg\")\n        elif initial_lang == \"Arabic\":\n            nlp = spacy.load('spacy.arabic.model')\n            nlp.tokenizer = Arabic_preprocessor(nlp.tokenizer)\n        elif initial_lang == \"English\":\n            nlp = spacy.load(\"en_core_web_lg\")\n        else:\n            raise ValueError(\"Language not supported\")\n\n        features = df.filtered_words.apply(lambda x: nlp(' '.join(x)).vector if len(x) != 0 else nlp(' ').vector).values\n        return np.stack(features, axis=0)\n\n    def get_word2id_emb(self, wordpath, embpath):\n        word2id = {}\n        with open(wordpath, 'r') as f:\n            for i, w in enumerate(list(f.readlines()[0].split())):\n                word2id[w] = i\n        embeddings = np.load(embpath)\n        return word2id, embeddings\n\n    def nonlinear_transform_features(self, wordpath, embpath, df):\n        word2id, embeddings = self.get_word2id_emb(wordpath, embpath)\n        features = df.filtered_words.apply(lambda x: [embeddings[word2id[w]] for w in x if w in word2id])\n        f_list = []\n        for f in features:\n            if len(f) != 0:\n                f_list.append(np.mean(f, axis=0))\n            else:\n                f_list.append(np.zeros((300,)))\n        return np.stack(f_list, axis=0)\n\n    def getlinear_transform_features(self, features, src, tgt):\n        W = torch.load(self.args.file_path + \"/LinearTranWeight/spacy_{}_{}/best_mapping.pth\".format(src, tgt))\n        return np.matmul(features, W)\n\n    def extract_time_feature(self, t_str):\n        t = datetime.fromisoformat(str(t_str))\n        OLE_TIME_ZERO = datetime(1899, 12, 30)\n        delta = t - OLE_TIME_ZERO\n        return [(float(delta.days) / 100000.), (float(delta.seconds) / 86400)]\n\n    def df_to_t_features(self, df):\n        return np.asarray([self.extract_time_feature(t_str) for t_str in df['created_at']])\n\n    def construct_graph(self):\n        # create save path\n        if self.args.is_static:\n            save_path = self.args.file_path + \"/hash_static-{}-{}/\".format(str(self.args.days), self.args.graph_lang)\n        else:\n            save_path = self.args.file_path + \"/{}/\".format(self.args.graph_lang)\n\n        os.makedirs(save_path, exist_ok=True)\n\n        # load df data\n        if self.args.graph_lang == \"French\":\n            df = Event2018().load_data()\n        elif self.args.graph_lang == \"Arabic\":\n            df = Arabic_Twitter().load_data()\n        elif self.args.graph_lang == \"English\":\n            df = Event2012().load_data()\n\n        print(\"{} Data converted to dataframe.\".format(self.args.graph_lang))\n\n        # sort data by time\n        df = df.sort_values(by='created_at').reset_index()\n        # append date\n        df['date'] = [d.date() for d in df['created_at']]\n\n        nf = None\n        # load features\n        f = np.load(self.args.file_path + '/features/features_69612_0709_spacy_lg_zero_multiclasses_filtered_{}.npy'.format(\n            self.args.graph_lang))\n        nonleafilename = self.args.file_path + \"/features/features_69612_0709_spacy_lg_zero_multiclasses_filtered_{}_{}.npy\".format(\n            self.args.graph_lang, self.args.tgtlang)\n        nf = np.load(nonleafilename)\n\n        # construct graph\n        message, data_split, all_graph_mins = self.construct_incremental_dataset(self.args, df, save_path, f, nf, False)\n        with open(save_path + \"node_edge_statistics.txt\", \"w\") as text_file:\n            text_file.write(message)\n        np.save(save_path + 'data_split.npy', np.asarray(data_split))\n        print(\"Data split: \", data_split)\n        np.save(save_path + 'all_graph_mins.npy', np.asarray(all_graph_mins))\n        print(\"Time sepnt on heterogeneous -> homogeneous graph conversions: \", all_graph_mins)\n\n    def construct_graph_from_df(self, df, G=None):\n        if G is None:\n            G = nx.Graph()\n        for _, row in df.iterrows():\n            tid = 't_' + str(row['tweet_id'])\n            G.add_node(tid)\n            G.nodes[tid]['tweet_id'] = True\n\n            user_ids = row['user_mentions']\n            user_ids.append(row['user_id'])\n            user_ids = ['u_' + str(each) for each in user_ids]\n            G.add_nodes_from(user_ids)\n            for each in user_ids:\n                G.nodes[each]['user_id'] = True\n\n            entities = row['entities']\n            entities = ['e_' + str(each) for each in entities]\n            G.add_nodes_from(entities)\n            for each in entities:\n                G.nodes[each]['entity'] = True\n\n            hashtags = row['hashtags']\n            hashtags = ['h_' + str(each) for each in hashtags]\n            G.add_nodes_from(hashtags)\n            for each in hashtags:\n                G.nodes[each]['hashtag'] = True\n\n            edges = []\n            edges += [(tid, each) for each in user_ids]\n            edges += [(tid, each) for each in entities]\n            edges += [(tid, each) for each in hashtags]\n            G.add_edges_from(edges)\n\n        return G\n\n    # convert networkx graph to dgl graph and store its sparse binary adjacency matrix\n    def networkx_to_dgl_graph(self, G, save_path=None):\n        message = ''\n        print('Start converting heterogeneous networkx graph to homogeneous dgl graph.')\n        message += 'Start converting heterogeneous networkx graph to homogeneous dgl graph.\\n'\n        all_start = time()\n\n        print('\\tGetting a list of all nodes ...')\n        message += '\\tGetting a list of all nodes ...\\n'\n        start = time()\n        all_nodes = list(G.nodes)\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\tGetting adjacency matrix ...')\n        message += '\\tGetting adjacency matrix ...\\n'\n        start = time()\n        A = nx.to_numpy_array(G)  # Returns the graph adjacency matrix as a NumPy matrix.\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # compute commuting matrices\n        print('\\tGetting lists of nodes of various types ...')\n        message += '\\tGetting lists of nodes of various types ...\\n'\n        start = time()\n        tid_nodes = list(nx.get_node_attributes(G, 'tweet_id').keys())\n        userid_nodes = list(nx.get_node_attributes(G, 'user_id').keys())\n        hash_nodes = list(nx.get_node_attributes(G, 'hashtag').keys())\n        entity_nodes = list(nx.get_node_attributes(G, 'entity').keys())\n        del G\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\tConverting node lists to index lists ...')\n        message += '\\tConverting node lists to index lists ...\\n'\n        start = time()\n        indices_tid = [all_nodes.index(x) for x in tid_nodes]\n        indices_userid = [all_nodes.index(x) for x in userid_nodes]\n        indices_hashtag = [all_nodes.index(x) for x in hash_nodes]\n        indices_entity = [all_nodes.index(x) for x in entity_nodes]\n        del tid_nodes\n        del userid_nodes\n        del hash_nodes\n        del entity_nodes\n        mins = (time() - start) / 60\n        print('\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # tweet-user-tweet\n        print('\\tStart constructing tweet-user-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-user matrix ...')\n        message += '\\tStart constructing tweet-user-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-user matrix ...\\n'\n        start = time()\n        w_tid_userid = A[np.ix_(indices_tid, indices_userid)]\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_userid = sparse.csr_matrix(w_tid_userid)\n        del w_tid_userid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_userid_tid = s_w_tid_userid.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-user * user-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-user * user-tweet ...\\n'\n        start = time()\n        s_m_tid_userid_tid = s_w_tid_userid * s_w_userid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_userid_tid.npz\", s_m_tid_userid_tid)\n            print(\"Sparse binary userid commuting matrix saved.\")\n            del s_m_tid_userid_tid\n        del s_w_tid_userid\n        del s_w_userid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # tweet-ent-tweet\n        print('\\tStart constructing tweet-ent-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-ent matrix ...')\n        message += '\\tStart constructing tweet-ent-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-ent matrix ...\\n'\n        start = time()\n        w_tid_entity = A[np.ix_(indices_tid, indices_entity)]\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_entity = sparse.csr_matrix(w_tid_entity)\n        del w_tid_entity\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_entity_tid = s_w_tid_entity.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-ent * ent-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-ent * ent-tweet ...\\n'\n        start = time()\n        s_m_tid_entity_tid = s_w_tid_entity * s_w_entity_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_entity_tid.npz\", s_m_tid_entity_tid)\n            print(\"Sparse binary entity commuting matrix saved.\")\n            del s_m_tid_entity_tid\n        del s_w_tid_entity\n        del s_w_entity_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # tweet-hashtag-tweet\n        print('\\tStart constructing tweet-hashtag-tweet commuting matrix ...')\n        print('\\t\\t\\tStart constructing tweet-hashtag matrix ...')\n        message += '\\tStart constructing tweet-hashtag-tweet commuting matrix ...\\n\\t\\t\\tStart constructing tweet-hashtag matrix ...\\n'\n        start = time()\n        w_tid_hash = A[np.ix_(indices_tid, indices_hashtag)]\n        del A\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # convert to scipy sparse matrix\n        print('\\t\\t\\tConverting to sparse matrix ...')\n        message += '\\t\\t\\tConverting to sparse matrix ...\\n'\n        start = time()\n        s_w_tid_hash = sparse.csr_matrix(w_tid_hash)\n        del w_tid_hash\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tTransposing ...')\n        message += '\\t\\t\\tTransposing ...\\n'\n        start = time()\n        s_w_hash_tid = s_w_tid_hash.transpose()\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tCalculating tweet-hashtag * hashtag-tweet ...')\n        message += '\\t\\t\\tCalculating tweet-hashtag * hashtag-tweet ...\\n'\n        start = time()\n        s_m_tid_hash_tid = s_w_tid_hash * s_w_hash_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        print('\\t\\t\\tSaving ...')\n        message += '\\t\\t\\tSaving ...\\n'\n        start = time()\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_m_tid_hash_tid.npz\", s_m_tid_hash_tid)\n            print(\"Sparse binary hashtag commuting matrix saved.\")\n            del s_m_tid_hash_tid\n        del s_w_tid_hash\n        del s_w_hash_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n\n        # compute tweet-tweet adjacency matrix\n        print('\\tComputing tweet-tweet adjacency matrix ...')\n        message += '\\tComputing tweet-tweet adjacency matrix ...\\n'\n        start = time()\n        if save_path is not None:\n            s_m_tid_userid_tid = sparse.load_npz(save_path + \"s_m_tid_userid_tid.npz\")\n            print(\"Sparse binary userid commuting matrix loaded.\")\n            s_m_tid_entity_tid = sparse.load_npz(save_path + \"s_m_tid_entity_tid.npz\")\n            print(\"Sparse binary entity commuting matrix loaded.\")\n            s_m_tid_hash_tid = sparse.load_npz(save_path + \"s_m_tid_hash_tid.npz\")\n            print(\"Sparse binary hashtag commuting matrix loaded.\")\n\n        s_A_tid_tid = s_m_tid_userid_tid + s_m_tid_entity_tid\n        del s_m_tid_userid_tid\n        del s_m_tid_entity_tid\n        s_bool_A_tid_tid = (s_A_tid_tid + s_m_tid_hash_tid).astype('bool')\n        del s_m_tid_hash_tid\n        del s_A_tid_tid\n        mins = (time() - start) / 60\n        print('\\t\\t\\tDone. Time elapsed: ', mins, ' mins\\n')\n        message += '\\t\\t\\tDone. Time elapsed: '\n        message += str(mins)\n        message += ' mins\\n'\n        all_mins = (time() - all_start) / 60\n        print('\\tOver all time elapsed: ', all_mins, ' mins\\n')\n        message += '\\tOver all time elapsed: '\n        message += str(all_mins)\n        message += ' mins\\n'\n\n        if save_path is not None:\n            sparse.save_npz(save_path + \"s_bool_A_tid_tid.npz\", s_bool_A_tid_tid)\n            print(\"Sparse binary adjacency matrix saved.\")\n            s_bool_A_tid_tid = sparse.load_npz(save_path + \"s_bool_A_tid_tid.npz\")\n            print(\"Sparse binary adjacency matrix loaded.\")\n\n        # create corresponding dgl graph\n        G = dgl.DGLGraph(s_bool_A_tid_tid)\n        print('We have %d nodes.' % G.number_of_nodes())\n        print('We have %d edges.' % G.number_of_edges())\n        message += 'We have '\n        message += str(G.number_of_nodes())\n        message += ' nodes.'\n        message += 'We have '\n        message += str(G.number_of_edges())\n        message += ' edges.\\n'\n\n        return all_mins, message\n\n    def construct_incremental_dataset(self, args, df, save_path, features, nfeatures, test=False):\n        data_split = []\n        all_graph_mins = []\n        message = \"\"\n        # extract distinct dates\n        distinct_dates = df.date.unique()\n        # print(\"Distinct dates: \", distinct_dates)\n        print(\"Number of distinct dates: \", len(distinct_dates))\n        message += \"Number of distinct dates: \"\n        message += str(len(distinct_dates))\n        message += \"\\n\"\n        print(\"Start constructing initial graph ...\")\n        message += \"\\nStart constructing initial graph ...\\n\"\n\n        if self.args.is_static:\n            ini_df = df.loc[df['date'].isin(distinct_dates[:self.args.days])]\n            days = self.args.days\n        else:\n            ini_df = df.loc[df['date'].isin(distinct_dates[:1])]\n            days = 1\n\n        print(\"Initial graph contains %d days\" % days)\n        message += \"Initial graph contains %d days\\n\" % days\n\n        path = save_path + '0/'\n        if not os.path.exists(path):\n            os.mkdir(path)\n\n        y = ini_df['event_id'].values\n        y = [int(each) for each in y]\n        np.save(path + 'labels.npy', np.asarray(y))\n\n        G = self.construct_graph_from_df(ini_df)\n        grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n        message += graph_message\n        print(\"Initial graph saved\")\n        message += \"Initial graph saved\\n\"\n        # record the total number of tweets\n        data_split.append(ini_df.shape[0])\n        # record the time spent for graph conversion\n        all_graph_mins.append(grap_mins)\n        # extract and save the labels of corresponding tweets\n        y = ini_df['event_id'].values\n        y = [int(each) for each in y]\n        np.save(path + 'labels.npy', np.asarray(y))\n        np.save(path + 'df.npy', ini_df)\n        # ini_df['created_at'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n        np.save(path + \"time.npy\", ini_df['created_at'].values)\n        print(\"Labels and times saved.\")\n        message += \"Labels and times saved.\\n\"\n        # extract and save the features of corresponding tweets\n        indices = ini_df['index'].values.tolist()\n        x = features[indices, :]\n        np.save(path + 'features.npy', x)\n        print(\"Features saved.\")\n        message += \"Features saved.\"\n        if nfeatures is not None:\n            # save trans nonlinear features\n            nx = nfeatures[indices, :]\n            np.save(path + '{}-{}-features.npy'.format(self.args.graph_lang, self.args.tgtlang), nx)\n            print(\"trans features saved\")\n            message += \"Nonlinear Trans Features saved.\\n\\n\"\n\n        if not self.args.is_static:\n            inidays = 1\n            j = 0\n\n            for i in range(inidays, len(distinct_dates)):\n                print(\"Start constructing graph \", str(i - j), \" ...\")\n                message += \"\\nStart constructing graph \"\n                message += str(i - j)\n                message += \" ...\\n\"\n                incr_df = df.loc[df['date'] == distinct_dates[i]]\n                path = save_path + str(i - j) + '/'\n                if not os.path.exists(path):\n                    os.mkdir(path)\n                np.save(path + \"/\" + \"dataframe.npy\", incr_df)\n\n                G = self.construct_graph_from_df(\n                    incr_df)  # remove obsolete, version 2: construct graph using only the data of the day\n                grap_mins, graph_message = self.networkx_to_dgl_graph(G, save_path=path)\n                message += graph_message\n                print(\"Graph \", str(i - j), \" saved\")\n                message += \"Graph \"\n                message += str(i - j)\n                message += \" saved\\n\"\n                # record the total number of tweets\n                data_split.append(incr_df.shape[0])\n                # record the time spent for graph conversion\n                all_graph_mins.append(grap_mins)\n                # extract and save the labels of corresponding tweets\n                # y = np.concatenate([y, incr_df['event_id'].values], axis = 0)\n                y = [int(each) for each in incr_df['event_id'].values]\n                np.save(path + 'labels.npy', y)\n                # incr_df['created_at'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n                np.save(path + \"time.npy\", incr_df['created_at'].values)\n                print(\"Labels saved.\")\n                message += \"Labels saved.\\n\"\n                # extract and save the features of corresponding tweets\n                indices = incr_df['index'].values.tolist()\n                x = features[indices, :]\n                # x = np.concatenate([x, x_incr], axis = 0)\n                np.save(path + 'features.npy', x)\n                np.save(path + 'df.npy', incr_df)\n                print(\"Features saved.\")\n                message += \"Features saved.\"\n                if nfeatures is not None:\n                    # save trans nonlinear features\n                    nx = nfeatures[indices, :]\n                    np.save(path + '{}-{}-features.npy'.format(self.args.graph_lang, self.args.tgtlang), nx)\n                    print(\"trans features saved\")\n                    message += \"trans features saved.\\n\"\n        return message, data_split, all_graph_mins\n\ndef infer(train_i, i, data_split, metrics, embedding_save_path, loss_fn, model=None):\n    save_path_i, in_feats, num_isolated_nodes, g, labels, test_indices = getdata(embedding_save_path, args.data_path,\n                                                                                 data_split, train_i, i, args,\n                                                                                 args.lang,\n                                                                                 args.Tealang)\n    # record the time spent in seconds on direct prediction\n    time_predict = []\n    # Directly predict\n    message = \"\\n------------ Directly predict on block \" + str(i) + \" ------------\\n\"\n    print(message)\n    with open(save_path_i + '/log.txt', 'a') as f:\n        f.write(message)\n    start = time()\n    # Infer the representations of all tweets\n    extract_nids, extract_features, extract_labels = extract_embeddings(g, model, len(labels), labels, args,\n                                                                        labels.device)\n    test_nmi = evaluate_model(extract_features, extract_labels, test_indices, -1, num_isolated_nodes, save_path_i,\n                              args.metrics, False)\n    seconds_spent = time() - start\n    message = '\\nDirect prediction took {:.2f} seconds'.format(seconds_spent)\n    print(message)\n    with open(save_path_i + '/log.txt', 'a') as f:\n        f.write(message)\n    time_predict.append(seconds_spent)\n    np.save(save_path_i + '/time_predict.npy', np.asarray(time_predict))\n    return model\n\ndef mutual_infer(embedding_save_path1, embedding_save_path2, data_split1, data_split2, train_i, i, loss_fn, metrics,\n                 model1, model2, device):\n    save_path_i1, in_feats1, num_isolated_nodes1, g1, labels1, test_indices1 = getdata(embedding_save_path1,\n                                                                                       args.data_path1, data_split1,\n                                                                                       train_i, i, args, args.lang1,\n                                                                                       args.lang2)\n    save_path_i2, in_feats2, num_isolated_nodes2, g2, labels2, test_indices2 = getdata(embedding_save_path2,\n                                                                                       args.data_path2, data_split2,\n                                                                                       train_i, i, args, args.lang2,\n                                                                                       args.lang1)\n\n    # model1\n    extract_nids, extract_features, extract_labels = mutual_extract_embeddings(g1, model1, model2, args.lang1,\n                                                                               args.lang2,\n                                                                               len(labels1), labels1, args, device)\n    test_value = evaluate_model(extract_features, extract_labels, test_indices1, -1, num_isolated_nodes2,\n                                save_path_i1, args.metrics, False)\n\n    # model2\n    extract_nids, extract_features, extract_labels = mutual_extract_embeddings(g2, model2, model1, args.lang2,\n                                                                               args.lang1,\n                                                                               len(labels2), labels2, args, device)\n\n    test_value = evaluate_model(extract_features, extract_labels, test_indices2, -1, num_isolated_nodes2,\n                                save_path_i2, args.metrics, False)\n    return model1, model2\n\ndef mutual_train(embedding_save_path1, embedding_save_path2, data_split1, data_split2, train_i, i, loss_fn, metrics,\n                 device):\n    save_path_i1, in_feats1, num_isolated_nodes1, g1, labels1, train_indices1, validation_indices1, test_indices1 = getdata(\n        embedding_save_path1, args.data_path1, data_split1, train_i, i, args, args.lang1, args.lang2)\n    save_path_i2, in_feats2, num_isolated_nodes2, g2, labels2, train_indices2, validation_indices2, test_indices2 = getdata(\n        embedding_save_path2, args.data_path2, data_split2, train_i, i, args, args.lang2, args.lang1)\n\n    model1 = GAT(in_feats1, args.hidden_dim, args.out_dim, args.num_heads, args.use_residual)\n    model2 = GAT(in_feats2, args.hidden_dim, args.out_dim, args.num_heads, args.use_residual)\n\n    # Optimizer\n    optimizer1 = optim.Adam(model1.parameters(), lr=args.lr, weight_decay=1e-4)\n    optimizer2 = optim.Adam(model2.parameters(), lr=args.lr, weight_decay=1e-4)\n    model1_data = {'opt': optimizer1, 'best_value': 1e-9, 'best_epoch': 0,\n                   'model': model1, 'peer': model2, 'src': args.lang1, 'tgt': args.lang2,\n                   'save_path_i': save_path_i1, 'num_iso_nodes': num_isolated_nodes1, 'g': g1, 'labels': labels1,\n                   'train_indices': train_indices1, 'vali_indices': validation_indices1, 'test_indices': test_indices1,\n                   'all_vali_nmi': [], 'seconds_train_batches': []}\n\n    model2_data = {'opt': optimizer2, 'best_value': 1e-9, 'best_epoch': 0,\n                   'model': model2, 'peer': model1, 'src': args.lang2, 'tgt': args.lang1,\n                   'save_path_i': save_path_i2, 'num_iso_nodes': num_isolated_nodes2, 'g': g2, 'labels': labels2,\n                   'train_indices': train_indices2, 'vali_indices': validation_indices2, 'test_indices': test_indices2,\n                   'all_vali_nmi': [], 'seconds_train_batches': []}\n    print(\"\\n------------ Start initial training / maintaining using blocks 0 to \" + str(i) + \" ------------\\n\")\n\n    if args.use_cuda:\n        model1.to(device)\n        model2.to(device)\n\n    for epoch in range(args.n_epochs):\n\n        for model_data in [model1_data, model2_data]:\n            losses = []\n            total_loss = 0\n            for metric in metrics:\n                metric.reset()\n\n            sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n            dataloader = dgl.dataloading.NodeDataLoader(\n                model_data['g'], model_data['train_indices'], sampler,\n                batch_size=args.batch_size,\n                shuffle=True,\n                drop_last=False,\n            )\n            for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n                start_batch = time()\n                model_data['model'].train()\n                model_data['peer'].eval()\n\n                blocks = [b.to(device) for b in blocks]\n                # forward\n                pred = model_data['model'](blocks, args)\n                batch_nids = blocks[-1].dstdata[dgl.NID].to(device=device, dtype=torch.long)\n                batch_labels = model_data['labels'].to(device)[batch_nids]\n                peerpred = None\n\n                if args.mode == 2 and epoch >= args.e:\n                    if args.add_mapping:\n                        peerpred = model_data['peer'](blocks, args, trans=True, src=model_data['src'],\n                                                      tgt=model_data['tgt'])\n                    else:\n                        peerpred = model_data['peer'](blocks, args)\n                    peerpred = peerpred.to(device)\n\n                if args.mode == 4 and epoch >= args.e:\n                    peerpred = model_data['peer'](blocks, args, trans=True)\n                    peerpred = peerpred.to(device)\n\n                loss_outputs = loss_fn(pred, batch_labels, args.rd, peerpred)\n                loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n\n                if (args.mode == 2 or args.mode == 4) and epoch >= args.e:\n                    l = nn.L1Loss(size_average=True, reduce=True, reduction='average')\n                    lkd = l(pred, peerpred.to(device))\n                    message = \"    \".join([\"add KD loss\", str(loss), str(lkd)])\n                    loss = loss + args.mt * lkd\n                    print(message)\n                    with open(save_path_i1 + '/log.txt', 'a') as f:\n                        f.write(message)\n\n                losses.append(loss.item())\n                total_loss += loss.item()\n                for metric in metrics:\n                    metric(pred, batch_labels, loss_outputs)\n                if batch_id % args.log_interval == 0:\n                    message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                        batch_id * args.batch_size, train_indices1.shape[0],\n                        100. * batch_id / ((train_indices1.shape[0] // args.batch_size) + 1), np.mean(losses))\n                    for metric in metrics:\n                        message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n                    print(message)\n                    with open(save_path_i1 + '/log.txt', 'a') as f:\n                        f.write(message)\n                    losses = []\n\n                model_data['opt'].zero_grad()\n                loss.backward()\n                model_data['opt'].step()\n                batch_seconds_spent = time() - start_batch\n                model_data['seconds_train_batches'].append(batch_seconds_spent)\n                # end one batch\n\n            total_loss /= (batch_id + 1)\n            message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch + 1, args.n_epochs, total_loss)\n            for metric in metrics:\n                message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n            message += '\\n'\n            print(message)\n            with open(model_data['save_path_i'] + '/log.txt', 'a') as f:\n                f.write(message)\n\n            for b in blocks:\n                del b\n            del pred\n            del input_nodes\n            del output_nodes\n            if peerpred != None:\n                del peerpred\n\n            # Validation\n            extract_nids, extract_features, extract_labels = extract_embeddings(model_data['g'], model_data['model'],\n                                                                                len(model_data['labels']),\n                                                                                model_data['labels'],\n                                                                                args,\n                                                                                device)\n            validation_value = evaluate_model(extract_features, extract_labels, model_data['vali_indices'], epoch,\n                                              model_data['num_iso_nodes'], model_data['save_path_i'], args.metrics,\n                                              True)\n\n            model_data['all_vali_nmi'].append(validation_value)\n            if validation_value > model_data['best_value']:\n                model_data['best_value'] = validation_value\n                model_data['best_epoch'] = epoch\n                # Save model\n                model_path = model_data['save_path_i'] + '/models'\n                if not os.path.isdir(model_path):\n                    os.mkdir(model_path)\n                p = model_path + '/best.pt'\n                torch.save(model_data['model'].state_dict(), p)\n                print(model_data['src'], ':', 'Best model was at epoch ', str(model_data['best_epoch']))\n\n            for metric in metrics:\n                metric.reset()\n\n    with open(save_path_i1 + '/evaluate.txt', 'a') as f:\n        message = 'Best model was at epoch ' + str(model1_data['best_epoch'])\n        f.write(message)\n    with open(save_path_i2 + '/evaluate.txt', 'a') as f:\n        message = 'Best model was at epoch ' + str(model2_data['best_epoch'])\n        f.write(message)\n    # Save all validation nmi\n    np.save(save_path_i1 + '/all_vali_nmi.npy', np.asarray(model1_data['all_vali_nmi']))\n    np.save(save_path_i2 + '/all_vali_nmi.npy', np.asarray(model2_data['all_vali_nmi']))\n    # save all seconds_train\n    np.save(save_path_i1 + '/seconds_train_batches.npy', np.asarray(model1_data['seconds_train_batches']))\n    np.save(save_path_i2 + '/seconds_train_batches.npy', np.asarray(model2_data['seconds_train_batches']))\n\n    extract_nids, extract_features, extract_labels = mutual_extract_embeddings(g1, model1, model2, args.lang1,\n                                                                               args.lang2,\n                                                                               len(labels1), labels1, args, device)\n    test_value = evaluate_model(extract_features, extract_labels, test_indices1, -1, num_isolated_nodes1,\n                                save_path_i1, args.metrics, False)\n\n    extract_nids, extract_features, extract_labels = mutual_extract_embeddings(g2, model2, model1, args.lang2,\n                                                                               args.lang1,\n                                                                               len(labels2), labels2, args, device)\n    test_value = evaluate_model(extract_features, extract_labels, test_indices2, -1, num_isolated_nodes2,\n                                save_path_i2, args.metrics, False)\n\n    return model1, model2\n\ndef initial_maintain(train_i, i, data_split, metrics, embedding_save_path, loss_fn, model=None):\n    save_path_i, in_feats, num_isolated_nodes, g, labels, train_indices, validation_indices, test_indices = getdata(\n        embedding_save_path, args.data_path, data_split, train_i, i, args, args.lang, args.Tealang)\n\n    if model is None:  # Construct the initial model\n        model = GAT(in_feats, args.hidden_dim, args.out_dim, args.num_heads, args.use_residual)\n        if args.use_cuda:\n            model.cuda()\n\n    if args.mode == 2 or args.mode == 4:\n        Tmodel = GAT(in_feats, args.hidden_dim, args.out_dim, args.num_heads, args.use_residual)\n        Tmodel_path = args.Tmodel_path + '/block_' + str(train_i) + '/models/best.pt'\n        Tmodel.load_state_dict(torch.load(Tmodel_path))\n        if args.use_cuda:\n            Tmodel.cuda()\n        Tmodel.eval()\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n    # Start training\n    message = \"\\n------------ Start initial training / maintaining using blocks 0 to \" + str(i) + \" ------------\\n\"\n    print(message)\n    with open(save_path_i + '/log.txt', 'a') as f:\n        f.write(message)\n    # record the highest validation nmi ever got for early stopping\n    best_vali_nmi = 1e-9\n    best_epoch = 0\n    wait = 0\n    # record validation nmi of all epochs before early stop\n    all_vali_nmi = []\n    # record the time spent in seconds on each batch of all training/maintaining epochs\n    seconds_train_batches = []\n    # record the time spent in mins on each epoch\n    mins_train_epochs = []\n    for epoch in range(args.n_epochs):\n        start_epoch = time()\n        losses = []\n        total_loss = 0\n        for metric in metrics:\n            metric.reset()\n\n        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n        dataloader = dgl.dataloading.NodeDataLoader(\n            g, train_indices, sampler,\n            batch_size=args.batch_size,\n            shuffle=True,\n            drop_last=False,\n        )\n        Tpred = None\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            start_batch = time()\n            model.train()\n            # forward\n            blocks = [b.to(train_indices.device) for b in blocks]\n            pred = model(blocks, args)  # Representations of the sampled nodes (in the last layer of the NodeFlow).\n            if args.mode == 2:\n                if args.add_mapping:\n                    Tpred = Tmodel(blocks, args, trans=True, src=args.lang, tgt=args.Tealang)\n                else:\n                    Tpred = Tmodel(blocks, args)\n            if args.mode == 4:\n                Tpred = Tmodel(blocks, args, trans=True)\n\n            batch_nids = blocks[-1].dstdata[dgl.NID].to(device=pred.device, dtype=torch.long)\n            batch_labels = labels[batch_nids]\n            loss_outputs = loss_fn(pred, batch_labels, args.rd, Tpred)\n            loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n\n            if args.mode == 2 or args.mode == 4:\n                # p = torch.matmul(pred,pred.T)\n                # Tp = torch.matmul(Tpred,Tpred.T)\n                # kl = F.kl_div(p.softmax(dim=-1).log(), Tp.softmax(dim=-1), reduction='sum')\n                l = nn.L1Loss(size_average=True, reduce=True, reduction='average')\n                # l = torch.nn.MSELoss(reduce=True, size_average=True)\n                lkd = l(pred, Tpred)\n                message = \"    \".join([\"add KD loss\", str(loss), str(lkd)])\n                print(message)\n                loss = loss + args.mt * lkd\n            losses.append(loss.item())\n            total_loss += loss.item()\n\n            for metric in metrics:\n                metric(pred, batch_labels, loss_outputs)\n\n            if batch_id % args.log_interval == 0:\n                message += 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    batch_id * args.batch_size, train_indices.shape[0],\n                    100. * batch_id / ((train_indices.shape[0] // args.batch_size) + 1), np.mean(losses))\n                for metric in metrics:\n                    message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n                print(message)\n                with open(save_path_i + '/log.txt', 'a') as f:\n                    f.write(message)\n                losses = []\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            batch_seconds_spent = time() - start_batch\n            seconds_train_batches.append(batch_seconds_spent)\n            # end one batch\n            del pred\n            if args.mode != 0:\n                del Tpred\n            for b in blocks:\n                del b\n\n        total_loss /= (batch_id + 1)\n        message = 'Epoch: {}/{}. Average loss: {:.4f}'.format(epoch + 1, args.n_epochs, total_loss)\n        for metric in metrics:\n            message += '\\t{}: {:.4f}'.format(metric.name(), metric.value())\n        mins_spent = (time() - start_epoch) / 60\n        message += '\\nThis epoch took {:.2f} mins'.format(mins_spent)\n        message += '\\n'\n        print(message)\n        with open(save_path_i + '/log.txt', 'a') as f:\n            f.write(message)\n        mins_train_epochs.append(mins_spent)\n\n        extract_nids, extract_features, extract_labels = extract_embeddings(g, model, len(labels), labels, args,\n                                                                            labels.device)\n        # save_embeddings(extract_nids, extract_features, extract_labels, extract_train_tags, save_path_i, epoch)\n        validation_nmi = evaluate_model(extract_features, extract_labels, validation_indices, epoch, num_isolated_nodes,\n                                        save_path_i, args.metrics, True)\n        all_vali_nmi.append(validation_nmi)\n\n        # Early stop\n        if validation_nmi > best_vali_nmi:\n            best_vali_nmi = validation_nmi\n            best_epoch = epoch\n            wait = 0\n            # Save model\n            model_path = save_path_i + '/models'\n            if (epoch == 0) and (not os.path.isdir(model_path)):\n                os.mkdir(model_path)\n            p = model_path + '/best.pt'\n            torch.save(model.state_dict(), p)\n            print('Best model saved after epoch ', str(epoch))\n        else:\n            wait += 1\n        if wait == args.patience:\n            print('Saved all_mins_spent')\n            print('Early stopping at epoch ', str(epoch))\n            print('Best model was at epoch ', str(best_epoch))\n            break\n        # end one epoch\n\n    # Save all validation nmi\n    np.save(save_path_i + '/all_vali_nmi.npy', np.asarray(all_vali_nmi))\n    # Save time spent on epochs\n    np.save(save_path_i + '/mins_train_epochs.npy', np.asarray(mins_train_epochs))\n    print('Saved mins_train_epochs.')\n    # Save time spent on batches\n    np.save(save_path_i + '/seconds_train_batches.npy', np.asarray(seconds_train_batches))\n    print('Saved seconds_train_batches.')\n    # Load the best model of the current block\n\n    best_model_path = save_path_i + '/models/best.pt'\n    model.load_state_dict(torch.load(best_model_path))\n    print(\"Best model loaded.\")\n    return model\n\ndef generateMasks(length, data_split, train_i, i, validation_percent=0.1, test_percent=0.2, save_path=None):\n    # verify total number of nodes\n    assert length == data_split[i]\n    if train_i == i:\n        # randomly shuffle the graph indices\n        train_indices = torch.randperm(length)\n        # get total number of validation indices\n        n_validation_samples = int(length * validation_percent)\n        # sample n_validation_samples validation indices and use the rest as training indices\n        validation_indices = train_indices[:n_validation_samples]\n        n_test_samples = n_validation_samples + int(length * test_percent)\n        test_indices = train_indices[n_validation_samples:n_test_samples]\n        train_indices = train_indices[n_test_samples:]\n\n        if save_path is not None:\n            torch.save(validation_indices, save_path + '/validation_indices.pt')\n            torch.save(train_indices, save_path + '/train_indices.pt')\n            torch.save(test_indices, save_path + '/test_indices.pt')\n            validation_indices = torch.load(save_path + '/validation_indices.pt')\n            train_indices = torch.load(save_path + '/train_indices.pt')\n            test_indices = torch.load(save_path + '/test_indices.pt')\n        return train_indices, validation_indices, test_indices\n    # If is in inference(prediction) epochs, generate test indices\n    else:\n        test_indices = torch.range(0, (data_split[i] - 1), dtype=torch.long)\n        if save_path is not None:\n            torch.save(test_indices, save_path + '/test_indices.pt')\n            test_indices = torch.load(save_path + '/test_indices.pt')\n        return test_indices\n\ndef getdata(embedding_save_path, data_path, data_split, train_i, i, args, src=None, tgt=None):\n    save_path_i = embedding_save_path + '/block_' + str(i)\n    if not os.path.isdir(save_path_i):\n        os.mkdir(save_path_i)\n    # load data\n    data = SocialDataset(data_path, i)\n    features = torch.FloatTensor(data.features)\n    labels = torch.LongTensor(data.labels)\n    in_feats = features.shape[1]  # feature dimension\n\n    g = dgl.DGLGraph(data.matrix,\n                     readonly=True)\n    num_isolated_nodes = graph_statistics(g, save_path_i)\n    g.set_n_initializer(dgl.init.zero_initializer)\n    g.readonly(readonly_state=True)\n    device = torch.device(\"cuda:{}\".format(args.gpuid) if args.use_cuda else \"cpu\")\n    g = g.to(device)\n\n    mask_path = save_path_i + '/masks'\n    if not os.path.isdir(mask_path):\n        os.mkdir(mask_path)\n\n    if train_i == i:\n        train_indices, validation_indices, test_indices = generateMasks(len(labels), data_split, train_i, i,\n                                                                        args.validation_percent,\n                                                                        args.test_percent,\n                                                                        mask_path)\n    else:\n        test_indices = generateMasks(len(labels), data_split, train_i, i, args.validation_percent,\n                                     args.test_percent,\n                                     mask_path)\n    if args.use_cuda:\n        features, labels = features.cuda(), labels.cuda()\n        test_indices = test_indices.cuda()\n        if train_i == i:\n            train_indices, validation_indices = train_indices.cuda(), validation_indices.cuda()\n    # features = F.normalize(features, p=2, dim=1)\n\n    g.ndata['h'] = features\n    if args.mode == 4:\n        tranfeatures = np.load(\n            data_path + '/' + str(i) + '/' + \"-\".join([src, tgt, 'features']) + '.npy')\n        tranfeatures = torch.FloatTensor(tranfeatures)\n        # tranfeatures = F.normalize(tranfeatures, p=2, dim=1)\n        if args.use_cuda:\n            tranfeatures = tranfeatures.cuda()\n        g.ndata['tranfeatures'] = tranfeatures\n\n    if train_i == i:\n        return save_path_i, in_feats, num_isolated_nodes, g, labels, train_indices, validation_indices, test_indices\n    else:\n        return save_path_i, in_feats, num_isolated_nodes, g, labels, test_indices\n\ndef extract_embeddings(g, model, num_all_samples, labels, args, device):\n    with torch.no_grad():\n        model.eval()\n        select_indices = torch.LongTensor(range(0, num_all_samples)).to(device)\n        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n        dataloader = dgl.dataloading.NodeDataLoader(\n            g, select_indices, sampler,\n            batch_size=int(args.batch_size),\n            shuffle=False,\n            drop_last=False,\n        )\n        labels = labels.cpu().detach().float()\n        fea_list = []\n        nid_list = []\n        label_list = []\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            blocks = [b.to(device) for b in blocks]\n            extract_features = model(blocks, args).float()\n            extract_features = extract_features.cpu().detach()\n            extract_nids = blocks[-1].dstdata[dgl.NID].data.cpu()  # node ids\n            extract_labels = labels[extract_nids].float()  # labels of all nodes\n            fea_list.append(extract_features.numpy())\n            nid_list.append(extract_nids.numpy())\n            label_list.append(extract_labels.numpy())\n\n        extract_features = np.concatenate(fea_list, axis=0).astype(np.float32)\n        extract_labels = np.concatenate(label_list, axis=0).astype(np.float32)\n        extract_nids = np.concatenate(nid_list, axis=0)\n\n    return (extract_nids, extract_features, extract_labels)\n\ndef mutual_extract_embeddings(g, model, peer, src, tgt, num_all_samples, labels, args, device):\n    with torch.no_grad():\n        model.eval()\n        peer.eval()\n        select_indices = torch.LongTensor(range(0, num_all_samples)).to(device)\n        sampler = dgl.dataloading.MultiLayerFullNeighborSampler(2)\n        dataloader = dgl.dataloading.NodeDataLoader(\n            g, select_indices, sampler,\n            batch_size=int(args.batch_size),\n            shuffle=False,\n            drop_last=False,\n        )\n        fea_list = []\n        nid_list = []\n        label_list = []\n        labels = labels.cpu().detach()\n        for batch_id, (input_nodes, output_nodes, blocks) in enumerate(dataloader):\n            blocks = [b.to(device) for b in blocks]\n            extract_features1 = model(blocks, args)\n            if (args.mode == 2 and args.add_mapping):\n                print(\"** add linear tran peer feature **\", src, tgt)\n                extract_features2 = peer(blocks, args, True, src=src, tgt=tgt)  # representations of all nodes\n            elif args.mode == 4:\n                print(\"** add nonlinear tran peer feature **\", src, tgt)\n                extract_features2 = peer(blocks, args, True)  # representations of all nodes\n            else:\n                print(\"** add feature **\")\n                extract_features2 = peer(blocks, args)\n            extract_nids = blocks[-1].dstdata[dgl.NID].cpu()\n            extract_labels = labels[extract_nids]  # labels of all nodes\n            extract_features1 = extract_features1.cpu().detach()\n            extract_features2 = extract_features2.cpu().detach()\n            extract_features = torch.cat((extract_features1, extract_features2), 1).numpy()\n            # extract_features = extract_features1.numpy()\n            fea_list.append(extract_features)\n            nid_list.append(extract_nids.numpy())\n            label_list.append(extract_labels.numpy())\n\n        for b in blocks:\n            del b\n        del input_nodes, output_nodes, select_indices\n\n        # assert batch_id == 0\n        # extract_nids = extract_nids.data.cpu().numpy()\n        # extract_features1 = extract_features1.data.cpu().detach()\n        # extract_features2 = extract_features2.data.cpu().detach()\n        # extract_features = torch.cat((extract_features1,extract_features2),1).numpy()\n        # extract_labels = extract_labels.data.cpu().detach().numpy()\n        extract_features = np.concatenate(fea_list, axis=0)\n        extract_labels = np.concatenate(label_list, axis=0)\n        extract_nids = np.concatenate(nid_list, axis=0)\n        # generate train/test mask\n        A = np.arange(num_all_samples)\n        # print(\"A\", A)\n        # assert (A == extract_nids).all()\n\n    return (extract_nids, extract_features, extract_labels)\n\ndef save_embeddings(extract_nids, extract_features, extract_labels, extract_train_tags, path, counter):\n    np.savetxt(path + '/features_' + str(counter) + '.tsv', extract_features, delimiter='\\t')\n    np.savetxt(path + '/labels_' + str(counter) + '.tsv', extract_labels, fmt='%i', delimiter='\\t')\n    with open(path + '/labels_tags_' + str(counter) + '.tsv', 'w') as f:\n        f.write('label\\tmessage_id\\ttrain_tag\\n')\n        for (label, mid, train_tag) in zip(extract_labels, extract_nids, extract_train_tags):\n            f.write(\"%s\\t%s\\t%s\\n\" % (label, mid, train_tag))\n    print(\"Embeddings after inference epoch \" + str(counter) + \" saved.\")\n\ndef intersection(lst1, lst2):\n    lst3 = [value for value in lst1 if value in lst2]\n    return lst3\n\ndef run_kmeans(extract_features, extract_labels, indices, metric, isoPath=None):\n    # Extract the features and labels of the test tweets\n    indices = indices.cpu().detach().numpy()\n\n    if isoPath is not None:\n        # Remove isolated points\n        temp = torch.load(isoPath)\n        temp = temp.cpu().detach().numpy()\n        non_isolated_index = list(np.where(temp != 1)[0])\n        indices = intersection(indices, non_isolated_index)\n\n    # Extract labels\n    labels_true = extract_labels[indices]\n    # Extract features\n    X = extract_features[indices, :]\n    assert labels_true.shape[0] == X.shape[0]\n    n_test_tweets = X.shape[0]\n\n    # Get the total number of classes\n    n_classes = len(set(list(labels_true)))\n\n    # kmeans clustering\n    kmeans = KMeans(n_clusters=n_classes, random_state=0).fit(X)\n    labels = kmeans.labels_\n    nmi = metrics.normalized_mutual_info_score(labels_true, labels)\n    ari = metrics.adjusted_rand_score(labels_true, labels)\n    ami = metrics.adjusted_mutual_info_score(labels_true, labels, average_method='arithmetic')\n    print(\"nmi:\", nmi, 'ami:', ami, 'ari:', ari)\n    value = nmi\n    global NMI\n    NMI = nmi\n    global AMI\n    AMI = ami\n    global ARI\n    ARI = ari\n\n    if metric == 'ari':\n        print('use ari')\n        value = ari\n    if metric == 'ami':\n        print('use ami')\n        value = ami\n    # Return number  of test tweets, number of classes covered by the test tweets, and kMeans cluatering NMI\n    return (n_test_tweets, n_classes, value)\n\ndef evaluate_model(extract_features, extract_labels, indices, epoch, num_isolated_nodes, save_path, metrics,\n                   is_validation=True,\n                   file_name='evaluate.txt'):\n    message = ''\n    message += '\\nEpoch '\n    message += str(epoch)\n    message += '\\n'\n\n    # with isolated nodes\n    n_tweets, n_classes, value = run_kmeans(extract_features, extract_labels, indices, metrics)\n    if is_validation:\n        split = 'validation'\n    else:\n        split = 'test'\n    message += '\\tNumber of ' + split + ' tweets: '\n    message += str(n_tweets)\n    message += '\\n\\tNumber of classes covered by ' + split + ' tweets: '\n    message += str(n_classes)\n    message += '\\n\\t' + split + ' '\n    message += metrics + ': '\n    message += str(value)\n    if num_isolated_nodes != 0:\n        # without isolated nodes\n        message += '\\n\\tWithout isolated nodes:'\n        n_tweets, n_classes, value = run_kmeans(extract_features, extract_labels, indices, metrics,\n                                                save_path + '/isolated_nodes.pt')\n        message += '\\tNumber of ' + split + ' tweets: '\n        message += str(n_tweets)\n        message += '\\n\\tNumber of classes covered by ' + split + ' tweets: '\n        message += str(n_classes)\n        message += '\\n\\t' + split + f' {metrics}: '\n        message += str(value)\n    message += '\\n'\n    global NMI\n    global AMI\n    global ARI\n    print(\"*********************************\")\n    with open(save_path + f'/{file_name}', 'a') as f:\n        f.write(message)\n        f.write('\\n')\n        f.write(\"NMI \" + str(NMI) + \" AMI \" + str(AMI) + ' ARI ' + str(ARI))\n    print(message)\n\n    return value\n\nclass Metric:\n    def __init__(self):\n        pass\n\n    def __call__(self, outputs, target, loss):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def value(self):\n        raise NotImplementedError\n\n    def name(self):\n        raise NotImplementedError\n\nclass AccumulatedAccuracyMetric(Metric):\n    \"\"\"\n    Works with classification model\n    \"\"\"\n\n    def __init__(self):\n        self.correct = 0\n        self.total = 0\n\n    def __call__(self, outputs, target, loss):\n        pred = outputs[0].data.max(1, keepdim=True)[1]\n        self.correct += pred.eq(target[0].data.view_as(pred)).cpu().sum()\n        self.total += target[0].size(0)\n        return self.value()\n\n    def reset(self):\n        self.correct = 0\n        self.total = 0\n\n    def value(self):\n        return 100 * float(self.correct) / self.total\n\n    def name(self):\n        return 'Accuracy'\n\nclass AverageNonzeroTripletsMetric(Metric):\n    '''\n    Counts average number of nonzero triplets found in minibatches\n    '''\n\n    def __init__(self):\n        self.values = []\n\n    def __call__(self, outputs, target, loss):\n        self.values.append(loss[1])\n        return self.value()\n\n    def reset(self):\n        self.values = []\n\n    def value(self):\n        return np.mean(self.values)\n\n    def name(self):\n        return 'Average nonzero triplets'\n\nclass OnlineTripletLoss(nn.Module):\n    \"\"\"\n    Online Triplets loss\n    Takes a batch of embeddings and corresponding labels.\n    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n    triplets\n    \"\"\"\n\n    def __init__(self, margin, triplet_selector):\n        super(OnlineTripletLoss, self).__init__()\n        self.margin = margin\n        self.triplet_selector = triplet_selector\n\n    def forward(self, embeddings, target, rd, peer_embeddings=None):\n        triplets = self.triplet_selector.get_triplets(embeddings, target)\n\n        if embeddings.is_cuda:\n            triplets = triplets.cuda()\n\n        ap_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 1]]).pow(2).sum(1)  # .pow(.5)\n        an_distances = (embeddings[triplets[:, 0]] - embeddings[triplets[:, 2]]).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(ap_distances - an_distances + self.margin)\n\n        if peer_embeddings != None:\n            peer_ap_distances = (peer_embeddings[triplets[:, 0]] - peer_embeddings[triplets[:, 1]]).pow(2).sum(1)\n            peer_an_distances = (peer_embeddings[triplets[:, 0]] - peer_embeddings[triplets[:, 2]]).pow(2).sum(1)\n            kd_ap_losses = F.relu(-peer_ap_distances + ap_distances)\n            kd_an_losses = F.relu(-an_distances + peer_an_distances)\n            print(\"losses.mean():\", losses.mean(), \"ap_mean:\", kd_ap_losses.mean(), \"an_mean:\", kd_an_losses.mean())\n            return losses.mean() + rd * kd_ap_losses.mean() + rd * kd_an_losses.mean(), len(triplets)\n        else:\n            return losses.mean(), len(triplets)\n\ndef pdist(vectors):\n    distance_matrix = -2 * vectors.mm(torch.t(vectors)) + vectors.pow(2).sum(dim=1).view(1, -1) + vectors.pow(2).sum(\n        dim=1).view(-1, 1)\n    return distance_matrix\n\nclass TripletSelector:\n    \"\"\"\n    Implementation should return indices of anchors, positive and negative samples\n    return np array of shape [N_triplets x 3]\n    \"\"\"\n\n    def __init__(self):\n        pass\n\n    def get_triplets(self, embeddings, labels):\n        raise NotImplementedError\n\nclass FunctionNegativeTripletSelector(TripletSelector):\n    \"\"\"\n    For each positive pair, takes the hardest negative sample (with the greatest triplet loss value) to create a triplet\n    Margin should match the margin used in triplet loss.\n    negative_selection_fn should take array of loss_values for a given anchor-positive pair and all negative samples\n    and return a negative index for that pair\n    \"\"\"\n\n    def __init__(self, margin, negative_selection_fn, cpu=True):\n        super(FunctionNegativeTripletSelector, self).__init__()\n        self.cpu = cpu\n        self.margin = margin\n        self.negative_selection_fn = negative_selection_fn\n\n    def get_triplets(self, embeddings, labels):\n        if self.cpu:\n            embeddings = embeddings.cpu()\n        distance_matrix = pdist(embeddings)\n        distance_matrix = distance_matrix.cpu()\n\n        labels = labels.cpu().data.numpy()\n        triplets = []\n\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            negative_indices = np.where(np.logical_not(label_mask))[0]\n            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n            anchor_positives = np.array(anchor_positives)\n\n            ap_distances = distance_matrix[anchor_positives[:, 0], anchor_positives[:, 1]]\n            for anchor_positive, ap_distance in zip(anchor_positives, ap_distances):\n                loss_values = ap_distance - distance_matrix[\n                    torch.LongTensor(np.array([anchor_positive[0]])), torch.LongTensor(negative_indices)] + self.margin\n                loss_values = loss_values.data.cpu().numpy()\n                hard_negative = self.negative_selection_fn(loss_values)\n                if hard_negative is not None:\n                    hard_negative = negative_indices[hard_negative]\n                    triplets.append([anchor_positive[0], anchor_positive[1], hard_negative])\n\n        if len(triplets) == 0:\n            triplets.append([anchor_positive[0], anchor_positive[1], negative_indices[0]])\n\n        triplets = np.array(triplets)\n\n        return torch.LongTensor(triplets)\n\ndef random_hard_negative(loss_values):\n    hard_negatives = np.where(loss_values > 0)[0]\n    return np.random.choice(hard_negatives) if len(hard_negatives) > 0 else None\n\ndef HardestNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                              negative_selection_fn=hardest_negative,\n                                                                                              cpu=cpu)\n\ndef RandomNegativeTripletSelector(margin, cpu=False): return FunctionNegativeTripletSelector(margin=margin,\n                                                                                             negative_selection_fn=random_hard_negative,\n                                                                                             cpu=cpu)\n\nclass GATLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, use_residual=False):\n        super(GATLayer, self).__init__()\n        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n        self.use_residual = use_residual\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        \"\"\"Reinitialize learnable parameters.\"\"\"\n        gain = nn.init.calculate_gain('relu')\n        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n\n    def edge_attention(self, edges):\n        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n        a = self.attn_fc(z2)\n        return {'e': F.leaky_relu(a)}\n\n    def message_func(self, edges):\n        return {'z': edges.src['z'], 'e': edges.data['e']}\n\n    def reduce_func(self, nodes):\n        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n        return {'h': h}\n\n    def forward(self, blocks, layer_id):\n        h = blocks[layer_id].srcdata['h'].float()  # 确保 h 为 Float 类型\n        z = self.fc(h)\n        blocks[layer_id].srcdata['z'] = z\n        z_dst = z[:blocks[layer_id].number_of_dst_nodes()]\n        blocks[layer_id].dstdata['z'] = z_dst\n\n        blocks[layer_id].apply_edges(self.edge_attention)\n        blocks[layer_id].update_all(\n            self.message_func,\n            self.reduce_func)\n\n        if self.use_residual:\n            return z_dst + blocks[layer_id].dstdata['h']  # residual connection\n        return blocks[layer_id].dstdata['h']\n\nclass MultiHeadGATLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, num_heads, merge='cat', use_residual=False):\n        super(MultiHeadGATLayer, self).__init__()\n        self.heads = nn.ModuleList()\n        for i in range(num_heads):\n            self.heads.append(GATLayer(in_dim, out_dim, use_residual))\n        self.merge = merge\n\n    def forward(self, blocks, layer_id):\n        head_outs = [attn_head(blocks, layer_id).float() for attn_head in self.heads]  # 确保 head_outs 为 Float 类型\n        if self.merge == 'cat':\n            return torch.cat(head_outs, dim=1)\n        else:\n            return torch.mean(torch.stack(head_outs))\n\nclass GAT(nn.Module):\n    def __init__(self, in_dim, hidden_dim, out_dim, num_heads, use_residual=False):\n        super(GAT, self).__init__()\n        self.layer1 = MultiHeadGATLayer(in_dim, hidden_dim, num_heads, 'cat', use_residual)\n        self.layer2 = MultiHeadGATLayer(hidden_dim * num_heads, out_dim, 1, 'cat', use_residual)\n\n    def forward(self, blocks, args, trans=False, src=None, tgt=None):\n        print(\"Entering forward function\")\n        if trans:\n            if args.mode == 4:\n                features = blocks[0].srcdata['tranfeatures'].float()\n                print(\"This is nonlinear trans!\")\n                blocks[0].srcdata['h'] = features\n            if args.mode == 2 and args.add_mapping:\n                features = blocks[0].srcdata['h'].cpu().detach().float()\n                W = torch.from_numpy(\n                    torch.load(\n                        args.file_path + '/LinearTranWeight/spacy_{}_{}/best_mapping.pth'.format(src, tgt))).float()\n                print(\"This is linear trans!\")\n                part1 = torch.index_select(features, 1, torch.tensor(range(0, args.word_embedding_dim)))\n                part1 = torch.matmul(part1, torch.FloatTensor(W))\n                part2 = torch.index_select(features, 1,\n                                           torch.tensor(range(args.word_embedding_dim, features.size()[1])))\n                features = torch.cat((part1, part2), 1).cuda()\n                blocks[0].srcdata['h'] = features\n\n        h = self.layer1(blocks, 0).float()\n        h = F.elu(h)\n        blocks[1].srcdata['h'] = h\n        h = self.layer2(blocks, 1).float()\n        return h\n\nclass Arabic_preprocessor:\n    def __init__(self, tokenizer, **cfg):\n        self.tokenizer = tokenizer\n\n    def clean_text(self, text):\n        search = [\"أ\", \"إ\", \"آ\", \"ة\", \"_\", \"-\", \"/\", \".\", \"،\", \" و \", \" يا \", '\"', \"ـ\", \"'\", \"ى\", \"\\\\\", '\\n', '\\t',\n                  '&quot;', '?', '؟', '!']\n        replace = [\"ا\", \"ا\", \"ا\", \"ه\", \" \", \" \", \"\", \"\", \"\", \" و\", \" يا\", \"\", \"\", \"\", \"ي\", \"\", ' ', ' ', ' ', ' ? ',\n                   ' ؟ ', ' ! ']\n\n        # remove tashkeel\n        p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n        text = re.sub(p_tashkeel, \"\", text)\n\n        # remove longation\n        p_longation = re.compile(r'(.)\\1+')\n        subst = r\"\\1\\1\"\n        text = re.sub(p_longation, subst, text)\n\n        text = text.replace('وو', 'و')\n        text = text.replace('يي', 'ي')\n        text = text.replace('اا', 'ا')\n\n        for i in range(len(search)):\n            text = text.replace(search[i], replace[i])\n\n        # trim\n        text = text.strip()\n\n        return text\n\n    def __call__(self, text):\n        preprocessed = self.clean_text(text)\n        return self.tokenizer(preprocessed)\n\n\n\n# load Dataset\nclass SocialDataset(Dataset):\n    def __init__(self, path, index):\n        self.features = np.load(path + '/' + str(index) + '/features.npy')\n        temp = np.load(path + '/' + str(index) + '/labels.npy', allow_pickle=True)\n        self.labels = np.asarray([int(each) for each in temp])\n        self.matrix = self.load_adj_matrix(path, index)\n\n    def __len__(self):\n        return len(self.features)\n\n    def __getitem__(self, idx):\n        return self.features[idx], self.labels[idx]\n\n    def load_adj_matrix(self, path, index):\n        s_bool_A_tid_tid = sparse.load_npz(path + '/' + str(index) + '/s_bool_A_tid_tid.npz')\n        print(\"Sparse binary adjacency matrix loaded.\")\n        return s_bool_A_tid_tid\n\n    def remove_obsolete_nodes(self, indices_to_remove=None):  # indices_to_remove: list\n        # torch.range(0, (self.labels.shape[0] - 1), dtype=torch.long)\n        if indices_to_remove is not None:\n            all_indices = np.arange(0, self.labels.shape[0]).tolist()\n            indices_to_keep = list(set(all_indices) - set(indices_to_remove))\n            self.features = self.features[indices_to_keep, :]\n            self.labels = self.labels[indices_to_keep]\n            self.matrix = self.matrix[indices_to_keep, :]\n            self.matrix = self.matrix[:, indices_to_keep]\n\n\n# save graph statistics to save path\ndef graph_statistics(G, save_path):\n    message = '\\nGraph statistics:\\n'\n    num_nodes = G.number_of_nodes()\n    num_edges = G.number_of_edges()\n    ave_degree = (num_edges / 2) // num_nodes\n    in_degrees = G.in_degrees()\n    isolated_nodes = torch.zeros([in_degrees.size()[0]], dtype=torch.long)\n    isolated_nodes = (in_degrees == isolated_nodes)\n    torch.save(isolated_nodes, save_path + '/isolated_nodes.pt')\n    num_isolated_nodes = torch.sum(isolated_nodes).item()\n\n    message += 'We have ' + str(num_nodes) + ' nodes.\\n'\n    message += 'We have ' + str(num_edges / 2) + ' in-edges.\\n'\n    message += 'Average degree: ' + str(ave_degree) + '\\n'\n    message += 'Number of isolated nodes: ' + str(num_isolated_nodes) + '\\n'\n    print(message)\n    with open(save_path + \"/graph_statistics.txt\", \"a\") as f:\n        f.write(message)\n\n    return num_isolated_nodes\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/wmd.py", "content": "import os\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom gensim.models import Word2Vec\nfrom gensim.similarities import WmdSimilarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom tqdm import tqdm\nimport multiprocessing\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nfrom dataset.dataloader import Event2012\n\n\n# event_id, filtered_words\nclass WMD:\n\n    r\"\"\"The WMD model for social event detection that uses Word Mover's Distance\n    to measure document similarity and detect events.\n\n    .. note::\n        This detector uses word embeddings and Word Mover's Distance to identify similar documents\n        and detect events in social media data. The model requires a dataset object with a \n        load_data() method.\n\n    Parameters\n    ----------\n    dataset : object\n        The dataset object containing social media data.\n        Must provide load_data() method that returns the raw data.\n    vector_size : int, optional\n        Dimensionality of word vectors. Default: ``100``.\n    window : int, optional\n        Maximum distance between current and predicted word. Default: ``5``.\n    min_count : int, optional\n        Minimum word frequency. Default: ``1``.\n    sg : int, optional\n        Training algorithm: Skip-gram (1) or CBOW (0). Default: ``1``.\n    num_best : int, optional\n        Number of best matches to return. Default: ``5``.\n    threshold : float, optional\n        Similarity threshold for event detection. Default: ``0.6``.\n    batch_size : int, optional\n        Batch size for processing. Default: ``1000``.\n    n_workers : int, optional\n        Number of worker processes. Default: ``CPU count - 1``.\n    file_path : str, optional\n        Path to save model files. Default: ``'../model/model_saved/WMD/'``.\n    \"\"\"\n    def __init__(self,\n                 dataset,\n                 vector_size=100,\n                 window=5,\n                 min_count=1,\n                 sg=1,\n                 num_best=5,\n                 threshold=0.6,\n                 batch_size=1000,  # 新增：批处理大小\n                 n_workers=None,   # 新增：进程数\n                 file_path='../model/model_saved/WMD/'):\n        self.dataset = dataset.load_data()\n        self.vector_size = vector_size\n        self.window = window\n        self.min_count = min_count\n        self.sg = sg\n        self.num_best = num_best\n        self.threshold = threshold\n        self.batch_size = batch_size\n        self.n_workers = n_workers or max(1, multiprocessing.cpu_count() - 1)\n        self.file_path = file_path\n        self.df = None\n        self.train_df = None\n        self.test_df = None\n        self.word2vec_model = None\n        self.model_path = os.path.join(self.file_path, 'word2vec_model.model')\n\n    def preprocess(self):\n        \"\"\"\n        优化的数据预处理\n        \"\"\"\n        df = self.dataset[['filtered_words', 'event_id']].copy()\n        # 使用列表推导式优化处理速度\n        df['processed_text'] = [\n            [str(word).lower() for word in x] if isinstance(x, list) else []\n            for x in df['filtered_words']\n        ]\n        # 过滤掉空文档\n        df = df[df['processed_text'].map(len) > 0]\n        self.df = df\n        return df\n\n    def fit(self):\n        \"\"\"\n        Train the Word2Vec model and save it to a file.\n        \"\"\"\n        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)\n\n        train_df, test_df = train_test_split(self.df, test_size=0.2, random_state=42)\n        self.train_df = train_df\n        self.test_df = test_df\n\n        sentences = train_df['processed_text'].tolist()\n\n        print(\"Training Word2Vec model...\")\n        word2vec_model = Word2Vec(sentences=sentences, \n                                vector_size=self.vector_size,\n                                window=self.window,\n                                min_count=self.min_count,\n                                sg=self.sg)\n        print(\"Word2Vec model trained successfully.\")\n\n        word2vec_model.save(self.model_path)\n        print(f\"Word2Vec model saved to {self.model_path}\")\n\n        self.word2vec_model = word2vec_model.wv\n\n    def detection(self):\n        \"\"\"\n        优化的事件检测\n        \"\"\"\n        if self.word2vec_model is None:\n            word2vec_model = Word2Vec.load(self.model_path)\n            self.word2vec_model = word2vec_model.wv\n\n        test_corpus = self.test_df['processed_text'].tolist()\n        train_corpus = self.train_df['processed_text'].tolist()\n\n        print(\"Calculating WMD distances...\")\n        instance = WmdSimilarity(train_corpus, self.word2vec_model, num_best=self.num_best)\n\n        # 使用多进程处理文档\n        process_doc = partial(\n            process_document,\n            instance=instance,\n            train_df=self.train_df,\n            threshold=self.threshold,\n            num_best=self.num_best\n        )\n\n        predictions = []\n        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:\n            # 批处理文档\n            for i in tqdm(range(0, len(test_corpus), self.batch_size)):\n                batch = test_corpus[i:i + self.batch_size]\n                batch_predictions = list(executor.map(process_doc, batch))\n                predictions.extend(batch_predictions)\n\n        # 处理未分配事件的文档\n        max_event_id = max(self.train_df['event_id'])\n        new_event_counter = 1\n        for i in range(len(predictions)):\n            if predictions[i] == -1:\n                predictions[i] = max_event_id + new_event_counter\n                new_event_counter += 1\n\n        ground_truths = self.test_df['event_id'].tolist()\n\n        # 保存结果\n        self._save_results(ground_truths, predictions)\n\n        return ground_truths, predictions\n\n    def _save_results(self, ground_truths, predictions):\n        \"\"\"\n        保存结果的辅助方法\n        \"\"\"\n        unique_ground_truths = list(set(ground_truths))\n        unique_predictions = list(set(predictions))\n\n        max_len = max(len(unique_ground_truths), len(unique_predictions))\n        unique_ground_truths.extend([None] * (max_len - len(unique_ground_truths)))\n        unique_predictions.extend([None] * (max_len - len(unique_predictions)))\n\n        data = {\n            'Unique Ground Truths': unique_ground_truths,\n            'Unique Predictions': unique_predictions\n        }\n        df = pd.DataFrame(data)\n        output_file = os.path.join(self.file_path, \"unique_ground_truths_predictions.csv\")\n        df.to_csv(output_file, index=False)\n        print(f\"Unique ground truths and predictions have been saved to {output_file}\")\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model and save results.\n        \"\"\"\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\n        current_datetime = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        with open(self.model_path + \"_evaluation.txt\", \"a\") as f:\n            f.write(f\"Date and Time: {current_datetime}\\n\")\n            f.write(f\"Normalized Mutual Information (NMI): {nmi}\\n\")\n            f.write(f\"Adjusted Mutual Information (AMI): {ami}\\n\")\n            f.write(f\"Adjusted Rand Index (ARI): {ari}\\n\")\n            f.write(\"\\n\")\n\n# 新增：计算单个文档的相似度\ndef process_document(doc, instance, train_df, threshold, num_best):\n    sims = instance[doc]\n    similar_events = []\n    for idx, score in sims[:num_best]:  # 只处理前num_best个结果\n        if score > threshold:\n            similar_events.append(train_df.iloc[idx]['event_id'])\n    \n    if similar_events:\n        prediction = max(set(similar_events), key=similar_events.count)\n    else:\n        prediction = -1  # 使用临时标记，后续处理\n    \n    return prediction\n\n\n\n\n"}
{"type": "source_file", "path": "SocialED/detector/hisevent.py", "content": "import os\nimport numpy as np\nimport pandas as pd\nfrom os.path import exists\nimport pickle\nimport torch\nfrom sentence_transformers import SentenceTransformer\nimport re\nfrom sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_mutual_info_score, adjusted_rand_score\nfrom sklearn import metrics\nfrom itertools import combinations, chain\nimport networkx as nx\nfrom datetime import datetime\nimport math\nfrom networkx.algorithms import cuts\nfrom sklearn.model_selection import train_test_split\nimport sys\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\nos.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n\nclass HISEvent():\n    \"\"\"HISEvent class for event detection.\n\n    This class implements hierarchical structure-based event detection.\n    \n    Args:\n        dataset: Input dataset\n        ...\n    \"\"\"\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.language = dataset.get_dataset_language()\n        self.dataset_name = dataset.get_dataset_name()\n        self.save_path = \"../model/model_saved/hisevent/\"+self.dataset_name+\"/\"\n\n    def preprocess(self):\n        preprocessor = Preprocessor(self.dataset)\n        preprocessor.preprocess()\n\n    def detection(self):\n        ground_truths, predictions = run_hier_2D_SE_mini_data(self.save_path, n=300, e_a=True, e_s=True)\n        return ground_truths, predictions\n\n    def evaluate(self, ground_truths, predictions):\n        \"\"\"\n        Evaluate the model.\n        \"\"\"\n        # Calculate Normalized Mutual Information (NMI)\n        nmi = metrics.normalized_mutual_info_score(ground_truths, predictions)\n        print(f\"Normalized Mutual Information (NMI): {nmi}\")\n\n        # Calculate Adjusted Mutual Information (AMI)\n        ami = metrics.adjusted_mutual_info_score(ground_truths, predictions)\n        print(f\"Adjusted Mutual Information (AMI): {ami}\")\n\n        # Calculate Adjusted Rand Index (ARI)\n        ari = metrics.adjusted_rand_score(ground_truths, predictions)\n        print(f\"Adjusted Rand Index (ARI): {ari}\")\n\nclass Preprocessor:\n    def __init__(self, dataset, mode='close'):\n        \"\"\"Initialize preprocessor\n        Args:\n            dataset: Dataset calss (e.g. Event2012, Event2018, etc.)\n            language: Language of the dataset (default 'English')\n            mode: 'open' or 'close' (default 'close') - determines preprocessing mode\n        \"\"\"\n        self.dataset = dataset.load_data()\n        self.language = dataset.get_dataset_language()\n        self.dataset_name = dataset.get_dataset_name()\n        self.mode = mode\n        self.columns = ['tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n                       'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions']\n\n    def get_closed_set_test_df(self, df):\n        \"\"\"Get closed set test dataframe\"\"\"\n        save_path = f'../model/model_saved/{self.dataset_name}/closed_set/'\n        if not exists(save_path):\n            os.makedirs(save_path)\n        \n        test_set_df_np_path = save_path + 'test_set.npy'\n        if not exists(test_set_df_np_path):\n            # Use 2012-style processing for all datasets\n            test_mask = torch.load(f'../model/model_saved/{self.dataset_name}/masks/test_mask.pt').cpu().detach().numpy()\n            test_mask = list(np.where(test_mask==True)[0])\n            test_df = df.iloc[test_mask]\n            \n            test_df_np = test_df.to_numpy()\n            np.save(test_set_df_np_path, test_df_np)\n        return\n\n    def get_closed_set_messages_embeddings(self):\n        \"\"\"Get SBERT embeddings for closed set messages\"\"\"\n        save_path = f'../model/model_saved/{self.dataset_name}/closed_set/'\n        \n        SBERT_embedding_path = f'{save_path}/SBERT_embeddings.pkl'\n        if not exists(SBERT_embedding_path):\n            test_set_df_np_path = save_path + 'test_set.npy'\n            test_df_np = np.load(test_set_df_np_path, allow_pickle=True)\n            \n            test_df = pd.DataFrame(data=test_df_np, columns=self.columns)\n            print(\"Dataframe loaded.\")\n\n            processed_text = [preprocess_sentence(s) for s in test_df['text'].values]\n            print('message text contents preprocessed.')\n\n            embeddings = SBERT_embed(processed_text, language=self.language)\n\n            with open(SBERT_embedding_path, 'wb') as fp:\n                pickle.dump(embeddings, fp)\n            print('SBERT embeddings stored.')\n        return\n\n    def get_open_set_messages_embeddings(self):\n        \"\"\"Get SBERT embeddings for open set messages\"\"\"\n        save_path = f'../model/model_saved/{self.dataset_name}/open_set/'\n        num_blocks = 21  # Use 2012-style processing for all datasets\n        \n        for i in range(num_blocks):\n            block = i + 1\n            print('\\n\\n====================================================')\n            print('block: ', block)\n\n            SBERT_embedding_path = f'{save_path}{block}/SBERT_embeddings.pkl'\n\n            if not exists(SBERT_embedding_path):\n                df_np = np.load(f'{save_path}{block}/{block}.npy', allow_pickle=True)\n                \n                df = pd.DataFrame(data=df_np, columns=self.columns + ['original_index', 'date'])\n                print(\"Dataframe loaded.\")\n\n                df['processed_text'] = [preprocess_sentence(s) for s in df['text']]\n                print('message text contents preprocessed.')\n\n                embeddings = SBERT_embed(df['processed_text'].tolist(), language=self.language)\n\n                with open(SBERT_embedding_path, 'wb') as fp:\n                    pickle.dump(embeddings, fp)\n                print('SBERT embeddings stored.')\n        return\n\n    def split_open_set(self, df, root_path):\n        \"\"\"Split data into open set blocks\"\"\"\n        if not exists(root_path):\n            os.makedirs(root_path)\n        \n        df = df.sort_values(by='created_at').reset_index()\n        df['date'] = [d.date() for d in df['created_at']]\n\n        distinct_dates = df.date.unique()\n\n        # First week -> block 0\n        folder = root_path + '0/'\n        if not exists(folder):\n            os.mkdir(folder)\n            \n        df_np_path = folder + '0.npy'\n        if not exists(df_np_path):\n            ini_df = df.loc[df['date'].isin(distinct_dates[:7])]\n            ini_df_np = ini_df.to_numpy()\n            np.save(df_np_path, ini_df_np)\n\n        # Following dates -> block 1, 2, ...\n        end = len(distinct_dates) - 1  # Use 2012-style processing\n        for i in range(7, end):\n            folder = root_path + str(i - 6) + '/'\n            if not exists(folder):\n                os.mkdir(folder)\n            \n            df_np_path = folder + str(i - 6) + '.npy'\n            if not exists(df_np_path):\n                incr_df = df.loc[df['date'] == distinct_dates[i]]\n                incr_df_np = incr_df.to_numpy()\n                np.save(df_np_path, incr_df_np)\n        return\n\n    def preprocess(self):\n        \"\"\"Main preprocessing function\"\"\"\n        # Load raw data using 2012-style processing\n        df_np = self.dataset\n        \n        print(\"Loaded data.\")\n        df = pd.DataFrame(data=df_np, columns=self.columns)\n        print(\"Data converted to dataframe.\")\n\n        \n        if self.mode == 'open':\n            # Open-set setting\n            root_path = f'../model/model_saved/{self.dataset_name}/open_set/'\n            self.split_open_set(df, root_path)\n            self.get_open_set_messages_embeddings()\n        else:\n            # Close-set setting\n            # Create masks directory and generate train/val/test splits\n            save_dir = os.path.join(f'../model/model_saved/{self.dataset_name}', 'masks')\n            os.makedirs(save_dir, exist_ok=True)\n            \n            # Split and save masks\n            self.split_and_save_masks(df, save_dir)\n            print(\"Generated and saved train/val/test masks.\")\n\n            self.get_closed_set_test_df(df)\n            self.get_closed_set_messages_embeddings()\n        \n        return\n\n    def split_and_save_masks(self, df, save_dir, train_size=0.7, val_size=0.1, test_size=0.2, random_seed=42):\n        \"\"\"\n        Splits the DataFrame into training, validation, and test sets, and saves the indices (masks) as .pt files.\n        \n        Parameters:\n        - df (pd.DataFrame): The DataFrame to be split\n        - save_dir (str): Directory to save the masks\n        - train_size (float): Proportion for training (default 0.7)\n        - val_size (float): Proportion for validation (default 0.1) \n        - test_size (float): Proportion for testing (default 0.2)\n        - random_seed (int): Random seed for reproducibility\n        \"\"\"\n        if train_size + val_size + test_size != 1.0:\n            raise ValueError(\"train_size + val_size + test_size must equal 1.0\")\n\n        if df.empty:\n            raise ValueError(\"The input DataFrame is empty.\")\n\n        print(f\"Total samples in DataFrame: {len(df)}\")\n        \n        # Set random seed\n        torch.manual_seed(random_seed)\n\n        # Split into train and temp\n        train_data, temp_data = train_test_split(df, train_size=train_size, random_state=random_seed)\n        \n        # Split temp into val and test\n        val_data, test_data = train_test_split(temp_data, \n                                             train_size=val_size/(val_size + test_size),\n                                             random_state=random_seed)\n\n        # Create boolean masks\n        full_train_mask = torch.zeros(len(df), dtype=torch.bool)\n        full_val_mask = torch.zeros(len(df), dtype=torch.bool)\n        full_test_mask = torch.zeros(len(df), dtype=torch.bool)\n\n        # Set indices\n        full_train_mask[train_data.index] = True\n        full_val_mask[val_data.index] = True  \n        full_test_mask[test_data.index] = True\n\n        print(f\"Training samples: {full_train_mask.sum()}\")\n        print(f\"Validation samples: {full_val_mask.sum()}\")\n        print(f\"Test samples: {full_test_mask.sum()}\")\n\n        # Save masks\n        mask_paths = {\n            'train_mask.pt': full_train_mask,\n            'val_mask.pt': full_val_mask, \n            'test_mask.pt': full_test_mask\n        }\n\n        for filename, mask in mask_paths.items():\n            mask_path = os.path.join(save_dir, filename)\n            if not os.path.exists(mask_path):\n                try:\n                    torch.save(mask, mask_path)\n                    print(f\"Saved {filename}\")\n                except Exception as e:\n                    print(f\"Error saving {filename}: {e}\")\n            else:\n                print(f\"{filename} already exists\")\n\n            # Verify saved file\n            if os.path.exists(mask_path):\n                saved_mask = torch.load(mask_path)\n                if saved_mask.numel() == 0:\n                    print(f\"Warning: {filename} is empty\")\n                else:\n                    print(f\"Verified {filename} with {saved_mask.numel()} elements\")\n\n        print(\"Mask generation completed\")\n\ndef get_stable_point(path):\n    stable_point_path = path + 'stable_point.pkl'\n    if not exists(stable_point_path):\n        embeddings_path = path + 'SBERT_embeddings.pkl'\n        with open(embeddings_path, 'rb') as f:\n            embeddings = pickle.load(f)\n        print(f\"Loaded embeddings: {embeddings}\")\n        print(f\"Shape of embeddings: {np.shape(embeddings)}\")\n\n        first_stable_point, global_stable_point = search_stable_points(embeddings)\n        stable_points = {'first': first_stable_point, 'global': global_stable_point}\n        with open(stable_point_path, 'wb') as fp:\n            pickle.dump(stable_points, fp)\n        print('stable points stored.')\n\n    with open(stable_point_path, 'rb') as f:\n        stable_points = pickle.load(f)\n    print('stable points loaded.')\n    return stable_points\n\ndef run_hier_2D_SE_mini_data(save_path, n=300, e_a=True, e_s=True):\n\n    # load test_set_df\n    test_set_df_np_path = save_path + 'test_set.npy'\n    test_df_np = np.load(test_set_df_np_path, allow_pickle=True)\n    test_df = pd.DataFrame(data=test_df_np,\n                           columns=['tweet_id', 'text', 'event_id', 'words', 'filtered_words',\n                       'entities', 'user_id', 'created_at', 'urls', 'hashtags', 'user_mentions'])\n    print(\"Dataframe loaded.\")\n    all_node_features = [[str(u)] + \\\n                         [str(each) for each in um] + \\\n                         [h.lower() for h in hs] + \\\n                         e \\\n                         for u, um, hs, e in \\\n                         zip(test_df['user_id'], test_df['user_mentions'], test_df['hashtags'], test_df['entities'])]\n\n    # load embeddings of the test set messages\n    with open(f'{save_path}/SBERT_embeddings.pkl', 'rb') as f:\n        embeddings = pickle.load(f)\n\n    stable_points = get_stable_point(save_path)\n    default_num_neighbors = stable_points['first']\n\n    global_edges = get_global_edges(all_node_features, embeddings, default_num_neighbors, e_a=e_a, e_s=e_s)\n    corr_matrix = np.corrcoef(embeddings)\n\n    np.fill_diagonal(corr_matrix, 0)\n    weighted_global_edges = [(edge[0], edge[1], corr_matrix[edge[0] - 1, edge[1] - 1]) for edge in global_edges \\\n                             if corr_matrix[edge[0] - 1, edge[1] - 1] > 0]  # node encoding starts from 1\n\n    division = hier_2D_SE_mini(weighted_global_edges, len(embeddings), n=n)\n    prediction = decode(division)\n\n    labels_true = test_df['event_id'].tolist()\n    n_clusters = len(list(set(labels_true)))\n    print('n_clusters gt: ', n_clusters)\n\n    return labels_true, prediction\n\ndef search_stable_points(embeddings, max_num_neighbors=50):\n    corr_matrix = np.corrcoef(embeddings)\n\n    np.fill_diagonal(corr_matrix, 0)\n    corr_matrix_sorted_indices = np.argsort(corr_matrix)\n\n    all_1dSEs = []\n    seg = None\n    for i in range(max_num_neighbors):\n        dst_ids = corr_matrix_sorted_indices[:, -(i + 1)]\n        knn_edges = [(s + 1, d + 1, corr_matrix[s, d]) \\\n                     for s, d in enumerate(dst_ids) if\n                     corr_matrix[s, d] > 0]  # (s+1, d+1): +1 as node indexing starts from 1 instead of 0\n        if i == 0:\n            g = nx.Graph()\n            g.add_weighted_edges_from(knn_edges)\n            seg = SE(g)\n            all_1dSEs.append(seg.calc_1dSE())\n        else:\n            all_1dSEs.append(seg.update_1dSE(all_1dSEs[-1], knn_edges))\n\n    # print('all_1dSEs: ', all_1dSEs)\n    stable_indices = []\n    for i in range(1, len(all_1dSEs) - 1):\n        if all_1dSEs[i] < all_1dSEs[i - 1] and all_1dSEs[i] < all_1dSEs[i + 1]:\n            stable_indices.append(i)\n    if len(stable_indices) == 0:\n        print('No stable points found after checking k = 1 to ', max_num_neighbors)\n        return 0, 0\n    else:\n        stable_SEs = [all_1dSEs[index] for index in stable_indices]\n        index = stable_indices[stable_SEs.index(min(stable_SEs))]\n        print('stable_indices: ', stable_indices)\n        print('stable_SEs: ', stable_SEs)\n        print('First stable point: k = ', stable_indices[0] + 1, ', correspoding 1dSE: ',\n              stable_SEs[0])  # n_neighbors should be index + 1\n        print('Global stable point within the searching range: k = ', index + 1, \\\n              ', correspoding 1dSE: ', all_1dSEs[index])  # n_neighbors should be index + 1\n    return stable_indices[0] + 1, index + 1  # first stable point, global stable point\n\ndef get_graph_edges(attributes):\n    attr_nodes_dict = {}\n    for i, l in enumerate(attributes):\n        for attr in l:\n            if attr not in attr_nodes_dict:\n                attr_nodes_dict[attr] = [i + 1]  # node indexing starts from 1\n            else:\n                attr_nodes_dict[attr].append(i + 1)\n\n    for attr in attr_nodes_dict.keys():\n        attr_nodes_dict[attr].sort()\n\n    graph_edges = []\n    for l in attr_nodes_dict.values():\n        graph_edges += list(combinations(l, 2))\n    return list(set(graph_edges))\n\ndef get_knn_edges(embeddings, default_num_neighbors):\n    corr_matrix = np.corrcoef(embeddings)\n    np.fill_diagonal(corr_matrix, 0)\n    corr_matrix_sorted_indices = np.argsort(corr_matrix)\n    knn_edges = []\n    for i in range(default_num_neighbors):\n        dst_ids = corr_matrix_sorted_indices[:, -(i + 1)]\n        knn_edges += [(s + 1, d + 1) if s < d else (d + 1, s + 1) \\\n                      for s, d in enumerate(dst_ids) if\n                      corr_matrix[s, d] > 0]  # (s+1, d+1): +1 as node indexing starts from 1 instead of 0\n    return list(set(knn_edges))\n\ndef get_global_edges(attributes, embeddings, default_num_neighbors, e_a=True, e_s=True):\n    graph_edges, knn_edges = [], []\n    if e_a == True:\n        graph_edges = get_graph_edges(attributes)\n    if e_s == True:\n        knn_edges = get_knn_edges(embeddings, default_num_neighbors)\n    return list(set(knn_edges + graph_edges))\n\ndef get_subgraphs_edges(clusters, graph_splits, weighted_global_edges):\n    \"\"\"Get subgraph edges.\n    \n    Args:\n        clusters: a list containing the current clusters, each cluster is a list of nodes of the original graph\n        graph_splits: a list of (start_index, end_index) pairs, each (start_index, end_index) pair indicates a subset of clusters, \n            which will serve as the nodes of a new subgraph\n        weighted_global_edges: a list of (start node, end node, edge weight) tuples, each tuple is an edge in the original graph\n\n    Returns:\n        all_subgraphs_edges: a list containing the edges of all subgraphs\n    \"\"\"\n    all_subgraphs_edges = []\n    for split in graph_splits:\n        subgraph_clusters = clusters[split[0]:split[1]]\n        subgraph_nodes = list(chain(*subgraph_clusters))\n        subgraph_edges = [edge for edge in weighted_global_edges if\n                          edge[0] in subgraph_nodes and edge[1] in subgraph_nodes]\n        all_subgraphs_edges.append(subgraph_edges)\n    return all_subgraphs_edges\n\ndef hier_2D_SE_mini(weighted_global_edges, n_messages, n=100):\n    '''\n    hierarchical 2D SE minimization\n    '''\n    ite = 1\n    # initially, each node (message) is in its own cluster\n    # node encoding starts from 1\n    clusters = [[i + 1] for i in range(n_messages)]\n    while True:\n        print('\\n=========Iteration ', str(ite), '=========')\n        n_clusters = len(clusters)\n        graph_splits = [(s, min(s + n, n_clusters)) for s in range(0, n_clusters, n)]  # [s, e)\n        all_subgraphs_edges = get_subgraphs_edges(clusters, graph_splits, weighted_global_edges)\n        last_clusters = clusters\n        clusters = []\n        print('Number of subgraphs: ', len(graph_splits))\n        for i, subgraph_edges in enumerate(all_subgraphs_edges):\n            print('\\tSubgraph ', str(i + 1))\n            g = nx.Graph()\n            g.add_weighted_edges_from(subgraph_edges)\n            seg = SE(g)\n            seg.division = {j: cluster for j, cluster in\n                            enumerate(last_clusters[graph_splits[i][0]:graph_splits[i][1]])}\n            seg.add_isolates()\n            for k in seg.division.keys():\n                for node in seg.division[k]:\n                    seg.graph.nodes[node]['comm'] = k\n            seg.update_struc_data()\n            seg.update_struc_data_2d()\n            seg.update_division_MinSE()\n\n            clusters += list(seg.division.values())\n        if len(graph_splits) == 1:\n            break\n        if clusters == last_clusters:\n            n *= 2\n    return clusters\n\n# SE\nclass SE:\n    def __init__(self, graph: nx.Graph):\n        self.graph = graph.copy()\n        self.vol = self.get_vol()\n        self.division = {}  # {comm1: [node11, node12, ...], comm2: [node21, node22, ...], ...}\n        self.struc_data = {}  # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }\n        self.struc_data_2d = {}  # {comm1: {comm2: [vol_after_merge, cut_after_merge, comm_node_SE_after_merge, leaf_nodes_SE_after_merge], comm3: [], ...}, ...}\n\n    def get_vol(self):\n        '''\n        get the volume of the graph\n        '''\n        return cuts.volume(self.graph, self.graph.nodes, weight='weight')\n\n    def calc_1dSE(self):\n        '''\n        get the 1D SE of the graph\n        '''\n        SE = 0\n        for n in self.graph.nodes:\n            d = cuts.volume(self.graph, [n], weight='weight')\n            SE += - (d / self.vol) * math.log2(d / self.vol)\n        return SE\n\n    def update_1dSE(self, original_1dSE, new_edges):\n        '''\n        get the updated 1D SE after new edges are inserted into the graph\n        '''\n\n        affected_nodes = []\n        for edge in new_edges:\n            affected_nodes += [edge[0], edge[1]]\n        affected_nodes = set(affected_nodes)\n\n        original_vol = self.vol\n        original_degree_dict = {node: 0 for node in affected_nodes}\n        for node in affected_nodes.intersection(set(self.graph.nodes)):\n            original_degree_dict[node] = self.graph.degree(node, weight='weight')\n\n        # insert new edges into the graph\n        self.graph.add_weighted_edges_from(new_edges)\n\n        self.vol = self.get_vol()\n        updated_vol = self.vol\n        updated_degree_dict = {}\n        for node in affected_nodes:\n            updated_degree_dict[node] = self.graph.degree(node, weight='weight')\n\n        updated_1dSE = (original_vol / updated_vol) * (original_1dSE - math.log2(original_vol / updated_vol))\n        for node in affected_nodes:\n            d_original = original_degree_dict[node]\n            d_updated = updated_degree_dict[node]\n            if d_original != d_updated:\n                if d_original != 0:\n                    updated_1dSE += (d_original / updated_vol) * math.log2(d_original / updated_vol)\n                updated_1dSE -= (d_updated / updated_vol) * math.log2(d_updated / updated_vol)\n\n        return updated_1dSE\n\n    def get_cut(self, comm):\n        '''\n        get the sum of the degrees of the cut edges of community comm\n        '''\n        return cuts.cut_size(self.graph, comm, weight='weight')\n\n    def get_volume(self, comm):\n        '''\n        get the volume of community comm\n        '''\n        return cuts.volume(self.graph, comm, weight='weight')\n\n    def calc_2dSE(self):\n        '''\n        get the 2D SE of the graph\n        '''\n        SE = 0\n        for comm in self.division.values():\n            g = self.get_cut(comm)\n            v = self.get_volume(comm)\n            SE += - (g / self.vol) * math.log2(v / self.vol)\n            for node in comm:\n                d = self.graph.degree(node, weight='weight')\n                SE += - (d / self.vol) * math.log2(d / v)\n        return SE\n\n    def show_division(self):\n        print(self.division)\n\n    def show_struc_data(self):\n        print(self.struc_data)\n\n    def show_struc_data_2d(self):\n        print(self.struc_data_2d)\n\n    def print_graph(self):\n        fig, ax = plt.subplots()\n        nx.draw(self.graph, ax=ax, with_labels=True)\n        plt.show()\n\n    def update_struc_data(self):\n        '''\n        calculate the volume, cut, communitiy mode SE, and leaf nodes SE of each cummunity, \n        then store them into self.struc_data\n        '''\n        self.struc_data = {}  # {comm1: [vol1, cut1, community_node_SE, leaf_nodes_SE], comm2:[vol2, cut2, community_node_SE, leaf_nodes_SE]，... }\n        for vname in self.division.keys():\n            comm = self.division[vname]\n            volume = self.get_volume(comm)\n            cut = self.get_cut(comm)\n            if volume == 0:\n                vSE = 0\n            else:\n                vSE = - (cut / self.vol) * math.log2(volume / self.vol)\n            vnodeSE = 0\n            for node in comm:\n                d = self.graph.degree(node, weight='weight')\n                if d != 0:\n                    vnodeSE -= (d / self.vol) * math.log2(d / volume)\n            self.struc_data[vname] = [volume, cut, vSE, vnodeSE]\n\n    def update_struc_data_2d(self):\n        '''\n        calculate the volume, cut, communitiy mode SE, and leaf nodes SE after merging each pair of cummunities, \n        then store them into self.struc_data_2d\n        '''\n        self.struc_data_2d = {}  # {(comm1, comm2): [vol_after_merge, cut_after_merge, comm_node_SE_after_merge, leaf_nodes_SE_after_merge], (comm1, comm3): [], ...}\n        comm_num = len(self.division)\n        for i in range(comm_num):\n            for j in range(i + 1, comm_num):\n                v1 = list(self.division.keys())[i]\n                v2 = list(self.division.keys())[j]\n                if v1 < v2:\n                    k = (v1, v2)\n                else:\n                    k = (v2, v1)\n\n                comm_merged = self.division[v1] + self.division[v2]\n                gm = self.get_cut(comm_merged)\n                vm = self.struc_data[v1][0] + self.struc_data[v2][0]\n                if self.struc_data[v1][0] == 0 or self.struc_data[v2][0] == 0:\n                    vmSE = self.struc_data[v1][2] + self.struc_data[v2][2]\n                    vmnodeSE = self.struc_data[v1][3] + self.struc_data[v2][3]\n                else:\n                    vmSE = - (gm / self.vol) * math.log2(vm / self.vol)\n                    vmnodeSE = self.struc_data[v1][3] - (self.struc_data[v1][0] / self.vol) * math.log2(\n                        self.struc_data[v1][0] / vm) + \\\n                               self.struc_data[v2][3] - (self.struc_data[v2][0] / self.vol) * math.log2(\n                        self.struc_data[v2][0] / vm)\n                self.struc_data_2d[k] = [vm, gm, vmSE, vmnodeSE]\n\n    def init_division(self):\n        '''\n        initialize self.division such that each node assigned to its own community\n        '''\n        self.division = {}\n        for node in self.graph.nodes:\n            new_comm = node\n            self.division[new_comm] = [node]\n            self.graph.nodes[node]['comm'] = new_comm\n\n    def add_isolates(self):\n        '''\n        add any isolated nodes into graph\n        '''\n        all_nodes = list(chain(*list(self.division.values())))\n        all_nodes.sort()\n        edge_nodes = list(self.graph.nodes)\n        edge_nodes.sort()\n        if all_nodes != edge_nodes:\n            for node in set(all_nodes) - set(edge_nodes):\n                self.graph.add_node(node)\n\n    def update_division_MinSE(self):\n        '''\n        greedily update the encoding tree to minimize 2D SE\n        '''\n\n        def Mg_operator(v1, v2):\n            '''\n            MERGE operator. It calculates the delta SE caused by mergeing communities v1 and v2, \n            without actually merging them, i.e., the encoding tree won't be changed\n            '''\n            v1SE = self.struc_data[v1][2]\n            v1nodeSE = self.struc_data[v1][3]\n\n            v2SE = self.struc_data[v2][2]\n            v2nodeSE = self.struc_data[v2][3]\n\n            if v1 < v2:\n                k = (v1, v2)\n            else:\n                k = (v2, v1)\n            vm, gm, vmSE, vmnodeSE = self.struc_data_2d[k]\n            delta_SE = vmSE + vmnodeSE - (v1SE + v1nodeSE + v2SE + v2nodeSE)\n            return delta_SE\n\n        # continue merging any two communities that can cause the largest decrease in SE, \n        # until the SE can't be further reduced\n        while True:\n            comm_num = len(self.division)\n            delta_SE = 99999\n            vm1 = None\n            vm2 = None\n            for i in range(comm_num):\n                for j in range(i + 1, comm_num):\n                    v1 = list(self.division.keys())[i]\n                    v2 = list(self.division.keys())[j]\n                    new_delta_SE = Mg_operator(v1, v2)\n                    if new_delta_SE < delta_SE:\n                        delta_SE = new_delta_SE\n                        vm1 = v1\n                        vm2 = v2\n\n            if delta_SE < 0:\n                # Merge v2 into v1, and update the encoding tree accordingly\n                for node in self.division[vm2]:\n                    self.graph.nodes[node]['comm'] = vm1\n                self.division[vm1] += self.division[vm2]\n                self.division.pop(vm2)\n\n                volume = self.struc_data[vm1][0] + self.struc_data[vm2][0]\n                cut = self.get_cut(self.division[vm1])\n                vmSE = - (cut / self.vol) * math.log2(volume / self.vol)\n                vmnodeSE = self.struc_data[vm1][3] - (self.struc_data[vm1][0] / self.vol) * math.log2(\n                    self.struc_data[vm1][0] / volume) + \\\n                           self.struc_data[vm2][3] - (self.struc_data[vm2][0] / self.vol) * math.log2(\n                    self.struc_data[vm2][0] / volume)\n                self.struc_data[vm1] = [volume, cut, vmSE, vmnodeSE]\n                self.struc_data.pop(vm2)\n\n                struc_data_2d_new = {}\n                for k in self.struc_data_2d.keys():\n                    if k[0] == vm2 or k[1] == vm2:\n                        continue\n                    elif k[0] == vm1 or k[1] == vm1:\n                        v1 = k[0]\n                        v2 = k[1]\n                        comm_merged = self.division[v1] + self.division[v2]\n                        gm = self.get_cut(comm_merged)\n                        vm = self.struc_data[v1][0] + self.struc_data[v2][0]\n                        if self.struc_data[v1][0] == 0 or self.struc_data[v2][0] == 0:\n                            vmSE = self.struc_data[v1][2] + self.struc_data[v2][2]\n                            vmnodeSE = self.struc_data[v1][3] + self.struc_data[v2][3]\n                        else:\n                            vmSE = - (gm / self.vol) * math.log2(vm / self.vol)\n                            vmnodeSE = self.struc_data[v1][3] - (self.struc_data[v1][0] / self.vol) * math.log2(\n                                self.struc_data[v1][0] / vm) + \\\n                                       self.struc_data[v2][3] - (self.struc_data[v2][0] / self.vol) * math.log2(\n                                self.struc_data[v2][0] / vm)\n                        struc_data_2d_new[k] = [vm, gm, vmSE, vmnodeSE]\n                    else:\n                        struc_data_2d_new[k] = self.struc_data_2d[k]\n                self.struc_data_2d = struc_data_2d_new\n            else:\n                break\n\ndef vanilla_2D_SE_mini(weighted_edges):\n    \"\"\"\n    vanilla (greedy) 2D SE minimization\n    \"\"\"\n    g = nx.Graph()\n    g.add_weighted_edges_from(weighted_edges)\n\n    seg = SE(g)\n    seg.init_division()\n    # seg.show_division()\n    SE1D = seg.calc_1dSE()\n\n    seg.update_struc_data()\n    # seg.show_struc_data()\n    seg.update_struc_data_2d()\n    # seg.show_struc_data_2d()\n    initial_SE2D = seg.calc_2dSE()\n\n    seg.update_division_MinSE()\n    communities = seg.division\n    minimized_SE2D = seg.calc_2dSE()\n\n    return SE1D, initial_SE2D, minimized_SE2D, communities\n\ndef test_vanilla_2D_SE_mini():\n    weighted_edges = [(1, 2, 2), (1, 3, 4)]\n\n    g = nx.Graph()\n    g.add_weighted_edges_from(weighted_edges)\n    A = nx.adjacency_matrix(g).todense()\n    print('adjacency matrix: \\n', A)\n    print('g.nodes: ', g.nodes)\n    print('g.edges: ', g.edges)\n    print('degrees of nodes: ', list(g.degree(g.nodes, weight='weight')))\n\n    SE1D, initial_SE2D, minimized_SE2D, communities = vanilla_2D_SE_mini(weighted_edges)\n    print('\\n1D SE of the graph: ', SE1D)\n    print('initial 2D SE of the graph: ', initial_SE2D)\n    print('the minimum 2D SE of the graph: ', minimized_SE2D)\n    print('communities detected: ', communities)\n    return\n\ndef replaceAtUser(text):\n    \"\"\" Replaces \"@user\" with \"\" \"\"\"\n    text = re.sub('@[^\\s]+|RT @[^\\s]+', '', text)\n    return text\n\n\ndef removeUnicode(text):\n    \"\"\" Removes unicode strings like \"\\u002c\" and \"x96\" \"\"\"\n    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)', r'', text)\n    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n    return text\n\n\ndef replaceURL(text):\n    \"\"\" Replaces url address with \"url\" \"\"\"\n    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'url', text)\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\n\ndef replaceMultiExclamationMark(text):\n    \"\"\" Replaces repetitions of exlamation marks \"\"\"\n    text = re.sub(r\"(\\!)\\1+\", '!', text)\n    return text\n\n\ndef replaceMultiQuestionMark(text):\n    \"\"\" Replaces repetitions of question marks \"\"\"\n    text = re.sub(r\"(\\?)\\1+\", '?', text)\n    return text\n\n\ndef removeEmoticons(text):\n    \"\"\" Removes emoticons from text \"\"\"\n    text = re.sub(\n        ':\\)|;\\)|:-\\)|\\(-:|:-D|=D|:P|xD|X-p|\\^\\^|:-*|\\^\\.\\^|\\^\\-\\^|\\^\\_\\^|\\,-\\)|\\)-:|:\\'\\(|:\\(|:-\\(|:\\S|T\\.T|\\.\\_\\.|:<|:-\\S|:-<|\\*\\-\\*|:O|=O|=\\-O|O\\.o|XO|O\\_O|:-\\@|=/|:/|X\\-\\(|>\\.<|>=\\(|D:',\n        '', text)\n    return text\n\ndef removeNewLines(text):\n    text = re.sub('\\n', '', text)\n    return text\n\ndef preprocess_sentence(s):\n    return removeNewLines(replaceAtUser(\n        removeEmoticons(replaceMultiQuestionMark(replaceMultiExclamationMark(removeUnicode(replaceURL(s)))))))\n\ndef preprocess_french_sentence(s):\n    return removeNewLines(\n        replaceAtUser(removeEmoticons(replaceMultiQuestionMark(replaceMultiExclamationMark(replaceURL(s))))))\n\ndef SBERT_embed(s_list, language):\n    '''\n    Use Sentence-BERT to embed sentences.\n    s_list: a list of sentences/ tokens to be embedded.\n    language: the language of the sentences ('English', 'French', 'Arabic').\n    output: the embeddings of the sentences/ tokens.\n    '''\n    # Model paths or names for each language\n    model_map = {\n        'English': '../model/model_needed/all-MiniLM-L6-v2',\n        'French': '../model/model_needed/distiluse-base-multilingual-cased-v1',\n        'Arabic': '../model/model_needed/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Default model for Hugging Face\n    hf_model_map = {\n        'English': 'sentence-transformers/all-MiniLM-L6-v2',\n        'French': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n        'Arabic': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'\n    }\n\n    # Print language and model being used\n    print(f\"Embedding sentences in language: {language}\")\n    \n    # Determine model path\n    model_path = model_map.get(language)\n    if not model_path:\n        raise ValueError(f\"Unsupported language: {language}. Supported languages are: {', '.join(model_map.keys())}\")\n\n    print(f\"Using model: {model_path}\")\n\n    # Load the model, downloading if necessary\n    try:\n        model = SentenceTransformer(model_path)\n        print(f\"Successfully loaded model from local path: {model_path}\")\n    except Exception as e:\n        print(f\"Model {model_path} not found locally. Attempting to download from Hugging Face...\")\n        model = SentenceTransformer(hf_model_map[language])\n        print(f\"Model downloaded from Hugging Face: {hf_model_map[language]}\")\n\n    # Compute embeddings\n    embeddings = model.encode(s_list, convert_to_tensor=True, normalize_embeddings=True)\n    print(f\"Computed embeddings for {len(s_list)} sentences/tokens.\")\n    \n    return embeddings.cpu()\n\n\n\ndef evaluate(labels_true, labels_pred):\n    nmi = normalized_mutual_info_score(labels_true, labels_pred)\n    ami = adjusted_mutual_info_score(labels_true, labels_pred)\n    ari = adjusted_rand_score(labels_true, labels_pred)\n    return nmi, ami, ari\n\n\ndef decode(division):\n    if type(division) is dict:\n        prediction_dict = {m: event for event, messages in division.items() for m in messages}\n    elif type(division) is list:\n        prediction_dict = {m: event for event, messages in enumerate(division) for m in messages}\n    prediction_dict_sorted = dict(sorted(prediction_dict.items()))\n    return list(prediction_dict_sorted.values())\n\n"}
