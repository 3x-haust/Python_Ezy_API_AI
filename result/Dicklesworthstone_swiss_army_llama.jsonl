{"repo_info": {"repo_name": "swiss_army_llama", "repo_owner": "Dicklesworthstone", "repo_url": "https://github.com/Dicklesworthstone/swiss_army_llama"}}
{"type": "test_file", "path": "tests/conftest.py", "content": ""}
{"type": "test_file", "path": "tests/test_sentiment_score_generation.py", "content": "import unittest\nimport asyncio\nfrom sentiment_score_generation import (\n    generate_all_prompts,\n    generate_llm_sentiment_score_prompt,\n    combine_populated_prompts_with_source_text,\n    validate_llm_generated_sentiment_response,\n    run_llm_in_process,\n    parallel_attempt,\n    combine_llm_generated_sentiment_responses,\n    analyze_focus_area_sentiments\n)\n\nclass TestSentimentScoreGeneration(unittest.TestCase):\n\n    def setUp(self):\n        self.focus_key = \"financial_investor_focus\"\n        self.scoring_scale_explanation = \"Test explanation\"\n        self.source_text_positive = \"The company has shown impressive growth this year.\"\n        self.source_text_negative = \"The company's performance has been disappointing.\"\n        self.model_name = \"Test model\"\n\n    def test_generate_all_prompts(self):\n        populated_prompts = generate_all_prompts(self.focus_key, self.scoring_scale_explanation)\n        self.assertIsInstance(populated_prompts, dict)\n        self.assertTrue('Optimistic' in populated_prompts)\n\n    def test_generate_llm_sentiment_score_prompt(self):\n        prompt = generate_llm_sentiment_score_prompt('Optimistic', 'Positive outlook', 'Investors', self.scoring_scale_explanation)\n        self.assertIsInstance(prompt, str)\n\n    def test_combine_populated_prompts_with_source_text(self):\n        combined = combine_populated_prompts_with_source_text('Test prompt', 'Test source')\n        self.assertIsInstance(combined, str)\n\n    def test_validate_llm_generated_sentiment_response(self):\n        sentiment_score, justification = validate_llm_generated_sentiment_response('5 | Test justification', -10, 10)\n        self.assertIsInstance(sentiment_score, float)\n        self.assertIsInstance(justification, str)\n\n    def test_run_llm_in_process(self):\n        result = run_llm_in_process('Test prompt', 'Test model')\n        self.assertIsInstance(result, dict)\n\n    def test_parallel_attempt(self):\n        loop = asyncio.get_event_loop()\n        result = loop.run_until_complete(parallel_attempt('Test prompt', 'Test model'))\n        self.assertIsInstance(result, str)\n\n    def test_combine_llm_generated_sentiment_responses(self):\n        outputs = ['5 | Justification', '6 | Justification']\n        mean_score, ci, iqr, iqr_pct, justifications = combine_llm_generated_sentiment_responses(outputs, -10, 10)\n        self.assertIsInstance(mean_score, float)\n        self.assertIsInstance(ci, list)\n        self.assertIsInstance(iqr, list)\n        self.assertIsInstance(iqr_pct, float)\n        self.assertIsInstance(justifications, list)\n\n    def test_analyze_focus_area_sentiments(self):\n        loop = asyncio.get_event_loop()\n        result = loop.run_until_complete(analyze_focus_area_sentiments(self.focus_key, self.scoring_scale_explanation, self.source_text, self.model_name))\n        self.assertIsInstance(result, dict)\n\n    def test_positive_sentiment(self):\n        loop = asyncio.get_event_loop()\n        result = loop.run_until_complete(analyze_focus_area_sentiments(self.focus_key, self.scoring_scale_explanation, self.source_text_positive, self.model_name))\n        self.assertIsInstance(result, dict)\n        self.assertGreater(result['individual_sentiment_report_dict']['Optimistic']['sentiment_scores_dict']['mean_sentiment_score'], 50)\n\n    def test_negative_sentiment(self):\n        loop = asyncio.get_event_loop()\n        result = loop.run_until_complete(analyze_focus_area_sentiments(self.focus_key, self.scoring_scale_explanation, self.source_text_negative, self.model_name))\n        self.assertIsInstance(result, dict)\n        self.assertLess(result['individual_sentiment_report_dict']['Optimistic']['sentiment_scores_dict']['mean_sentiment_score'], -50)\n        \nif __name__ == '__main__':\n    unittest.main()\n"}
{"type": "test_file", "path": "tests/swiss_army_llama/test_build_faiss_indexes.py", "content": "import pytest\nimport json\nfrom unittest.mock import AsyncMock\nimport numpy as np\nfrom swiss_army_llama import build_faiss_indexes, AsyncSessionLocal\nimport faiss\n\n@pytest.mark.asyncio\nasync def test_build_faiss_indexes(monkeypatch):\n    # Mocking data returned from the database for embeddings\n    mock_embedding_data = [(\"model1\", \"text1\", json.dumps([1.0, 1.0])), (\"model1\", \"text2\", json.dumps([1.0, 1.0]))]\n    \n    # Mocking SQLAlchemy execute method to return our mock data\n    async def mock_execute(*args, **kwargs):\n        if \"SELECT llm_model_name, text, embedding_json FROM embeddings\" in args[0]:\n            return AsyncMock(fetchall=AsyncMock(return_value=mock_embedding_data))()\n\n    # Mocking the database session\n    monkeypatch.setattr(AsyncSessionLocal, \"execute\", mock_execute)\n    \n    # Run the function to test\n    faiss_indexes, associated_texts_by_model_and_pooling_method = await build_faiss_indexes()\n    \n    # Verify that FAISS indexes have been built for the mock data\n    assert \"model1\" in faiss_indexes\n    \n    # Verify that associated texts have been correctly identified\n    assert associated_texts_by_model_and_pooling_method[\"model1\"] == [\"text1\", \"text2\"]\n    \n    # Verify that the FAISS index is valid\n    embedding_array = np.array([[1.0, 1.0], [1.0, 1.0]]).astype('float32')\n    faiss.normalize_L2(embedding_array)\n    assert faiss_indexes[\"model1\"].ntotal == len(embedding_array)\n"}
{"type": "test_file", "path": "tests/swiss_army_llama/test_database_operations.py", "content": "import pytest\nimport asyncio\nfrom datetime import datetime\nfrom sqlalchemy import select\nfrom swiss_army_llama import DatabaseWriter, execute_with_retry, engine, AsyncSessionLocal\nfrom embeddings_data_models import Base, TextEmbedding, DocumentEmbedding, Document, AudioTranscript\nfrom sqlalchemy.exc import OperationalError\n\n@pytest.fixture(scope='module')\nasync def setup_db():\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n    yield\n    await engine.dispose()\n\n@pytest.fixture\ndef db_writer():\n    queue = asyncio.Queue()\n    return DatabaseWriter(queue)\n\n@pytest.mark.asyncio\n@pytest.mark.usefixtures(\"setup_db\")\nasync def test_enqueue_text_embedding_write(db_writer):\n    async with AsyncSessionLocal() as session:\n        text_embedding = TextEmbedding(\n            text=\"text\",\n            llm_model_name=\"model\",\n            embedding_json=\"{}\",\n            ip_address=\"127.0.0.1\",\n            request_time=datetime.now(),\n            response_time=datetime.now(),\n            total_time=1.0\n        )\n        await db_writer.enqueue_write([text_embedding])\n        await db_writer.dedicated_db_writer()\n        result = await execute_with_retry(session, select(TextEmbedding).where(TextEmbedding.text == \"text\"), OperationalError)\n        assert result.scalar_one().text == \"text\"\n\n@pytest.mark.asyncio\n@pytest.mark.usefixtures(\"setup_db\")\nasync def test_enqueue_document_embedding_write(db_writer):\n    async with AsyncSessionLocal() as session:\n        doc_embedding = DocumentEmbedding(\n            document_hash=\"doc_hash\",\n            filename=\"file\",\n            mimetype=\"text\",\n            document_file_hash=\"document_file_hash\",\n            llm_model_name=\"model\",\n            file_data=b\"data\",\n            document_embedding_results_json={},\n            ip_address=\"127.0.0.1\",\n            request_time=datetime.now(),\n            response_time=datetime.now(),\n            total_time=1.0\n        )\n        await db_writer.enqueue_write([doc_embedding])\n        await db_writer.dedicated_db_writer()\n        result = await execute_with_retry(session, select(DocumentEmbedding).where(DocumentEmbedding.filename == \"file\"), OperationalError)\n        assert result.scalar_one().filename == \"file\"\n\n@pytest.mark.asyncio\n@pytest.mark.usefixtures(\"setup_db\")\nasync def test_enqueue_document_write(db_writer):\n    async with AsyncSessionLocal() as session:\n        document = Document(\n            llm_model_name=\"model\",\n            document_hash=\"doc_hash\"\n        )\n        await db_writer.enqueue_write([document])\n        await db_writer.dedicated_db_writer()\n        result = await execute_with_retry(session, select(Document).where(Document.document_hash == \"doc_hash\"), OperationalError)\n        assert result.scalar_one().document_hash == \"doc_hash\"\n\n@pytest.mark.asyncio\n@pytest.mark.usefixtures(\"setup_db\")\nasync def test_enqueue_audio_transcript_write(db_writer):\n    async with AsyncSessionLocal() as session:\n        audio_transcript = AudioTranscript(\n            audio_file_hash=\"audio_hash\",\n            audio_file_name=\"audio_name\",\n            audio_file_size_mb=1.0,\n            segments_json={},\n            combined_transcript_text=\"text\",\n            combined_transcript_text_list_of_metadata_dicts={},\n            info_json={},\n            ip_address=\"127.0.0.1\",\n            request_time=datetime.now(),\n            response_time=datetime.now(),\n            total_time=1.0\n        )\n        await db_writer.enqueue_write([audio_transcript])\n        await db_writer.dedicated_db_writer()\n        result = await execute_with_retry(session, select(AudioTranscript).where(AudioTranscript.audio_file_hash == \"audio_hash\"), OperationalError)\n        assert result.scalar_one().audio_file_hash == \"audio_hash\"\n"}
{"type": "test_file", "path": "tests/swiss_army_llama/test_audio_transcription_functions.py", "content": "import pytest\nimport json\nimport os\nimport re\nimport shutil\nimport tempfile\nfrom hashlib import sha3_256\nfrom datetime import datetime\nfrom fastapi import Request\nfrom fastapi.datastructures import UploadFile\nfrom sqlalchemy import create_engine, text as sql_text\nfrom sqlalchemy.ext.asyncio import AsyncSession, create_async_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom swiss_army_llama import (normalize_logprobs, remove_pagination_breaks, sophisticated_sentence_splitter, get_transcript_from_db, save_transcript_to_db, execute_with_retry, db_writer,\n                            merge_transcript_segments_into_combined_text, compute_and_store_transcript_embeddings, compute_transcript_with_whisper_from_audio_func, get_or_compute_transcript)\nfrom embeddings_data_models import  AudioTranscript, AudioTranscriptResponse\n\n\nDATABASE_URL = \"sqlite+aiosqlite:///test_swiss_army_llama.sqlite\"\nengine = create_engine(DATABASE_URL)\nasync_engine = create_async_engine(DATABASE_URL)\n\nTestingSessionLocal = sessionmaker(\n    bind=engine,\n    expire_on_commit=False,\n)\n\n# Async Session for testing\nAsyncTestingSessionLocal = sessionmaker(\n    bind=async_engine,\n    class_=AsyncSession,\n    expire_on_commit=False,\n)\n\n@pytest.mark.asyncio\nasync def test_get_and_save_transcript():\n    audio_file_hash = \"test_audio_file_hash\"\n    audio_file_name = \"test_audio_file_name\"\n    audio_file_size_mb = 1.0\n    transcript_segments = json.dumps({\"test\": \"segment\"})\n    info = json.dumps({\"test\": \"info\"})\n    ip_address = \"127.0.0.1\"\n    request_time = datetime.now()\n    response_time = datetime.now()\n    total_time = 1.0\n    combined_transcript_text = \"test text\"\n    combined_transcript_text_list_of_metadata_dicts = json.dumps({\"test\": \"metadata\"})\n    \n    # Save transcript to DB\n    await save_transcript_to_db(\n        audio_file_hash, audio_file_name, audio_file_size_mb, transcript_segments, info,\n        ip_address, request_time, response_time, total_time, combined_transcript_text,\n        combined_transcript_text_list_of_metadata_dicts\n    )\n    await db_writer.dedicated_db_writer()\n\n    # Raw SQL query to validate data using sql_text\n    async with AsyncTestingSessionLocal() as session:\n        query = sql_text(\"SELECT audio_file_name FROM audio_transcripts WHERE audio_file_hash=:audio_file_hash\")\n        result = await session.execute(query, {\"audio_file_hash\": audio_file_hash})\n        row = result.fetchone()\n        assert row[0] == audio_file_name\n    \n    # Get transcript from DB using execute_with_retry\n    result = await execute_with_retry(get_transcript_from_db, audio_file_hash)\n        \n    assert isinstance(result, AudioTranscriptResponse)\n    assert result.audio_file_name == audio_file_name\n    assert result.audio_file_size_mb == audio_file_size_mb\n    \n    # Raw SQL query to validate data using sql_text\n    async with AsyncTestingSessionLocal() as session:\n        query = sql_text(\"SELECT audio_file_name FROM audio_transcripts WHERE audio_file_hash=:audio_file_hash\")\n        result = await session.execute(query, {\"audio_file_hash\": audio_file_hash})\n        row = result.fetchone()\n        assert row[0] == audio_file_name\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef setup_and_teardown():\n    # Setup: Create tables\n    async with AsyncTestingSessionLocal() as session:\n        await session.run_sync(AudioTranscript.metadata.create_all)\n    yield\n    # Teardown: Drop tables\n    async with AsyncTestingSessionLocal() as session:\n        await session.run_sync(AudioTranscript.metadata.drop_all)\n\n@pytest.mark.asyncio\nasync def test_text_related_functions():\n    # Testing normalize_logprobs\n    assert normalize_logprobs(5, 2, 10) == 0.375\n\n    # Testing remove_pagination_breaks (utilizes 're' module)\n    assert remove_pagination_breaks(\"This is a test-\\nexample.\") == \"This is a testexample.\"\n    \n    # Using re for an additional test\n    assert (re.match(r'^This is', 'This is a test')) is True\n\n    # Testing sophisticated_sentence_splitter\n    assert sophisticated_sentence_splitter(\"This is a test. And another.\") == [\"This is a test.\", \"And another.\"]\n\n\n@pytest.mark.asyncio\nasync def test_merge_transcript_segments_into_combined_text():\n    segments = [{\"start\": 0, \"end\": 2, \"text\": \"Hi\", \"avg_logprob\": -0.5},\n                {\"start\": 2, \"end\": 5, \"text\": \"there\", \"avg_logprob\": -0.7}]\n    combined_text, metadata_dicts, sentences = merge_transcript_segments_into_combined_text(segments)\n    assert combined_text == \"Hi there \"\n    assert metadata_dicts[0]['model_confidence_score'] == 1.0\n\n\n@pytest.mark.asyncio\nasync def test_get_or_compute_transcript():\n    # Preparing a fake audio file\n    audio_content = b\"fake_audio_data\"\n    audio_file = UploadFile(\"fake_audio.wav\", file=tempfile.NamedTemporaryFile(delete=False))\n    audio_file.file.write(audio_content)\n    audio_file.file.seek(0)\n    \n    # Hashing the audio content using sha3_256\n    audio_hash = sha3_256(audio_content).hexdigest()\n\n    # Simulate a request object\n    req = Request({\"type\": \"http\", \"client\": (\"127.0.0.1\", 12345)}, {})\n            \n    # Actual function call\n    result = await get_or_compute_transcript(audio_file, True, \"test_model\", req)\n\n    # Validate\n    assert isinstance(result, AudioTranscriptResponse)\n    assert result.audio_file_name == \"fake_audio.wav\"\n\n    # Compute the hash and validate\n    audio_hash = sha3_256(audio_content).hexdigest()\n    assert result.audio_file_hash == audio_hash  # Using the hash here for validation\n\n    # Compute and store transcript embeddings (Mocking the function for test)\n    # Here, you can replace 'dummy_transcript' and 'dummy_model' with actual data if available\n    await compute_and_store_transcript_embeddings('dummy_transcript', [], 'dummy_model', '127.0.0.1', 'dummy_text', req)\n\n    # Cleanup\n    audio_file.file.close()\n    shutil.rmtree('generated_transcript_embeddings_zip_files', ignore_errors=True)\n    os.remove(audio_file.file.name)\n    \n\n@pytest.mark.asyncio\nasync def test_compute_transcript_with_whisper_from_audio_func():\n    audio_file_hash = \"test_audio_file_hash\"\n    audio_file_path = \"/path/to/audio/file\"\n    audio_file_name = \"test_audio_file_name.wav\"\n    audio_file_size_mb = 1.0\n    ip_address = \"127.0.0.1\"\n\n    # Simulate a request object\n    req = Request({\"type\": \"http\", \"client\": (\"127.0.0.1\", 12345)}, {})\n\n    # Calling compute_transcript_with_whisper_from_audio_func\n    segment_details, info_dict, combined_transcript_text, combined_transcript_text_list_of_metadata_dicts, request_time, response_time, total_time, download_url = await compute_transcript_with_whisper_from_audio_func(\n        audio_file_hash, audio_file_path, audio_file_name, audio_file_size_mb, ip_address, req\n    )\n\n    # Validate (since this is a test, you may need to adjust these assertions based on what compute_transcript_with_whisper_from_audio_func actually returns)\n    assert segment_details is not None\n    assert info_dict is not None\n    assert combined_transcript_text is not None\n    assert combined_transcript_text_list_of_metadata_dicts is not None\n    assert request_time is not None\n    assert response_time is not None\n    assert total_time is not None    "}
{"type": "test_file", "path": "tests/test_log_viewer_functions.py", "content": "import os\nimport pytest\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\nfrom log_viewer_functions import safe_highlight_func, highlight_rules_func, show_logs_func, show_logs_incremental_func\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef prepare_log_file():\n    # Create a more realistic sample log file for testing\n    log_file_path = 'swiss_army_llama.log'\n    now = datetime.now(timezone('UTC'))\n    five_minutes_ago = now - timedelta(minutes=5)\n    sample_logs = f\"\"\"{five_minutes_ago.strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]} - This is a success.\n{now.strftime('%Y-%m-%d %H:%M:%S,%f')[:-3]} - This is an error.\n\"\"\"\n    with open(log_file_path, 'w') as f:\n        f.write(sample_logs)\n    yield\n    os.remove(log_file_path)\n\ndef test_safe_highlight_func():\n    assert safe_highlight_func(\"Hello world\", r\"world\", \"WORLD\") == \"Hello WORLD\"\n    assert safe_highlight_func(\"Hello world\", r\"[\", \"WORLD\") == \"Hello world\"\n\ndef test_highlight_rules_func():\n    assert highlight_rules_func(\"This is a success.\") == '<span style=\"color: #baffc9;\">This</span> is a <span style=\"color: #baffc9;\">success</span>.'\n\ndef test_show_logs_func(prepare_log_file):\n    logs = show_logs_func(5)\n    assert \"success\" in logs\n    assert \"error\" in logs\n\ndef test_show_logs_incremental_func(prepare_log_file):\n    result = show_logs_incremental_func(5, 0)\n    assert \"success\" in result[\"logs\"]\n    assert \"error\" in result[\"logs\"]\n"}
{"type": "test_file", "path": "tests/swiss_army_llama/test_ramdisk_management.py", "content": "import pytest\nimport os\nimport subprocess\nfrom swiss_army_llama import check_that_user_has_required_permissions_to_manage_ramdisks, setup_ramdisk, copy_models_to_ramdisk, clear_ramdisk, RAMDISK_PATH\n\n@pytest.mark.skipif(not os.environ.get('RUN_SUDO_TESTS'), reason=\"requires admin rights\")\ndef test_check_user_permission_for_ramdisk():\n    assert check_that_user_has_required_permissions_to_manage_ramdisks() is True\n\n@pytest.mark.skipif(not os.environ.get('RUN_SUDO_TESTS'), reason=\"requires admin rights\")\ndef test_setup_ramdisk():\n    setup_ramdisk()\n    cmd_check = f\"mount | grep {RAMDISK_PATH}\"\n    result = subprocess.run(cmd_check, shell=True, stdout=subprocess.PIPE).stdout.decode('utf-8')\n    assert RAMDISK_PATH in result\n\n@pytest.mark.skipif(not os.environ.get('RUN_SUDO_TESTS'), reason=\"requires admin rights\")\ndef test_copy_models_to_ramdisk():\n    models_directory = \"./test_models/\"\n    ramdisk_directory = RAMDISK_PATH\n    os.makedirs(models_directory, exist_ok=True)\n    with open(f\"{models_directory}/dummy_model.bin\", \"wb\") as f:\n        f.write(b\"Dummy data\")\n    copy_models_to_ramdisk(models_directory, ramdisk_directory)\n    assert os.path.exists(os.path.join(ramdisk_directory, \"dummy_model.bin\")) is True\n\n@pytest.mark.skipif(not os.environ.get('RUN_SUDO_TESTS'), reason=\"requires admin rights\")\ndef test_clear_ramdisk():\n    clear_ramdisk()\n    cmd_check = f\"mount | grep {RAMDISK_PATH}\"\n    result = subprocess.run(cmd_check, shell=True, stdout=subprocess.PIPE).stdout.decode('utf-8')\n    assert RAMDISK_PATH not in result\n"}
{"type": "test_file", "path": "tests/swiss_army_llama/test_service_functions.py", "content": ""}
{"type": "test_file", "path": "tests/swiss_army_llama/test_views.py", "content": ""}
{"type": "source_file", "path": "swiss_army_llama.py", "content": "import shared_resources\nfrom shared_resources import initialize_globals, download_models, is_gpu_available\nfrom logger_config import setup_logger\nfrom database_functions import AsyncSessionLocal\nfrom ramdisk_functions import clear_ramdisk\nfrom misc_utility_functions import  build_faiss_indexes, configure_redis_optimally\nfrom embeddings_data_models import DocumentEmbedding, ShowLogsIncrementalModel\nfrom embeddings_data_models import EmbeddingRequest, SemanticSearchRequest, AdvancedSemanticSearchRequest, SimilarityRequest, TextCompletionRequest, AddGrammarRequest\nfrom embeddings_data_models import EmbeddingResponse, SemanticSearchResponse, AdvancedSemanticSearchResponse, SimilarityResponse, AllStringsResponse, AllDocumentsResponse, TextCompletionResponse, AudioTranscriptResponse, ImageQuestionResponse, AddGrammarResponse\nfrom service_functions import get_or_compute_embedding, get_or_compute_transcript, add_model_url, download_file, start_resource_monitoring, end_resource_monitoring, decompress_data\nfrom service_functions import get_list_of_corpus_identifiers_from_list_of_embedding_texts, compute_embeddings_for_document, parse_submitted_document_file_into_sentence_strings_func\nfrom service_functions import generate_completion_from_llm, ask_question_about_image, validate_bnf_grammar_func, convert_document_to_sentences_func, get_audio_duration_seconds, prepare_string_for_embedding\nfrom grammar_builder import GrammarBuilder\nfrom log_viewer_functions import show_logs_incremental_func, show_logs_func\nfrom uvicorn_config import option\nimport asyncio\nimport glob\nimport json\nimport os \nimport sys\nimport random\nimport tempfile\nimport traceback\nimport zipfile\nfrom pathlib import Path\nfrom datetime import datetime\nfrom hashlib import sha3_256\nfrom typing import List, Optional, Dict, Any\nfrom urllib.parse import unquote\nimport numpy as np\nfrom decouple import config\nimport uvicorn\nimport fastapi\nfrom fastapi import FastAPI, HTTPException, Request, UploadFile, File, Form\nfrom fastapi.responses import JSONResponse, FileResponse, HTMLResponse\nfrom contextlib import asynccontextmanager\nfrom sqlalchemy import select\nfrom sqlalchemy import text as sql_text\nfrom sqlalchemy.exc import SQLAlchemyError\nimport faiss\nimport fast_vector_similarity as fvs\nimport uvloop\nfrom magika import Magika\n\nasyncio.set_event_loop_policy(uvloop.EventLoopPolicy())\nlogger = setup_logger()\nmagika = Magika()\ngpu_check_results = is_gpu_available()\nconfigure_redis_optimally()\nlogger.info(f\"\\nGPU check results:\\n {gpu_check_results}\\n\")\n\nclass GracefulExit(BaseException):\n    pass\n\ndef raise_graceful_exit():\n    raise GracefulExit()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Startup code\n    await initialize_globals()\n    yield\n    # Shutdown code (if any)\n    pass\n\n# Note: the Ramdisk setup and teardown requires sudo; to enable password-less sudo, edit your sudoers file with `sudo visudo`.\n# Add the following lines, replacing username with your actual username\n# username ALL=(ALL) NOPASSWD: /bin/mount -t tmpfs -o size=*G tmpfs /mnt/ramdisk\n# username ALL=(ALL) NOPASSWD: /bin/umount /mnt/ramdisk\n\n# Global variables\nuse_hardcoded_security_token = 0\nif use_hardcoded_security_token:\n    SECURITY_TOKEN = \"Test123$\"\n    USE_SECURITY_TOKEN = config(\"USE_SECURITY_TOKEN\", default=False, cast=bool)\nelse:\n    USE_SECURITY_TOKEN = False\nDEFAULT_MODEL_NAME = config(\"DEFAULT_MODEL_NAME\", default=\"Meta-Llama-3-8B-Instruct.Q3_K_S\", cast=str) \nDEFAULT_EMBEDDING_MODEL_NAME = config(\"DEFAULT_EMBEDDING_MODEL_NAME\", default=\"nomic-embed-text-v1.5.Q6_K\", cast=str)\nDEFAULT_MULTI_MODAL_MODEL_NAME = config(\"DEFAULT_MULTI_MODAL_MODEL_NAME\", default=\"llava-llama-3-8b-v1_1-int4\", cast=str)\nUSE_RAMDISK = config(\"USE_RAMDISK\", default=False, cast=bool)\nUSE_RESOURCE_MONITORING = config(\"USE_RESOURCE_MONITORING\", default=1, cast=bool)\nRAMDISK_PATH = config(\"RAMDISK_PATH\", default=\"/mnt/ramdisk\", cast=str)\nBASE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\nMAX_THOUSANDS_OF_WORDs_FOR_DOCUMENT_EMBEDDING = config(\"MAX_THOUSANDS_OF_WORDs_FOR_DOCUMENT_EMBEDDING\", default=100, cast=int)\nDEFAULT_COMPLETION_TEMPERATURE = config(\"DEFAULT_COMPLETION_TEMPERATURE\", default=0.7, cast=float)\nDEFAULT_MAX_COMPLETION_TOKENS = config(\"DEFAULT_MAX_COMPLETION_TOKENS\", default=1000, cast=int)\nDEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE = config(\"DEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE\", default=1, cast=int)\nDEFAULT_EMBEDDING_POOLING_METHOD = config(\"DEFAULT_EMBEDDING_POOLING_METHOD\", default=\"mean\", cast=str)\n\nlogger.info(f\"USE_RAMDISK is set to: {USE_RAMDISK}\")\n\ndescription_string = \"\"\"\nðŸ‡¨ðŸ‡­ðŸŽ–ï¸ðŸ¦™ Swiss Army Llama is your One-Stop-Shop to Quickly and Conveniently Integrate Powerful Local LLM Functionality into your Project via a REST API.\n\"\"\"\napp = FastAPI(title=\"Swiss Army Llama\", description=description_string, docs_url=\"/\", lifespan=lifespan)  # Set the Swagger UI to root\n\n    \n@app.exception_handler(SQLAlchemyError) \nasync def sqlalchemy_exception_handler(request: Request, exc: SQLAlchemyError) -> JSONResponse:\n    logger.exception(exc)\n    return JSONResponse(status_code=500, content={\"message\": \"Database error occurred\"})\n\n@app.exception_handler(Exception)\nasync def general_exception_handler(request: Request, exc: Exception) -> JSONResponse:\n    logger.exception(exc)\n    return JSONResponse(status_code=500, content={\"message\": \"An unexpected error occurred\"})\n\n\n@app.get(\"/\", include_in_schema=False)\nasync def custom_swagger_ui_html():\n    return fastapi.templating.get_swagger_ui_html(openapi_url=\"/openapi.json\", title=app.title, swagger_favicon_url=app.swagger_ui_favicon_url)\n\n\n@app.get(\"/get_list_of_available_model_names/\",\n        summary=\"Retrieve Available Model Names\",\n        description=\"\"\"Retrieve the list of available model names for generating embeddings.\n\n### Parameters:\n- `token`: Security token (optional).\n\n### Response:\nThe response will include a JSON object containing the list of available model names. Note that these are all GGML format models designed to work with llama_cpp.\n\n### Example Response:\n```json\n{\n    \"model_names\": [\"Meta-Llama-3-8B-Instruct.Q3_K_S\", \"Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M\", \"my_super_custom_model\"]\n}\n```\"\"\",\n        response_description=\"A JSON object containing the list of available model names.\")\nasync def get_list_of_available_model_names(token: str = None) -> Dict[str, List[str]]:\n    if USE_SECURITY_TOKEN and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    models_dir = os.path.join(RAMDISK_PATH, 'models') if USE_RAMDISK else os.path.join(BASE_DIRECTORY, 'models')\n    logger.info(f\"Looking for models in: {models_dir}\") # Add this line for debugging\n    logger.info(f\"Directory content: {os.listdir(models_dir)}\") # Add this line for debugging\n    model_files = glob.glob(os.path.join(models_dir, \"*.bin\")) + glob.glob(os.path.join(models_dir, \"*.gguf\")) # Find all files with .bin or .gguf extension\n    model_names = sorted([os.path.splitext(os.path.basename(model_file))[0] for model_file in model_files]) # Remove both extensions, but ignore other periods in the filename\n    return {\"model_names\": model_names}\n\n\n@app.get(\"/get_list_of_available_bnf_grammars\",\n        response_model=List[str],\n        summary=\"Get Available BNF Grammars\",\n        description=\"Returns a list of all the valid .gbnf files in the grammar_files directory.\",\n        response_description=\"A list containing the names of all valid .gbnf files.\")\nasync def get_list_of_available_bnf_grammars(token: str = None) -> List[str]:\n    if USE_SECURITY_TOKEN and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")    \n    try:\n        grammar_files_dir = 'grammar_files'\n        if not os.path.exists(grammar_files_dir):\n            os.makedirs(grammar_files_dir)\n        valid_files = [f for f in os.listdir(grammar_files_dir) if f.endswith('.gbnf')]\n        return valid_files\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"An error occurred: {e}\")\n\n\n\n@app.get(\"/get_all_stored_strings/\",\n        summary=\"Retrieve All Strings\",\n        description=\"\"\"Retrieve a list of all stored strings from the database for which embeddings have been computed.\n\n### Parameters:\n- `token`: Security token (optional).\n\n### Response:\nThe response will include a JSON object containing the list of all stored strings with computed embeddings.\n\n### Example Response:\n```json\n{\n    \"strings\": [\"The quick brown fox jumps over the lazy dog\", \"To be or not to be\", \"Hello, World!\"]\n}\n```\"\"\",\n        response_description=\"A JSON object containing the list of all strings with computed embeddings.\")\nasync def get_all_stored_strings(req: Request, token: str = None) -> AllStringsResponse:\n    logger.info(\"Received request to retrieve all stored strings for which embeddings have been computed\")\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        logger.warning(f\"Unauthorized request to retrieve all stored strings for which embeddings have been computed from {req.client.host}\")\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    try:\n        logger.info(\"Retrieving all stored strings with computed embeddings from the database\")\n        async with AsyncSessionLocal() as session:\n            result = await session.execute(sql_text(\"SELECT DISTINCT text FROM embeddings\"))\n            all_strings = [row[0] for row in result.fetchall()]\n        logger.info(f\"Retrieved {len(all_strings):,} stored strings with computed embeddings from the database; Last 10 embedded strings: {all_strings[-10:]}\")\n        return {\"strings\": all_strings}\n    except Exception as e:\n        logger.error(f\"An error occurred while processing the request: {e}\")\n        logger.error(traceback.format_exc())  # Print the traceback\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n\n@app.get(\"/get_all_stored_documents/\",\n        summary=\"Retrieve All Stored Documents\",\n        description=\"\"\"Retrieve a list of all stored documents from the database for which embeddings have been computed.\n\n### Parameters:\n- `token`: Security token (optional).\n\n### Response:\nThe response will include a JSON object containing the list of all stored documents with computed embeddings.\n\n### Example Response:\n```json\n{\n    \"documents\": [\"document1.pdf\", \"document2.txt\", \"document3.md\", \"document4.json\"]\n}\n```\"\"\",\n        response_description=\"A JSON object containing the list of all documents with computed embeddings.\")\nasync def get_all_stored_documents(req: Request, token: str = None) -> AllDocumentsResponse:\n    logger.info(\"Received request to retrieve all stored documents with computed embeddings\")\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        logger.warning(f\"Unauthorized request to retrieve all stored documents for which all sentence embeddings have been computed from {req.client.host}\")\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    try:\n        logger.info(\"Retrieving all stored documents with computed embeddings from the database\")\n        async with AsyncSessionLocal() as session:\n            result = await session.execute(sql_text(\"SELECT DISTINCT filename FROM document_embeddings\"))\n            all_documents = [row[0] for row in result.fetchall()]\n        logger.info(f\"Retrieved {len(all_documents):,} stored documents with computed embeddings from the database; Last 10 processed document filenames: {all_documents[-10:]}\")\n        return {\"documents\": all_documents}\n    except Exception as e:\n        logger.error(f\"An error occurred while processing the request: {e}\")\n        logger.error(traceback.format_exc())  # Print the traceback\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n\n@app.post(\"/add_new_model/\",\n        summary=\"Add New Model by URL\",\n        description=\"\"\"Submit a new model URL for download and use. The model must satisfy the following criteria:\n\n1. Must be in `.gguf` format.\n2. Must be larger than 100 MB to ensure it's a valid model file.\n\n### Parameters:\n- `model_url`: The URL of the model weight file, which must end with `.gguf`. For example: `https://huggingface.co/kirp/TinyLlama-1.1B-Chat-v0.2-gguf/blob/main/ggml-model-q5_k_m.gguf`\n- `token`: Security token (optional).\n\n### Response:\nThe response will include a JSON object indicating whether the model was successfully added and downloaded. Possible status values are:\n\n- `success`: Model was added and downloaded successfully.\n- `failure`: Model download failed, likely because it's not a valid model file.\n- `error`: The URL did not point to a `.gguf` file.\n- `unknown`: An unexpected error occurred.\n\n### Example Response:\n```json\n{\n    \"status\": \"success\",\n    \"message\": \"Model added and downloaded successfully.\"\n}\n```\n\"\"\",\n        response_description=\"A JSON object indicating the status of the model addition and download.\")\nasync def add_new_model(model_url: str, token: str = None) -> Dict[str, Any]:\n    if USE_SECURITY_TOKEN and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    unique_id = f\"add_model_{hash(model_url)}\"     # Generate a unique lock ID based on the model_url\n    lock = await shared_resources.lock_manager.lock(unique_id)\n    if lock.valid:\n        try:\n            decoded_model_url = unquote(model_url)\n            if not decoded_model_url.endswith('.gguf'):\n                return {\"status\": \"error\", \"message\": \"Model URL must point to a .gguf file.\"}\n            corrected_model_url = add_model_url(decoded_model_url)\n            _, download_status = download_models()\n            status_dict = {status[\"url\"]: status for status in download_status}\n            if corrected_model_url in status_dict:\n                return {\"status\": status_dict[corrected_model_url][\"status\"], \"message\": status_dict[corrected_model_url][\"message\"]}\n            return {\"status\": \"unknown\", \"message\": \"Unexpected error.\"}\n        finally:\n            await shared_resources.lock_manager.unlock(lock)\n    else:\n        return {\"status\": \"already processing\", \"message\": \"Another worker is already processing this model URL.\"}\n\n\n\n@app.post(\"/get_embedding_vector_for_string/\",\n        response_model=EmbeddingResponse,\n        summary=\"Retrieve Embedding Vector for a Given Text String\",\n        description=\"\"\"Retrieve the embedding vector for a given input text string using the specified model.\n\n### Parameters:\n- `request`: A JSON object containing the input text string (`text`) and the model name.\n- `token`: Security token (optional).\n- `document_file_hash`: The SHA3-256 hash of the document file, if applicable (optional).\n\n### Request JSON Format:\nThe request must contain the following attributes:\n- `text`: The input text for which the embedding vector is to be retrieved.\n- `llm_model_name`: The model used to calculate the embedding (optional, will use the default model if not provided).\n- `embedding_pooling_method`: The method used to pool the embeddings (Choices: 'mean', 'mins_maxes', 'svd', 'svd_first_four', 'ica', 'factor_analysis', 'gaussian_random_projection'; default is 'mean').\n\n### Example (note that `llm_model_name` is optional):\n```json\n{\n    \"text\": \"This is a sample text.\",\n    \"llm_model_name\": \"nomic-embed-text-v1.5.Q6_K\",\n    \"embedding_pooling_method\": \"svd\",\n    \"corpus_identifier_string\": \"pastel_related_documentation_corpus\"\n}\n```\n\n### Response:\nThe response will include the embedding vector for the input text string.\n\n### Example Response:\n```json\n{\n    \"embedding\": [0.1234, 0.5678, ...]\n}\n```\"\"\", response_description=\"A JSON object containing the embedding vector for the input text.\")\nasync def get_embedding_vector_for_string(request: EmbeddingRequest, req: Request = None, token: str = None, client_ip: str = None, document_file_hash: str = None) -> EmbeddingResponse:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        logger.warning(f\"Unauthorized request from client IP {client_ip}\")\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    try:\n        request.text = prepare_string_for_embedding(request.text)\n        unique_id = f\"get_embedding_{request.text}_{request.llm_model_name}_{request.embedding_pooling_method}\"\n        lock = await shared_resources.lock_manager.lock(unique_id)\n        if lock.valid:\n            try:\n                input_data = {\"text\": request.text}\n                context = start_resource_monitoring(\"get_embedding_vector_for_string\", input_data, req.client.host if req else \"localhost\")\n                return await get_or_compute_embedding(request, req, client_ip, document_file_hash)\n            finally:\n                end_resource_monitoring(context)\n                await shared_resources.lock_manager.unlock(lock)\n        else:\n            return {\"status\": \"already processing\"}\n    except Exception as e:\n        logger.error(f\"An error occurred while processing the request: {e}\")\n        logger.error(traceback.format_exc()) # Print the traceback\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n\n\n@app.post(\"/compute_similarity_between_strings/\",\n        response_model=SimilarityResponse,\n        summary=\"Compute Similarity Between Two Strings\",\n        description=\"\"\"Compute the similarity between two given input strings using specified model embeddings and a selected similarity measure.\n\n### Parameters:\n- `request`: A JSON object containing the two strings, the model name, and the similarity measure.\n- `token`: Security token (optional).\n\n### Request JSON Format:\nThe request must contain the following attributes:\n- `text1`: The first input text.\n- `text2`: The second input text.\n- `llm_model_name`: The model used to calculate embeddings (optional).\n- `similarity_measure`: The similarity measure to be used. Supported measures include `all`, `spearman_rho`, `kendall_tau`, `approximate_distance_correlation`, `jensen_shannon_dependency_measure`, `normalized_mutual_information`, and `hoeffding_d` (optional, default is `all`).\n\n### Example Request (note that `llm_model_name` and `similarity_measure` are optional):\n```json\n{\n    \"text1\": \"This is a sample text.\",\n    \"text2\": \"This is another sample text.\",\n    \"llm_model_name\": \"nomic-embed-text-v1.5.Q6_K\",\n    \"similarity_measure\": \"all\"\n}\n```\"\"\")\nasync def compute_similarity_between_strings(request: SimilarityRequest, req: Request, token: str = None) -> SimilarityResponse:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    logger.info(f\"Received request: {request}\")\n    request_time = datetime.utcnow()\n    request.text1 = prepare_string_for_embedding(request.text1)\n    request.text2 = prepare_string_for_embedding(request.text2)\n    similarity_measure = request.similarity_measure.lower()\n    unique_id = f\"compute_similarity_{request.text1}_{request.text2}_{request.llm_model_name}_{request.embedding_pooling_method}_{similarity_measure}\"\n    lock = await shared_resources.lock_manager.lock(unique_id)\n    if lock.valid:\n        try:\n            client_ip = req.client.host if req else \"localhost\"\n            embedding_request1 = EmbeddingRequest(text=request.text1, llm_model_name=request.llm_model_name, embedding_pooling_method=request.embedding_pooling_method)\n            embedding_request2 = EmbeddingRequest(text=request.text2, llm_model_name=request.llm_model_name, embedding_pooling_method=request.embedding_pooling_method)\n            embedding1_response = await get_or_compute_embedding(request=embedding_request1, req=req, client_ip=client_ip, use_verbose=False)\n            embedding2_response = await get_or_compute_embedding(request=embedding_request2, req=req, client_ip=client_ip, use_verbose=False)\n            embedding1 = np.array(embedding1_response[\"embedding\"])\n            embedding2 = np.array(embedding2_response[\"embedding\"])\n            if embedding1.size == 0 or embedding2.size == 0:\n                raise HTTPException(status_code=400, detail=\"Could not calculate embeddings for the given texts\")\n            params = {\n                \"vector_1\": embedding1.tolist(),\n                \"vector_2\": embedding2.tolist(),\n                \"similarity_measure\": similarity_measure\n            }\n            similarity_stats_str = fvs.py_compute_vector_similarity_stats(json.dumps(params))\n            similarity_stats_json = json.loads(similarity_stats_str)\n            if similarity_measure == 'all':\n                similarity_score = similarity_stats_json\n            else:\n                similarity_score = similarity_stats_json.get(similarity_measure, None)\n                if similarity_score is None:\n                    raise HTTPException(status_code=400, detail=\"Invalid similarity measure specified\")\n            response_time = datetime.utcnow()\n            total_time = (response_time - request_time).total_seconds()\n            logger.info(f\"Computed similarity using {similarity_measure} in {total_time:,.2f} seconds; similarity score: {similarity_score}\")\n            return {\n                \"text1\": request.text1,\n                \"text2\": request.text2,\n                \"similarity_measure\": similarity_measure,\n                \"similarity_score\": similarity_score,\n                \"embedding1\": embedding1.tolist(),\n                \"embedding2\": embedding2.tolist()\n            }\n        except Exception as e:\n            logger.error(f\"An error occurred while processing the request: {e}\")\n            raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n        finally:\n            await shared_resources.lock_manager.unlock(lock)\n    else:\n        return {\"status\": \"already processing\"}\n\n\n\n@app.post(\"/search_stored_embeddings_with_query_string_for_semantic_similarity/\",\n        response_model=SemanticSearchResponse,\n        summary=\"Get Most Similar Strings from Stored Embeddings in Database\",\n        description=\"\"\"Find the most similar strings in the database to the given input \"query\" text. This endpoint uses a pre-computed FAISS index to quickly search for the closest matching strings.\n\n### Parameters:\n- `request`: A JSON object containing the query text, model name, an optional corpus identifier string to restrict the search to a specific corpus, and an optional number of most semantically similar strings to return.\n- `req`: HTTP request object (internal use).\n- `token`: Security token (optional).\n\n### Request JSON Format:\nThe request must contain the following attributes:\n- `query_text`: The input text for which to find the most similar string.\n- `llm_model_name`: The model used to calculate embeddings.\n- `embedding_pooling_method`: The method used to pool the embeddings (Choices: 'mean', 'mins_maxes', 'svd', 'svd_first_four', 'ica', 'factor_analysis', 'gaussian_random_projection'; default is 'mean').\n- `corpus_identifier_string`: An optional string identifier to restrict the search to a specific corpus.\n- `number_of_most_similar_strings_to_return`: (Optional) The number of most similar strings to return, defaults to 10.\n\n### Example:\n```json\n{\n    \"query_text\": \"Find me the most similar string!\",\n    \"llm_model_name\": \"nomic-embed-text-v1.5.Q6_K\",\n    \"corpus_identifier_string\": \"pastel_related_documentation_corpus\",\n    \"embedding_pooling_method\": \"mean\",\n    \"number_of_most_similar_strings_to_return\": 5\n}\n```\n\n### Response:\nThe response will include the most similar strings found in the database, along with the similarity scores and the corpus identifier string used for the search.\n\n### Example Response:\n```json\n{\n    \"query_text\": \"Find me the most similar string!\",  \n    \"results\": [\n        {\"search_result_text\": \"This is the most similar string!\", \"similarity_to_query_text\": 0.9823},\n        {\"search_result_text\": \"Another similar string.\", \"similarity_to_query_text\": 0.9721},\n        ...\n    ]\n}\n```\"\"\",\n        response_description=\"A JSON object containing the query text along with the most similar strings and similarity scores.\")\nasync def search_stored_embeddings_with_query_string_for_semantic_similarity(request: SemanticSearchRequest, req: Request, token: str = None) -> SemanticSearchResponse:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")                            \n    global faiss_indexes, associated_texts_by_model_and_pooling_method\n    request_time = datetime.utcnow()\n    request.query_text = prepare_string_for_embedding(request.query_text)\n    unique_id = f\"semantic_search_{request.query_text}_{request.llm_model_name}_{request.embedding_pooling_method}_{request.corpus_identifier_string}_{request.number_of_most_similar_strings_to_return}\"  # Unique ID for this operation\n    lock = await shared_resources.lock_manager.lock(unique_id)        \n    if lock.valid:\n        try:\n            faiss_indexes, associated_texts_by_model_and_pooling_method = await build_faiss_indexes(force_rebuild=True)\n            try:\n                faiss_index = faiss_indexes[(request.llm_model_name, request.embedding_pooling_method)]\n            except KeyError:\n                raise HTTPException(status_code=400, detail=f\"No FAISS index found for model: {request.llm_model_name} and pooling method: {request.embedding_pooling_method}\")\n            llm_model_name = request.llm_model_name\n            embedding_pooling_method = request.embedding_pooling_method\n            num_results = request.number_of_most_similar_strings_to_return\n            num_results_before_corpus_filter = num_results*25\n            total_entries = len(associated_texts_by_model_and_pooling_method[llm_model_name][embedding_pooling_method])  # Get the total number of entries for the model and pooling method\n            num_results = min(num_results, total_entries)  # Ensure num_results doesn't exceed the total number of entries\n            num_results_before_corpus_filter = min(num_results_before_corpus_filter, total_entries)  # Ensure num_results_before_corpus_filter doesn't exceed the total number of entries\n            logger.info(f\"Received request to find {num_results:,} most similar strings for query text: `{request.query_text}` using model: {llm_model_name}, pooling method: {embedding_pooling_method}, and corpus: {request.corpus_identifier_string}\")\n            try:\n                logger.info(f\"Computing embedding for input text: {request.query_text}\")\n                embedding_request = EmbeddingRequest(text=request.query_text, llm_model_name=request.llm_model_name, embedding_pooling_method=request.embedding_pooling_method, corpus_identifier_string=request.corpus_identifier_string)\n                embedding_response = await get_or_compute_embedding(embedding_request, req)                \n                embedding_json = embedding_response[\"text_embedding_dict\"][\"embedding_json\"]\n                embedding_vector = json.loads(embedding_json)\n                input_embedding = np.array(embedding_vector).astype('float32').reshape(1, -1)\n                faiss.normalize_L2(input_embedding)  # Normalize the input vector for cosine similarity\n                results = []  # Create an empty list to store the results\n                faiss_index = faiss_indexes[(llm_model_name, embedding_pooling_method)]\n                associated_texts = associated_texts_by_model_and_pooling_method[llm_model_name][embedding_pooling_method]\n                list_of_corpus_identifier_strings = await get_list_of_corpus_identifiers_from_list_of_embedding_texts(associated_texts, llm_model_name, embedding_pooling_method)\n                logger.info(f\"Searching for the most similar string in the FAISS index using {embedding_pooling_method} embeddings\")\n                if faiss_index is None:\n                    raise HTTPException(status_code=400, detail=f\"No FAISS index found for model: {llm_model_name} and pooling method: {embedding_pooling_method}\")\n                similarities, indices = faiss_index.search(input_embedding.reshape(1, -1), num_results_before_corpus_filter)  # Search for num_results similar strings\n                for ii in range(num_results_before_corpus_filter):\n                    index = indices[0][ii]\n                    if index < len(associated_texts):\n                        similarity = float(similarities[0][ii])  # Convert numpy.float32 to native float\n                        most_similar_text = associated_texts[index]\n                        corpus_identifier_string = list_of_corpus_identifier_strings[index]\n                        if (corpus_identifier_string == request.corpus_identifier_string) and (most_similar_text != request.query_text) and (len(results) <= num_results):\n                            results.append({\"search_result_text\": most_similar_text, \"similarity_to_query_text\": similarity})\n                    else:\n                        logger.warning(f\"Index {index} out of range for model {llm_model_name} and pooling method {embedding_pooling_method}\")\n                response_time = datetime.utcnow()\n                total_time = (response_time - request_time).total_seconds()\n                logger.info(f\"Finished searching for the most similar string in the FAISS index in {total_time:,.2f} seconds. Found {len(results):,} results, returning the top {num_results:,}.\")\n                logger.info(f\"Found most similar strings for query string {request.query_text}: {results}\")\n                if len(results) == 0:\n                    logger.info(f\"No results found for query string {request.query_text}.\")\n                    raise HTTPException(status_code=400, detail=f\"No results found for query string {request.query_text} and model {llm_model_name} and pooling method {embedding_pooling_method} and corpus {request.corpus_identifier_string}.\")\n                return {\"query_text\": request.query_text, \"corpus_identifier_string\": request.corpus_identifier_string, \"embedding_pooling_method\": embedding_pooling_method, \"results\": results}  # Return the response matching the SemanticSearchResponse model\n            except Exception as e:\n                logger.error(f\"An error occurred while processing the request: {e}\")\n                logger.error(traceback.format_exc())  # Print the traceback\n                raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n        finally:\n            await shared_resources.lock_manager.unlock(lock)\n    else:\n        return {\"status\": \"already processing\"}\n    \n\n\n@app.post(\"/advanced_search_stored_embeddings_with_query_string_for_semantic_similarity/\",\n        response_model=AdvancedSemanticSearchResponse,\n        summary=\"Advanced Semantic Search with Two-Step Similarity Measures\",\n        description=\"\"\"Perform an advanced semantic search by first using FAISS and cosine similarity to narrow down the most similar strings in the database, and then applying additional similarity measures for finer comparison.\n\n### Parameters:\n- `request`: A JSON object containing the query text, model name, an optional corpus identifier string to restrict the search to a specific corpus, an optional similarity filter percentage, and an optional number of most similar strings to return.\n- `req`: HTTP request object (internal use).\n- `token`: Security token (optional).\n\n### Request JSON Format:\nThe request must contain the following attributes:\n- `query_text`: The input text for which to find the most similar string.\n- `llm_model_name`: The model used to calculate embeddings.\n- `embedding_pooling_method`: The method used to pool the embeddings (Choices: 'mean', 'mins_maxes', 'svd', 'svd_first_four', 'ica', 'factor_analysis', 'gaussian_random_projection'; default is 'mean').\n- `corpus_identifier_string`: An optional string identifier to restrict the search to a specific corpus.\n- `similarity_filter_percentage`: (Optional) The percentage of embeddings to filter based on cosine similarity, defaults to 0.02 (i.e., top 2%).\n- `number_of_most_similar_strings_to_return`: (Optional) The number of most similar strings to return after applying the second similarity measure, defaults to 10.\n- `result_sorting_metric`: (Optional) The metric to sort the results by, defaults to 'hoeffding_d'. Choices: 'hoeffding_d', 'cosine_similarity', 'spearman_rho', 'kendall_tau', 'approximate_distance_correlation', 'jensen_shannon_dependency_measure', 'hamming_distance'.\n\n### Example:\n```json\n{\n    \"query_text\": \"Find me the most similar string!\",\n    \"llm_model_name\": \"nomic-embed-text-v1.5.Q6_K\",\n    \"embedding_pooling_method\": \"mean\",\n    \"corpus_identifier_string\": \"specific_corpus\"\n    \"similarity_filter_percentage\": 0.02,\n    \"number_of_most_similar_strings_to_return\": 5,\n    \"result_sorting_metric\": \"hoeffding_d\"\n}\n```\n\n### Response:\nThe response will include the most similar strings found in the database, along with their similarity scores for multiple measures.\n\n### Example Response:\n```json\n{\n    \"query_text\": \"Find me the most similar string!\",\n    \"results\": [\n        {\"search_result_text\": \"This is the most similar string!\", \"similarity_to_query_text\": {\"cosine_similarity\": 0.9823, \"spearman_rho\": 0.8, ... }},\n        {\"search_result_text\": \"Another similar string.\", \"similarity_to_query_text\": {\"cosine_similarity\": 0.9721, \"spearman_rho\": 0.75, ... }},\n        ...\n    ]\n}\n```\"\"\", response_description=\"A JSON object containing the query text and the most similar strings, along with their similarity scores for multiple measures.\")\nasync def advanced_search_stored_embeddings_with_query_string_for_semantic_similarity(request: AdvancedSemanticSearchRequest, req: Request, token: str = None) -> AdvancedSemanticSearchResponse:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    global faiss_indexes, associated_texts_by_model_and_pooling_method\n    request_time = datetime.utcnow()\n    request.query_text = prepare_string_for_embedding(request.query_text)   \n    unique_id = f\"advanced_semantic_search_{request.query_text}_{request.llm_model_name}_{request.embedding_pooling_method}_{request.similarity_filter_percentage}_{request.number_of_most_similar_strings_to_return}\"\n    lock = await shared_resources.lock_manager.lock(unique_id)        \n    if lock.valid:\n        try:                \n            context = start_resource_monitoring(\"advanced_search_stored_embeddings_with_query_string_for_semantic_similarity\", request.dict(), req.client.host if req else \"localhost\")            \n            faiss_indexes, associated_texts_by_model_and_pooling_method = await build_faiss_indexes(force_rebuild=True)\n            try:\n                faiss_index = faiss_indexes[(request.llm_model_name, request.embedding_pooling_method)]\n            except KeyError:\n                raise HTTPException(status_code=400, detail=f\"No FAISS index found for model: {request.llm_model_name} and pooling method: {request.embedding_pooling_method}\")            \n            llm_model_name = request.llm_model_name\n            embedding_pooling_method = request.embedding_pooling_method\n            num_results_before_corpus_filter = request.number_of_most_similar_strings_to_return*25\n            logger.info(f\"Received request to find most similar strings for query text: `{request.query_text}` using model: {llm_model_name}\")\n            try:\n                logger.info(f\"Computing embedding for input text: {request.query_text}\")\n                embedding_request = EmbeddingRequest(text=request.query_text, llm_model_name=llm_model_name, embedding_pooling_method=embedding_pooling_method)\n                embedding_response = await get_or_compute_embedding(embedding_request, req)                \n                embedding_json = embedding_response[\"text_embedding_dict\"][\"embedding_json\"]\n                embedding_vector = json.loads(embedding_json)\n                input_embedding = np.array(embedding_vector).astype('float32').reshape(1, -1)                \n                faiss.normalize_L2(input_embedding)\n                logger.info(f\"Computed embedding for input text: {request.query_text}\")\n                final_results = []\n                faiss_index = faiss_indexes[(llm_model_name, embedding_pooling_method)]\n                if faiss_index is None:\n                    raise HTTPException(status_code=400, detail=f\"No FAISS index found for model: {llm_model_name} and pooling method: {embedding_pooling_method}\")\n                num_results = max([1, int((1 - request.similarity_filter_percentage) * len(associated_texts_by_model_and_pooling_method[llm_model_name][embedding_pooling_method]))])\n                num_results_before_corpus_filter = min(num_results_before_corpus_filter, len(associated_texts_by_model_and_pooling_method[llm_model_name][embedding_pooling_method]))\n                similarities, indices = faiss_index.search(input_embedding, num_results_before_corpus_filter)\n                filtered_indices = indices[0]\n                filtered_similarities = similarities[0]\n                similarity_results = []\n                associated_texts = associated_texts_by_model_and_pooling_method[llm_model_name][embedding_pooling_method]\n                list_of_corpus_identifier_strings = await get_list_of_corpus_identifiers_from_list_of_embedding_texts(associated_texts, llm_model_name, embedding_pooling_method)\n                for idx, similarity in zip(filtered_indices, filtered_similarities):\n                    if idx < len(associated_texts) and list_of_corpus_identifier_strings[idx] == request.corpus_identifier_string:\n                        associated_text = associated_texts[idx]\n                        similarity_results.append((similarity, associated_text))\n                similarity_results = sorted(similarity_results, key=lambda x: x[0], reverse=True)[:num_results]\n                for _, associated_text in similarity_results:\n                    embedding_request = EmbeddingRequest(text=associated_text, llm_model_name=llm_model_name, embedding_pooling_method=embedding_pooling_method)\n                    embedding_response = await get_or_compute_embedding(request=embedding_request, req=req, use_verbose=False)           \n                    embedding_json = embedding_response[\"text_embedding_dict\"][\"embedding_json\"]\n                    embedding_vector = json.loads(embedding_json)\n                    comparison__embedding = np.array(embedding_vector).astype('float32').reshape(1, -1)                 \n                    params = {\n                        \"vector_1\": input_embedding.tolist()[0],\n                        \"vector_2\": comparison__embedding.tolist()[0],\n                        \"similarity_measure\": \"all\"\n                    }\n                    similarity_stats_str = fvs.py_compute_vector_similarity_stats(json.dumps(params))\n                    similarity_stats_json = json.loads(similarity_stats_str)\n                    final_results.append({\n                        \"search_result_text\": associated_text,\n                        \"similarity_to_query_text\": similarity_stats_json\n                    })\n                num_to_return = request.number_of_most_similar_strings_to_return if request.number_of_most_similar_strings_to_return is not None else len(final_results)\n                results = sorted(final_results, key=lambda x: x[\"similarity_to_query_text\"][request.result_sorting_metric], reverse=True)[:num_to_return]\n                response_time = datetime.utcnow()\n                total_time = (response_time - request_time).total_seconds()\n                logger.info(f\"Finished advanced search in {total_time} seconds. Found {len(results)} results.\")\n                return {\"query_text\": request.query_text, \"corpus_identifier_string\": request.corpus_identifier_string, \"embedding_pooling_method\": request.embedding_pooling_method, \"results\": results}\n            except Exception as e:\n                logger.error(f\"An error occurred while processing the request: {e}\")\n                traceback.print_exc()\n                raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n        finally:\n            await shared_resources.lock_manager.unlock(lock)\n            end_resource_monitoring(context)            \n    else:\n        return {\"status\": \"already processing\"}\n    \n\n\n@app.post(\"/get_all_embedding_vectors_for_document/\",\n    summary=\"Get Embeddings for a Document\",\n    description=\"\"\"Extract text embeddings for a document. This endpoint supports plain text, .doc/.docx (MS Word), PDF files, images (using Tesseract OCR), and many other file types supported by the textract library.\n\n### Parameters:\n- `file`: The uploaded document file (either plain text, .doc/.docx, PDF, etc.).\n- `url`: URL of the document file to download (optional; in lieu of `file`).\n- `hash`: SHA3-256 hash of the document file to verify integrity (optional; in lieu of `file`).\n- `size`: Size of the document file in bytes to verify completeness (optional; in lieu of `file`).\n- `llm_model_name`: The model used to calculate embeddings (optional).\n- `embedding_pooling_method`: The method used to pool the embeddings (Choices: 'mean', 'mins_maxes', 'svd', 'svd_first_four', 'ica', 'factor_analysis', 'gaussian_random_projection'; default is 'mean').\n- `corpus_identifier_string`: An optional string identifier for grouping documents into a specific corpus.\n- `json_format`: The format of the JSON response (optional, see details below).\n- `send_back_json_or_zip_file`: Whether to return a JSON file or a ZIP file containing the embeddings file (optional, defaults to `zip`).\n- `query_text`: An optional query text to perform a semantic search with the same parameters used for the document embedding request.\n- `token`: Security token (optional).\n\n### JSON Format Options:\nThe format of the JSON string returned by the endpoint (default is `records`; these are the options supported by the Pandas `to_json()` function):\n\n- `split` : dict like {`index` -> [index], `columns` -> [columns], `data` -> [values]}\n- `records` : list like [{column -> value}, â€¦ , {column -> value}]\n- `index` : dict like {index -> {column -> value}}\n- `columns` : dict like {column -> {index -> value}}\n- `values` : just the values array\n- `table` : dict like {`schema`: {schema}, `data`: {data}}\n\n### Examples:\n- Plain Text: Submit a file containing plain text.\n- MS Word: Submit a `.doc` or `.docx` file.\n- PDF: Submit a `.pdf` file.\"\"\",\n    response_description=\"Either a ZIP file containing the embeddings JSON file or a direct JSON response, depending on the value of `send_back_json_or_zip_file`.\")\nasync def get_all_embedding_vectors_for_document(\n    file: UploadFile = File(None),\n    url: str = Form(None),\n    hash: str = Form(None),\n    size: int = Form(None),\n    llm_model_name: str = DEFAULT_EMBEDDING_MODEL_NAME,\n    embedding_pooling_method: str = DEFAULT_EMBEDDING_POOLING_METHOD,\n    corpus_identifier_string: str = \"\", \n    json_format: str = 'records',\n    send_back_json_or_zip_file: str = 'zip',\n    query_text: str = None,\n    token: str = None,\n    req: Request = None\n):\n    logger.info(f\"Received request with embedding_pooling_method: {embedding_pooling_method}\")\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    client_ip = req.client.host if req else \"localhost\"\n    request_time = datetime.utcnow()\n    if file:\n        input_data_binary = await file.read()\n        result = magika.identify_bytes(input_data_binary)\n        detected_data_type = result.output.ct_label\n        temp_file = tempfile.NamedTemporaryFile(suffix=f\".{detected_data_type}\", delete=False)\n        temp_file_path = temp_file.name\n        logger.info(f\"Temp file path: {temp_file_path}\")\n        with open(temp_file_path, 'wb') as buffer:\n            buffer.write(input_data_binary)\n    elif url and hash and size:\n        temp_file_path = await download_file(url, size, hash)\n        with open(temp_file_path, 'rb') as f:\n            input_data_binary = f.read()\n            result = magika.identify_bytes(input_data_binary)\n            detected_data_type = result.output.ct_label\n            new_temp_file_path = temp_file_path + f\".{detected_data_type}\"\n            os.rename(temp_file_path, new_temp_file_path)\n            temp_file_path = new_temp_file_path\n    else:\n        raise HTTPException(status_code=400, detail=\"Invalid input. Provide either a file or URL with hash and size.\")\n    try:\n        hash_obj = sha3_256()\n        with open(temp_file_path, 'rb') as buffer:\n            for chunk in iter(lambda: buffer.read(1024), b''):\n                hash_obj.update(chunk)\n        document_file_hash = hash_obj.hexdigest()\n        logger.info(f\"SHA3-256 hash of submitted file: {document_file_hash}\")\n        if corpus_identifier_string == \"\":\n            corpus_identifier_string = document_file_hash\n        unique_id = f\"document_embedding_{document_file_hash}_{llm_model_name}_{embedding_pooling_method}\"\n        # Exponential backoff with jitter for acquiring lock\n        max_retries = 5\n        for attempt in range(max_retries):\n            try:\n                lock = await shared_resources.lock_manager.lock(unique_id)\n                if lock.valid:\n                    break\n            except Exception as e:\n                wait_time = (2 ** attempt) + (random.randint(0, 1000) / 1000)\n                logger.warning(f\"Attempt {attempt + 1}: Failed to acquire lock: {e}. Retrying in {wait_time:,.2f} seconds.\")\n                await asyncio.sleep(wait_time)  # Wait before retrying\n        if not lock or not lock.valid:\n            raise HTTPException(status_code=503, detail=\"Service temporarily unavailable. Please try again later.\")\n        try:\n            async with AsyncSessionLocal() as session:\n                result = await session.execute(select(DocumentEmbedding).filter(DocumentEmbedding.document_file_hash == document_file_hash, DocumentEmbedding.llm_model_name == llm_model_name, DocumentEmbedding.embedding_pooling_method == embedding_pooling_method))\n                existing_document_embedding = result.scalar_one_or_none()\n                if existing_document_embedding:\n                    logger.info(\"Document has been processed before, returning existing result\")\n                    sentences = existing_document_embedding.sentences\n                    document_embedding_results_json_compressed_binary = existing_document_embedding.document_embedding_results_json_compressed_binary\n                    document_embedding_results_json_decompressed_binary = decompress_data(document_embedding_results_json_compressed_binary)\n                    json_content = document_embedding_results_json_decompressed_binary.decode('utf-8')\n                    if len(json_content) == 0:\n                        raise HTTPException(status_code=400, detail=\"Could not retrieve document embedding results.\")\n                    existing_document = 1\n                    document_embedding_request = {}\n                else:\n                    document_embedding_request = {}\n                    existing_document = 0\n                    with open(temp_file_path, 'rb') as f:\n                        input_data_binary = f.read()\n                    result = magika.identify_bytes(input_data_binary)\n                    mime_type = result.output.mime_type\n                    sentences, thousands_of_input_words = await parse_submitted_document_file_into_sentence_strings_func(temp_file_path, mime_type)\n                    document_embedding_request['mime_type'] = mime_type\n                    document_embedding_request['sentences'] = sentences\n                    document_embedding_request['total_number_of_sentences'] = len(sentences)\n                    document_embedding_request['total_words'] = sum(len(sentence.split()) for sentence in sentences)\n                    document_embedding_request['total_characters'] = sum(len(sentence) for sentence in sentences)\n                    document_embedding_request['thousands_of_input_words'] = thousands_of_input_words\n                    document_embedding_request['file_size_mb'] = os.path.getsize(temp_file_path) / (1024 * 1024)\n                    document_embedding_request['corpus_identifier_string'] = corpus_identifier_string\n                    document_embedding_request['embedding_pooling_method'] = embedding_pooling_method\n                    document_embedding_request['llm_model_name'] = llm_model_name\n                    document_embedding_request['document_file_hash'] = document_file_hash\n                    if thousands_of_input_words > MAX_THOUSANDS_OF_WORDs_FOR_DOCUMENT_EMBEDDING:\n                        raise HTTPException(status_code=400, detail=f\"Document contains ~{int(thousands_of_input_words*1000):,} words, more than the maximum of {MAX_THOUSANDS_OF_WORDs_FOR_DOCUMENT_EMBEDDING*1000:,} words, which would take too long to compute embeddings for. Please submit a smaller document.\") \n                    first_10_words_of_input_text = ' '.join(' '.join(sentences).split()[:10])\n                    logger.info(f\"Received request to extract embeddings for document with MIME type: {mime_type} and size: {os.path.getsize(temp_file_path):,} bytes from IP address: {client_ip}; First 10 words of the document: '{first_10_words_of_input_text}...'\")\n                    logger.info(f\"Document contains ~{int(thousands_of_input_words*1000):,} words, which is within the maximum of {MAX_THOUSANDS_OF_WORDs_FOR_DOCUMENT_EMBEDDING*1000:,} words. Proceeding with embedding computation using {llm_model_name} and pooling method {embedding_pooling_method}.\") \n                    input_data = {\n                        \"sentences\": sentences,\n                        \"file_size_mb\": os.path.getsize(temp_file_path) / (1024 * 1024),\n                        \"mime_type\": mime_type\n                    }\n                    context = start_resource_monitoring(\"get_all_embedding_vectors_for_document\", input_data, client_ip)\n                    try:\n                        json_content = await compute_embeddings_for_document(sentences=sentences, llm_model_name=llm_model_name, embedding_pooling_method=embedding_pooling_method, corpus_identifier_string=corpus_identifier_string, client_ip=client_ip, document_file_hash=document_file_hash, file=file, original_file_content=input_data_binary, json_format=json_format)\n                        logger.info(f\"Done getting all regular embeddings for document containing {len(sentences):,} sentences with model {llm_model_name} and embedding pooling method {embedding_pooling_method} and corpus {corpus_identifier_string}\")\n                \n                    except Exception as e:\n                        logger.error(f\"Error while computing embeddings for document: {e}\")\n                        traceback.print_exc()\n                        raise HTTPException(status_code=400, detail=\"Error while computing embeddings for document\")\n                    finally:\n                        end_resource_monitoring(context)\n            if query_text:\n                use_advanced_semantic_search = 0\n                if use_advanced_semantic_search:\n                    search_request = AdvancedSemanticSearchRequest(\n                        query_text=query_text,\n                        llm_model_name=llm_model_name,\n                        embedding_pooling_method=embedding_pooling_method,\n                        corpus_identifier_string=corpus_identifier_string,\n                        similarity_filter_percentage=0.01,\n                        result_sorting_metric=\"hoeffding_d\",\n                        number_of_most_similar_strings_to_return=10\n                    )\n                    logger.info(f\"Performing advanced semantic search for model {llm_model_name} and pooling method {embedding_pooling_method}...\")\n                    search_response = await advanced_search_stored_embeddings_with_query_string_for_semantic_similarity(search_request, req, token)\n                    search_results = search_response[\"results\"]\n                else:\n                    search_request = SemanticSearchRequest(\n                        query_text=query_text,\n                        llm_model_name=llm_model_name,\n                        embedding_pooling_method=embedding_pooling_method,\n                        corpus_identifier_string=corpus_identifier_string,\n                        number_of_most_similar_strings_to_return=10\n                    )\n                    logger.info(f\"Performing semantic search for model {llm_model_name} and pooling method {embedding_pooling_method}...\")\n                    search_response = await search_stored_embeddings_with_query_string_for_semantic_similarity(search_request, req, token)\n                    search_results = search_response[\"results\"]\n                logger.info(f\"Advanced semantic search completed. Results for query text '{query_text}'\\n: {search_results}\")\n                json_content_dict = {\"document_embedding_request\": document_embedding_request, \"document_embedding_results\": json.loads(json_content), \"semantic_search_request\": dict(search_request), \"semantic_search_results\": search_results} \n                json_content = json.dumps(json_content_dict)\n            else:\n                json_content_dict = {\"document_embedding_request\": document_embedding_request, \"document_embedding_results\": json.loads(json_content)}\n                json_content = json.dumps(json_content_dict)                                \n            overall_total_time = (datetime.utcnow() - request_time).total_seconds()\n            json_content_length = len(json_content)\n            if json_content_length > 0:\n                if not existing_document:\n                    logger.info(f\"The response took {overall_total_time:,.2f} seconds to generate, or {float(overall_total_time / (thousands_of_input_words)):,.2f} seconds per thousand input tokens and {overall_total_time / (float(json_content_length) / 1000000.0):,.2f} seconds per million output characters.\") \n                if send_back_json_or_zip_file == 'json':\n                    logger.info(f\"Returning JSON response for document containing {len(sentences):,} sentences with model {llm_model_name}; first 100 characters out of {json_content_length:,} total of JSON response: {json_content[:100]}\" if 'sentences' in locals() else f\"Returning JSON response; first 100 characters out of {json_content_length:,} total of JSON response: {json_content[:100]}\")\n                    return JSONResponse(content=json.loads(json_content.decode()))\n                else:\n                    original_filename_without_extension, _ = os.path.splitext(file.filename if file else os.path.basename(url))\n                    json_file_path = f\"/tmp/{original_filename_without_extension}.json\"\n                    with open(json_file_path, 'w') as json_file:\n                        json_file.write(json_content)\n                    zip_file_path = f\"/tmp/{original_filename_without_extension}.zip\"\n                    with zipfile.ZipFile(zip_file_path, 'w', compression=zipfile.ZIP_DEFLATED) as zipf:\n                        zipf.write(json_file_path, os.path.basename(json_file_path))\n                    logger.info(f\"Returning ZIP response for document containing {len(sentences):,} sentences with model {llm_model_name}; first 100 characters out of {json_content_length:,} total of JSON response: {json_content[:100]}\")\n                    return FileResponse(zip_file_path, headers={\"Content-Disposition\": f\"attachment; filename={original_filename_without_extension}.zip\"})\n        finally:\n            await shared_resources.lock_manager.unlock(lock)\n    except Exception as e:\n        logger.error(f\"Error in processing: {e}\")\n        traceback.print_exc()\n        raise HTTPException(status_code=500, detail=\"An error occurred while processing the request.\")\n\n    \n\n@app.post(\"/get_text_completions_from_input_prompt/\",\n        response_model=List[TextCompletionResponse],\n        summary=\"Generate Text Completions for a Given Input Prompt\",\n        description=\"\"\"Generate text completions for a given input prompt string using the specified model.\n### Parameters:\n- `request`: A JSON object containing the input prompt string (`input_prompt`), the model name, an optional grammar file, an optional number of tokens to generate, and an optional number of completions to generate.\n- `token`: Security token (optional).\n\n### Request JSON Format:\nThe request must contain the following attributes:\n- `input_prompt`: The input prompt from which to generate a completion with the LLM model.\n- `llm_model_name`: The model used to calculate the embedding (optional, will use the default model if not provided).\n- `temperature`: The temperature to use for text generation (optional, defaults to 0.7).\n- `grammar_file_string`: The grammar file used to restrict text generation (optional; default is to not use any grammar file). Examples: `json`, `list`)\n- `number_of_completions_to_generate`: The number of completions to generate (optional, defaults to 1).\n- `number_of_tokens_to_generate`: The number of tokens to generate (optional, defaults to 1000).\n\n### Example (note that `llm_model_name` is optional):\n```json\n{\n    \"input_prompt\": \"The Kings of France in the 17th Century:\",\n    \"llm_model_name\": \"Meta-Llama-3-8B-Instruct.Q3_K_S\",\n    \"temperature\": 0.95,\n    \"grammar_file_string\": \"json\",\n    \"number_of_tokens_to_generate\": 500,\n    \"number_of_completions_to_generate\": 3\n}\n```\n\n### Response:\nThe response will include the generated text completion, the time taken to compute the generation in seconds, and the request details (input prompt, model name, grammar file, and number of tokens to generate).\n\n### Example Response:\n```json\n[\n    {\n        \"input_prompt\": \"The Kings of France in the 17th Century:\",\n        \"llm_model_name\": \"Meta-Llama-3-8B-Instruct.Q3_K_S\",\n        \"grammar_file_string\": \"json\",\n        \"number_of_tokens_to_generate\": 500,\n        \"number_of_completions_to_generate\": 3,\n        \"time_taken_in_seconds\": 67.17,\n        \"generated_text\": \"{\\\"kings\\\":[\\\\n    {\\\\n        \\\"name\\\": \\\"Henry IV\\\",\\\\n        \\\"reign_start\\\": 1589,\\\\n        \\\"reign_end\\\": 1610\\\\n    },\\\\n    {\\\\n        \\\"name\\\": \\\"Louis XIII\\\",\\\\n        \\\"reign_start\\\": 1610,\\\\n        \\\"reign_end\\\": 1643\\\\n    },\\\\n    {\\\\n        \\\"name\\\": \\\"Louis XIV\\\",\\\\n        \\\"reign_start\\\": 1643,\\\\n        \\\"reign_end\\\": 1715\\\\n    },\\\\n    {\\\\n        \\\"name\\\": \\\"Louis XV\\\",\\\\n        \\\"reign_start\\\": 1715,\\\\n        \\\"reign_end\\\": 1774\\\\n    },\\\\n    {\\\\n        \\\"name\\\": \\\"Louis XVI\\\",\\\\n        \\\"reign_start\\\": 1774,\\\\n        \\\"reign_end\\\": 1792\\\\n    }\\\\n]}\",\n        \"finish_reason\": \"stop\",\n        \"llm_model_usage_json\": \"{\\\"prompt_tokens\\\": 13, \\\"completion_tokens\\\": 218, \\\"total_tokens\\\": 231}\"\n    },\n    {\n        \"input_prompt\": \"The Kings of France in the 17th Century:\",\n        \"llm_model_name\": \"Meta-Llama-3-8B-Instruct.Q3_K_S\",\n        \"grammar_file_string\": \"json\",\n        \"number_of_tokens_to_generate\": 500,\n        \"number_of_completions_to_generate\": 3,\n        \"time_taken_in_seconds\": 67.17,\n        \"generated_text\": \"{\\\"kings\\\":\\\\n   [ {\\\"name\\\": \\\"Henry IV\\\",\\\\n      \\\"reignStart\\\": \\\"1589\\\",\\\\n      \\\"reignEnd\\\": \\\"1610\\\"},\\\\n     {\\\"name\\\": \\\"Louis XIII\\\",\\\\n      \\\"reignStart\\\": \\\"1610\\\",\\\\n      \\\"reignEnd\\\": \\\"1643\\\"},\\\\n     {\\\"name\\\": \\\"Louis XIV\\\",\\\\n      \\\"reignStart\\\": \\\"1643\\\",\\\\n      \\\"reignEnd\\\": \\\"1715\\\"}\\\\n   ]}\",\n        \"finish_reason\": \"stop\",\n        \"llm_model_usage_json\": \"{\\\"prompt_tokens\\\": 13, \\\"completion_tokens\\\": 115, \\\"total_tokens\\\": 128}\"\n    },\n    {\n        \"input_prompt\": \"The Kings of France in the 17th Century:\",\n        \"llm_model_name\": \"Meta-Llama-3-8B-Instruct.Q3_K_S\",\n        \"grammar_file_string\": \"json\",\n        \"number_of_tokens_to_generate\": 500,\n        \"number_of_completions_to_generate\": 3,\n        \"time_taken_in_seconds\": 67.17,\n        \"generated_text\": \"{\\\\n\\\"Henri IV\\\": \\\"1589-1610\\\",\\\\n\\\"Louis XIII\\\": \\\"1610-1643\\\",\\\\n\\\"Louis XIV\\\": \\\"1643-1715\\\",\\\\n\\\"Louis XV\\\": \\\"1715-1774\\\",\\\\n\\\"Louis XVI\\\": \\\"1774-1792\\\",\\\\n\\\"Louis XVIII\\\": \\\"1814-1824\\\",\\\\n\\\"Charles X\\\": \\\"1824-1830\\\",\\\\n\\\"Louis XIX (previously known as Charles X): \\\" \\\\n    : \\\"1824-1830\\\",\\\\n\\\"Charles X (previously known as Louis XIX)\\\": \\\"1824-1830\\\"}\",\n        \"finish_reason\": \"stop\",\n        \"llm_model_usage_json\": \"{\\\"prompt_tokens\\\": 13, \\\"completion_tokens\\\": 168, \\\"total_tokens\\\": 181}\"\n    }\n]\n```\"\"\", response_description=\"A JSON object containing the the generated text completion of the input prompt and the request details.\")\nasync def get_text_completions_from_input_prompt(request: TextCompletionRequest, req: Request = None, token: str = None, client_ip: str = None) -> List[TextCompletionResponse]:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        logger.warning(f\"Unauthorized request from client IP {client_ip}\")\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    context = start_resource_monitoring(\"get_text_completions_from_input_prompt\", request.dict(), client_ip)\n    try:\n        unique_id = f\"text_completion_{hash(request.input_prompt)}_{request.llm_model_name}\"\n        lock = await shared_resources.lock_manager.lock(unique_id)\n        if lock.valid:\n            try:\n                response = await generate_completion_from_llm(request, req, client_ip)\n                return response\n            finally:\n                await shared_resources.lock_manager.unlock(lock)\n        else:\n            return {\"status\": \"already processing\"}\n    except Exception as e:\n        logger.error(f\"An error occurred while processing the request: {e}\")\n        logger.error(traceback.format_exc())  # Print the traceback\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n    finally:\n        end_resource_monitoring(context)\n\n\n\n@app.post(\"/ask_question_about_image/\",\n        response_model=List[ImageQuestionResponse],\n        summary=\"Ask a Question About an Image\",\n        description=\"\"\"Ask a question about an image using a specified LLaVA model.\n### Parameters:\n- `image`: The image file to ask a question about. Can be any common image format, such as PNG, JPEG, or GIF (it will be automatically converted to a PNG file for processing).\n- `question`: The question to ask about the image.\n- `llm_model_name`: The model used to answer the question (must include 'llava').\n- `temperature`: The temperature to use for text generation (optional, defaults to 0.7).\n- `number_of_tokens_to_generate`: The number of tokens to generate (optional, defaults to 256).\n- `number_of_completions_to_generate`: The number of completions to generate (optional, defaults to 1).\n- `token`: Security token (optional).\n\n### Example Request:\nSubmit a file and a JSON request for processing.\n\n### Example Response:\n```json\n[\n    {\n        \"question\": \"What is happening in this image?\",\n        \"llm_model_name\": \"llava-llama-3-8b-v1_1-int4\",\n        \"image_hash\": \"e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\",\n        \"time_taken_in_seconds\": 12.34,\n        \"number_of_tokens_to_generate\": 256,\n        \"number_of_completions_to_generate\": 1,\n        \"generated_text\": \"The image shows a sunset over a mountain range.\",\n        \"finish_reason\": \"stop\",\n        \"llm_model_usage_json\": \"{\\\"prompt_tokens\\\": 13, \\\"completion_tokens\\\": 218, \\\"total_tokens\\\": 231}\"\n    }\n]\n\"\"\",\n        response_description=\"A JSON object containing the generated answer(s) to the question about the image and the request details.\")\nasync def ask_question_about_image_endpoint(\n    image: UploadFile = File(...),\n    question: str = Form(\"What is happening in this image?\"),\n    llm_model_name: str = Form(DEFAULT_MULTI_MODAL_MODEL_NAME),\n    temperature: float = Form(DEFAULT_COMPLETION_TEMPERATURE),\n    number_of_tokens_to_generate: int = Form((int(DEFAULT_MAX_COMPLETION_TOKENS//3))),\n    number_of_completions_to_generate: int = Form(DEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE),\n    req: Request = None,\n    token: str = None,\n    client_ip: str = None\n) -> List[ImageQuestionResponse]:\n    if USE_SECURITY_TOKEN and (token is None or token != SECURITY_TOKEN):\n        logger.warning(f\"Unauthorized request from client IP {client_ip}\")\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")    \n    context = start_resource_monitoring(\"ask_question_about_image\", {\n        \"question\": question,\n        \"llm_model_name\": llm_model_name,\n        \"temperature\": temperature,\n        \"number_of_tokens_to_generate\": number_of_tokens_to_generate,\n        \"number_of_completions_to_generate\": number_of_completions_to_generate,\n        \"client_ip\": client_ip,\n        \"timestamp\": datetime.utcnow().isoformat(),\n    }, client_ip)\n    try:\n        unique_id = f\"image_question_{hash(question)}_{llm_model_name}\"\n        lock = await shared_resources.lock_manager.lock(unique_id)\n        if lock.valid:\n            try:\n                response = await ask_question_about_image(question, llm_model_name, temperature, number_of_tokens_to_generate, number_of_completions_to_generate, image, req, client_ip)\n                return response\n            finally:\n                await shared_resources.lock_manager.unlock(lock)\n        else:\n            return {\"status\": \"already processing\"}\n    except Exception as e:\n        logger.error(f\"An error occurred while processing the request: {e}\")\n        logger.error(traceback.format_exc())  # Print the traceback\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n    finally:\n        end_resource_monitoring(context)\n\n\n\n@app.post(\"/turn_sample_json_into_bnf_grammar_for_llm/\",\n        summary=\"Generate BNF Grammar from Sample JSON\",\n        description=\"\"\"Generate BNF grammar based on a sample JSON string or an uploaded JSON file, with optional token-based authentication.\n### Parameters:\n- `sample_json`: The sample JSON data as a string (optional if file is uploaded).\n- `file`: The sample JSON file to upload (optional if JSON data is provided as `sample_json`). File must be JSON type and not exceed 100KB.\n- `token`: A security token for authentication (optional).\n\n### Constraints:\n- Uploaded files must be of type JSON and not exceed 100KB.\n\n### Validation:\n- The generated BNF grammar will be validated. If validation fails, an error message will be returned.\n\n### Example Request with JSON Data:\nUse `multipart/form-data` to provide `sample_json` as a string.\n\n### Example Request with File Upload:\nUse `multipart/form-data` to upload a JSON file.\n\n### Example Request with Token:\nAdd a `token` parameter to your request for authentication.\n\n### Response:\nThe response will be the generated BNF grammar based on the sample JSON provided. If the generated BNF grammar fails validation, an error message will be returned.\n\n### Example Response:\n\"root ::= '{' ws root_pair_list ws '}' ws ...\"\n\"\"\",\n        response_description=\"A string containing the generated BNF grammar, or an error message if the grammar fails validation.\")\nasync def turn_sample_json_into_bnf_grammar_for_llm(\n    sample_json: str = Form(None),\n    file: UploadFile = File(None),\n    token: str = Form(None)\n) -> str:\n    if sample_json is None and file is None:\n        raise HTTPException(status_code=400, detail=\"Either sample_json or file must be provided\")\n    if file:\n        if file.content_type != \"application/json\":\n            raise HTTPException(status_code=400, detail=\"Invalid file type. Only JSON is accepted.\")\n        file_size = file.file._file.tell()\n        if file_size > 102400:\n            raise HTTPException(status_code=400, detail=\"File size exceeds 100KB.\")\n    gb = GrammarBuilder()\n    if sample_json:\n        try:\n            json_content = json.loads(sample_json)\n        except json.JSONDecodeError as e:\n            raise HTTPException(status_code=400, detail=f\"Invalid JSON: {e}\")\n        bnf_grammar = gb.json_to_bnf(json.dumps(json_content))\n    else:\n        file_content = await file.read()\n        try:\n            json_content = json.loads(file_content.decode('utf-8'))\n        except json.JSONDecodeError as e:\n            raise HTTPException(status_code=400, detail=f\"Invalid JSON: {e}\")\n        bnf_grammar = gb.json_to_bnf(json.dumps(json_content))\n    is_valid_grammar, validation_message = validate_bnf_grammar_func(bnf_grammar)\n    if not is_valid_grammar:\n        raise HTTPException(status_code=400, detail=f\"Generated BNF grammar could not be validated: {validation_message}\")\n    return bnf_grammar\n\n\n\n@app.post(\"/turn_pydantic_model_description_into_bnf_grammar_for_llm/\",\n        summary=\"Generate BNF Grammar from Pydantic Model Description\",\n        description=\"\"\"Generate BNF grammar based on a Pydantic model description string. This endpoint allows you to turn a Pydantic model definition into a corresponding BNF grammar.\n        \n### Parameters:\n- `pydantic_model_description`: The Pydantic model description as a string. Must include the class definition, fields, and their types.\n\n### Validation:\n- The generated BNF grammar will be validated. If validation fails, an error message will be returned.\n\n### Authentication:\n- `token`: Security token for authorized access (optional if security is disabled).\n\n### Example Request:\nUse `multipart/form-data` to provide `pydantic_model_description` as a string.\n\n### Response:\nThe response will be the generated BNF grammar based on the Pydantic model description provided. If the generated BNF grammar fails validation, an error message will be returned.\n\n### Example Response:\n\"root ::= '{' ws root_pair_list ws '}' ws ...\"\n\"\"\",\n        response_description=\"A string containing the generated BNF grammar, or an error message if the grammar fails validation.\")\nasync def turn_pydantic_model_description_into_bnf_grammar_for_llm(\n    pydantic_model_description: str = Form(...),\n    token: str = Form(None)\n) -> str:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")        \n    if not pydantic_model_description:\n        raise HTTPException(status_code=400, detail=\"Pydantic model description must be provided\")\n    gb = GrammarBuilder()\n    bnf_grammar = gb.pydantic_to_json_bnf(pydantic_model_description)\n    is_valid_grammar, validation_message = validate_bnf_grammar_func(bnf_grammar)\n    if not is_valid_grammar:\n        raise HTTPException(status_code=400, detail=f\"Generated BNF grammar could not be validated: {validation_message}\")\n    return bnf_grammar\n\n\n\n@app.post(\"/compute_transcript_with_whisper_from_audio/\",\n        response_model=AudioTranscriptResponse,\n        summary=\"Transcribe and Embed Audio using Whisper and LLM\",\n        description=\"\"\"Transcribe an audio file and optionally compute document embeddings. This endpoint uses the Whisper model for transcription and a specified or default language model for embeddings. The transcription and embeddings are then stored, and a ZIP file containing the embeddings can be downloaded.\n\n### Parameters:\n- `file`: The uploaded audio file.\n- `url`: URL of the audio file to download.\n- `hash`: SHA3-256 hash of the audio file to verify integrity.\n- `size`: Size of the audio file in bytes to verify completeness.\n- `compute_embeddings_for_resulting_transcript_document`: Boolean to indicate if document embeddings should be computed (optional, defaults to True).\n- `llm_model_name`: The language model used for computing embeddings (optional, defaults to the default model name).\n- `embedding_pooling_method`: The method used to pool the embeddings (Choices: 'mean', 'mins_maxes', 'svd', 'svd_first_four', 'ica', 'factor_analysis', 'gaussian_random_projection'; default is 'mean').\n- `req`: HTTP Request object for additional request metadata (optional).\n- `token`: Security token for API access (optional).\n- `client_ip`: Client IP for logging and security (optional).\n- `corpus_identifier_string`: An optional string identifier for grouping transcripts into a specific corpus.\n\n### Examples:\n- Audio File: Submit an audio file for transcription.\n- Audio File with Embeddings: Submit an audio file and set `compute_embeddings_for_resulting_transcript_document` to True to also get embeddings.\n\n### Authentication:\n- If security tokens are enabled (`USE_SECURITY_TOKEN=True` and `use_hardcoded_security_token=True`), then the `token` parameter must match the hardcoded `SECURITY_TOKEN`.\n\n### Error Handling:\n- Unauthorized requests are logged and result in a 403 status.\n- All other errors result in a 500 status and are logged with their tracebacks.\"\"\",\n        response_description=\"A JSON object containing the complete transcription details, computational times, and an optional URL for downloading a ZIP file of the document embeddings.\")\nasync def compute_transcript_with_whisper_from_audio(\n    file: UploadFile = File(None),\n    url: str = Form(None),\n    hash: str = Form(None),\n    size: int = Form(None),\n    compute_embeddings_for_resulting_transcript_document: Optional[bool] = True,\n    llm_model_name: str = DEFAULT_EMBEDDING_MODEL_NAME, \n    embedding_pooling_method: str = \"mean\",\n    corpus_identifier_string: str = \"\",\n    req: Request = None,\n    token: str = None,\n    client_ip: str = None\n) -> AudioTranscriptResponse:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        logger.warning(f\"Unauthorized request from client_ip {client_ip}\")\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")    \n    if file:\n        input_data_binary = await file.read()\n        result = magika.identify_bytes(input_data_binary)\n        detected_data_type = result.output.ct_label\n        temp_file = tempfile.NamedTemporaryFile(suffix=f\".{detected_data_type}\", delete=False)\n        temp_file_path = temp_file.name\n        logger.info(f\"Temp file path: {temp_file_path}\")\n        with open(temp_file_path, 'wb') as buffer:\n            buffer.write(input_data_binary)\n    elif url and hash and size:\n        temp_file_path = await download_file(url, size, hash)\n        with open(temp_file_path, 'rb') as file:\n            input_data_binary = file.read()\n            result = magika.identify_bytes(input_data_binary)\n            detected_data_type = result.output.ct_label\n            new_temp_file_path = temp_file_path + f\".{detected_data_type}\"\n            os.rename(temp_file_path, new_temp_file_path)\n            temp_file_path = new_temp_file_path\n        # Create a new UploadFile object using the downloaded file path\n        file = UploadFile(filename=os.path.basename(temp_file_path), file=open(temp_file_path, 'rb'))\n    else:\n        raise HTTPException(status_code=400, detail=\"Invalid input. Provide either a file or URL with hash and size.\")\n    audio_file_size_mb = os.path.getsize(temp_file_path) / (1024 * 1024)\n    logger.info(f\"Now determining the duration of the audio file with path {temp_file_path} which is {audio_file_size_mb:,.2f} MB in size...\")\n    audio_duration_seconds = get_audio_duration_seconds(temp_file_path)\n    input_data = {\n        \"file_size_mb\": audio_file_size_mb,\n        \"audio_duration_seconds\": round(audio_duration_seconds, 2)\n    }\n    context = start_resource_monitoring(\"compute_transcript_with_whisper_from_audio\", input_data, req.client.host if req else \"localhost\")\n    try:\n        audio_transcript = await get_or_compute_transcript(\n            file=file,\n            compute_embeddings_for_resulting_transcript_document=compute_embeddings_for_resulting_transcript_document,\n            llm_model_name=llm_model_name,\n            embedding_pooling_method=embedding_pooling_method,\n            corpus_identifier_string=corpus_identifier_string,\n            req=req,\n        )\n        return audio_transcript\n    except Exception as e:\n        try:\n            os.remove(temp_file_path)\n        except Exception as e:  # noqa: F841\n            pass\n        logger.error(f\"An error occurred while processing the request: {e}\")\n        logger.error(traceback.format_exc())  # Print the traceback\n        raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n    finally:\n        try:\n            os.remove(temp_file_path)\n        except Exception as e:  # noqa: F841\n            pass\n        end_resource_monitoring(context)\n\n\n\n@app.post(\"/add_new_grammar_definition_file/\",\n        response_model=AddGrammarResponse,\n        summary=\"Add or Update a Grammar Definition File\",\n        description=\"\"\"Add a new BNF grammar definition file or update an existing one.\n        \n### Parameters:\n- `bnf_grammar`: The BNF grammar string.\n- `grammar_file_name`: The name for the new or existing grammar file.\n\nIf a grammar file with the given name already exists, this endpoint will compare the existing content with the new submission. If the content is different, the file will be overwritten.\n\n### Example Request:\n```json\n{\n    \"bnf_grammar\": \"root ::= '{' ws root_pair_list ws '}' ws ...\",\n    \"grammar_file_name\": \"new_grammar\"\n}\n```\n\n### Response:\nThe response will include a list of all valid grammar files in the `grammar_files` directory.\n\n### Example Response:\n```json\n{\n    \"valid_grammar_files\": [\"new_grammar.gbnf\", \"another_grammar.gbnf\"]\n}\n```\"\"\",\n        response_description=\"A JSON object containing a list of all valid grammar files.\")\nasync def add_new_grammar_definition_file(request: AddGrammarRequest, token: str = None) -> AddGrammarResponse:\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")    \n    is_valid_grammar, validation_message = validate_bnf_grammar_func(request.bnf_grammar)\n    if not is_valid_grammar:\n        raise HTTPException(status_code=400, detail=f\"Invalid BNF grammar: {validation_message}\")\n    grammar_files_dir = 'grammar_files'\n    if not os.path.exists(grammar_files_dir):\n        os.makedirs(grammar_files_dir)\n    grammar_file_name_with_extension = f\"{request.grammar_file_name}.gbnf\"\n    grammar_file_path = Path(grammar_files_dir) / grammar_file_name_with_extension\n    existing_files = await get_list_of_available_bnf_grammars()\n    if grammar_file_name_with_extension in existing_files:\n        with open(grammar_file_path, \"r\") as f:\n            existing_content = f.read()\n        if existing_content != request.bnf_grammar:\n            logger.info(f\"Grammar file {grammar_file_name_with_extension} already exists, but newly submitted grammar is different-- overwriting!\")\n            with open(grammar_file_path, \"w\") as f:\n                f.write(request.bnf_grammar)\n        else:\n            logger.info(f\"Grammar file {grammar_file_name_with_extension} already exists and is the same-- not overwriting!\")\n    else:\n        logger.info(f\"Grammar file {grammar_file_name_with_extension} does not exist-- creating!\")\n        with open(grammar_file_path, \"w\") as f:\n            f.write(request.bnf_grammar)\n    valid_grammar_files = [f.name for f in Path(grammar_files_dir).glob(\"*.gbnf\")]\n    return {\"valid_grammar_files\": valid_grammar_files}\n\n\n\n@app.post(\"/clear_ramdisk/\")\nasync def clear_ramdisk_endpoint(token: str = None):\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    if USE_RAMDISK:\n        clear_ramdisk()\n        return {\"message\": \"RAM Disk cleared successfully.\"}\n    return {\"message\": \"RAM Disk usage is disabled.\"}\n\n\n\n@app.get(\"/download/{file_name}\",\n        summary=\"Download File by Name\",\n        description=\"\"\"Download a file by its name from the 'generated_transcript_embeddings_zip_files' directory, with optional token-based authentication.\n### Parameters:\n- `file_name`: The name of the file to download.\n- `token`: A security token for authentication (optional).\n\n### Example Request with Token:\nAdd a `token` parameter to your request for authentication.\n\n### Response:\nThe response will be the requested file in ZIP format if it exists, or a 404 status code if the file is not found.\n\n### Security:\nIf a security token is required by the application configuration, you must provide a valid `token` to access this endpoint. Unauthorized access will result in a 403 status code.\"\"\",\n        response_description=\"The ZIP file that was requested, or a status code indicating an error.\")\nasync def download_file_endpoint(file_name: str, token: str = None):\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")    \n    decoded_file_name = unquote(file_name)\n    file_path = os.path.join(\"generated_transcript_embeddings_zip_files\", decoded_file_name)\n    absolute_file_path = os.path.abspath(file_path)\n    logger.info(f\"Trying to fetch file from: {absolute_file_path}\")\n    if os.path.exists(absolute_file_path):\n        with open(absolute_file_path, 'rb') as f:\n            logger.info(f\"File first 10 bytes: {f.read(10)}\")\n        return FileResponse(absolute_file_path, media_type=\"application/zip\", filename=decoded_file_name)\n    else:\n        logger.error(f\"File not found at: {absolute_file_path}\")\n        raise HTTPException(status_code=404, detail=\"File not found\")\n    \n\n\n@app.get(\"/show_logs_incremental/{minutes}/{last_position}\", response_model=ShowLogsIncrementalModel)\ndef show_logs_incremental(minutes: int, last_position: int):\n    return show_logs_incremental_func(minutes, last_position)\n\n\n\n@app.get(\"/show_logs/{minutes}\", response_class=HTMLResponse)\ndef show_logs(minutes: int = 5):\n    return show_logs_func(minutes)\n\n\n        \n@app.get(\"/show_logs\",\n        response_class=HTMLResponse,\n        summary=\"Show Recent Logs\",\n        description=\"\"\"Displays the most recent logs from the 'swiss_army_llama.log' file, highlighting key words and patterns for easier readability. The logs are displayed as HTML content with inline styles and Javascript for dynamic refreshing.\n        \n### Behavior:\n- By default, shows logs from the last 5 minutes.\n- Log entries are color-coded based on keywords like 'success', 'error', 'pending', etc.\n- Log entries are continuously fetched every 20 seconds.\n- Provides options to copy and download the logs.\n\n### Response:\nThe response will be an HTML page that displays the logs in a human-readable format, highlighting various types of information.\n\n### Additional Features:\n- Users can copy or download the logs using buttons provided on the page.\"\"\",\n        response_description=\"An HTML page containing the most recent logs with dynamic updating and interactive features.\")\ndef show_logs_default():\n    return show_logs_func(5)\n\nif __name__ == \"__main__\":\n    try:\n        uvicorn.run(\"swiss_army_llama:app\", **option)\n    except GracefulExit:\n        logger.info(\"Received signal to terminate. Shutting down gracefully...\")\n        sys.exit(0)\n    except KeyboardInterrupt:\n        logger.info(\"Received KeyboardInterrupt. Shutting down gracefully...\")\n        sys.exit(0)\n    except Exception:\n        logger.exception(\"Unhandled exception occurred during shutdown.\")\n        sys.exit(1)\n\n\n\n@app.post(\"/convert_document_to_sentences/\",\n    summary=\"Convert Document to Sentences\",\n    description=\"\"\"Convert an uploaded document into individual sentences and return various statistics.\n    \n### Parameters:\n- `file`: The uploaded document file (supports plain text, .doc/.docx, PDF files, images using Tesseract OCR, and many other file types supported by the textract library).\n- `url`: URL of the file to download.\n- `hash`: SHA3-256 hash of the file to verify integrity.\n- `size`: Size of the file in bytes to verify completeness.\n- `token`: Security token (optional).\n\n### Response:\nThe response will include a JSON object with the following keys:\n- `individual_sentences`: A list of individual sentences extracted from the document.\n- `total_number_of_sentences`: The total number of sentences extracted.\n- `average_words_per_sentence`: The average number of words per sentence.\n- `total_input_file_size_in_bytes`: The total size of the input file in bytes.\n- `total_text_size_in_characters`: The total size of the text extracted from the document in characters.\n\n### Example Request:\nSubmit a file for conversion.\n\n### Example Response:\n```json\n{\n    \"individual_sentences\": [\"This is the first sentence.\", \"Here is another one.\"],\n    \"total_number_of_sentences\": 2,\n    \"average_words_per_sentence\": 5.0,\n    \"total_input_file_size_in_bytes\": 2048,\n    \"total_text_size_in_characters\": 50\n}\n```\"\"\",\n    response_description=\"A JSON object containing the sentences extracted from the document and various statistics.\"\n)\nasync def convert_document_to_sentences(\n    file: UploadFile = File(None),\n    url: str = Form(None),\n    hash: str = Form(None),\n    size: int = Form(None),\n    token: str = Form(None)\n):\n    if USE_SECURITY_TOKEN and use_hardcoded_security_token and (token is None or token != SECURITY_TOKEN):\n        raise HTTPException(status_code=403, detail=\"Unauthorized\")\n    temp_file_path = None\n    if file:\n        input_data_binary = await file.read()\n        result = magika.identify_bytes(input_data_binary)\n        detected_data_type = result.output.ct_label\n        temp_file = tempfile.NamedTemporaryFile(suffix=f\".{detected_data_type}\", delete=False)\n        temp_file_path = temp_file.name\n        logger.info(f\"Temp file path: {temp_file_path}\")\n        with open(temp_file_path, 'wb') as buffer:\n            buffer.write(input_data_binary)\n    elif url and hash and size:\n        temp_file_path = await download_file(url, size, hash)\n        with open(temp_file_path, 'rb') as file:\n            input_data_binary = file.read()\n            result = magika.identify_bytes(input_data_binary)\n            detected_data_type = result.output.ct_label\n            new_temp_file_path = temp_file_path + f\".{detected_data_type}\"\n            os.rename(temp_file_path, new_temp_file_path)\n            temp_file_path = new_temp_file_path\n    else:\n        raise HTTPException(status_code=400, detail=\"Invalid input. Provide either a file or URL with hash and size.\")\n    \n    try:\n        result = await convert_document_to_sentences_func(temp_file_path, result.output.mime_type)\n    finally:\n        os.remove(temp_file_path)\n    return JSONResponse(content=result)\n"}
{"type": "source_file", "path": "shared_resources.py", "content": "from misc_utility_functions import  is_redis_running, start_redis_server, build_faiss_indexes\nfrom database_functions import DatabaseWriter, initialize_db, AsyncSessionLocal, delete_expired_rows \nfrom ramdisk_functions import setup_ramdisk, copy_models_to_ramdisk, check_that_user_has_required_permissions_to_manage_ramdisks\nfrom logger_config import setup_logger\nfrom aioredlock import Aioredlock\nimport aioredis\nimport asyncio\nimport urllib.request\nimport os\nimport glob\nimport json\nfrom filelock import FileLock, Timeout\nimport traceback\nimport llama_cpp\nfrom typing import List, Tuple, Dict\nfrom decouple import config\nfrom fastapi import HTTPException\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\ntry:\n    import nvgpu\n    GPU_AVAILABLE = True\nexcept ImportError:\n    GPU_AVAILABLE = False\nlogger = setup_logger()\n\nembedding_model_cache = {} # Model cache to store loaded models\ntext_completion_model_cache = {} # Model cache to store loaded text completion models\n\nSWISS_ARMY_LLAMA_SERVER_LISTEN_PORT = config(\"SWISS_ARMY_LLAMA_SERVER_LISTEN_PORT\", default=8089, cast=int)\nDEFAULT_MODEL_NAME = config(\"DEFAULT_MODEL_NAME\", default=\"openchat_v3.2_super\", cast=str) \nLLM_CONTEXT_SIZE_IN_TOKENS = config(\"LLM_CONTEXT_SIZE_IN_TOKENS\", default=512, cast=int)\nTEXT_COMPLETION_CONTEXT_SIZE_IN_TOKENS = config(\"TEXT_COMPLETION_CONTEXT_SIZE_IN_TOKENS\", default=4000, cast=int)\nDEFAULT_MAX_COMPLETION_TOKENS = config(\"DEFAULT_MAX_COMPLETION_TOKENS\", default=100, cast=int)\nDEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE = config(\"DEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE\", default=4, cast=int)\nDEFAULT_COMPLETION_TEMPERATURE = config(\"DEFAULT_COMPLETION_TEMPERATURE\", default=0.7, cast=float)\nMINIMUM_STRING_LENGTH_FOR_DOCUMENT_EMBEDDING = config(\"MINIMUM_STRING_LENGTH_FOR_DOCUMENT_EMBEDDING\", default=15, cast=int)\nUSE_PARALLEL_INFERENCE_QUEUE = config(\"USE_PARALLEL_INFERENCE_QUEUE\", default=False, cast=bool)\nMAX_CONCURRENT_PARALLEL_INFERENCE_TASKS = config(\"MAX_CONCURRENT_PARALLEL_INFERENCE_TASKS\", default=10, cast=int)\nUSE_RAMDISK = config(\"USE_RAMDISK\", default=False, cast=bool)\nUSE_VERBOSE = config(\"USE_VERBOSE\", default=False, cast=bool)\nRAMDISK_PATH = config(\"RAMDISK_PATH\", default=\"/mnt/ramdisk\", cast=str)\nBASE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\nUSE_AUTOMATIC_PURGING_OF_EXPIRED_RECORDS = config(\"USE_AUTOMATIC_PURGING_OF_EXPIRED_RECORDS\", default=1, cast=bool)\n\nif USE_AUTOMATIC_PURGING_OF_EXPIRED_RECORDS:\n    scheduler = AsyncIOScheduler()\n    scheduler.add_job(delete_expired_rows(AsyncSessionLocal), 'interval', hours=1)\n    scheduler.start()\n\ndef is_gpu_available():\n    if not GPU_AVAILABLE:\n        return {\n            \"gpu_found\": False,\n            \"num_gpus\": 0,\n            \"first_gpu_vram\": 0,\n            \"total_vram\": 0,\n            \"error\": \"nvgpu module not found\"\n        }\n    try:\n        gpu_info = nvgpu.gpu_info()\n        num_gpus = len(gpu_info)\n        if num_gpus == 0:\n            return {\n                \"gpu_found\": False,\n                \"num_gpus\": 0,\n                \"first_gpu_vram\": 0,\n                \"total_vram\": 0\n            }\n        first_gpu_vram = gpu_info[0]['mem_total']\n        total_vram = sum(gpu['mem_total'] for gpu in gpu_info)\n        return {\n            \"gpu_found\": True,\n            \"num_gpus\": num_gpus,\n            \"first_gpu_vram\": first_gpu_vram,\n            \"total_vram\": total_vram,\n            \"gpu_info\": gpu_info\n        }\n    except Exception as e:\n        return {\n            \"gpu_found\": False,\n            \"num_gpus\": 0,\n            \"first_gpu_vram\": 0,\n            \"total_vram\": 0,\n            \"error\": str(e)\n        }\n        \nasync def initialize_globals():\n    global db_writer, faiss_indexes, associated_texts_by_model_and_pooling_method, redis, lock_manager\n    if not is_redis_running():\n        logger.info(\"Starting Redis server...\")\n        start_redis_server()\n        await asyncio.sleep(1)  # Sleep for 1 second to give Redis time to start\n    redis = await aioredis.create_redis_pool('redis://localhost')\n    lock_manager = Aioredlock([redis])\n    lock_manager.default_lock_timeout = 20000  # Lock timeout in milliseconds (20 seconds)\n    lock_manager.retry_count = 5  # Number of retries\n    lock_manager.retry_delay_min = 100  # Minimum delay between retries in milliseconds\n    lock_manager.retry_delay_max = 1000  # Maximum delay between retries in milliseconds\n    await initialize_db()\n    queue = asyncio.Queue()\n    db_writer = DatabaseWriter(queue)\n    await db_writer.initialize_processing_hashes()\n    asyncio.create_task(db_writer.dedicated_db_writer())\n    global USE_RAMDISK\n    if USE_RAMDISK and not check_that_user_has_required_permissions_to_manage_ramdisks():\n        USE_RAMDISK = False\n    elif USE_RAMDISK:\n        setup_ramdisk()\n    list_of_downloaded_model_names, download_status = download_models()\n    faiss_indexes, associated_texts_by_model_and_pooling_method = await build_faiss_indexes()\n\n\n# other shared variables and methods\ndb_writer = None\nfaiss_indexes = None\nassociated_texts_by_model_and_pooling_method = None\nredis = None\nlock_manager = None\n\ndef download_models() -> Tuple[List[str], List[Dict[str, str]]]:\n    download_status = []    \n    json_path = os.path.join(BASE_DIRECTORY, \"model_urls.json\")\n    if not os.path.exists(json_path):\n        initial_model_urls = [\n            \"https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-Instruct-Merged-DPO-Q4_K_M.gguf\",\n            \"https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct.Q3_K_S.gguf\",\n            \"https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf\"\n        ]\n        with open(json_path, \"w\") as f:\n            json.dump(initial_model_urls, f)\n    with open(json_path, \"r\") as f:\n        list_of_model_download_urls = json.load(f)\n    model_names = [os.path.basename(url) for url in list_of_model_download_urls]\n    current_file_path = os.path.abspath(__file__)\n    base_dir = os.path.dirname(current_file_path)\n    models_dir = os.path.join(base_dir, 'models')\n    logger.info(\"Checking models directory...\")\n    if USE_RAMDISK:\n        ramdisk_models_dir = os.path.join(RAMDISK_PATH, 'models')\n        if not os.path.exists(RAMDISK_PATH):\n            setup_ramdisk()\n        if all(os.path.exists(os.path.join(ramdisk_models_dir, llm_model_name)) for llm_model_name in model_names):\n            logger.info(\"Models found in RAM Disk.\")\n            for url in list_of_model_download_urls:\n                download_status.append({\"url\": url, \"status\": \"success\", \"message\": \"Model found in RAM Disk.\"})\n            return model_names, download_status\n    if not os.path.exists(models_dir):\n        os.makedirs(models_dir)\n        logger.info(f\"Created models directory: {models_dir}\")\n    else:\n        logger.info(f\"Models directory exists: {models_dir}\")\n    lock = FileLock(os.path.join(models_dir, \"download.lock\"))\n    for url, model_name_with_extension in zip(list_of_model_download_urls, model_names):\n        status = {\"url\": url, \"status\": \"success\", \"message\": \"File already exists.\"}\n        filename = os.path.join(models_dir, model_name_with_extension)\n        try:\n            with lock.acquire(timeout=1200): # Wait up to 20 minutes for the file to be downloaded before returning failure\n                if not os.path.exists(filename):\n                    logger.info(f\"Downloading model {model_name_with_extension} from {url}...\")\n                    urllib.request.urlretrieve(url, filename)\n                    file_size = os.path.getsize(filename) / (1024 * 1024)  # Convert bytes to MB\n                    if file_size < 100:\n                        os.remove(filename)\n                        status[\"status\"] = \"failure\"\n                        status[\"message\"] = \"Downloaded file is too small, probably not a valid model file.\"\n                    else:\n                        logger.info(f\"Downloaded: {filename}\")\n                else:\n                    logger.info(f\"File already exists: {filename}\")\n        except Timeout:\n            logger.warning(f\"Could not acquire lock for downloading {model_name_with_extension}\")\n            status[\"status\"] = \"failure\"\n            status[\"message\"] = \"Could not acquire lock for downloading.\"\n        download_status.append(status)\n    if USE_RAMDISK:\n        copy_models_to_ramdisk(models_dir, ramdisk_models_dir)\n    logger.info(\"Model downloads completed.\")\n    return model_names, download_status\n\ndef load_model(llm_model_name: str, raise_http_exception: bool = True):\n    global USE_VERBOSE\n    model_instance = None\n    try:\n        models_dir = os.path.join(RAMDISK_PATH, 'models') if USE_RAMDISK else os.path.join(BASE_DIRECTORY, 'models')\n        if llm_model_name in embedding_model_cache:\n            return embedding_model_cache[llm_model_name]\n        matching_files = glob.glob(os.path.join(models_dir, f\"{llm_model_name}*\"))\n        if not matching_files:\n            logger.error(f\"No model file found matching: {llm_model_name}\")\n            raise FileNotFoundError\n        matching_files.sort(key=os.path.getmtime, reverse=True)\n        model_file_path = matching_files[0]\n        gpu_info = is_gpu_available()\n        if 'llava' in llm_model_name:\n            is_llava_multimodal_model = 1\n        else:\n            is_llava_multimodal_model = 0\n        if not is_llava_multimodal_model:\n            if gpu_info['gpu_found']:\n                try:\n                    model_instance = llama_cpp.Llama(model_path=model_file_path, embedding=True, n_ctx=LLM_CONTEXT_SIZE_IN_TOKENS, verbose=USE_VERBOSE, n_gpu_layers=-1) # Load the model with GPU acceleration\n                except Exception as e:  # noqa: F841\n                    model_instance = llama_cpp.Llama(model_path=model_file_path, embedding=True, n_ctx=LLM_CONTEXT_SIZE_IN_TOKENS, verbose=USE_VERBOSE)\n            else:\n                model_instance = llama_cpp.Llama(model_path=model_file_path, embedding=True, n_ctx=LLM_CONTEXT_SIZE_IN_TOKENS, verbose=USE_VERBOSE) # Load the model without GPU acceleration        \n            embedding_model_cache[llm_model_name] = model_instance\n        return model_instance\n    except TypeError as e:\n        logger.error(f\"TypeError occurred while loading the model: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Exception occurred while loading the model: {e}\")\n        traceback.print_exc()\n        if raise_http_exception:\n            raise HTTPException(status_code=404, detail=\"Model file not found\")\n        else:\n            raise FileNotFoundError(f\"No model file found matching: {llm_model_name}\")"}
{"type": "source_file", "path": "uvicorn_config.py", "content": "from decouple import config\nSWISS_ARMY_LLAMA_SERVER_LISTEN_PORT = config(\"SWISS_ARMY_LLAMA_SERVER_LISTEN_PORT\", default=8089, cast=int)\nUVICORN_NUMBER_OF_WORKERS = config(\"UVICORN_NUMBER_OF_WORKERS\", default=3, cast=int)\n\noption = {\n    \"host\": \"0.0.0.0\",\n    \"port\": SWISS_ARMY_LLAMA_SERVER_LISTEN_PORT,\n    \"workers\": UVICORN_NUMBER_OF_WORKERS\n}\n"}
{"type": "source_file", "path": "log_viewer_functions.py", "content": "import re\nimport logging\nimport html\nfrom datetime import datetime, timedelta\nfrom pytz import timezone\n\nlog_file_path = 'swiss_army_llama.log'\n\ndef safe_highlight_func(text, pattern, replacement):\n    try:\n        return re.sub(pattern, replacement, text)\n    except Exception as e:\n        logging.warning(f\"Failed to apply highlight rule: {e}\")\n        return text\n\n\ndef highlight_rules_func(text):\n    rules = [\n        (re.compile(r\"\\b(success\\w*)\\b\", re.IGNORECASE), '#COLOR1_OPEN#', '#COLOR1_CLOSE#'),\n        (re.compile(r\"\\b(error|fail\\w*)\\b\", re.IGNORECASE), '#COLOR2_OPEN#', '#COLOR2_CLOSE#'),\n        (re.compile(r\"\\b(pending)\\b\", re.IGNORECASE), '#COLOR3_OPEN#', '#COLOR3_CLOSE#'),\n        (re.compile(r\"\\b(response)\\b\", re.IGNORECASE), '#COLOR4_OPEN#', '#COLOR4_CLOSE#'),\n        (re.compile(r'\\\"(.*?)\\\"', re.IGNORECASE), '#COLOR5_OPEN#', '#COLOR5_CLOSE#'),\n        (re.compile(r\"\\'(.*?)\\'\", re.IGNORECASE), \"#COLOR6_OPEN#\", '#COLOR6_CLOSE#'),\n        (re.compile(r\"\\`(.*?)\\`\", re.IGNORECASE), '#COLOR7_OPEN#', '#COLOR7_CLOSE#'),\n        (re.compile(r\"\\b(https?://\\S+)\\b\", re.IGNORECASE), '#COLOR8_OPEN#', '#COLOR8_CLOSE#'),\n        (re.compile(r\"\\b(\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2},\\d{3})\\b\", re.IGNORECASE), '#COLOR9_OPEN#', '#COLOR9_CLOSE#'),\n        (re.compile(r\"\\b(_{100,})\\b\", re.IGNORECASE), '#COLOR10_OPEN#', '#COLOR10_CLOSE#'),\n        (re.compile(r\"\\b(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d+)\\b\", re.IGNORECASE), '#COLOR11_OPEN#', '#COLOR11_CLOSE#'),\n        (re.compile(r\"\\b([a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12})\\b\", re.IGNORECASE), '#COLOR12_OPEN#', '#COLOR12_CLOSE#'),\n        (re.compile(r\"\\b([a-f0-9]{64})\\b\", re.IGNORECASE), '#COLOR13_OPEN#', '#COLOR13_CLOSE#')                                \n    ]\n    for pattern, replacement_open, replacement_close in rules:\n        text = pattern.sub(f\"{replacement_open}\\\\1{replacement_close}\", text)\n    text = html.escape(text)\n    text = text.replace('#COLOR1_OPEN#', '<span style=\"color: #baffc9;\">').replace('#COLOR1_CLOSE#', '</span>')\n    text = text.replace('#COLOR2_OPEN#', '<span style=\"color: #ffb3ba;\">').replace('#COLOR2_CLOSE#', '</span>')\n    text = text.replace('#COLOR3_OPEN#', '<span style=\"color: #ffdfba;\">').replace('#COLOR3_CLOSE#', '</span>')\n    text = text.replace('#COLOR4_OPEN#', '<span style=\"color: #ffffba;\">').replace('#COLOR4_CLOSE#', '</span>')\n    text = text.replace('#COLOR5_OPEN#', '<span style=\"color: #bdc7e7;\">').replace('#COLOR5_CLOSE#', '</span>')\n    text = text.replace('#COLOR6_OPEN#', \"<span style='color: #d5db9c;'>\").replace('#COLOR6_CLOSE#', '</span>')\n    text = text.replace('#COLOR7_OPEN#', '<span style=\"color: #a8d8ea;\">').replace('#COLOR7_CLOSE#', '</span>')\n    text = text.replace('#COLOR8_OPEN#', '<span style=\"color: #e2a8a8;\">').replace('#COLOR8_CLOSE#', '</span>')\n    text = text.replace('#COLOR9_OPEN#', '<span style=\"color: #ece2d0;\">').replace('#COLOR9_CLOSE#', '</span>')\n    text = text.replace('#COLOR10_OPEN#', '<span style=\"color: #d6e0f0;\">').replace('#COLOR10_CLOSE#', '</span>')\n    text = text.replace('#COLOR11_OPEN#', '<span style=\"color: #f2d2e2;\">').replace('#COLOR11_CLOSE#', '</span>')\n    text = text.replace('#COLOR12_OPEN#', '<span style=\"color: #d5f2ea;\">').replace('#COLOR12_CLOSE#', '</span>')\n    text = text.replace('#COLOR13_OPEN#', '<span style=\"color: #f2ebd3;\">').replace('#COLOR13_CLOSE#', '</span>')\n    return text\n\ndef show_logs_incremental_func(minutes: int, last_position: int):\n    new_logs = []\n    now = datetime.now(timezone('UTC'))  # get current time, make it timezone-aware\n    with open(log_file_path, \"r\") as f:\n        f.seek(last_position)  # seek to `last_position`\n        while True:\n            line = f.readline()\n            if line == \"\":  # if EOF\n                break\n            if line.strip() == \"\":\n                continue\n            try:  # Try to parse the datetime\n                log_datetime_str = line.split(\" - \")[0]  # assuming the datetime is at the start of each line\n                log_datetime = datetime.strptime(log_datetime_str, \"%Y-%m-%d %H:%M:%S,%f\")  # parse the datetime string to a datetime object\n                log_datetime = log_datetime.replace(tzinfo=timezone('UTC'))  # set the datetime object timezone to UTC to match `now`\n                if now - log_datetime > timedelta(minutes=minutes):  # if the log is older than `minutes` minutes from now\n                    continue  # ignore the log and continue with the next line\n            except ValueError:\n                pass  # If the line does not start with a datetime, ignore the ValueError and process the line anyway\n            new_logs.append(highlight_rules_func(line.rstrip('\\n')))  # add the highlighted log to the list and strip any newline at the end\n        last_position = f.tell()  # get the last position\n    new_logs_as_string = \"<br>\".join(new_logs)  # joining with <br> directly\n    return {\"logs\": new_logs_as_string, \"last_position\": last_position}  # also return the last position\n\n\ndef show_logs_func(minutes: int = 5):\n    with open(log_file_path, \"r\") as f:\n        lines = f.readlines()\n    logs = []\n    now = datetime.now(timezone('UTC'))\n    for line in lines:\n        if line.strip() == \"\":\n            continue\n        try:\n            log_datetime_str = line.split(\" - \")[0]\n            log_datetime = datetime.strptime(log_datetime_str, \"%Y-%m-%d %H:%M:%S,%f\")\n            log_datetime = log_datetime.replace(tzinfo=timezone('UTC'))\n            if now - log_datetime <= timedelta(minutes=minutes):\n                logs.append(highlight_rules_func(line.rstrip('\\n')))\n        except (ValueError, IndexError):\n            logs.append(highlight_rules_func(line.rstrip('\\n')))  # Line didn't meet datetime parsing criteria, continue with processing\n    logs_as_string = \"<br>\".join(logs)\n    logs_as_string_newlines_rendered = logs_as_string.replace(\"\\n\", \"<br>\")\n    logs_as_string_newlines_rendered_font_specified = \"\"\"\n    <html>\n    <head>\n    <link href=\"https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;500&display=swap\" rel=\"stylesheet\">\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css\" rel=\"stylesheet\">\n    <script>\n    var logContainer;\n    var lastLogs = `{0}`;\n    var shouldScroll = true;\n    var userScroll = false;\n    var lastPosition = 0;\n    var minutes = {1};\n    function fetchLogs() {{\n        if (typeof minutes !== 'undefined' && typeof lastPosition !== 'undefined') {{\n            fetch('/show_logs_incremental/' + minutes + '/' + lastPosition)\n            .then(response => response.json())\n            .then(data => {{\n                if (logContainer) {{\n                    var div = document.createElement('div');\n                    div.innerHTML = data.logs;\n                    if (div.innerHTML) {{\n                        lastLogs += div.innerHTML;\n                        lastPosition = data.last_position;\n                    }}\n                    logContainer.innerHTML = lastLogs;\n                    if (shouldScroll) {{\n                        logContainer.scrollTop = logContainer.scrollHeight;\n                    }}\n                }}\n            }});\n        }}\n    }}\n    function checkScroll() {{\n        if(logContainer.scrollTop + logContainer.clientHeight < logContainer.scrollHeight) {{\n            userScroll = true;\n            shouldScroll = false;\n        }} else {{\n            userScroll = false;\n        }}\n        if (!userScroll) {{\n            setTimeout(function(){{ shouldScroll = true; }}, 10000);\n        }}\n    }}\n    window.onload = function() {{\n        let p = document.getElementsByTagName('p');\n        for(let i = 0; i < p.length; i++) {{\n            let color = window.getComputedStyle(p[i]).getPropertyValue('color');\n            p[i].style.textShadow = `0 0 5px ${{color}}, 0 0 10px ${{color}}, 0 0 15px ${{color}}, 0 0 20px ${{color}}`;\n        }}\n        document.querySelector('#copy-button').addEventListener('click', function() {{\n            var text = document.querySelector('#log-container').innerText;\n            navigator.clipboard.writeText(text).then(function() {{\n                console.log('Copying to clipboard was successful!');\n            }}, function(err) {{\n                console.error('Could not copy text: ', err);\n            }});\n        }});\n        document.querySelector('#download-button').addEventListener('click', function() {{\n            var text = document.querySelector('#log-container').innerText;\n            var element = document.createElement('a');\n            element.setAttribute('href', 'data:text/plain;charset=utf-8,' + encodeURIComponent(text));\n            element.setAttribute('download', 'swiss_army_llama_monitor_log__' + new Date().toISOString() + '.txt');\n            element.style.display = 'none';\n            document.body.appendChild(element);\n            element.click();\n            document.body.removeChild(element);\n        }});\n    }}\n    document.addEventListener('DOMContentLoaded', (event) => {{\n        logContainer = document.getElementById('log-container');\n        logContainer.innerHTML = lastLogs;\n        logContainer.addEventListener('scroll', checkScroll);\n        fetchLogs();\n        setInterval(fetchLogs, 20000);  // Fetch the logs every 20 seconds\n    }});\n    </script>\n    </head>        \n    <style>\n    .log-container {{\n        scroll-behavior: smooth;\n        background-color: #2b2b2b; /* dark gray */\n        color: #d3d3d3; /* light gray */\n        background-image: linear-gradient(rgba(0,0,0,0.1) 1px, transparent 1px), linear-gradient(90deg, rgba(0,0,0,0.1) 1px, transparent 1px);\n        background-size: 100% 10px, 10px 100%;\n        background-position: 0 0, 0 0;\n        animation: scan 1s linear infinite;\n        @keyframes scan {{\n            0% {{\n                background-position: 0 0, 0 0;\n            }}\n            100% {{\n                background-position: -10px -10px, -10px -10px;\n            }}\n        }}\n        font-size: 14px;\n        font-family: monospace;\n        padding: 10px;\n        height: 100vh;\n        margin: 0;\n        box-sizing: border-box;\n        overflow: auto;\n    }}\n    .icon-button {{\n        position: fixed;\n        right: 10px;\n        margin: 10px;\n        background-color: #555;\n        color: white;\n        border: none;\n        cursor: pointer;\n        border-radius: 50%;\n        width: 60px;\n        height: 60px;\n        font-size: 30px;\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        text-decoration: none;\n    }}\n    #copy-button {{\n        bottom: 80px;  // Adjust this value as needed\n    }}\n    #download-button {{\n        bottom: 10px;\n    }}\n    </style>\n    <body>\n    <pre id=\"log-container\" class=\"log-container\"></pre>\n    <button id=\"copy-button\" class=\"icon-button\"><i class=\"fas fa-copy\"></i></button>\n    <button id=\"download-button\" class=\"icon-button\"><i class=\"fas fa-download\"></i></button>\n    </body>\n    </html>\"\"\".format(logs_as_string_newlines_rendered, minutes)\n    return logs_as_string_newlines_rendered_font_specified"}
{"type": "source_file", "path": "database_functions.py", "content": "from embeddings_data_models import Base, TextEmbedding, DocumentEmbedding, Document, AudioTranscript\nfrom logger_config import setup_logger\nimport traceback\nimport asyncio\nimport random\nfrom sqlalchemy import select, update, UniqueConstraint, exists\nfrom sqlalchemy import text as sql_text\nfrom sqlalchemy.exc import SQLAlchemyError, OperationalError, IntegrityError\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\nfrom decouple import config\nfrom datetime import datetime, timedelta\n\nlogger = setup_logger()\ndb_writer = None\nDATABASE_URL = \"sqlite+aiosqlite:///swiss_army_llama.sqlite\"\nMAX_RETRIES = config(\"MAX_RETRIES\", default=3, cast=int)\nDB_WRITE_BATCH_SIZE = config(\"DB_WRITE_BATCH_SIZE\", default=25, cast=int) \nRETRY_DELAY_BASE_SECONDS = config(\"RETRY_DELAY_BASE_SECONDS\", default=1, cast=int)\nJITTER_FACTOR = config(\"JITTER_FACTOR\", default=0.1, cast=float)\nTIME_IN_DAYS_BEFORE_RECORDS_ARE_PURGED = config(\"TIME_IN_DAYS_BEFORE_RECORDS_ARE_PURGED\", default=2, cast=int)\n\nengine = create_async_engine(DATABASE_URL, echo=False, connect_args={\"check_same_thread\": False})\nAsyncSessionLocal = sessionmaker(bind=engine, class_=AsyncSession, expire_on_commit=False, autoflush=False)\n\nasync def consolidate_wal_data():\n    consolidate_command = \"PRAGMA wal_checkpoint(FULL);\"\n    try:\n        async with engine.begin() as conn:\n            result = await conn.execute(sql_text(consolidate_command))\n            result_fetch = result.fetchone()\n            return result_fetch\n    except Exception as e:\n        logger.error(f\"Error during WAL consolidation: {e}\")\n        return None\n\nclass DatabaseWriter:\n    def __init__(self, queue):\n        self.queue = queue\n        self.processing_hashes = set()\n\n    def _get_hash_from_operation(self, operation):\n        if isinstance(operation, TextEmbedding):\n            return f\"{operation.embedding_hash}\"\n        elif isinstance(operation, DocumentEmbedding):\n            return f\"{operation.document_embedding_results_json_compressed_binary}\"\n        elif isinstance(operation, Document):\n            return operation.document_hash\n        elif isinstance(operation, AudioTranscript):\n            return operation.audio_file_hash\n        return None\n\n    async def initialize_processing_hashes(self, chunk_size=1000):\n        start_time = datetime.utcnow()\n        async with AsyncSessionLocal() as session:\n            queries = [\n                (select(TextEmbedding.embedding_hash), TextEmbedding),\n                (select(DocumentEmbedding.document_embedding_results_json_compressed_binary), DocumentEmbedding),\n                (select(Document.document_hash), Document),\n                (select(AudioTranscript.audio_file_hash), AudioTranscript)\n            ]\n            for query, model_class in queries:\n                offset = 0\n                while True:\n                    result = await session.execute(query.limit(chunk_size).offset(offset))\n                    rows = result.fetchall()\n                    if not rows:\n                        break\n                    for row in rows:\n                        if model_class == TextEmbedding:\n                            hash_with_model = row[0]\n                        elif model_class == DocumentEmbedding:\n                            hash_with_model = row[0]\n                        elif model_class == Document:\n                            hash_with_model = row[0]\n                        elif model_class == AudioTranscript:\n                            hash_with_model = row[0]\n                        self.processing_hashes.add(hash_with_model)\n                    offset += chunk_size\n        end_time = datetime.utcnow()\n        total_time = (end_time - start_time).total_seconds()\n        if len(self.processing_hashes) > 0:\n            logger.info(f\"Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: {len(self.processing_hashes)}; Took {total_time} seconds, for an average of {total_time / len(self.processing_hashes)} seconds per hash.\")\n\n    async def _record_exists(self, session, operation):\n        model_class = type(operation)\n        if model_class == TextEmbedding:\n            return await session.execute(select(exists().where(TextEmbedding.embedding_hash == operation.embedding_hash)))\n        elif model_class == DocumentEmbedding:\n            return await session.execute(select(exists().where(DocumentEmbedding.document_embedding_results_json_compressed_binary == operation.document_embedding_results_json_compressed_binary)))\n        elif model_class == Document:\n            return await session.execute(select(exists().where(Document.document_hash == operation.document_hash)))\n        elif model_class == AudioTranscript:\n            return await session.execute(select(exists().where(AudioTranscript.audio_file_hash == operation.audio_file_hash)))\n        return None\n\n    async def dedicated_db_writer(self):\n        while True:\n            write_operations_batch = await self.queue.get()\n            async with AsyncSessionLocal() as session:\n                filtered_operations = []\n                try:\n                    if write_operations_batch:\n                        for write_operation in write_operations_batch:\n                            existing_record = await self._record_exists(session, write_operation)\n                            if not existing_record.scalar():\n                                filtered_operations.append(write_operation)\n                                hash_value = self._get_hash_from_operation(write_operation)\n                                if hash_value:\n                                    self.processing_hashes.add(hash_value)\n                            else:\n                                await self._update_existing_record(session, write_operation)\n                        if filtered_operations:\n                            await consolidate_wal_data()  # Consolidate WAL before performing writes\n                            session.add_all(filtered_operations)\n                            await session.flush()  # Flush to get the IDs\n                            await session.commit()\n                            for write_operation in filtered_operations:\n                                hash_to_remove = self._get_hash_from_operation(write_operation)\n                                if hash_to_remove is not None and hash_to_remove in self.processing_hashes:\n                                    self.processing_hashes.remove(hash_to_remove)\n                except IntegrityError as e:\n                    await self._handle_integrity_error(e, write_operation, session)\n                except SQLAlchemyError as e:\n                    logger.error(f\"Database error: {e}\")\n                    await session.rollback()\n                except Exception as e:\n                    tb = traceback.format_exc()\n                    logger.error(f\"Unexpected error: {e}\\n{tb}\")\n                    await session.rollback()\n                self.queue.task_done()\n\n    async def _update_existing_record(self, session, operation):\n        model_class = type(operation)\n        primary_keys = [key.name for key in model_class.__table__.primary_key]\n        unique_constraints = [c for c in model_class.__table__.constraints if isinstance(c, UniqueConstraint)]\n        conditions = []\n        for constraint in unique_constraints:\n            if set(constraint.columns.keys()).issubset(set(operation.__dict__.keys())):\n                for col in constraint.columns.keys():\n                    conditions.append(getattr(model_class, col) == getattr(operation, col))\n                break\n        if not conditions:\n            for pk in primary_keys:\n                conditions.append(getattr(model_class, pk) == getattr(operation, pk))\n        values = {col: getattr(operation, col) for col in operation.__dict__.keys() if col in model_class.__table__.columns.keys()}\n        stmt = update(model_class).where(*conditions).values(**values)\n        await session.execute(stmt)\n        await session.commit()\n\n    async def _handle_integrity_error(self, e, write_operation, session):\n        unique_constraint_msg = {\n            TextEmbedding: \"embeddings.embedding_hash\",\n            DocumentEmbedding: \"document_embeddings.document_embedding_results_json_compressed_binary\",\n            Document: \"documents.document_hash\",\n            AudioTranscript: \"audio_transcripts.audio_file_hash\"\n        }.get(type(write_operation))\n        if unique_constraint_msg and unique_constraint_msg in str(e):\n            logger.warning(f\"Embedding already exists in the database for given input: {e}\")\n            await self._update_existing_record(session, write_operation)\n        else:\n            raise        \n\n    async def enqueue_write(self, write_operations):\n        write_operations = [op for op in write_operations if self._get_hash_from_operation(op) not in self.processing_hashes]\n        if not write_operations:\n            return\n        for op in write_operations:\n            hash_value = self._get_hash_from_operation(op)\n            if hash_value:\n                self.processing_hashes.add(hash_value)\n        await self.queue.put(write_operations)\n\n\nasync def execute_with_retry(func, *args, **kwargs):\n    retries = 0\n    while retries < MAX_RETRIES:\n        try:\n            return await func(*args, **kwargs)\n        except OperationalError as e:\n            if 'database is locked' in str(e):\n                retries += 1\n                sleep_time = RETRY_DELAY_BASE_SECONDS * (2 ** retries) + (random.random() * JITTER_FACTOR) # Implementing exponential backoff with jitter\n                logger.warning(f\"Database is locked. Retrying ({retries}/{MAX_RETRIES})... Waiting for {sleep_time} seconds\")\n                await asyncio.sleep(sleep_time)\n            else:\n                raise\n    raise OperationalError(\"Database is locked after multiple retries\")\n\nasync def initialize_db(use_verbose = 0):\n    logger.info(\"Initializing database, creating tables, and setting SQLite PRAGMAs...\")\n    list_of_sqlite_pragma_strings = [\n        \"PRAGMA journal_mode=WAL;\", \n        \"PRAGMA synchronous = NORMAL;\", \n        \"PRAGMA cache_size = -1048576;\", \n        \"PRAGMA busy_timeout = 2000;\", \n        \"PRAGMA wal_autocheckpoint = 100;\"\n    ]\n    list_of_sqlite_pragma_justification_strings = [\n        \"Set SQLite to use Write-Ahead Logging (WAL) mode (from default DELETE mode) so that reads and writes can occur simultaneously\",\n        \"Set synchronous mode to NORMAL (from FULL) so that writes are not blocked by reads\",\n        \"Set cache size to 1GB (from default 2MB) so that more data can be cached in memory and not read from disk; to make this 256MB, set it to -262144 instead\",\n        \"Increase the busy timeout to 2 seconds so that the database waits\",\n        \"Set the WAL autocheckpoint to 100 (from default 1000) so that the WAL file is checkpointed more frequently\"\n    ]\n    assert len(list_of_sqlite_pragma_strings) == len(list_of_sqlite_pragma_justification_strings)\n    async with engine.begin() as conn:\n        for pragma_string in list_of_sqlite_pragma_strings:\n            await conn.execute(sql_text(pragma_string))\n            if use_verbose:\n                logger.info(f\"Executed SQLite PRAGMA: {pragma_string}\")\n                logger.info(f\"Justification: {list_of_sqlite_pragma_justification_strings[list_of_sqlite_pragma_strings.index(pragma_string)]}\")\n        try:\n            await conn.run_sync(Base.metadata.create_all)  # Create tables if they don't exist\n        except Exception as e:  # noqa: F841\n            pass\n    logger.info(\"Database initialization completed.\")\n\ndef get_db_writer() -> DatabaseWriter:\n    return db_writer  # Return the existing DatabaseWriter instance\n\ndef delete_expired_rows(session_factory):\n    async def async_delete_expired_rows():\n        async with session_factory() as session:\n            expiration_time = datetime.utcnow() - timedelta(days=TIME_IN_DAYS_BEFORE_RECORDS_ARE_PURGED)\n            models = [TextEmbedding, DocumentEmbedding, Document, AudioTranscript]\n            for model in models:\n                expired_rows = await session.execute(\n                    select(model).where(model.created_at < expiration_time)\n                )\n                expired_rows = expired_rows.scalars().all()\n                for row in expired_rows:\n                    await session.delete(row)\n            await session.commit()\n    return async_delete_expired_rows\n"}
{"type": "source_file", "path": "logger_config.py", "content": "import logging\nimport os\nimport shutil\nimport queue\nfrom logging.handlers import RotatingFileHandler, QueueHandler, QueueListener\n\nlogger = logging.getLogger(\"swiss_army_llama\")\n\ndef setup_logger():\n    if logger.handlers:\n        return logger\n    old_logs_dir = 'old_logs'\n    if not os.path.exists(old_logs_dir):\n        os.makedirs(old_logs_dir)\n    logger.setLevel(logging.INFO)\n    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n    log_file_path = 'swiss_army_llama.log'\n    log_queue = queue.Queue(-1)  # Create a queue for the handlers\n    fh = RotatingFileHandler(log_file_path, maxBytes=10*1024*1024, backupCount=5)\n    fh.setFormatter(formatter)\n    def namer(default_log_name):  # Function to move rotated logs to the old_logs directory\n        return os.path.join(old_logs_dir, os.path.basename(default_log_name))\n    def rotator(source, dest):\n        shutil.move(source, dest)\n    fh.namer = namer\n    fh.rotator = rotator\n    sh = logging.StreamHandler()  # Stream handler\n    sh.setFormatter(formatter)\n    queue_handler = QueueHandler(log_queue)  # Create QueueHandler\n    queue_handler.setFormatter(formatter)\n    logger.addHandler(queue_handler)\n    listener = QueueListener(log_queue, sh)\n    listener.start()\n    logging.getLogger('sqlalchemy.engine').setLevel(logging.WARNING)  # Configure SQLalchemy logging\n    return logger"}
{"type": "source_file", "path": "embeddings_data_models.py", "content": "from sqlalchemy import Column, String, Float, DateTime, Integer, UniqueConstraint, ForeignKey, LargeBinary\nfrom sqlalchemy.dialects.sqlite import JSON\nfrom sqlalchemy.orm import declarative_base, relationship\nfrom sqlalchemy.ext.declarative import declared_attr\nfrom hashlib import sha3_256\nfrom pydantic import BaseModel, field_validator\nfrom typing import List, Optional, Union, Dict\nfrom decouple import config\nfrom sqlalchemy import event\nfrom datetime import datetime\n\nBase = declarative_base()\nDEFAULT_MODEL_NAME = config(\"DEFAULT_MODEL_NAME\", default=\"Meta-Llama-3-8B-Instruct.Q3_K_S\", cast=str) \nDEFAULT_EMBEDDING_MODEL_NAME = config(\"DEFAULT_EMBEDDING_MODEL_NAME\", default=\"nomic-embed-text-v1.5.Q6_K\", cast=str)\nDEFAULT_MULTI_MODAL_MODEL_NAME = config(\"DEFAULT_MULTI_MODAL_MODEL_NAME\", default=\"llava-llama-3-8b-v1_1-int4\", cast=str)\nDEFAULT_MAX_COMPLETION_TOKENS = config(\"DEFAULT_MAX_COMPLETION_TOKENS\", default=100, cast=int)\nDEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE = config(\"DEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE\", default=4, cast=int)\nDEFAULT_COMPLETION_TEMPERATURE = config(\"DEFAULT_COMPLETION_TEMPERATURE\", default=0.7, cast=float)\nDEFAULT_EMBEDDING_POOLING_METHOD = config(\"DEFAULT_EMBEDDING_POOLING_METHOD\", default=\"mean\", cast=str)\n\nclass SerializerMixin:\n    @declared_attr\n    def __tablename__(cls):\n        return cls.__name__.lower()\n\n    def as_dict(self):\n        return {c.key: getattr(self, c.key) for c in self.__table__.columns}\n    \nclass TextEmbedding(Base, SerializerMixin):\n    __tablename__ = \"embeddings\"\n    id = Column(Integer, primary_key=True, index=True)\n    text = Column(String, index=True)\n    text_hash = Column(String, index=True)\n    embedding_pooling_method = Column(String, index=True)\n    embedding_hash = Column(String, index=True)\n    llm_model_name = Column(String, index=True)\n    corpus_identifier_string = Column(String, index=True)    \n    embedding_json = Column(String)\n    ip_address = Column(String)\n    request_time = Column(DateTime)\n    response_time = Column(DateTime)\n    total_time = Column(Float)\n    document_file_hash = Column(String, ForeignKey('document_embeddings.document_file_hash'))\n    document = relationship(\"DocumentEmbedding\", back_populates=\"embeddings\", foreign_keys=[document_file_hash, corpus_identifier_string])\n    __table_args__ = (UniqueConstraint('embedding_hash', name='_embedding_hash_uc'),)\n\nclass DocumentEmbedding(Base):\n    __tablename__ = \"document_embeddings\"\n    id = Column(Integer, primary_key=True, index=True)\n    document_hash = Column(String, ForeignKey('documents.document_hash'))\n    filename = Column(String)\n    mimetype = Column(String)\n    document_file_hash = Column(String, index=True)\n    embedding_pooling_method = Column(String, index=True)\n    llm_model_name = Column(String, index=True)\n    corpus_identifier_string = Column(String, index=True)\n    file_data = Column(LargeBinary)  # To store the original file\n    sentences = Column(String)\n    document_embedding_results_json_compressed_binary = Column(LargeBinary)  # To store the embedding results JSON\n    ip_address = Column(String)\n    request_time = Column(DateTime)\n    response_time = Column(DateTime)\n    total_time = Column(Float)\n    embeddings = relationship(\"TextEmbedding\", back_populates=\"document\", foreign_keys=[TextEmbedding.document_file_hash])\n    __table_args__ = (UniqueConstraint('document_embedding_results_json_compressed_binary', name='_document_embedding_results_json_compressed_binary_uc'),)\n    document = relationship(\"Document\", back_populates=\"document_embeddings\", foreign_keys=[document_hash])\n\nclass Document(Base):\n    __tablename__ = \"documents\"\n    id = Column(Integer, primary_key=True, index=True)\n    llm_model_name = Column(String, index=True)\n    corpus_identifier_string = Column(String, index=True)    \n    document_hash = Column(String, index=True)\n    document_embeddings = relationship(\"DocumentEmbedding\", back_populates=\"document\", foreign_keys=[DocumentEmbedding.document_hash])\n    def update_hash(self):  # Concatenate specific attributes from the document_embeddings relationship\n        hash_data = \"\".join([emb.filename + emb.mimetype for emb in self.document_embeddings])\n        self.document_hash = sha3_256(hash_data.encode('utf-8')).hexdigest()\n@event.listens_for(Document.document_embeddings, 'append')\ndef update_document_hash_on_append(target, value, initiator):\n    target.update_hash()\n@event.listens_for(Document.document_embeddings, 'remove')\ndef update_document_hash_on_remove(target, value, initiator):\n    target.update_hash()\n\n# Request/Response models start here:\n\nclass EmbeddingRequest(BaseModel):\n    text: str = \"\"\n    llm_model_name: str = DEFAULT_EMBEDDING_MODEL_NAME\n    embedding_pooling_method: str = DEFAULT_EMBEDDING_POOLING_METHOD\n    corpus_identifier_string: str = \"\"\n\nclass SimilarityRequest(BaseModel):\n    text1: str = \"\"\n    text2: str = \"\"\n    llm_model_name: str = DEFAULT_EMBEDDING_MODEL_NAME\n    embedding_pooling_method: str = DEFAULT_EMBEDDING_POOLING_METHOD\n    similarity_measure: str = \"all\"\n    @field_validator('similarity_measure')\n    def validate_similarity_measure(cls, value):\n        valid_measures = [\"all\", \"spearman_rho\", \"kendall_tau\", \"approximate_distance_correlation\", \"jensen_shannon_dependency_measure\", \"hoeffding_d\"]\n        if value.lower() not in valid_measures:\n            raise ValueError(f\"Invalid similarity measure. Supported measures are: {', '.join(valid_measures)}\")\n        return value.lower()\n    \nclass SemanticSearchRequest(BaseModel):\n    query_text: str = \"\"\n    number_of_most_similar_strings_to_return: int = 10\n    llm_model_name: str = DEFAULT_EMBEDDING_MODEL_NAME\n    embedding_pooling_method: str = DEFAULT_EMBEDDING_POOLING_METHOD\n    corpus_identifier_string: str = \"\"\n        \nclass SemanticSearchResponse(BaseModel):\n    query_text: str\n    corpus_identifier_string: str\n    embedding_pooling_method: str\n    results: List[dict]  # List of similar strings and their similarity scores using cosine similarity with Faiss (in descending order)\n\nclass AdvancedSemanticSearchRequest(BaseModel):\n    query_text: str = \"\"\n    llm_model_name: str = DEFAULT_EMBEDDING_MODEL_NAME\n    embedding_pooling_method: str = DEFAULT_EMBEDDING_POOLING_METHOD\n    corpus_identifier_string: str = \"\"\n    similarity_filter_percentage: float = 0.01\n    number_of_most_similar_strings_to_return: int = 10\n    result_sorting_metric: str = \"hoeffding_d\"\n    @field_validator('result_sorting_metric')\n    def validate_similarity_measure(cls, value):\n        valid_measures = [\"spearman_rho\", \"kendall_tau\", \"approximate_distance_correlation\", \"jensen_shannon_dependency_measure\", \"hoeffding_d\"]\n        if value.lower() not in valid_measures:\n            raise ValueError(f\"Invalid similarity measure. Supported measures are: {', '.join(valid_measures)}\")\n        return value.lower()\n    \nclass AdvancedSemanticSearchResponse(BaseModel):\n    query_text: str\n    corpus_identifier_string: str\n    embedding_pooling_method: str\n    results: List[Dict[str, Union[str, float, Dict[str, float]]]]\n\nclass EmbeddingResponse(BaseModel):\n    id: int\n    text: str\n    text_hash: str\n    embedding_pooling_method: str\n    embedding_hash: str\n    llm_model_name: str\n    corpus_identifier_string: str\n    embedding_json: str\n    ip_address: Optional[str]\n    request_time: datetime\n    response_time: datetime\n    total_time: float\n    document_file_hash: Optional[str]\n    embedding: List[float]\n\nclass SimilarityResponse(BaseModel):\n    text1: str\n    text2: str\n    similarity_measure: str\n    embedding_pooling_method: str\n    similarity_score: Union[float, Dict[str, float]]  # Now can be either a float or a dictionary\n    embedding1: List[float]\n    embedding2: List[float]\n        \nclass AllStringsResponse(BaseModel):\n    strings: List[str]\n\nclass AllDocumentsResponse(BaseModel):\n    documents: List[str]\n\nclass TextCompletionRequest(BaseModel):\n    input_prompt: str = \"\"\n    llm_model_name: str = DEFAULT_MODEL_NAME\n    temperature: float = DEFAULT_COMPLETION_TEMPERATURE\n    grammar_file_string: str = \"\"\n    number_of_tokens_to_generate: int = DEFAULT_MAX_COMPLETION_TOKENS\n    number_of_completions_to_generate: int = DEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE\n    \nclass TextCompletionResponse(BaseModel):\n    input_prompt: str\n    llm_model_name: str\n    grammar_file_string: str\n    number_of_tokens_to_generate: int\n    number_of_completions_to_generate: int\n    time_taken_in_seconds: float\n    generated_text: str\n    finish_reason: str\n    llm_model_usage_json: str\n\nclass ImageQuestionResponse(BaseModel):\n    question: str\n    llm_model_name: str\n    image_hash: str\n    time_taken_in_seconds: float\n    number_of_tokens_to_generate: int\n    number_of_completions_to_generate: int\n    time_taken_in_seconds: float\n    generated_text: str\n    finish_reason: str\n    llm_model_usage_json: str\n    \nclass AudioTranscript(Base):\n    __tablename__ = \"audio_transcripts\"\n    audio_file_hash = Column(String, primary_key=True, index=True)\n    audio_file_name = Column(String, index=True)\n    audio_file_size_mb = Column(Float)  # File size in MB\n    segments_json = Column(JSON)  # Transcribed segments as JSON\n    combined_transcript_text = Column(String)\n    combined_transcript_text_list_of_metadata_dicts = Column(JSON)\n    info_json = Column(JSON)  # Transcription info as JSON\n    ip_address = Column(String)\n    request_time = Column(DateTime)\n    response_time = Column(DateTime)\n    total_time = Column(Float)\n    corpus_identifier_string = Column(String, index=True)\n\nclass AudioTranscriptResponse(BaseModel):\n    audio_file_hash: str\n    audio_file_name: str\n    audio_file_size_mb: float\n    segments_json: List[dict]\n    combined_transcript_text: str\n    combined_transcript_text_list_of_metadata_dicts: List[dict]\n    info_json: dict\n    url_to_download_zip_file_of_embeddings: str\n    ip_address: str\n    request_time: datetime\n    response_time: datetime\n    total_time: float\n    url_to_download_zip_file_of_embeddings: str\n    llm_model_name: str\n    embedding_pooling_method: str\n    corpus_identifier_string: str\n    \nclass ShowLogsIncrementalModel(BaseModel):\n    logs: str\n    last_position: int\n\nclass AddGrammarRequest(BaseModel):\n    bnf_grammar: str\n    grammar_file_name: str\n\nclass AddGrammarResponse(BaseModel):\n    valid_grammar_files: List[str]\n"}
{"type": "source_file", "path": "grammar_builder.py", "content": "from service_functions import validate_bnf_grammar_func\nfrom typing import List, Dict\nimport json\n\nuse_grammarbuilder_demo = 0\n\ndef normalize_json(json_str):\n    output = []\n    in_string = False\n    escape_char = False\n    for char in json_str:\n        if char == \"\\\\\" and not escape_char:\n            escape_char = True\n            output.append(char)\n            continue\n        if char == '\"' and not escape_char:\n            in_string = not in_string\n        if in_string:\n            output.append(char)\n        else:\n            if char.strip():\n                output.append(char)\n        if escape_char:\n            escape_char = False\n    return ''.join(output)\n\nclass GrammarBuilder:\n    type_to_bnf: Dict[str, str] = {\n        \"str\": \"string\",\n        \"float\": \"number\",\n        \"int\": \"number\",\n        \"bool\": \"bool\",\n        \"datetime\": \"datetime\",\n        \"List\": \"list\",\n        \"Dict\": \"dict\",\n        \"Optional\": \"optional\"\n    }\n\n    def __init__(self):\n        self.rules = {\n            \"ws\": \"([ \\\\t\\\\n] ws)?\",\n            \"string\": '\\\\\" ([^\"\\\\\\\\] | \"\\\\\\\\\" ([\"\\\\\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]))* \\\\\" ws',\n            \"number\": '(\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws',\n            \"bool\": \"('true' | 'false') ws\",\n            \"datetime\": \"string\",\n            \"dict\": \"'{' ws dict_pair_list ws '}' ws\",\n            \"dict_pair_list\": \"dict_pair (',' ws dict_pair)*\",\n            \"dict_pair\": \"string ':' ws value ws\",\n            \"list\": \"'[' ws list_items ws ']' ws\",\n            \"list_items\": \"value (',' ws value)*\"\n        }\n\n\n    def generate_bnf_from_fields(self, fields: List[str], parent=\"root\") -> str:\n        bnf = []\n        keys = ' | '.join([f'\"{field.split(\":\")[0].strip()}\"' for field in fields])\n        bnf.append(f\"{parent} ::= '{{' ws {parent}_pair_list ws '}}' ws\")\n        bnf.append(f\"{parent}_pair_list ::= {parent}_pair (',' ws {parent}_pair)*\")\n        bnf.append(f\"{parent}_pair ::= allowed_keys_{parent} ':' ws value ws\")\n        bnf.append(f\"allowed_keys_{parent} ::= {keys}\")\n        value_types = set()\n        for field in fields:\n            field_name, field_type = field.split(\":\")\n            field_name, field_type = field_name.strip(), field_type.strip()\n            parsed_type = self.type_to_bnf.get(field_type, field_type)\n            if field_type.startswith(\"List\"):\n                parsed_type = \"list\"\n            value_types.add(parsed_type)\n        bnf.append(f\"value ::= {' | '.join(value_types)}\")\n        return \"\\n\".join(bnf)\n\n    def pydantic_to_json_bnf(self, model_description: str) -> str:\n        lines = model_description.strip().split('\\n')[1:]\n        fields = [line.strip() for line in lines if ':' in line]\n        bnf_for_fields = self.generate_bnf_from_fields(fields)\n        return f\"{bnf_for_fields}\\n{self.generate_base_rules()}\"\n\n    def generate_base_rules(self):\n        return \"\\n\".join([f\"{key} ::= {value}\" for key, value in self.rules.items()])\n\n    def generate_bnf(self, data, parent=\"root\"):\n        bnf = []\n        if isinstance(data, dict):\n            keys = ' | '.join([f'\\\"{key}\\\"' for key in data.keys()])\n            bnf.append(f\"{parent} ::= '{{' ws {parent}_pair_list ws '}}' ws\")\n            bnf.append(f\"{parent}_pair_list ::= {parent}_pair (',' ws {parent}_pair)*\")\n            bnf.append(f\"{parent}_pair ::= allowed_keys_{parent} ':' ws value ws\")\n            bnf.append(f\"allowed_keys_{parent} ::= {keys}\")\n            sample_key = next(iter(data.keys()))\n            if isinstance(data[sample_key], dict):\n                bnf.append(f\"value ::= {self.generate_bnf(data[sample_key], 'nested_value')}\")\n        elif isinstance(data, list):\n            if len(data) > 0:\n                sample_item = data[0]\n                rule_name = f\"{parent}_item\"\n                bnf.append(f\"{parent} ::= '[' ws {rule_name} (',' ws {rule_name})* ']' ws\")\n                bnf.append(f\"{rule_name} ::= {self.type_to_bnf.get(type(sample_item).__name__, type(sample_item).__name__)}\")\n            else:\n                bnf.append(f\"{parent} ::= '[' ws ']' ws\")\n        else:\n            bnf.append(f\"{parent} ::= {self.type_to_bnf.get(type(data).__name__, type(data).__name__)} ws\")\n        return \"\\n\".join(bnf)\n                \n    def json_to_bnf(self, json_str):\n        normalized_str = normalize_json(json_str)\n        try:\n            parsed_data = json.loads(normalized_str)\n        except json.JSONDecodeError as e:\n            return f\"Invalid JSON: {e}\"\n        bnf_grammar = self.generate_bnf(parsed_data)\n        return f\"{bnf_grammar}\\n{self.generate_base_rules()}\"\n\n\nif use_grammarbuilder_demo:\n    gb = GrammarBuilder()\n    sample_json = '''\n    {\n    \"Optimistic\": {\n        \"score\": 70.0,\n        \"explanation\": \"The statement talks about secular industry tailwinds and expectations to grow the business at a rate exceeding global GDP.\"\n    },\n    \"Pessimistic\": {\n        \"score\": -20.0,\n        \"explanation\": \"The paragraph acknowledges that they've experienced equity losses year-to-date.\"\n    },\n    \"Confident\": {\n        \"score\": 60.0,\n        \"explanation\": \"The text shows belief in their people, platform, and their prospect of gaining market share.\"\n    },\n    \"Cautious\": {\n        \"score\": 40.0,\n        \"explanation\": \"Mentions the possibility of falling below the target margins but aims to stay within the range.\"\n    },\n    \"Transparent\": {\n        \"score\": 80.0,\n        \"explanation\": \"Provides clear information on financial outlook, including specifics about Adjusted EBITDA.\"\n    },\n    \"Vague\": {\n        \"score\": -80.0,\n        \"explanation\": \"The text is quite specific and does not evade details.\"\n    },\n    \"Upbeat\": {\n        \"score\": 20.0,\n        \"explanation\": \"The tone is more balanced and not overtly enthusiastic.\"\n    },\n    \"Disappointed\": {\n        \"score\": -10.0,\n        \"explanation\": \"Acknowledges equity losses but doesn't express dissatisfaction.\"\n    },\n    \"Reassuring\": {\n        \"score\": 50.0,\n        \"explanation\": \"Tries to reassure by focusing on core business and tailwinds.\"\n    },\n    \"Evasive\": {\n        \"score\": -100.0,\n        \"explanation\": \"No signs of avoiding any topics; quite straightforward.\"\n    },\n    \"Committed\": {\n        \"score\": 60.0,\n        \"explanation\": \"Shows dedication to running the core business within the stated margin.\"\n    },\n    \"Analytical\": {\n        \"score\": 70.0,\n        \"explanation\": \"Provides a breakdown of the financial situation and market conditions.\"\n    },\n    \"Ambitious\": {\n        \"score\": 50.0,\n        \"explanation\": \"Talks about exceeding global GDP growth.\"\n    },\n    \"Concerned\": {\n        \"score\": -10.0,\n        \"explanation\": \"Reflects worry about equity losses but not overly so.\"\n    },\n    \"Focused\": {\n        \"score\": 80.0,\n        \"explanation\": \"Focuses on core business and previously stated margin.\"\n    },\n    \"Uncertain\": {\n        \"score\": -90.0,\n        \"explanation\": \"No ambiguity in the statements; quite specific.\"\n    },\n    \"Responsive\": {\n        \"score\": 60.0,\n        \"explanation\": \"Directly addresses the financial outlook and plans.\"\n    },\n    \"Defensive\": {\n        \"score\": -100.0,\n        \"explanation\": \"No signs of defending or justifying decisions.\"\n    },\n    \"Strategic\": {\n        \"score\": 60.0,\n        \"explanation\": \"Discusses gaining share and investment in people and platform.\"\n    },\n    \"Realistic\": {\n        \"score\": 40.0,\n        \"explanation\": \"Acknowledges challenges but maintains a balanced view.\"\n    }\n    }\n    '''\n    print('\\n' + '_' * 80 + '\\n')\n    bnf_grammar = gb.json_to_bnf(sample_json)\n    print(bnf_grammar)\n    print('\\n' + '_' * 80 + '\\n')\n    print(\"Validating grammar...\")\n    is_valid, validation_message = validate_bnf_grammar_func(bnf_grammar)\n    print(validation_message)\n\n    print('\\n\\n\\n')\n\n    gb = GrammarBuilder()\n    sample_pydantic_model_description = '''\n    class AudioTranscriptResponse(BaseModel):\n        audio_file_hash: str\n        audio_file_name: str\n        audio_file_size_mb: float\n        segments_json: List[dict]\n        combined_transcript_text: str\n        combined_transcript_text_list_of_metadata_dicts: List[dict]\n        info_json: dict\n        url_to_download_zip_file_of_embeddings: str\n        ip_address: str\n        request_time: datetime\n        response_time: datetime\n        total_time: float\n    '''\n    \n    bnf_grammar = gb.pydantic_to_json_bnf(sample_pydantic_model_description)\n    print(bnf_grammar)\n    print('\\n' + '_' * 80 + '\\n')\n    print(\"Validating grammar...\")\n    is_valid, validation_message = validate_bnf_grammar_func(bnf_grammar)\n    print(validation_message)\n\n"}
{"type": "source_file", "path": "ramdisk_functions.py", "content": "from logger_config import setup_logger\nimport os\nimport subprocess\nimport shutil\nimport psutil\nfrom decouple import config\nlogger = setup_logger()\n\nRAMDISK_PATH = config(\"RAMDISK_PATH\", default=\"/mnt/ramdisk\", cast=str)\nRAMDISK_SIZE_IN_GB = config(\"RAMDISK_SIZE_IN_GB\", default=1, cast=int)\n\ndef check_that_user_has_required_permissions_to_manage_ramdisks():\n    try: # Try to run a harmless command with sudo to test if the user has password-less sudo permissions\n        result = subprocess.run([\"sudo\", \"ls\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n        if \"password\" in result.stderr.lower():\n            raise PermissionError(\"Password required for sudo\")\n        logger.info(\"User has sufficient permissions to manage RAM Disks.\")\n        return True\n    except (PermissionError, subprocess.CalledProcessError) as e:\n        logger.info(\"Sorry, current user does not have sufficient permissions to manage RAM Disks! Disabling RAM Disks for now...\")\n        logger.debug(f\"Permission check error detail: {e}\")\n        return False\n    \ndef setup_ramdisk():\n    if os.environ.get(\"RAMDISK_SETUP_DONE\") == \"1\":\n        logger.info(\"RAM Disk setup already completed by another worker. Skipping.\")\n        return    \n    cmd_check = f\"sudo mount | grep {RAMDISK_PATH}\" # Check if RAM disk already exists at the path\n    result = subprocess.run(cmd_check, shell=True, stdout=subprocess.PIPE).stdout.decode('utf-8')\n    if RAMDISK_PATH in result:\n        logger.info(f\"RAM Disk already set up at {RAMDISK_PATH}. Skipping setup.\")\n        return\n    total_ram_gb = psutil.virtual_memory().total / (1024 ** 3)\n    free_ram_gb = psutil.virtual_memory().free / (1024 ** 3)\n    buffer_gb = 2  # buffer to ensure we don't use all the free RAM\n    ramdisk_size_gb = max(min(RAMDISK_SIZE_IN_GB, free_ram_gb - buffer_gb), 0.1)\n    ramdisk_size_mb = int(ramdisk_size_gb * 1024)\n    ramdisk_size_str = f\"{ramdisk_size_mb}M\"\n    logger.info(f\"Total RAM: {total_ram_gb}G\")\n    logger.info(f\"Free RAM: {free_ram_gb}G\")\n    logger.info(f\"Calculated RAM Disk Size: {ramdisk_size_gb}G\")\n    if RAMDISK_SIZE_IN_GB > total_ram_gb:\n        raise ValueError(f\"Cannot allocate {RAMDISK_SIZE_IN_GB}G for RAM Disk. Total system RAM is {total_ram_gb:.2f}G.\")\n    logger.info(\"Setting up RAM Disk...\")\n    os.makedirs(RAMDISK_PATH, exist_ok=True)\n    mount_command = [\"sudo\", \"mount\", \"-t\", \"tmpfs\", \"-o\", f\"size={ramdisk_size_str}\", \"tmpfs\", RAMDISK_PATH]\n    subprocess.run(mount_command, check=True)\n    logger.info(f\"RAM Disk set up at {RAMDISK_PATH} with size {ramdisk_size_gb}G\")\n    os.environ[\"RAMDISK_SETUP_DONE\"] = \"1\"\n\ndef copy_models_to_ramdisk(models_directory, ramdisk_directory):\n    total_size = sum(os.path.getsize(os.path.join(models_directory, model)) for model in os.listdir(models_directory))\n    free_ram = psutil.virtual_memory().free\n    if total_size > free_ram:\n        logger.warning(f\"Not enough space on RAM Disk. Required: {total_size}, Available: {free_ram}. Rebuilding RAM Disk.\")\n        clear_ramdisk()\n        free_ram = psutil.virtual_memory().free  # Recompute the available RAM after clearing the RAM disk\n        if total_size > free_ram:\n            logger.error(f\"Still not enough space on RAM Disk even after clearing. Required: {total_size}, Available: {free_ram}.\")\n            raise ValueError(\"Not enough RAM space to copy models.\")\n    os.makedirs(ramdisk_directory, exist_ok=True)\n    for model in os.listdir(models_directory):\n        src_path = os.path.join(models_directory, model)\n        dest_path = os.path.join(ramdisk_directory, model)\n        if os.path.exists(dest_path) and os.path.getsize(dest_path) == os.path.getsize(src_path): # Check if the file already exists in the RAM disk and has the same size\n            logger.info(f\"Model {model} already exists in RAM Disk and is the same size. Skipping copy.\")\n            continue\n        shutil.copyfile(src_path, dest_path)\n        logger.info(f\"Copied model {model} to RAM Disk at {dest_path}\")\n\ndef clear_ramdisk():\n    while True:\n        cmd_check = f\"sudo mount | grep {RAMDISK_PATH}\"\n        result = subprocess.run(cmd_check, shell=True, stdout=subprocess.PIPE).stdout.decode('utf-8')\n        if RAMDISK_PATH not in result:\n            break  # Exit the loop if the RAMDISK_PATH is not in the mount list\n        cmd_umount = f\"sudo umount -l {RAMDISK_PATH}\"\n        subprocess.run(cmd_umount, shell=True, check=True)\n    logger.info(f\"Cleared RAM Disk at {RAMDISK_PATH}\")"}
{"type": "source_file", "path": "service_functions.py", "content": "from logger_config import setup_logger\nimport shared_resources\nfrom shared_resources import load_model, text_completion_model_cache, is_gpu_available\nfrom database_functions import AsyncSessionLocal, execute_with_retry\nfrom misc_utility_functions import clean_filename_for_url_func,  FakeUploadFile, sophisticated_sentence_splitter, merge_transcript_segments_into_combined_text, suppress_stdout_stderr, image_to_base64_data_uri, process_image, find_clip_model_path\nfrom embeddings_data_models import TextEmbedding, DocumentEmbedding, Document, AudioTranscript\nfrom embeddings_data_models import EmbeddingRequest, TextCompletionRequest\nfrom embeddings_data_models import TextCompletionResponse,  AudioTranscriptResponse, ImageQuestionResponse\nimport os\nimport re\nimport unicodedata\nimport shutil\nimport psutil\nimport glob\nimport json\nimport io\nimport zipfile\nimport tempfile\nimport traceback\nimport time\nfrom datetime import datetime\nfrom hashlib import sha3_256\nfrom urllib.parse import quote\nimport numpy as np\nimport pandas as pd\nimport textract\nimport zstandard as zstd\nfrom sqlalchemy import select\nfrom sqlalchemy.orm import joinedload\nfrom sqlalchemy.inspection import inspect\nfrom fastapi import HTTPException, Request, UploadFile\nfrom fastapi.concurrency import run_in_threadpool\nfrom typing import List, Optional, Dict, Any\nfrom decouple import config\nfrom faster_whisper import WhisperModel\nfrom llama_cpp import Llama, LlamaGrammar\nfrom llama_cpp.llama_chat_format import Llava16ChatHandler\nfrom llama_cpp import llama_types\nfrom mutagen import File as MutagenFile\nfrom magika import Magika\nimport httpx\nfrom sklearn.decomposition import TruncatedSVD, FastICA, FactorAnalysis\nfrom sklearn.random_projection import GaussianRandomProjection\n\nlogger = setup_logger()\nmagika = Magika()\n\nSWISS_ARMY_LLAMA_SERVER_LISTEN_PORT = config(\"SWISS_ARMY_LLAMA_SERVER_LISTEN_PORT\", default=8089, cast=int)\nDEFAULT_MODEL_NAME = config(\"DEFAULT_MODEL_NAME\", default=\"openchat_v3.2_super\", cast=str) \nLLM_CONTEXT_SIZE_IN_TOKENS = config(\"LLM_CONTEXT_SIZE_IN_TOKENS\", default=512, cast=int)\nTEXT_COMPLETION_CONTEXT_SIZE_IN_TOKENS = config(\"TEXT_COMPLETION_CONTEXT_SIZE_IN_TOKENS\", default=4000, cast=int)\nDEFAULT_MAX_COMPLETION_TOKENS = config(\"DEFAULT_MAX_COMPLETION_TOKENS\", default=100, cast=int)\nDEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE = config(\"DEFAULT_NUMBER_OF_COMPLETIONS_TO_GENERATE\", default=4, cast=int)\nDEFAULT_COMPLETION_TEMPERATURE = config(\"DEFAULT_COMPLETION_TEMPERATURE\", default=0.7, cast=float)\nMINIMUM_STRING_LENGTH_FOR_DOCUMENT_EMBEDDING = config(\"MINIMUM_STRING_LENGTH_FOR_DOCUMENT_EMBEDDING\", default=15, cast=int)\nUSE_PARALLEL_INFERENCE_QUEUE = config(\"USE_PARALLEL_INFERENCE_QUEUE\", default=False, cast=bool)\nMAX_CONCURRENT_PARALLEL_INFERENCE_TASKS = config(\"MAX_CONCURRENT_PARALLEL_INFERENCE_TASKS\", default=10, cast=int)\nUSE_RAMDISK = config(\"USE_RAMDISK\", default=False, cast=bool)\nUSE_VERBOSE = config(\"USE_VERBOSE\", default=False, cast=bool)\nUSE_RESOURCE_MONITORING = config(\"USE_RESOURCE_MONITORING\", default=1, cast=bool)\nUSE_FLASH_ATTENTION = config(\"USE_FLASH_ATTENTION\", default=True, cast=bool)\nRAMDISK_PATH = config(\"RAMDISK_PATH\", default=\"/mnt/ramdisk\", cast=str)\nBASE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\n\n    \n# Core embedding functions start here:    \n    \ndef prepare_string_for_embedding(text: str) -> str:\n    # Normalize Unicode characters to NFKC form\n    text = unicodedata.normalize('NFKC', text)\n    # Define all possible newline and carriage return characters\n    newline_chars = [\n        '\\r', '\\n', '\\r\\n', '\\u2028', '\\u2029', '\\v', '\\f', \n        '\\x85', '\\u000A', '\\u000B', '\\u000C', '\\u000D', '\\u0085',\n        '\\u000D\\u000A'\n    ]\n    # Replace all newline characters with a space\n    for nl in newline_chars:\n        text = text.replace(nl, ' ')\n    # Replace any sequence of whitespace characters (including non-breaking spaces) with a single space\n    text = re.sub(r'\\s+', ' ', text)\n    # Remove leading and trailing whitespace\n    text = text.strip()\n    # Remove leading comma followed by whitespace if present\n    if text.startswith(', '):\n        text = text[2:].strip()\n    # Remove all control characters and non-printable characters\n    text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n    # Ensure text is ASCII-encoded to catch any remaining unusual characters\n    text = text.encode('ascii', 'ignore').decode('ascii')\n    # Truncate to a maximum length of 5000 characters\n    if len(text) > 5000:\n        text = text[:5000]\n    # Eliminate all blank lines\n    text = ' '.join(line for line in text.splitlines() if line.strip() != '')\n    #Final trimming\n    text = text.strip()\n    return text\n\ndef compress_data(input_data):\n    if isinstance(input_data, str):\n        input_data = input_data.encode('utf-8')\n    zstd_compression_level = 15 # 22 is the highest compression level; 15 is a good balance between compression and speed\n    zstandard_compressor = zstd.ZstdCompressor(level=zstd_compression_level, write_content_size=True, write_checksum=True)\n    zstd_compressed_data = zstandard_compressor.compress(input_data)\n    return zstd_compressed_data\n\ndef decompress_data(compressed_data):\n    return zstd.decompress(compressed_data)\n\ndef add_model_url(new_url: str) -> str:\n    corrected_url = new_url\n    if '/blob/main/' in new_url:\n        corrected_url = new_url.replace('/blob/main/', '/resolve/main/')\n    json_path = os.path.join(BASE_DIRECTORY, \"model_urls.json\")\n    with open(json_path, \"r\") as f:\n        existing_urls = json.load(f)\n    if corrected_url not in existing_urls:\n        logger.info(f\"Model URL not found in database. Adding {new_url} now...\")\n        existing_urls.append(corrected_url)\n        with open(json_path, \"w\") as f:\n            json.dump(existing_urls, f)\n        logger.info(f\"Model URL added: {new_url}\")\n    else:\n        logger.info(\"Model URL already exists.\")        \n    return corrected_url  \n\nasync def get_embedding_from_db(text: str, llm_model_name: str, embedding_pooling_method: str):\n    text_hash = sha3_256(text.encode('utf-8')).hexdigest()\n    return await execute_with_retry(_get_embedding_from_db, text_hash, llm_model_name, embedding_pooling_method)\n\nasync def _get_embedding_from_db(text_hash: str, llm_model_name: str, embedding_pooling_method: str) -> Optional[TextEmbedding]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(TextEmbedding)\n            .filter(TextEmbedding.text_hash == text_hash,\n                    TextEmbedding.llm_model_name == llm_model_name,\n                    TextEmbedding.embedding_pooling_method == embedding_pooling_method)\n        )\n        return result.scalars().first()\n    \nasync def get_corpus_identifier_from_embedding_text(text: str, llm_model_name: str, embedding_pooling_method: str):\n    text_hash = sha3_256(text.encode('utf-8')).hexdigest()\n    return await execute_with_retry(_get_corpus_identifier_from_embedding_text, text_hash, llm_model_name, embedding_pooling_method)\n\nasync def _get_corpus_identifier_from_embedding_text(text_hash: str, llm_model_name: str, embedding_pooling_method: str) -> Optional[str]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(TextEmbedding.corpus_identifier_string)\n            .filter(TextEmbedding.text_hash == text_hash,\n                    TextEmbedding.llm_model_name == llm_model_name,\n                    TextEmbedding.embedding_pooling_method == embedding_pooling_method)\n        )\n        return result.scalar()\n\nasync def get_list_of_corpus_identifiers_from_list_of_embedding_texts(list_of_texts: List[str], llm_model_name: str, embedding_pooling_method: str):\n    list_of_text_hashes = [sha3_256(text.encode('utf-8')).hexdigest() for text in list_of_texts]\n    return await execute_with_retry(_get_list_of_corpus_identifiers_from_list_of_embedding_texts, list_of_text_hashes, llm_model_name, embedding_pooling_method)\n\nasync def _get_list_of_corpus_identifiers_from_list_of_embedding_texts(list_of_text_hashes: List[str], llm_model_name: str, embedding_pooling_method: str) -> List[str]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(TextEmbedding.corpus_identifier_string)\n            .filter(TextEmbedding.text_hash.in_(list_of_text_hashes),\n                    TextEmbedding.llm_model_name == llm_model_name,\n                    TextEmbedding.embedding_pooling_method == embedding_pooling_method)\n        )\n        rows = result.scalars().all()\n        return rows\n    \nasync def get_texts_for_corpus_identifier(corpus_identifier_string: str) -> Dict[str, List[str]]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(DocumentEmbedding)\n            .options(joinedload(DocumentEmbedding.embeddings))\n            .filter(DocumentEmbedding.corpus_identifier_string == corpus_identifier_string)\n        )\n        document_embeddings = result.unique().scalars().all()\n        texts_by_model_and_embedding_pooling_method = {(doc.llm_model_name, doc.embedding_pooling_method): [] for doc in document_embeddings}\n        for document_embedding in document_embeddings:\n            texts_by_model_and_embedding_pooling_method[(document_embedding.llm_model_name, document_embedding.embedding_pooling_method)].extend(\n                [embedding.text for embedding in document_embedding.embeddings]\n            )\n    return texts_by_model_and_embedding_pooling_method\n\nasync def get_texts_for_model_and_embedding_pooling_method(llm_model_name: str, embedding_pooling_method: str) -> Dict[str, List[str]]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(DocumentEmbedding)\n            .options(joinedload(DocumentEmbedding.embeddings))\n            .filter(DocumentEmbedding.llm_model_name == llm_model_name, DocumentEmbedding.embedding_pooling_method == embedding_pooling_method)\n        )\n        document_embeddings = result.unique().scalars().all()\n        texts_by_model_and_embedding_pooling_method = {(doc.llm_model_name, doc.embedding_pooling_method): [] for doc in document_embeddings}\n        for document_embedding in document_embeddings:\n            texts_by_model_and_embedding_pooling_method[(document_embedding.llm_model_name, document_embedding.embedding_pooling_method)].extend(\n                [embedding.text for embedding in document_embedding.embeddings]\n            )\n    return texts_by_model_and_embedding_pooling_method\n\nasync def get_or_compute_embedding(request: EmbeddingRequest, req: Request = None, client_ip: str = None, document_file_hash: str = None, use_verbose: bool = True) -> dict:\n    request_time = datetime.utcnow()  # Capture request time as datetime object\n    ip_address = (\n        client_ip or (req.client.host if req else \"localhost\")\n    )  # If client_ip is provided, use it; otherwise, try to get from req; if not available, default to \"localhost\"\n    if use_verbose:\n        logger.info(f\"Received request for embedding for '{request.text}' using model '{request.llm_model_name}' and embedding pooling method '{request.embedding_pooling_method}' from IP address '{ip_address}'\")\n    text_embedding_instance = await get_embedding_from_db(\n        request.text, request.llm_model_name, request.embedding_pooling_method\n    )\n    if text_embedding_instance is not None: # Check if embedding exists in the database\n        response_time = datetime.utcnow()  # Capture response time as datetime object\n        total_time = (\n            response_time - request_time\n        ).total_seconds()  # Calculate time taken in seconds\n        if use_verbose:\n            logger.info(f\"Embedding found in database for '{request.text}' using model '{request.llm_model_name}' and embedding pooling method '{request.embedding_pooling_method}'; returning in {total_time:.4f} seconds\")\n        return {\"text_embedding_dict\": text_embedding_instance.as_dict()}\n    model = load_model(request.llm_model_name)\n    # Compute the embedding if not in the database\n    list_of_embedding_entry_dicts = await calculate_sentence_embeddings_list(model, [request.text], request.embedding_pooling_method)\n    embedding_entry_dict = list_of_embedding_entry_dicts[0]\n    if embedding_entry_dict is None:\n        logger.error(\n            f\"Could not calculate the embedding for the given text: '{request.text}' using model '{request.llm_model_name} and embedding pooling method '{request.embedding_pooling_method}!'\"\n        )\n        raise HTTPException(\n            status_code=400,\n            detail=\"Could not calculate the embedding for the given text\",\n        )\n    else:\n        embedding = embedding_entry_dict['embedding']\n        embedding_hash = embedding_entry_dict['embedding_hash']\n        text = request.text\n        text_hash = sha3_256(text.encode('utf-8')).hexdigest()\n        embedding_json = json.dumps(embedding)\n        request_time = datetime.utcnow()\n        response_time = datetime.utcnow()\n        total_time = (response_time - request_time).total_seconds()\n        embedding_instance = TextEmbedding(\n            text=text,\n            text_hash=text_hash,\n            embedding_hash=embedding_hash,\n            llm_model_name=request.llm_model_name,\n            embedding_pooling_method=request.embedding_pooling_method,\n            corpus_identifier_string=request.corpus_identifier_string,\n            embedding_json=embedding_json,\n            ip_address=client_ip,\n            request_time=request_time,\n            response_time=response_time,\n            total_time=total_time,\n            document_file_hash=document_file_hash,\n        )\n    word_length_of_input_text = len(request.text.split())\n    if word_length_of_input_text > 0:\n        if use_verbose:\n            logger.info(f\"Embedding calculated for '{request.text}' using model '{request.llm_model_name}' and embedding pooling method '{request.embedding_pooling_method}' in {total_time:,.2f} seconds, or an average of {total_time/word_length_of_input_text :.2f} seconds per word. Now saving to database...\")\n    await shared_resources.db_writer.enqueue_write([embedding_instance])  # Enqueue the write operation using the db_writer instance\n    return {\"text_embedding_dict\": embedding_instance.as_dict()}\n\nasync def calculate_sentence_embeddings_list(llama, texts: list, embedding_pooling_method: str) -> list:\n    start_time = datetime.utcnow()\n    total_number_of_sentences = len(texts)\n    total_characters = sum(len(s) for s in texts)\n    sentence_embeddings_object = llama.create_embedding(texts)\n    sentence_embeddings_list = sentence_embeddings_object['data']\n    if len(sentence_embeddings_list) != len(texts):\n        raise ValueError(\"Inconsistent number of embeddings found.\")\n    list_of_embedding_entry_dicts = []\n    cnt = 0\n    for i, current_text in enumerate(texts):\n        current_set_of_embeddings = sentence_embeddings_list[i]['embedding']\n        if isinstance(current_set_of_embeddings[0], list):\n            number_of_embeddings = len(current_set_of_embeddings)\n        else:\n            number_of_embeddings = 1\n            current_set_of_embeddings = [current_set_of_embeddings]\n        logger.info(f\"Sentence {i + 1:,} of {len(texts):,} has {number_of_embeddings:,} embeddings for text '{current_text[:50]}...'\")\n        embeddings = np.array(current_set_of_embeddings)\n        dimension_of_token_embeddings = embeddings.shape[1]\n        # Ensure embeddings have enough dimensions for the pooling method\n        required_components = {\n            \"svd\": 2,\n            \"svd_first_four\": 4,\n            \"ica\": 2,\n            \"factor_analysis\": 2,\n            \"gaussian_random_projection\": 2\n        }\n        if number_of_embeddings > 1:\n            min_components = required_components.get(embedding_pooling_method, 1)\n            if number_of_embeddings < min_components:\n                padding = np.zeros((min_components - number_of_embeddings, dimension_of_token_embeddings))\n                embeddings = np.vstack([embeddings, padding])\n            if embedding_pooling_method == \"mean\":\n                element_wise_mean = np.mean(embeddings, axis=0)\n                flattened_vector = element_wise_mean.flatten()\n            elif embedding_pooling_method == \"mins_maxes\":\n                element_wise_min = np.min(embeddings, axis=0)\n                element_wise_max = np.max(embeddings, axis=0)\n                flattened_vector = np.concatenate([element_wise_min, element_wise_max], axis=0)\n            elif embedding_pooling_method == \"svd\":\n                svd = TruncatedSVD(n_components=2)\n                svd_embeddings = svd.fit_transform(embeddings.T)\n                flattened_vector = svd_embeddings.flatten()\n            elif embedding_pooling_method == \"svd_first_four\":\n                svd = TruncatedSVD(n_components=4)\n                svd_embeddings = svd.fit_transform(embeddings.T)\n                flattened_vector = svd_embeddings.flatten()\n            elif embedding_pooling_method == \"ica\":\n                ica = FastICA(n_components=2)\n                ica_embeddings = ica.fit_transform(embeddings.T)\n                flattened_vector = ica_embeddings.flatten()\n            elif embedding_pooling_method == \"factor_analysis\":\n                fa = FactorAnalysis(n_components=2)\n                fa_embeddings = fa.fit_transform(embeddings.T)\n                flattened_vector = fa_embeddings.flatten()           \n            elif embedding_pooling_method == \"gaussian_random_projection\":\n                grp = GaussianRandomProjection(n_components=2)\n                grp_embeddings = grp.fit_transform(embeddings.T)\n                flattened_vector = grp_embeddings.flatten()                 \n            else:\n                raise ValueError(f\"Unknown embedding_pooling_method: {embedding_pooling_method}\")\n            combined_embedding = flattened_vector.tolist()\n        else:\n            flattened_vector = embeddings.flatten().tolist()\n            combined_embedding = embeddings.flatten().tolist()\n        embedding_length = len(combined_embedding)\n        cnt += 1\n        embedding_json = json.dumps(combined_embedding)\n        embedding_hash = sha3_256(embedding_json.encode('utf-8')).hexdigest()\n        embedding_entry_dict = {'text_index': i, 'text': current_text, 'embedding_pooling_method': embedding_pooling_method, 'number_of_token_embeddings_used': number_of_embeddings, 'embedding_length': embedding_length, 'embedding_hash': embedding_hash, 'embedding': combined_embedding}\n        list_of_embedding_entry_dicts.append(embedding_entry_dict)\n    end_time = datetime.utcnow()\n    total_time = (end_time - start_time).total_seconds()\n    logger.info(f\"Calculated {len(flattened_vector):,}-dimensional embeddings (relative to the underlying token embedding dimensions of {dimension_of_token_embeddings:,}) for {total_number_of_sentences:,} sentences in a total of {total_time:,.1f} seconds.\")\n    logger.info(f\"That's an average of {1000*total_time/total_number_of_sentences:,.2f} ms per sentence and {total_number_of_sentences/total_time:,.3f} sentences per second (and {total_characters/(1000*total_time):,.4f} total characters per ms) using pooling method '{embedding_pooling_method}'\")\n    return list_of_embedding_entry_dicts\n\nasync def batch_save_embeddings_to_db(embeddings: List[TextEmbedding]):\n    async with AsyncSessionLocal() as session:\n        # Extract the unique embedding_hashes from the embeddings list\n        embedding_hashes = [embedding.embedding_hash for embedding in embeddings]\n        # Query the database for existing embeddings with the same hashes\n        existing_embeddings_query = select(TextEmbedding.embedding_hash).where(TextEmbedding.embedding_hash.in_(embedding_hashes))\n        result = await session.execute(existing_embeddings_query)\n        existing_embedding_hashes = {row.embedding_hash for row in result}\n        # Filter out embeddings that already exist in the database\n        embeddings_to_insert = [embedding for embedding in embeddings if embedding.embedding_hash not in existing_embedding_hashes]\n        # Batch insert the remaining embeddings\n        if embeddings_to_insert:\n            session.add_all(embeddings_to_insert)\n            await session.commit()\n            \nasync def compute_embeddings_for_document(sentences: list, llm_model_name: str, embedding_pooling_method: str, corpus_identifier_string: str, client_ip: str, document_file_hash: str, file: UploadFile, original_file_content: bytes, json_format: str = 'records') -> list:\n    request_time = datetime.utcnow()\n    sentences = [prepare_string_for_embedding(text) for text in sentences]\n    model = load_model(llm_model_name)\n    try:\n        list_of_embedding_entry_dicts = await calculate_sentence_embeddings_list(model, sentences, embedding_pooling_method)\n    except Exception as e:\n        logger.error(f\"Error computing embeddings for batch: {e}\")\n        logger.error(traceback.format_exc())\n        raise\n    embeddings_to_save = []\n    list_of_embedding_hashes_added = []\n    for embedding_entry_dict in list_of_embedding_entry_dicts:\n        embedding = embedding_entry_dict['embedding']\n        embedding_hash = embedding_entry_dict['embedding_hash']\n        if embedding_hash in list_of_embedding_hashes_added:\n            continue\n        text_index = embedding_entry_dict['text_index']\n        text = sentences[text_index]\n        text_hash = sha3_256(text.encode('utf-8')).hexdigest()\n        embedding_json = json.dumps(embedding)\n        response_time = datetime.utcnow()\n        total_time = (response_time - request_time).total_seconds()\n        embedding_instance = TextEmbedding(\n            text=text,\n            text_hash=text_hash,\n            embedding_hash=embedding_hash,\n            llm_model_name=llm_model_name,\n            embedding_pooling_method=embedding_pooling_method,\n            corpus_identifier_string=corpus_identifier_string,\n            embedding_json=embedding_json,\n            ip_address=client_ip,\n            request_time=request_time,\n            response_time=response_time,\n            total_time=total_time,\n            document_file_hash=document_file_hash,\n        )\n        embeddings_to_save.append(embedding_instance)\n        list_of_embedding_hashes_added.append(embedding_hash)\n    logger.info(f\"Storing {len(embeddings_to_save):,} text embeddings in database...\")\n    await batch_save_embeddings_to_db(embeddings_to_save)\n    logger.info(f\"Done storing {len(embeddings_to_save):,} text embeddings in database.\")\n    document_embedding_results_df = pd.DataFrame(list_of_embedding_entry_dicts)\n    json_content = document_embedding_results_df.to_json(orient=json_format or 'records').encode()\n    if file is not None:\n        await store_document_embeddings_in_db(\n            file=file,\n            document_file_hash=document_file_hash,\n            original_file_content=original_file_content,\n            sentences=sentences,\n            json_content=json_content,\n            llm_model_name=llm_model_name,\n            embedding_pooling_method=embedding_pooling_method,\n            corpus_identifier_string=corpus_identifier_string,\n            client_ip=client_ip,\n            request_time=request_time,\n        )    \n    return json_content\n\nasync def parse_submitted_document_file_into_sentence_strings_func(temp_file_path: str, mime_type: str):\n    content = \"\"\n    try:\n        content = textract.process(temp_file_path, method='pdfminer', encoding='utf-8')\n        content = content.decode('utf-8')\n    except Exception as e:\n        logger.error(f\"Error while processing file: {e}, mime_type: {mime_type}\")\n        logger.error(traceback.format_exc())\n        raise HTTPException(status_code=400, detail=f\"Unsupported file type or error: {e}\")\n    sentences = sophisticated_sentence_splitter(content)\n    if len(sentences) == 0 and temp_file_path.lower().endswith('.pdf'):\n        logger.info(\"No sentences found, attempting OCR using Tesseract.\")\n        try:\n            content = textract.process(temp_file_path, method='tesseract', encoding='utf-8')\n            content = content.decode('utf-8')\n            sentences = sophisticated_sentence_splitter(content)\n        except Exception as e:\n            logger.error(f\"Error while processing file with OCR: {e}\")\n            logger.error(traceback.format_exc())\n            raise HTTPException(status_code=400, detail=\"OCR failed: {e}\")\n    if len(sentences) == 0:\n        logger.info(\"No sentences found in the document\")\n        raise HTTPException(status_code=400, detail=\"No sentences found in the document\")\n    strings = [s.strip() for s in sentences if len(s.strip()) > MINIMUM_STRING_LENGTH_FOR_DOCUMENT_EMBEDDING]\n    thousands_of_input_words = round(sum(len(s.split()) for s in strings) / 1000, 2)\n    return strings, thousands_of_input_words\n\nasync def _get_document_from_db(document_file_hash: str):\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(select(Document).filter(Document.document_hash == document_file_hash))\n        return result.scalar_one_or_none()\n\nasync def store_document_embeddings_in_db(file, document_file_hash: str, original_file_content: bytes, sentences: List[str], json_content: bytes, llm_model_name: str, embedding_pooling_method:str, corpus_identifier_string: str, client_ip: str, request_time: datetime):\n    if file is None:\n        logger.error(\"Received a None file object in store_document_embeddings_in_db\")\n    else:\n        logger.info(f\"Received file: {file.filename} with content type: {file.content_type}\")\n    sentences = json.dumps(sentences)\n    document = await _get_document_from_db(document_file_hash)\n    if not document:\n        document = Document(document_hash=document_file_hash, llm_model_name=llm_model_name, corpus_identifier_string=corpus_identifier_string)\n        await shared_resources.db_writer.enqueue_write([document])\n    document_embedding_results_json_compressed_binary = compress_data(json_content)\n    document_embedding = DocumentEmbedding(\n        filename=file.filename,\n        mimetype=file.content_type,\n        document_file_hash=document_file_hash,\n        llm_model_name=llm_model_name,\n        embedding_pooling_method=embedding_pooling_method,\n        corpus_identifier_string=corpus_identifier_string,\n        file_data=original_file_content,\n        sentences=sentences,\n        document_embedding_results_json_compressed_binary=document_embedding_results_json_compressed_binary,\n        ip_address=client_ip,\n        request_time=request_time,\n        response_time=datetime.utcnow(),\n        total_time=(datetime.utcnow() - request_time).total_seconds()\n    )\n    document.document_embeddings.append(document_embedding)\n    document.update_hash()\n    await shared_resources.db_writer.enqueue_write([document, document_embedding])\n    \ndef load_text_completion_model(llm_model_name: str, raise_http_exception: bool = True):\n    global USE_VERBOSE\n    try:\n        if llm_model_name in text_completion_model_cache:\n            return text_completion_model_cache[llm_model_name]\n        models_dir = os.path.join(RAMDISK_PATH, 'models') if USE_RAMDISK else os.path.join(BASE_DIRECTORY, 'models')\n        matching_files = glob.glob(os.path.join(models_dir, f\"{llm_model_name}*\"))\n        if not matching_files:\n            logger.error(f\"No model file found matching: {llm_model_name}\")\n            raise FileNotFoundError\n        matching_files.sort(key=os.path.getmtime, reverse=True)\n        model_file_path = matching_files[0]\n        is_llava_multimodal_model = 'llava' in llm_model_name and 'mmproj' not in llm_model_name\n        chat_handler = None # Determine the appropriate chat handler based on the model name\n        if 'llava' in llm_model_name:\n            clip_model_path = find_clip_model_path(llm_model_name)\n            if clip_model_path is None:\n                raise FileNotFoundError\n            chat_handler = Llava16ChatHandler(clip_model_path=clip_model_path)\n        with suppress_stdout_stderr():\n            gpu_info = is_gpu_available()\n            if gpu_info:\n                num_gpus = gpu_info['num_gpus']\n                if num_gpus > 1:\n                    llama_split_mode = 2 # 2, // split rows across GPUs | 1, // split layers and KV across GPUs\n                else:\n                    llama_split_mode = 0\n            else:\n                num_gpus = 0\n            try:                \n                model_instance = Llama(\n                    model_path=model_file_path,\n                    embedding=True if is_llava_multimodal_model else False,\n                    n_ctx=TEXT_COMPLETION_CONTEXT_SIZE_IN_TOKENS,\n                    flash_attn=USE_FLASH_ATTENTION,\n                    verbose=USE_VERBOSE,\n                    llama_split_mode=llama_split_mode,\n                    n_gpu_layers=-1 if gpu_info['gpu_found'] else 0,\n                    clip_model_path=clip_model_path if is_llava_multimodal_model else None,\n                    chat_handler=chat_handler\n                )\n            except Exception as e:  # noqa: F841\n                model_instance = Llama(\n                    model_path=model_file_path,\n                    embedding=True if is_llava_multimodal_model else False,\n                    n_ctx=TEXT_COMPLETION_CONTEXT_SIZE_IN_TOKENS,\n                    flash_attn=USE_FLASH_ATTENTION,\n                    verbose=USE_VERBOSE,\n                    clip_model_path=clip_model_path if is_llava_multimodal_model else None,\n                    chat_handler=chat_handler\n                )                \n        text_completion_model_cache[llm_model_name] = model_instance\n        return model_instance\n    except TypeError as e:\n        logger.error(f\"TypeError occurred while loading the model: {e}\")\n        logger.error(traceback.format_exc())        \n        raise\n    except Exception as e:\n        logger.error(f\"Exception occurred while loading the model: {e}\")\n        logger.error(traceback.format_exc())\n        if raise_http_exception:\n            raise HTTPException(status_code=404, detail=\"Model file not found\")\n        else:\n            raise FileNotFoundError(f\"No model file found matching: {llm_model_name}\")\n        \nasync def generate_completion_from_llm(request: TextCompletionRequest, req: Request = None, client_ip: str = None) -> List[TextCompletionResponse]:\n    request_time = datetime.utcnow()\n    logger.info(f\"Starting text completion calculation using model: '{request.llm_model_name}'for input prompt: '{request.input_prompt}'\")\n    logger.info(f\"Loading model: '{request.llm_model_name}'\")\n    llm = load_text_completion_model(request.llm_model_name)\n    logger.info(f\"Done loading model: '{request.llm_model_name}'\")\n    list_of_llm_outputs = []\n    grammar_file_string_lower = request.grammar_file_string.lower() if request.grammar_file_string else \"\"\n    chat_handler = llm.chat_handler # Use the appropriate chat handler based on the model name\n    if chat_handler is None: # Use the default code path if no chat handler is found\n        for ii in range(request.number_of_completions_to_generate):\n            logger.info(f\"Generating completion {ii+1} of {request.number_of_completions_to_generate} with model {request.llm_model_name} for input prompt: '{request.input_prompt}'\")\n            output = llm(prompt=request.input_prompt, max_tokens=request.number_of_tokens_to_generate, temperature=request.temperature)\n            list_of_llm_outputs.append(output)\n    else:\n        if grammar_file_string_lower:\n            list_of_grammar_files = glob.glob(\"./grammar_files/*.gbnf\")\n            matching_grammar_files = [x for x in list_of_grammar_files if grammar_file_string_lower in os.path.splitext(os.path.basename(x).lower())[0]]\n            if len(matching_grammar_files) == 0:\n                logger.error(f\"No grammar file found matching: {request.grammar_file_string}\")\n                raise FileNotFoundError\n            matching_grammar_files.sort(key=os.path.getmtime, reverse=True)\n            grammar_file_path = matching_grammar_files[0]\n            logger.info(f\"Loading selected grammar file: '{grammar_file_path}'\")\n            llama_grammar = LlamaGrammar.from_file(grammar_file_path)\n            for ii in range(request.number_of_completions_to_generate):\n                logger.info(f\"Generating completion {ii+1} of {request.number_of_completions_to_generate} with model {request.llm_model_name} for input prompt: '{request.input_prompt}'\")\n                output = chat_handler(\n                    llama=llm,\n                    messages=[llama_types.ChatCompletionRequestUserMessage(content=request.input_prompt)],\n                    grammar=llama_grammar,\n                    max_tokens=request.number_of_tokens_to_generate,\n                    temperature=request.temperature,\n                )\n                list_of_llm_outputs.append(output)\n        else:\n            for ii in range(request.number_of_completions_to_generate):\n                logger.info(f\"Generating completion {ii+1} of {request.number_of_completions_to_generate} with model {request.llm_model_name} for input prompt: '{request.input_prompt}'\")\n                output = chat_handler(\n                    llama=llm,\n                    messages=[llama_types.ChatCompletionRequestUserMessage(content=request.input_prompt)],\n                    max_tokens=request.number_of_tokens_to_generate,\n                    temperature=request.temperature,\n                )\n                list_of_llm_outputs.append(output)\n    response_time = datetime.utcnow()\n    total_time_per_completion = ((response_time - request_time).total_seconds()) / request.number_of_completions_to_generate\n    list_of_responses = []\n    for idx, current_completion_output in enumerate(list_of_llm_outputs):\n        model_output = current_completion_output['choices'][0]\n        if 'message' in model_output.keys():            \n            generated_text = model_output['message']['content']\n        else:\n            generated_text = model_output['text']\n        if request.grammar_file_string == 'json':\n            generated_text = generated_text.encode('unicode_escape').decode()\n        finish_reason = str(model_output['finish_reason'])                \n        llm_model_usage_json = json.dumps(current_completion_output['usage'])\n        logger.info(f\"Completed text completion {idx:,} in an average of {total_time_per_completion:,.2f} seconds for input prompt: '{request.input_prompt}'; Beginning of generated text: \\n'{generated_text[:100]}'...\")\n        response = TextCompletionResponse(input_prompt = request.input_prompt,\n                                            llm_model_name = request.llm_model_name,\n                                            grammar_file_string = request.grammar_file_string,\n                                            number_of_tokens_to_generate = request.number_of_tokens_to_generate,\n                                            number_of_completions_to_generate = request.number_of_completions_to_generate,\n                                            time_taken_in_seconds = float(total_time_per_completion),\n                                            generated_text = generated_text,\n                                            finish_reason = finish_reason,\n                                            llm_model_usage_json = llm_model_usage_json)\n        list_of_responses.append(response)\n    return list_of_responses\n\nasync def ask_question_about_image(\n    question: str,\n    llm_model_name: str,\n    temperature: float,\n    number_of_tokens_to_generate: int,\n    number_of_completions_to_generate: int,\n    image: UploadFile,\n    req: Request = None,\n    client_ip: str = None\n) -> List[ImageQuestionResponse]:\n    if 'llava' not in llm_model_name:\n        logger.error(f\"Model '{llm_model_name}' is not a valid LLaVA model.\")\n        raise HTTPException(status_code=400, detail=\"Model name must include 'llava'\")\n    request_time = datetime.utcnow()\n    logger.info(f\"Starting image question calculation using model: '{llm_model_name}' for question: '{question}'\")\n    logger.info(f\"Loading model: '{llm_model_name}'\")\n    llm = load_text_completion_model(llm_model_name)\n    logger.info(f\"Done loading model: '{llm_model_name}'\")\n    original_image_path = f\"/tmp/{image.filename}\"\n    with open(original_image_path, \"wb\") as image_file:\n        image_file.write(await image.read())\n    processed_image_path = process_image(original_image_path)\n    image_hash = sha3_256(open(processed_image_path, 'rb').read()).hexdigest()\n    data_uri = image_to_base64_data_uri(processed_image_path)\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri }},\n            {\"type\": \"text\", \"text\": question}\n        ]},\n    ]\n    responses = []\n    for completion_count in range(number_of_completions_to_generate):\n        with suppress_stdout_stderr():\n            llm_output = llm.create_chat_completion(\n                messages=messages,\n                max_tokens=number_of_tokens_to_generate,\n                temperature=temperature,\n                top_p=0.95,\n                stream=False,\n            )\n        response_time = datetime.utcnow()\n        total_time_taken = (response_time - request_time).total_seconds()\n        model_output = llm_output['choices'][0]\n        generated_text = model_output['message']['content']      \n        finish_reason = str(model_output['finish_reason'])\n        llm_model_usage_json = json.dumps(llm_output['usage'])\n        response = ImageQuestionResponse(\n            question=question,\n            llm_model_name=llm_model_name,\n            image_hash=image_hash,\n            time_taken_in_seconds=total_time_taken,\n            number_of_tokens_to_generate=number_of_tokens_to_generate,\n            number_of_completions_to_generate=number_of_completions_to_generate,\n            generated_text=generated_text,\n            finish_reason=finish_reason,\n            llm_model_usage_json=llm_model_usage_json\n        )\n        logger.info(f\"Completed image question calculation in {total_time_taken:.2f} seconds for question: '{question}'; Beginning of generated text: \\n'{generated_text[:100]}'...\")\n        responses.append(response)\n    return responses\n\ndef validate_bnf_grammar_func(grammar: str):\n    defined_rules, used_rules = set(), set()\n    for line in grammar.strip().split('\\n'):\n        if '::=' not in line: \n            continue\n        parts = line.split('::=')\n        rule = parts[0].strip()\n        if rule in defined_rules:\n            return False, f\"Rule {rule} is defined more than once.\"\n        defined_rules.add(rule)\n        expression = parts[-1]\n        # Tokenize the expression using regex\n        tokens = re.findall(r'\\b[\\w-]+\\b|\\[.*?\\]|\\(.*?\\)|\".*?\"', expression)\n        # Additional handling for complex expressions\n        complex_tokens = re.findall(r'[\\w-]+\\[[\\w-]+\\]', expression)\n        tokens.extend(complex_tokens)\n        for token in tokens:\n            if token.startswith('[') or token.startswith('(') or token.startswith('\"'):\n                continue  # Skip character classes, optional constructs, and string literals\n            if '[' in token and ']' in token:  # Split complex tokens into individual rules\n                sub_parts = token.split('[')\n                used_rules.add(sub_parts[0])\n                used_rules.add(sub_parts[1][:-1])\n                continue\n            used_rules.add(token)\n    for rule in used_rules:\n        if rule not in defined_rules:\n            return False, f\"Used rule {rule} is not defined.\"\n    return True, \"Valid BNF Grammar\"\n\nasync def convert_document_to_sentences_func(file_path: str, mime_type: str) -> Dict[str, Any]:\n    sentences, thousands_of_input_words = await parse_submitted_document_file_into_sentence_strings_func(file_path, mime_type)\n    total_number_of_sentences = len(sentences)\n    total_input_file_size_in_bytes = os.path.getsize(file_path)\n    total_text_size_in_characters = sum(len(sentence) for sentence in sentences)\n    total_words = sum(len(sentence.split()) for sentence in sentences)\n    average_words_per_sentence = total_words / total_number_of_sentences if total_number_of_sentences else 0\n    result = {\n        \"individual_sentences\": sentences,\n        \"total_number_of_sentences\": total_number_of_sentences,\n        \"average_words_per_sentence\": average_words_per_sentence,\n        \"total_input_file_size_in_bytes\": total_input_file_size_in_bytes,\n        \"total_text_size_in_characters\": total_text_size_in_characters,\n        \"thousands_of_input_words\": thousands_of_input_words\n    }\n    return result\n\nasync def download_file(url: str, expected_size: int, expected_hash: str) -> str:\n    temp_file = tempfile.NamedTemporaryFile(delete=False)\n    temp_file_path = temp_file.name\n    hash_obj = sha3_256()\n    downloaded_size = 0\n    async with httpx.AsyncClient() as client:\n        async with client.stream(\"GET\", url) as response:\n            if response.status_code != 200:\n                raise HTTPException(status_code=400, detail=\"Failed to download file\")\n            async for chunk in response.aiter_bytes():\n                downloaded_size += len(chunk)\n                if downloaded_size > expected_size:\n                    os.remove(temp_file_path)\n                    raise HTTPException(status_code=400, detail=\"Downloaded file size exceeds expected size\")\n                temp_file.write(chunk)\n                hash_obj.update(chunk)\n    temp_file.close()\n    if downloaded_size != expected_size:\n        os.remove(temp_file_path)\n        raise HTTPException(status_code=400, detail=\"Downloaded file size does not match expected size\")\n    if hash_obj.hexdigest() != expected_hash:\n        os.remove(temp_file_path)\n        raise HTTPException(status_code=400, detail=\"File hash mismatch\")\n    return temp_file_path\n\n# Audio Transcript functions start here:\n\ndef object_as_dict(obj):\n    return {c.key: getattr(obj, c.key) for c in inspect(obj).mapper.column_attrs}\n\ndef convert_to_pydantic_response(audio_transcript, compute_embeddings_for_resulting_transcript_document, llm_model_name, embedding_pooling_method, download_url):\n    audio_transcript_dict = object_as_dict(audio_transcript)\n    # Convert JSON fields from strings to proper lists/dictionaries using json.loads\n    audio_transcript_dict['segments_json'] = json.loads(audio_transcript_dict['segments_json'])\n    audio_transcript_dict['combined_transcript_text_list_of_metadata_dicts'] = json.loads(audio_transcript_dict['combined_transcript_text_list_of_metadata_dicts'])\n    # Ensure info_json is a dictionary\n    info_json = json.loads(audio_transcript_dict['info_json'])\n    if isinstance(info_json, list):\n        # Convert list to dictionary if necessary\n        info_json = {str(i): info_json[i] for i in range(len(info_json))}\n    audio_transcript_dict['info_json'] = info_json\n    # Update fields based on the request\n    audio_transcript_dict['url_to_download_zip_file_of_embeddings'] = download_url\n    if compute_embeddings_for_resulting_transcript_document:\n        audio_transcript_dict['llm_model_name'] = llm_model_name\n        audio_transcript_dict['embedding_pooling_method'] = embedding_pooling_method\n    else:\n        audio_transcript_dict['llm_model_name'] = \"\"\n        audio_transcript_dict['embedding_pooling_method'] = \"\"\n    return audio_transcript_dict\n\ndef generate_download_url(audio_file_name: str, req: Request) -> str:\n    sanitized_file_name = clean_filename_for_url_func(audio_file_name)\n    document_name = f\"automatic_whisper_transcript_of__{sanitized_file_name}\"\n    download_url = f\"download/{quote(document_name)}.zip\"\n    return f\"{req.base_url}{download_url}\"\n\nasync def get_transcript_from_db(audio_file_hash: str) -> Optional[AudioTranscript]:\n    return await execute_with_retry(_get_transcript_from_db, audio_file_hash)\n\nasync def _get_transcript_from_db(audio_file_hash: str) -> Optional[AudioTranscript]:\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(\n            select(AudioTranscript).filter(AudioTranscript.audio_file_hash == audio_file_hash)\n        )\n        transcript = result.scalars().first()\n        return transcript\n\nasync def save_transcript_to_db(audio_file_hash: str, audio_file_name: str, audio_file_size_mb: float, transcript_segments: json.dumps, info: json.dumps, ip_address: str, request_time: datetime, response_time: datetime, total_time: float, combined_transcript_text: str, combined_transcript_text_list_of_metadata_dicts: json.dumps, corpus_identifier_string: str):\n    audio_transcript = AudioTranscript(\n        audio_file_hash=audio_file_hash,\n        audio_file_name=audio_file_name,\n        audio_file_size_mb=audio_file_size_mb,\n        segments_json=json.dumps(transcript_segments),\n        combined_transcript_text=combined_transcript_text,\n        combined_transcript_text_list_of_metadata_dicts=json.dumps(combined_transcript_text_list_of_metadata_dicts),\n        info_json=json.dumps(info),\n        ip_address=ip_address,\n        request_time=request_time,\n        response_time=response_time,\n        total_time=total_time,\n        corpus_identifier_string=corpus_identifier_string\n    )\n    await shared_resources.db_writer.enqueue_write([audio_transcript])\n\nasync def compute_and_store_transcript_embeddings(audio_file_name: str, sentences: list, llm_model_name: str, embedding_pooling_method: str, corpus_identifier_string: str, ip_address: str, combined_transcript_text: str, req: Request):\n    request_time=datetime.utcnow()\n    logger.info(f\"Now computing embeddings for entire transcript of {audio_file_name}...\")\n    zip_dir = 'generated_transcript_embeddings_zip_files'\n    if not os.path.exists(zip_dir):\n        os.makedirs(zip_dir)\n    sanitized_file_name = clean_filename_for_url_func(audio_file_name)\n    document_name = f\"automatic_whisper_transcript_of__{sanitized_file_name}\"\n    document_file_hash = sha3_256(combined_transcript_text.encode('utf-8')).hexdigest()\n    sentences = sophisticated_sentence_splitter(combined_transcript_text)\n    computed_embeddings = await compute_embeddings_for_document(\n        sentences=sentences,\n        llm_model_name=llm_model_name,\n        embedding_pooling_method=embedding_pooling_method,\n        corpus_identifier_string=corpus_identifier_string,\n        client_ip=ip_address,\n        document_file_hash=document_file_hash,\n        file=None,\n        original_file_content=combined_transcript_text.encode(),\n        json_format=\"records\",\n    )\n    zip_file_path = f\"{zip_dir}/{quote(document_name)}.zip\"\n    # Ensure computed_embeddings is JSON serializable\n    if isinstance(computed_embeddings, bytes):\n        computed_embeddings = computed_embeddings.decode('utf-8')    \n    zip_file_path = f\"{zip_dir}/{quote(document_name)}.zip\"\n    with zipfile.ZipFile(zip_file_path, 'w') as zipf:\n        zipf.writestr(\"embeddings.txt\", json.dumps(computed_embeddings))\n    download_url = f\"download/{quote(document_name)}.zip\"\n    full_download_url = f\"{req.base_url}{download_url}\"\n    logger.info(f\"Generated download URL for transcript embeddings: {full_download_url}\")\n    fake_upload_file = FakeUploadFile(filename=document_name, content=combined_transcript_text.encode(), content_type='text/plain')\n    logger.info(f\"Storing transcript embeddings for {audio_file_name} in the database...\")\n    await store_document_embeddings_in_db(\n        file=fake_upload_file,\n        document_file_hash=document_file_hash,\n        original_file_content=combined_transcript_text.encode('utf-8'),\n        sentences=sentences,\n        json_content=json.dumps(computed_embeddings).encode('utf-8'),\n        llm_model_name=llm_model_name,\n        embedding_pooling_method=embedding_pooling_method,\n        corpus_identifier_string=corpus_identifier_string,\n        client_ip=ip_address,\n        request_time=request_time,\n    )\n    return full_download_url\n\nasync def compute_transcript_with_whisper_from_audio_func(audio_file_hash, audio_file_path, audio_file_name, audio_file_size_mb, ip_address, req: Request, corpus_identifier_string: str, embedding_pooling_method: str, compute_embeddings_for_resulting_transcript_document=True, llm_model_name=DEFAULT_MODEL_NAME):\n    model_size = \"large-v3\"\n    logger.info(f\"Loading Whisper model {model_size}...\")\n    num_workers = 1 if psutil.virtual_memory().total < 32 * (1024 ** 3) else min(4, max(1, int((psutil.virtual_memory().total - 32 * (1024 ** 3)) / (4 * (1024 ** 3))))) # Only use more than 1 worker if there is at least 32GB of RAM; then use 1 worker per additional 4GB of RAM up to 4 workers max\n    with suppress_stdout_stderr():\n        gpu_info = is_gpu_available()\n        if gpu_info['gpu_found']:\n            model = await run_in_threadpool(WhisperModel, model_size, device=\"cuda\", compute_type=\"auto\")\n        else:\n            model = await run_in_threadpool(WhisperModel, model_size, device=\"cpu\", compute_type=\"auto\", cpu_threads=os.cpu_count(), num_workers=num_workers)\n    request_time = datetime.utcnow()\n    logger.info(f\"Computing transcript for {audio_file_name} which has a {audio_file_size_mb :.2f}MB file size...\")\n    segments, info = await run_in_threadpool(model.transcribe, audio_file_path, beam_size=20)\n    if not segments:\n        logger.warning(f\"No segments were returned for file {audio_file_name}.\")\n        return [], {}, \"\", [], request_time, datetime.utcnow(), 0, \"\"    \n    segment_details = []\n    for idx, segment in enumerate(segments):\n        details = {\n            \"start\": round(segment.start, 2),\n            \"end\": round(segment.end, 2),\n            \"text\": segment.text,\n            \"avg_logprob\": round(segment.avg_logprob, 2)\n        }\n        logger.info(f\"Details of transcript segment {idx:,} from file {audio_file_name}: {details}\")\n        segment_details.append(details)\n    combined_transcript_text, combined_transcript_text_list_of_metadata_dicts, list_of_transcript_sentences = merge_transcript_segments_into_combined_text(segment_details)    \n    if compute_embeddings_for_resulting_transcript_document:\n        download_url = await compute_and_store_transcript_embeddings(\n            audio_file_name=audio_file_name,\n            sentences=list_of_transcript_sentences,\n            llm_model_name=llm_model_name,\n            embedding_pooling_method=embedding_pooling_method,\n            corpus_identifier_string=corpus_identifier_string,\n            ip_address=ip_address,\n            combined_transcript_text=combined_transcript_text,\n            req=req,\n        )\n    else:\n        download_url = ''\n    response_time = datetime.utcnow()\n    total_time = (response_time - request_time).total_seconds()\n    logger.info(f\"Transcript computed in {total_time:,.2f} seconds.\")\n    await save_transcript_to_db(\n        audio_file_hash=audio_file_hash,\n        audio_file_name=audio_file_name,\n        audio_file_size_mb=audio_file_size_mb,\n        transcript_segments=segment_details,\n        info=info,\n        ip_address=ip_address,\n        request_time=request_time,\n        response_time=response_time,\n        total_time=total_time,\n        combined_transcript_text=combined_transcript_text,\n        combined_transcript_text_list_of_metadata_dicts=combined_transcript_text_list_of_metadata_dicts,\n        corpus_identifier_string=corpus_identifier_string\n    )\n    info_dict = info._asdict()\n    return segment_details, info_dict, combined_transcript_text, combined_transcript_text_list_of_metadata_dicts, request_time, response_time, total_time, download_url\n    \nasync def get_or_compute_transcript(file: UploadFile,\n                                    compute_embeddings_for_resulting_transcript_document: bool,\n                                    llm_model_name: str,\n                                    embedding_pooling_method: str,\n                                    corpus_identifier_string: str,\n                                    req: Request = None\n                                    ) -> AudioTranscriptResponse:\n    request_time = datetime.utcnow()\n    ip_address = req.client.host if req else \"127.0.0.1\"\n    file_contents = await file.read()\n    audio_file_hash = sha3_256(file_contents).hexdigest()\n    file.file.seek(0)  # Reset file pointer after read\n    unique_id = f\"transcript_{audio_file_hash}_{llm_model_name}_{embedding_pooling_method}\"\n    lock = await shared_resources.lock_manager.lock(unique_id)\n    if lock.valid:\n        try:\n            existing_audio_transcript = await get_transcript_from_db(audio_file_hash)\n            if existing_audio_transcript:\n                # Generate the download URL based on the existing audio transcript data\n                download_url = generate_download_url(existing_audio_transcript.audio_file_name, req)\n                existing_audio_transcript_dict = convert_to_pydantic_response(\n                    existing_audio_transcript, \n                    compute_embeddings_for_resulting_transcript_document, \n                    llm_model_name, \n                    embedding_pooling_method,\n                    download_url\n                )\n                return AudioTranscriptResponse(**existing_audio_transcript_dict)\n            current_position = file.file.tell()\n            file.file.seek(0, os.SEEK_END)\n            audio_file_size_mb = file.file.tell() / (1024 * 1024)\n            file.file.seek(current_position)\n            \n            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n                shutil.copyfileobj(file.file, tmp_file)\n                audio_file_name = tmp_file.name\n            if corpus_identifier_string == \"\":\n                corpus_identifier_string = audio_file_hash\n            (\n                segment_details,\n                info,\n                combined_transcript_text,\n                combined_transcript_text_list_of_metadata_dicts,\n                request_time,\n                response_time,\n                total_time,\n                download_url,\n            ) = await compute_transcript_with_whisper_from_audio_func(\n                audio_file_hash=audio_file_hash,\n                audio_file_path=audio_file_name,\n                audio_file_name=file.filename,\n                audio_file_size_mb=audio_file_size_mb,\n                ip_address=ip_address,\n                req=req,\n                corpus_identifier_string=corpus_identifier_string,\n                embedding_pooling_method=embedding_pooling_method,\n                compute_embeddings_for_resulting_transcript_document=compute_embeddings_for_resulting_transcript_document,\n                llm_model_name=llm_model_name,\n            )\n            audio_transcript_response = {\n                \"audio_file_hash\": audio_file_hash,\n                \"audio_file_name\": file.filename,\n                \"audio_file_size_mb\": audio_file_size_mb,\n                \"segments_json\": segment_details,\n                \"combined_transcript_text\": combined_transcript_text,\n                \"combined_transcript_text_list_of_metadata_dicts\": combined_transcript_text_list_of_metadata_dicts,\n                \"info_json\": info,\n                \"ip_address\": ip_address,\n                \"request_time\": request_time,\n                \"response_time\": response_time,\n                \"total_time\": total_time,\n                \"url_to_download_zip_file_of_embeddings\": download_url if compute_embeddings_for_resulting_transcript_document else \"\",\n                \"llm_model_name\": llm_model_name if compute_embeddings_for_resulting_transcript_document else \"\",\n                \"embedding_pooling_method\": embedding_pooling_method if compute_embeddings_for_resulting_transcript_document else \"\",\n                \"corpus_identifier_string\": corpus_identifier_string if compute_embeddings_for_resulting_transcript_document else \"\",\n            }\n            try:\n                os.remove(audio_file_name)\n            except Exception as e:  # noqa: F841\n                pass\n            return AudioTranscriptResponse(**audio_transcript_response)\n        finally:\n            await shared_resources.lock_manager.unlock(lock)\n    else:\n        return {\"status\": \"already processing\"}\n    \ndef get_audio_duration_seconds(audio_input) -> float:\n    if isinstance(audio_input, bytes):\n        audio_file = io.BytesIO(audio_input)\n        audio = MutagenFile(audio_file)\n    elif isinstance(audio_input, str):\n        audio = MutagenFile(audio_input)\n    else:\n        raise ValueError(\"audio_input must be either bytes or a file path string.\")\n    if audio is None or not hasattr(audio.info, 'length'):\n        raise ValueError(\"Could not determine the length of the audio file.\")\n    return audio.info.length\n\ndef start_resource_monitoring(endpoint_name: str, input_data: Dict[str, Any], client_ip: str) -> Dict[str, Any]:\n    if not USE_RESOURCE_MONITORING:\n        return {}\n    initial_memory = psutil.virtual_memory().used\n    initial_cpu_times = psutil.cpu_times_percent(interval=None)\n    start_time = time.time()\n    request_details = {}\n    if endpoint_name == \"get_embedding_vector_for_string\":\n        text = input_data.get(\"text\", \"\")\n        request_details = {\n            \"num_words\": len(text.split()),\n            \"num_characters\": len(text)\n        }\n    elif endpoint_name == \"get_all_embedding_vectors_for_document\":\n        sentences = input_data.get(\"sentences\", [])\n        file_size_mb = input_data.get(\"file_size_mb\", 0)\n        mime_type = input_data.get(\"mime_type\", \"\")\n        request_details = {\n            \"num_sentences\": len(sentences),\n            \"total_words\": sum(len(sentence.split()) for sentence in sentences),\n            \"total_characters\": sum(len(sentence) for sentence in sentences),\n            \"file_size_mb\": file_size_mb,\n            \"mime_type\": mime_type\n        }\n    elif endpoint_name == \"compute_transcript_with_whisper_from_audio\":\n        transcript_details = input_data.get(\"transcript_details\", {})\n        file_size_mb = input_data.get(\"file_size_mb\", 0)\n        audio_duration_seconds = input_data.get(\"audio_duration_seconds\", 0)\n        request_details = {\n            \"file_size_mb\": file_size_mb,\n            \"audio_duration_seconds\": audio_duration_seconds,\n            \"num_sentences\": len(transcript_details.get(\"sentences\", [])),\n            \"total_words\": sum(len(sentence.split()) for sentence in transcript_details.get(\"sentences\", [])),\n            \"total_characters\": sum(len(sentence) for sentence in transcript_details.get(\"sentences\", []))\n        }\n    elif endpoint_name == \"get_text_completions_from_input_prompt\":\n        input_prompt = input_data.get(\"input_prompt\", \"\")\n        request_details = {\n            \"num_words\": len(input_prompt.split()),\n            \"num_characters\": len(input_prompt),\n            \"llm_model_name\": input_data.get(\"llm_model_name\", \"\"),\n            \"temperature\": input_data.get(\"temperature\", 0.7),\n            \"grammar_file_string\": input_data.get(\"grammar_file_string\", \"\"),\n            \"number_of_completions_to_generate\": input_data.get(\"number_of_completions_to_generate\", 1),\n            \"number_of_tokens_to_generate\": input_data.get(\"number_of_tokens_to_generate\", 1000)\n        }\n    elif endpoint_name == \"ask_question_about_image\":\n        question = input_data.get(\"question\", \"\")\n        request_details = {\n            \"question\": question,\n            \"num_words_in_question\": len(question.split()),\n            \"num_characters_in_question\": len(question),\n            \"llm_model_name\": input_data.get(\"llm_model_name\", \"\"),\n            \"temperature\": input_data.get(\"temperature\", 0.7),\n            \"number_of_tokens_to_generate\": input_data.get(\"number_of_tokens_to_generate\", 256),\n            \"number_of_completions_to_generate\": input_data.get(\"number_of_completions_to_generate\", 1),\n            \"image_filename\": input_data.get(\"image\").filename if input_data.get(\"image\") else \"\"\n        }\n    elif endpoint_name == \"advanced_search_stored_embeddings_with_query_string_for_semantic_similarity\":\n        request_details = {\n            \"query_text\": input_data.get(\"query_text\", \"\"),\n            \"llm_model_name\": input_data.get(\"llm_model_name\", \"\"),\n            \"embedding_pooling_method\": input_data.get(\"embedding_pooling_method\", \"\"),\n            \"corpus_identifier_string\": input_data.get(\"corpus_identifier_string\", \"\"),\n            \"similarity_filter_percentage\": input_data.get(\"similarity_filter_percentage\", 0.02),\n            \"number_of_most_similar_strings_to_return\": input_data.get(\"number_of_most_similar_strings_to_return\", 10),\n            \"result_sorting_metric\": input_data.get(\"result_sorting_metric\", \"hoeffding_d\")\n        }        \n    context = {\n        \"endpoint_name\": endpoint_name,\n        \"start_time\": start_time,\n        \"initial_memory\": initial_memory,\n        \"initial_cpu_times\": initial_cpu_times,\n        \"request_details\": request_details,\n        \"client_ip\": client_ip,\n        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(start_time))\n    }\n    return context\n\ndef end_resource_monitoring(context: Dict[str, Any]):\n    if not USE_RESOURCE_MONITORING or not context:\n        return\n    # Retrieve initial state from context\n    endpoint_name = context[\"endpoint_name\"]\n    start_time = context[\"start_time\"]\n    initial_memory = context[\"initial_memory\"]\n    initial_cpu_times = context[\"initial_cpu_times\"]\n    request_details = context[\"request_details\"]\n    client_ip = context[\"client_ip\"]\n    timestamp = context[\"timestamp\"]\n    # Capture final system resource usage\n    end_time = time.time()\n    final_memory = psutil.virtual_memory().used\n    final_cpu_times = psutil.cpu_times_percent(interval=None)\n    # Calculate the metrics\n    memory_used = final_memory - initial_memory\n    cpu_used = {\n        \"user\": final_cpu_times.user - initial_cpu_times.user,\n        \"system\": final_cpu_times.system - initial_cpu_times.system,\n        \"idle\": final_cpu_times.idle - initial_cpu_times.idle\n    }\n    time_taken = end_time - start_time\n    # Combine all metrics into a result dictionary\n    result = {\n        \"timestamp\": timestamp,\n        \"client_ip\": client_ip,\n        \"endpoint_name\": endpoint_name,\n        \"request_details\": request_details,\n        \"memory_used\": memory_used,\n        \"cpu_used\": cpu_used,\n        \"time_taken\": time_taken\n    }\n    # Append the result to the log file\n    log_file_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"resource_monitoring_logs.json\")\n    try:\n        with open(log_file_path, \"a\") as log_file:\n            log_file.write(json.dumps(result) + \"\\n\")\n    except Exception as e:\n        logger.error(f\"Failed to write resource monitoring log: {e}\")\n        logger.error(traceback.format_exc())\n    logger.info(f\"Request data and system resources used: {result}\")"}
{"type": "source_file", "path": "misc_utility_functions.py", "content": "from logger_config import setup_logger\nfrom embeddings_data_models import TextEmbedding\nimport socket\nimport os\nimport re\nimport json\nimport io\nimport glob\nimport redis\nimport sys\nimport threading\nimport numpy as np\nimport faiss\nimport base64\nfrom typing import Optional\nfrom pathlib import Path\nfrom typing import Any\nfrom database_functions import AsyncSessionLocal\nfrom sqlalchemy import select\nfrom collections import defaultdict\nfrom PIL import Image\nfrom decouple import config\n\nlogger = setup_logger()\nUSE_RAMDISK = config(\"USE_RAMDISK\", default=False, cast=bool)\nRAMDISK_PATH = config(\"RAMDISK_PATH\", default=\"/mnt/ramdisk\", cast=str)\nBASE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\n\nclass suppress_stdout_stderr(object):\n    def __enter__(self):\n        self.outnull_file = open(os.devnull, 'w')\n        self.errnull_file = open(os.devnull, 'w')\n        self.old_stdout_fileno_undup    = sys.stdout.fileno()\n        self.old_stderr_fileno_undup    = sys.stderr.fileno()\n        self.old_stdout_fileno = os.dup ( sys.stdout.fileno() )\n        self.old_stderr_fileno = os.dup ( sys.stderr.fileno() )\n        self.old_stdout = sys.stdout\n        self.old_stderr = sys.stderr\n        os.dup2 ( self.outnull_file.fileno(), self.old_stdout_fileno_undup )\n        os.dup2 ( self.errnull_file.fileno(), self.old_stderr_fileno_undup )\n        sys.stdout = self.outnull_file        \n        sys.stderr = self.errnull_file\n        return self\n\n    def __exit__(self, *_):        \n        sys.stdout = self.old_stdout\n        sys.stderr = self.old_stderr\n        os.dup2 ( self.old_stdout_fileno, self.old_stdout_fileno_undup )\n        os.dup2 ( self.old_stderr_fileno, self.old_stderr_fileno_undup )\n        os.close ( self.old_stdout_fileno )\n        os.close ( self.old_stderr_fileno )\n        self.outnull_file.close()\n        self.errnull_file.close()\n    \ndef safe_path(base_path, file_name):\n    abs_base_path = os.path.abspath(base_path)\n    abs_user_path = os.path.abspath(os.path.join(base_path, file_name))\n    return abs_user_path.startswith(abs_base_path), abs_user_path\n\ndef clean_filename_for_url_func(dirty_filename: str) -> str:\n    clean_filename = re.sub(r'[^\\w\\s]', '', dirty_filename) # Remove special characters and replace spaces with underscores\n    clean_filename = clean_filename.replace(' ', '_')\n    return clean_filename\n\ndef is_redis_running(host='localhost', port=6379):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((host, port))\n        return True\n    except ConnectionRefusedError:\n        return False\n    finally:\n        s.close()\n        \ndef start_redis_server():\n    try:\n        result = os.system(\"sudo service redis-server start\")\n        if result == 0:\n            print(\"Redis server started successfully.\")\n        else:\n            logger.error(f\"Failed to start Redis server, return code: {result}\")\n            raise Exception(\"Failed to start Redis server.\")\n    except Exception as e:\n        logger.error(f\"Failed to start Redis server: {e}\")\n        raise\n\ndef restart_redis_server():\n    try:\n        result = os.system(\"sudo service redis-server stop\")\n        if result != 0:\n            logger.warning(f\"Failed to stop Redis server, it might not be running. Return code: {result}\")\n        result = os.system(\"sudo service redis-server start\")\n        if result == 0:\n            print(\"Redis server started successfully.\")\n        else:\n            logger.error(f\"Failed to start Redis server, return code: {result}\")\n            raise Exception(\"Failed to start Redis server.\")\n    except Exception as e:\n        logger.error(f\"Failed to restart Redis server: {e}\")\n        raise\n\ndef configure_redis_optimally(redis_host='localhost', redis_port=6379, maxmemory='1gb'):\n    configured_file = 'redis_configured.txt'\n    if os.path.exists(configured_file):\n        print(\"Redis has already been configured. Skipping configuration.\")\n        return\n    if not is_redis_running(redis_host, redis_port):\n        start_redis_server()\n    r = redis.StrictRedis(host=redis_host, port=redis_port, decode_responses=True)\n    output = []\n    def set_config(key, value):\n        try:\n            response = r.config_set(key, value)\n            msg = f\"Successfully set {key} to {value}\" if response else f\"Failed to set {key} to {value}\"\n            output.append(msg)\n            print(msg)\n        except redis.exceptions.ConnectionError as e:\n            logger.error(f\"Failed to set config {key}: {e}\")\n            raise\n    set_config('maxmemory', maxmemory)\n    set_config('maxmemory-policy', 'allkeys-lru')\n    max_clients = min(os.cpu_count() * 1000, 50000)\n    set_config('maxclients', max_clients)\n    set_config('timeout', 300)\n    set_config('save', '900 1 300 10 60 10000')\n    set_config('appendonly', 'yes')\n    set_config('appendfsync', 'everysec')\n    set_config('stop-writes-on-bgsave-error', 'no')\n    output.append(\"Redis configuration optimized successfully.\")\n    output.append(\"Restarting Redis server to apply changes...\")\n    with open(configured_file, 'w') as f:\n        f.write(\"\\n\".join(output))\n    print(\"\\n\".join(output))\n    restart_redis_server()\n    \ndef configure_redis_in_background():\n    threading.Thread(target=configure_redis_optimally).start()\n    \nasync def build_faiss_indexes(force_rebuild=False):\n    global faiss_indexes, associated_texts_by_model_and_pooling_method\n    if os.environ.get(\"FAISS_SETUP_DONE\") == \"1\" and not force_rebuild:\n        return faiss_indexes, associated_texts_by_model_and_pooling_method\n    faiss_indexes = {}\n    associated_texts_by_model_and_pooling_method = defaultdict(lambda: defaultdict(list))  # Create a nested dictionary to store associated texts by model name and pooling method\n    async with AsyncSessionLocal() as session:\n        result = await session.execute(select(TextEmbedding.llm_model_name, TextEmbedding.text, TextEmbedding.embedding_json, TextEmbedding.embedding_pooling_method))\n        embeddings_by_model_and_pooling = defaultdict(lambda: defaultdict(list))\n        for row in result.fetchall():  # Process regular embeddings\n            llm_model_name = row[0]\n            embedding_pooling_method = row[3]\n            associated_texts_by_model_and_pooling_method[llm_model_name][embedding_pooling_method].append(row[1])  # Store the associated text by model name and pooling method\n            embeddings_by_model_and_pooling[llm_model_name][embedding_pooling_method].append((row[1], json.loads(row[2])))\n        for llm_model_name, embeddings_by_pooling in embeddings_by_model_and_pooling.items():\n            for embedding_pooling_method, embeddings in embeddings_by_pooling.items():\n                logger.info(f\"Building Faiss index over embeddings for model {llm_model_name} with pooling method {embedding_pooling_method}...\")\n                embeddings_array = np.array([e[1] for e in embeddings]).astype('float32')\n                if embeddings_array.size == 0:\n                    logger.error(f\"No embeddings were loaded from the database for model {llm_model_name} with pooling method {embedding_pooling_method}, so nothing to build the Faiss index with!\")\n                    continue\n                faiss.normalize_L2(embeddings_array)  # Normalize the vectors for cosine similarity\n                faiss_index = faiss.IndexFlatIP(embeddings_array.shape[1])  # Use IndexFlatIP for cosine similarity\n                faiss_index.add(embeddings_array)\n                faiss_indexes[(llm_model_name, embedding_pooling_method)] = faiss_index  # Store the index by model name and pooling method\n    os.environ[\"FAISS_SETUP_DONE\"] = \"1\"\n    return faiss_indexes, associated_texts_by_model_and_pooling_method\n\ndef normalize_logprobs(avg_logprob, min_logprob, max_logprob):\n    range_logprob = max_logprob - min_logprob\n    return (avg_logprob - min_logprob) / range_logprob if range_logprob != 0 else 0.5\n\ndef truncate_string(s: str, max_length: int = 100) -> str:\n    return s[:max_length]\n\ndef remove_pagination_breaks(text: str) -> str:\n    text = re.sub(r'-(\\n)(?=[a-z])', '', text) # Remove hyphens at the end of lines when the word continues on the next line\n    text = re.sub(r'(?<=\\w)(?<![.?!-]|\\d)\\n(?![\\nA-Z])', ' ', text) # Replace line breaks that are not preceded by punctuation or list markers and not followed by an uppercase letter or another line break   \n    return text\n\ndef sophisticated_sentence_splitter(text):\n    text = remove_pagination_breaks(text)\n    pattern = r'\\.(?!\\s*(com|net|org|io)\\s)(?![0-9])'  # Split on periods that are not followed by a space and a top-level domain or a number\n    pattern += r'|[.!?]\\s+'  # Split on whitespace that follows a period, question mark, or exclamation point\n    pattern += r'|\\.\\.\\.(?=\\s)'  # Split on ellipses that are followed by a space\n    sentences = re.split(pattern, text)\n    refined_sentences = []\n    temp_sentence = \"\"\n    for sentence in sentences:\n        if sentence is not None:\n            temp_sentence += sentence\n            if temp_sentence.count('\"') % 2 == 0:  # If the number of quotes is even, then we have a complete sentence\n                refined_sentences.append(temp_sentence.strip())\n                temp_sentence = \"\"\n    if temp_sentence:\n        refined_sentences[-1] += temp_sentence\n    return [s.strip() for s in refined_sentences if s.strip()]\n\ndef merge_transcript_segments_into_combined_text(segments):\n    if not segments:\n        return \"\", [], []\n    min_logprob = min(segment['avg_logprob'] for segment in segments)\n    max_logprob = max(segment['avg_logprob'] for segment in segments)\n    combined_text = \"\"\n    sentence_buffer = \"\"\n    list_of_metadata_dicts = []\n    list_of_sentences = []\n    char_count = 0\n    time_start = None\n    time_end = None\n    total_logprob = 0.0\n    segment_count = 0\n    for segment in segments:\n        if time_start is None:\n            time_start = segment['start']\n        time_end = segment['end']\n        total_logprob += segment['avg_logprob']\n        segment_count += 1\n        sentence_buffer += segment['text'] + \" \"\n        sentences = sophisticated_sentence_splitter(sentence_buffer)\n        for sentence in sentences:\n            combined_text += sentence.strip() + \" \"\n            list_of_sentences.append(sentence.strip())\n            char_count += len(sentence.strip()) + 1  # +1 for the space\n            avg_logprob = total_logprob / segment_count\n            model_confidence_score = normalize_logprobs(avg_logprob, min_logprob, max_logprob)\n            metadata = {\n                'start_char_count': char_count - len(sentence.strip()) - 1,\n                'end_char_count': char_count - 2,\n                'time_start': time_start,\n                'time_end': time_end,\n                'model_confidence_score': model_confidence_score\n            }\n            list_of_metadata_dicts.append(metadata)\n        sentence_buffer = sentences[-1] if len(sentences) % 2 != 0 else \"\"\n    return combined_text, list_of_metadata_dicts, list_of_sentences\n    \nclass JSONAggregator:\n    def __init__(self):\n        self.completions = []\n        self.aggregate_result = None\n\n    @staticmethod\n    def weighted_vote(values, weights):\n        tally = defaultdict(float)\n        for v, w in zip(values, weights):\n            tally[v] += w\n        return max(tally, key=tally.get)\n\n    @staticmethod\n    def flatten_json(json_obj, parent_key='', sep='->'):\n        items = {}\n        for k, v in json_obj.items():\n            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n            if isinstance(v, dict):\n                items.update(JSONAggregator.flatten_json(v, new_key, sep=sep))\n            else:\n                items[new_key] = v\n        return items\n\n    @staticmethod\n    def get_value_by_path(json_obj, path, sep='->'):\n        keys = path.split(sep)\n        item = json_obj\n        for k in keys:\n            item = item[k]\n        return item\n\n    @staticmethod\n    def set_value_by_path(json_obj, path, value, sep='->'):\n        keys = path.split(sep)\n        item = json_obj\n        for k in keys[:-1]:\n            item = item.setdefault(k, {})\n        item[keys[-1]] = value\n\n    def calculate_path_weights(self):\n        all_paths = []\n        for j in self.completions:\n            all_paths += list(self.flatten_json(j).keys())\n        path_weights = defaultdict(float)\n        for path in all_paths:\n            path_weights[path] += 1.0\n        return path_weights\n\n    def aggregate(self):\n        path_weights = self.calculate_path_weights()\n        aggregate = {}\n        for path, weight in path_weights.items():\n            values = [self.get_value_by_path(j, path) for j in self.completions if path in self.flatten_json(j)]\n            weights = [weight] * len(values)\n            aggregate_value = self.weighted_vote(values, weights)\n            self.set_value_by_path(aggregate, path, aggregate_value)\n        self.aggregate_result = aggregate\n\nclass FakeUploadFile:\n    def __init__(self, filename: str, content: Any, content_type: str = 'text/plain'):\n        self.filename = filename\n        self.content_type = content_type\n        self.file = io.BytesIO(content)\n    def read(self, size: int = -1) -> bytes:\n        return self.file.read(size)\n    def seek(self, offset: int, whence: int = 0) -> int:\n        return self.file.seek(offset, whence)\n    def tell(self) -> int:\n        return self.file.tell()\n    \ndef process_image(image_path, max_dimension=1024):\n    original_path = Path(image_path)\n    processed_image_path = original_path.with_stem(original_path.stem + \"_processed\").with_suffix(original_path.suffix)\n    with Image.open(image_path) as img:\n        img.thumbnail((max_dimension, max_dimension), Image.LANCZOS)\n        img.save(processed_image_path)\n    return processed_image_path\n\ndef alpha_remover_func(img):\n    if img.mode != 'RGBA':\n        return img\n    canvas = Image.new('RGBA', img.size, (255, 255, 255, 255))\n    canvas.paste(img, mask=img)\n    return canvas.convert('RGB')\n\ndef image_to_base64_data_uri(file_path):\n    with open(file_path, \"rb\") as img_file:\n        base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n        return f\"data:image/png;base64,{base64_data}\"    \n    \ndef find_clip_model_path(llm_model_name: str) -> Optional[str]:\n    models_dir = os.path.join(RAMDISK_PATH, 'models') if USE_RAMDISK else os.path.join(BASE_DIRECTORY, 'models')\n    base_name = os.path.splitext(os.path.basename(llm_model_name))[0]\n    mmproj_model_name = base_name.replace(\"-f16\", \"-mmproj-f16\").replace(\"-int4\", \"-mmproj-f16\")\n    mmproj_files = glob.glob(os.path.join(models_dir, f\"{mmproj_model_name}.gguf\"))\n    if not mmproj_files:\n        logger.error(f\"No mmproj file found matching: {mmproj_model_name}\")\n        return None\n    return mmproj_files[0]    \n"}
{"type": "source_file", "path": "sentiment_score_generation.py", "content": "from swiss_army_llama import configured_logger as logger\nimport asyncio\nimport psutil\nimport glob\nimport os\nimport sys\nimport re\nimport json\nimport traceback\nimport numpy as np\nimport pandas as pd\nfrom filelock import FileLock\nfrom datetime import datetime\nfrom concurrent.futures import ProcessPoolExecutor\nfrom functools import partial\nfrom llama_cpp import Llama\nBASE_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\n\n\nfinancial_investor_focus_presets = \"\"\"\n    Optimistic: Reflecting a positive outlook or confidence in future success.\n    Pessimistic: Expressing a lack of hope or a negative view of future prospects.\n    Confident: Demonstrating self-assurance and belief in strategies or decisions.\n    Uncertain: Highlighting doubt or lack of clarity regarding future events.\n    Positive: Conveying a favorable or encouraging sentiment.\n    Negative: Indicating an unfavorable or discouraging sentiment.\n    Cautious: Showing carefulness, often related to risks or potential challenges.\n    Aggressive: Reflecting a forceful or assertive approach, often in strategy.\n    Satisfied: Expressing contentment or approval with current conditions.\n    Disappointed: Showing dissatisfaction or displeasure with outcomes.\n    Enthusiastic: Conveying strong excitement or eagerness about something.\n    Worried: Reflecting concern or anxiety about potential problems.\n    Transparent: Indicating openness and clarity in communication.\n    Ambiguous: Conveying uncertainty or vagueness, often leading to confusion.\n    Stable: Reflecting steadiness, consistency, or lack of volatility.\n    Volatile: Indicating instability, unpredictability, or rapid changes.\n    Committed: Demonstrating dedication or strong allegiance to a cause or plan.\n    Indifferent: Showing lack of interest, concern, or importance.\n    Receptive: Indicating willingness to consider new ideas or feedback.\n    Defensive: Reflecting a protective stance, often in response to criticism or challenges.\n\"\"\"  \n\npublic_company_earnings_call_text = \"\"\"\n    Optimistic: Expressing confidence in future growth, profitability, or success.\n    Pessimistic: Showing concern or lack of confidence in future prospects.\n    Confident: Demonstrating strong belief in strategies, products, or market position.\n    Cautious: Reflecting careful consideration, especially regarding risks or challenges.\n    Transparent: Providing clear and open information, often related to finances or strategies.\n    Vague: Lacking clarity or specific details, possibly to avoid commitment.\n    Upbeat: Conveying a positive and enthusiastic tone about performance or opportunities.\n    Disappointed: Expressing dissatisfaction with results, outcomes, or specific areas.\n    Reassuring: Offering assurances or comfort regarding concerns or uncertainties.\n    Evasive: Avoiding direct answers or clear statements on sensitive topics.\n    Committed: Demonstrating dedication to initiatives, strategies, or goals.\n    Analytical: Providing a detailed and thoughtful analysis of performance and trends.\n    Ambitious: Reflecting strong desire for growth, innovation, or market leadership.\n    Concerned: Showing worry or apprehension, often related to external factors or challenges.\n    Focused: Emphasizing specific priorities, goals, or areas of attention.\n    Uncertain: Highlighting ambiguity or lack of clarity on specific matters or outlook.\n    Responsive: Addressing questions or concerns promptly and thoroughly.\n    Defensive: Protecting or justifying decisions, results, or strategies.\n    Strategic: Emphasizing long-term planning, direction, and competitive positioning.\n    Realistic: Providing a grounded and practical perspective on performance and expectations.\n\"\"\"\n\ncustomer_feedback_focus_presets = \"\"\"\n    Satisfied: Expressing contentment or approval with the product or service.\n    Dissatisfied: Showing dissatisfaction or displeasure with the product or service.\n    Impressed: Reflecting a strong positive impression or admiration.\n    Frustrated: Indicating irritation or annoyance, often due to unmet expectations.\n    Appreciative: Conveying gratitude or appreciation for a particular aspect.\n    Confused: Reflecting uncertainty or confusion about a feature or experience.\n    Delighted: Showing great pleasure, satisfaction, or happiness.\n    Disappointed: Expressing discontentment or letdown with the overall experience.\n    Concerned: Reflecting worry or concern about a specific issue or experience.\n    Excited: Conveying anticipation or eagerness about a new feature or update.\n    Annoyed: Indicating minor irritation or displeasure with an aspect of the service.\n    Surprised: Reflecting an unexpected experience, either positive or negative.\n    Comfortable: Indicating ease or comfort with using the product or service.\n    Inconvenienced: Reflecting a sense of hassle or difficulty in interaction.\n    Trusting: Demonstrating confidence or trust in the brand or product.\n    Skeptical: Showing doubt or skepticism towards claims or promises.\n    Loyal: Reflecting a strong allegiance or loyalty to the brand or product.\n    Indifferent: Conveying a lack of strong feelings one way or another.\n    Curious: Indicating an interest or curiosity about future developments.\n    Unsupported: Feeling a lack of support or assistance when needed.\n\"\"\"\n\ncustomer_service_interaction_focus_presets = \"\"\"\n    Empathetic: Showing understanding and compassion for the customer's situation.\n    Unsympathetic: Lacking concern or understanding for the customer's needs.\n    Helpful: Providing valuable assistance, guidance, or solutions.\n    Unhelpful: Failing to provide meaningful support or resolution.\n    Patient: Taking time to listen and understand the customer's concerns.\n    Impatient: Showing frustration or haste, possibly dismissing the customer's needs.\n    Clear: Communicating information in an easy-to-understand and straightforward manner.\n    Confusing: Providing information that may lead to misunderstandings or confusion.\n    Responsive: Reacting promptly and appropriately to questions or concerns.\n    Slow: Delaying responses or taking excessive time to resolve issues.\n    Friendly: Expressing warmth, politeness, and a welcoming tone.\n    Cold: Reflecting a lack of warmth, possibly perceived as indifferent or distant.\n    Professional: Maintaining a respectful and business-like demeanor.\n    Informal: Using casual language or tone, possibly perceived as unprofessional.\n    Reassuring: Offering comfort or confidence that issues will be resolved.\n    Dismissive: Indicating a lack of interest or disregarding the customer's concerns.\n    Appreciative: Expressing gratitude for the customer's patience or feedback.\n    Apologetic: Offering apologies for mistakes or inconveniences.\n    Inquisitive: Asking questions to clarify or understand the customer's needs.\n    Uninterested: Showing lack of engagement or interest in assisting the customer.\n\"\"\"\n\nmarketing_campaign_focus_presets = \"\"\"\n    Persuasive: Effectively convincing or influencing the audience.\n    Engaging: Capturing interest and encouraging interaction or participation.\n    Innovative: Introducing new ideas or creative approaches.\n    ClichÃ©d: Relying on overused or predictable concepts and phrases.\n    Inspirational: Motivating or uplifting, often evoking emotional responses.\n    Confusing: Lacking clarity or causing misunderstandings in messaging.\n    Aggressive: Using forceful or assertive tactics to promote a product or idea.\n    Subtle: Communicating messages in a less obvious or understated manner.\n    Authentic: Conveying a genuine or sincere brand image or message.\n    Misleading: Providing information that may deceive or create false impressions.\n    Inclusive: Emphasizing diversity and broad appeal to various audiences.\n    Exclusive: Targeting a specific or niche audience, possibly excluding others.\n    Visually_Stunning: Utilizing impressive visual design or imagery.\n    Monotonous: Lacking variety or excitement in content or presentation.\n    Responsive: Adapting to audience feedback or market trends.\n    Rigid: Sticking strictly to a specific concept or style, lacking flexibility.\n    Emotional: Evoking strong emotional reactions or connections.\n    Factual: Relying on facts, data, or evidence in messaging.\n    Trendy: Aligning with current trends or popular culture.\n    Timeless: Conveying messages that are enduring and not tied to temporary trends.\n\"\"\"\n\nproduct_review_focus_presets = \"\"\"\n    Satisfied: Expressing contentment or approval with the product's quality or performance.\n    Dissatisfied: Showing dissatisfaction or displeasure with the product's attributes.\n    Impressed: Reflecting a positive impression or admiration for the product.\n    Disappointed: Expressing discontentment or letdown with the overall experience.\n    Excited: Conveying enthusiasm or eagerness about the product's features or design.\n    Frustrated: Indicating irritation or annoyance with the product's functionality or support.\n    Valuable: Perceiving the product as worth the price or offering good value.\n    Overpriced: Reflecting the belief that the product is too expensive for what it offers.\n    Innovative: Recognizing the product as new, creative, or cutting-edge.\n    Outdated: Viewing the product as old-fashioned or behind current trends.\n    User-friendly: Finding the product easy to use or intuitive.\n    Complicated: Experiencing difficulty or confusion in using the product.\n    Reliable: Trusting the product to perform consistently and without issues.\n    Unreliable: Encountering inconsistencies or problems in the product's performance.\n    Attractive: Appreciating the product's appearance or aesthetic design.\n    Unappealing: Finding the product's appearance or design unattractive or displeasing.\n    Essential: Viewing the product as a must-have or vital to specific needs.\n    Redundant: Perceiving the product as unnecessary or superfluous.\n    High-Quality: Recognizing the product's superior craftsmanship or materials.\n    Low-Quality: Identifying flaws or shortcomings in the product's build or design.\n\"\"\"\n\nemail_correspondence_focus_presets = \"\"\"\n    Formal: Adhering to conventional or traditional standards in language and tone.\n    Informal: Using a casual or conversational tone.\n    Courteous: Expressing politeness, respect, or consideration.\n    Abrupt: Conveying a sudden or brusque tone, possibly perceived as rude.\n    Clear: Providing information that is easy to understand and unambiguous.\n    Confusing: Containing elements that may lead to misunderstanding or confusion.\n    Persuasive: Intending to convince or influence the recipient.\n    Indifferent: Lacking interest or concern in the subject matter.\n    Appreciative: Expressing gratitude or thanks.\n    Apologetic: Conveying regret or offering an apology for something.\n    Enthusiastic: Showing excitement or strong interest in the topic.\n    Disinterested: Reflecting a lack of personal interest or bias.\n    Urgent: Conveying a sense of immediacy or time-sensitivity.\n    Nonchalant: Reflecting a lack of concern or appearing casually unconcerned.\n    Encouraging: Offering support or motivation.\n    Critical: Expressing criticism or pointing out faults.\n    Responsive: Providing a timely and relevant reply.\n    Unresponsive: Lacking a reply or failing to address the subject.\n    Inquisitive: Asking questions or seeking more information.\n    Directive: Providing clear instructions or directions.\n\"\"\"\n\ngithub_issues_focus_presets = \"\"\"\n    Constructive: Providing useful feedback or contributing positively to the discussion.\n    Critical: Pointing out problems or issues, possibly in a negative tone.\n    Appreciative: Expressing gratitude or appreciation for work done.\n    Frustrated: Indicating irritation or annoyance with a bug or lack of progress.\n    Urgent: Conveying a sense of immediacy or importance in resolving an issue.\n    Confused: Reflecting uncertainty or confusion about code, features, or requirements.\n    Collaborative: Showing a willingness to work together or assist others.\n    Dismissive: Indicating a lack of interest or disregard for the issue or comment.\n    Informative: Providing detailed information, context, or guidance.\n    Vague: Lacking clarity or specific details, possibly leading to misunderstandings.\n    Supportive: Offering assistance, encouragement, or backing for an idea or request.\n    Argumentative: Engaging in disagreement or debate, possibly in a confrontational manner.\n    Responsive: Reacting promptly and appropriately to questions or concerns.\n    Unresponsive: Failing to provide timely or relevant responses.\n    Encouraging: Motivating or inspiring further work or discussion.\n    Inquisitive: Seeking more information, asking clarifying questions.\n    Detailed: Providing a thorough and comprehensive explanation or description.\n    Concise: Communicating information in a clear and brief manner.\n    Receptive: Open to feedback, suggestions, or new ideas.\n    Defensive: Reacting protectively, possibly in response to criticism or feedback.\n\"\"\"\n\nsocial_media_sentiment_focus_presets = \"\"\"\n    Positive: Expressing favorable opinions or support.\n    Negative: Conveying criticism or opposition.\n    Neutral: Lacking strong emotion or bias; impartial.\n    Engaging: Capturing interest; encouraging interaction or response.\n    Polarizing: Provoking strong disagreement or contrasting opinions.\n    Inspirational: Providing motivation or encouragement.\n    Sarcastic: Conveying irony or mocking insincerity.\n    Humorous: Intending to entertain or amuse.\n    Controversial: Addressing subjects likely to arouse strong opinions or debate.\n    Supportive: Offering sympathy, encouragement, or assistance.\n    Hostile: Expressing aggression, antagonism, or conflict.\n    Curious: Seeking information or expressing genuine interest.\n    Informative: Providing useful information or insights.\n    Emotional: Reflecting strong emotions or feelings.\n    Detached: Lacking emotional involvement or personal bias.\n    Celebratory: Expressing joy, congratulations, or festivity.\n    Mourning: Expressing grief, sorrow, or remembrance.\n    Respectful: Showing consideration and regard for others.\n    Disrespectful: Lacking courtesy; rude or offensive.\n    Trendy: Reflecting current trends, fads, or popular culture.\n\"\"\"\n\nemployee_feedback_focus_presets = \"\"\"\n    Positive: Conveying encouragement, praise, or satisfaction with performance.\n    Negative: Indicating dissatisfaction, criticism, or areas for improvement.\n    Constructive: Providing actionable insights and suggestions for growth.\n    Supportive: Showing empathy, understanding, and encouragement.\n    Disengaged: Reflecting a lack of interest or connection with the employee's situation.\n    Appreciative: Expressing gratitude or recognition for efforts and achievements.\n    Concerned: Reflecting worry or unease about performance or behavior.\n    Fair: Providing balanced and unbiased feedback.\n    Biased: Showing favoritism or prejudice, affecting the objectivity of feedback.\n    Motivational: Inspiring or encouraging further growth and development.\n    Demotivational: Discouraging or undermining confidence and enthusiasm.\n    Specific: Providing clear and detailed insights or directions.\n    Vague: Lacking clarity or specificity, making it hard to act upon.\n    Respectful: Treating the employee with dignity and consideration.\n    Disrespectful: Conveying a lack of respect or courtesy.\n    Empathetic: Showing understanding and compassion for the employee's feelings or situation.\n    Impersonal: Lacking warmth or personal connection, possibly feeling mechanical.\n    Engaging: Encouraging dialogue, questions, and active participation.\n    Confident: Expressing strong belief in decisions, assessments, or directions.\n    Uncertain: Lacking conviction or clarity in the feedback provided.\n\"\"\"\n\ncrisis_communication_focus_presets = \"\"\"\n    Urgent: Conveying a sense of immediate need or priority.\n    Reassuring: Providing comfort or confidence in the face of uncertainty.\n    Clear: Communicating information in an unambiguous and straightforward manner.\n    Confusing: Lacking clarity or leading to misunderstandings.\n    Calm: Reflecting a composed or steady demeanor.\n    Panicked: Showing signs of fear or anxiety, possibly leading to hasty decisions.\n    Empathetic: Demonstrating understanding and compassion for those affected.\n    Detached: Lacking emotion or personal connection, possibly perceived as cold.\n    Informative: Providing essential details, updates, or guidance.\n    Vague: Being unclear or lacking specific information, causing uncertainty.\n    Responsive: Reacting promptly and appropriately to evolving situations.\n    Slow: Delaying responses or updates, possibly leading to frustration.\n    Honest: Conveying truthful and transparent information.\n    Evasive: Avoiding direct answers or withholding information.\n    Authoritative: Speaking with command and expertise, instilling trust.\n    Hesitant: Showing uncertainty or reluctance in communication.\n    Encouraging: Inspiring hope or confidence during a challenging time.\n    Alarming: Causing unnecessary fear or concern, possibly due to exaggerated language.\n    Collaborative: Working together with others, coordinating efforts.\n    Defensive: Reacting protectively or defensively, often in response to criticism.\n\"\"\"\n\npolitical_speech_focus_presets = \"\"\"\n    Inspirational: Providing motivation and encouragement, often appealing to shared values or goals.\n    Divisive: Creating or emphasizing division or disagreement among different groups.\n    Diplomatic: Expressing ideas in a sensitive, tactful, and balanced manner.\n    Aggressive: Using forceful or confrontational language to make a point.\n    Patriotic: Expressing love, support, or devotion to one's country.\n    Critical: Pointing out problems or criticizing opponents or policies.\n    Visionary: Outlining a future goal or plan in an imaginative or idealistic way.\n    Defensive: Responding to criticism or defending a position or policy.\n    Conciliatory: Seeking to reconcile differences and bring about harmony or agreement.\n    Evocative: Eliciting strong emotions or memories, often through vivid language.\n    Rational: Based on logic and reason, often using facts and evidence to support arguments.\n    Emotional: Appealing to the emotions, possibly using anecdotes or passionate language.\n    Authoritative: Speaking with confidence and authority, often citing expertise or credentials.\n    Populist: Appealing to the concerns and interests of ordinary people.\n    Evasive: Avoiding direct answers or clear statements on specific issues.\n    Respectful: Showing regard and consideration for others, even when disagreeing.\n    Condescending: Talking down to the audience or treating them as inferior or less knowledgeable.\n    Unifying: Seeking to bring people together, emphasizing common goals or values.\n    Provocative: Intended to provoke thought or reaction, possibly through controversial statements.\n    Compassionate: Expressing sympathy or concern for the needs and suffering of others.\n\"\"\"\n\nhealthcare_patient_feedback_focus_presets = \"\"\"\n    Satisfied: Expressing contentment or approval with the care received.\n    Dissatisfied: Showing dissatisfaction or displeasure with healthcare services.\n    Grateful: Conveying gratitude or appreciation for medical assistance.\n    Anxious: Reflecting anxiety or concern about a diagnosis, treatment, or recovery.\n    Confident: Demonstrating trust or confidence in healthcare providers.\n    Uncertain: Expressing doubt or confusion about medical information or decisions.\n    Compassionate: Reflecting empathy, warmth, or understanding from healthcare providers.\n    Neglected: Feeling overlooked or lacking attention from medical staff.\n    Informed: Feeling well-informed and educated about medical conditions and treatments.\n    Misinformed: Receiving incorrect or misleading information.\n    Comfortable: Indicating physical or emotional comfort during medical care.\n    Uncomfortable: Reflecting physical discomfort or unease with medical procedures.\n    Hopeful: Conveying optimism or hope regarding recovery or treatment outcomes.\n    Desperate: Expressing a strong need or desire for immediate help or relief.\n    Supported: Feeling supported or cared for by healthcare providers.\n    Unsupported: Lacking support or empathy from medical staff.\n    Empowered: Feeling control or influence over medical decisions and care.\n    Vulnerable: Feeling exposed or at risk, possibly due to health conditions or treatments.\n    Appreciative: Expressing thanks or positive recognition for care provided.\n    Disheartened: Feeling discouraged or dispirited about medical outcomes or experiences.\n\"\"\"\n\neducational_content_focus_presets = \"\"\"\n    Informative: Providing valuable information or insights.\n    Confusing: Lacking clarity or causing misunderstandings.\n    Engaging: Capturing interest and encouraging active participation.\n    Dull: Lacking excitement or interest; monotonous.\n    Challenging: Presenting difficulties or stimulating intellectual effort.\n    Simplistic: Overly simple or lacking depth.\n    Comprehensive: Covering a subject thoroughly or in detail.\n    Superficial: Lacking depth or detailed coverage of a topic.\n    Relevant: Directly related to the subject or current trends.\n    Outdated: Containing information that is no longer current or applicable.\n    Practical: Providing hands-on experience or applicable knowledge.\n    Theoretical: Focusing on theories or abstract concepts without practical application.\n    Inspiring: Encouraging creativity, motivation, or a positive attitude toward learning.\n    Discouraging: Dampening interest or enthusiasm in the subject.\n    Balanced: Presenting multiple perspectives or a fair representation of a subject.\n    Biased: Showing prejudice or favoritism toward a particular view or group.\n    Interactive: Encouraging active participation or engagement with the content.\n    Passive: Requiring little or no interaction or engagement.\n    Structured: Organized and logically arranged for ease of understanding.\n    Disorganized: Lacking clear structure or logical sequence.\n\"\"\"\n\njob_interview_focus_presets = \"\"\"\n    Professional: Adhering to formal language, attire, and conduct.\n    Casual: Adopting a relaxed and informal approach or tone.\n    Inquisitive: Asking detailed and probing questions to understand the candidate's experience and thinking.\n    Superficial: Sticking to surface-level questions without digging into depth or details.\n    Technical: Focusing on specific skills, tools, or methodologies relevant to the job.\n    Behavioral: Concentrating on attitudes, ethics, and interpersonal skills.\n    Motivational: Exploring the candidate's passions, goals, and what drives them.\n    Structured: Following a set pattern or sequence of questions and activities.\n    Unstructured: Lacking a clear pattern, possibly leading to a more organic and free-flowing conversation.\n    Friendly: Creating a warm and welcoming atmosphere to make the candidate comfortable.\n    Intimidating: Taking a more aggressive or challenging approach that might test the candidate's ability to handle pressure.\n    Balanced: Providing a fair mix of technical, behavioral, and motivational questions.\n    Biased: Showing favoritism or prejudice towards certain backgrounds, experiences, or views.\n    Transparent: Clearly communicating the interview process, expectations, and any feedback.\n    Confidential: Emphasizing the privacy of the candidate's responses and information.\n    Encouraging: Building confidence and enthusiasm during the interview process.\n    Discouraging: Creating a negative or disheartening atmosphere, possibly through overly critical or harsh questioning.\n    Adaptive: Tailoring questions and approach based on the candidate's responses and unique background.\n    Rigid: Sticking strictly to predetermined questions without flexibility or adaptation to the candidate's specific situation.\n    Holistic: Considering all aspects of the candidate, including skills, personality, values, and fit with the organization's culture.\n    Narrow: Focusing on one or a few specific areas, potentially overlooking a broader view of the candidate's suitability.\n\"\"\"\n\nsales_call_focus_presets = \"\"\"\n    Persuasive: Utilizing convincing language and arguments to promote the product or service.\n    Informative: Focusing on providing detailed information about the product or service.\n    Aggressive: Employing a forceful and assertive approach to secure a sale.\n    Consultative: Engaging in a dialogue to understand needs and offering tailored solutions.\n    Scripted: Following a predetermined script or set of talking points.\n    Spontaneous: Adapting and reacting to the prospect's responses without strict adherence to a script.\n    Friendly: Creating a warm and approachable atmosphere to build rapport.\n    Formal: Maintaining a professional tone and language aligned with corporate standards.\n    Solution-Oriented: Concentrating on how the product or service solves specific problems or fulfills needs.\n    Feature-Focused: Highlighting the unique features and specifications of the product or service.\n    Customer-Centric: Tailoring the conversation around the customer's specific needs and interests.\n    Product-Centric: Centering the conversation mainly on the product or service being offered, possibly overlooking customer needs.\n    Transparent: Clearly communicating pricing, terms, and any relevant details without hidden agendas.\n    Evasive: Avoiding direct answers or details, possibly creating confusion or mistrust.\n    Collaborative: Working with the prospect to find the best solution, encouraging their input and feedback.\n    Competitive: Emphasizing why the product or service is superior to competitors.\n    Empathetic: Showing understanding and compassion for the prospect's situation or needs.\n    Time-Sensitive: Creating urgency through limited-time offers or emphasizing immediate needs.\n    Patient: Taking the time to listen, explain, and guide without rushing the prospect.\n    Closing-Focused: Prioritizing closing the deal, possibly at the expense of building a relationship or understanding needs.\n    Relationship-Building: Emphasizing long-term relationships over immediate sales, investing time in understanding and connecting with the prospect.\n\"\"\"\n\nreal_estate_listing_focus_presets = \"\"\"\n    Luxurious: Highlighting high-end finishes, amenities, and exclusive features.\n    Affordable: Emphasizing the value or competitive pricing of the property.\n    Family-Friendly: Targeting families by showcasing features like spacious rooms, yards, and proximity to schools.\n    Modern: Focusing on contemporary design, technology, and aesthetics.\n    Vintage: Celebrating historical or traditional architectural features and charm.\n    Eco-Friendly: Promoting energy efficiency, sustainability, or environmentally friendly aspects.\n    Investment-Oriented: Tailoring the listing to appeal to investors, emphasizing rental income or appreciation potential.\n    Location-Centric: Concentrating on the location's benefits, such as convenience, views, or prestige.\n    Community-Focused: Highlighting community amenities, neighbors, or local culture and vibe.\n    Practical: Providing clear and concise information about the property without embellishments.\n    Visual: Relying heavily on high-quality images, videos, or virtual tours to showcase the property.\n    Detailed: Offering an exhaustive description, including every feature, measurement, and specification.\n    Minimalist: Using a brief and to-the-point description, relying on key selling points.\n    Pet-Friendly: Emphasizing features that cater to pet owners, such as a fenced yard or nearby parks.\n    Accessible: Highlighting accessibility features for individuals with disabilities or mobility challenges.\n    Renovation Potential: Pointing out opportunities for improvements or customization.\n    Move-In Ready: Emphasizing that the property is in turnkey condition without need for immediate repairs or updates.\n    Resort-Style: Showcasing amenities and features that create a vacation-like living experience.\n    Urban: Focusing on the benefits of city living, such as proximity to public transport, dining, and entertainment.\n    Rural: Emphasizing the tranquility, space, and natural surroundings typical of countryside properties.\n    Transparent: Clearly stating all costs, potential issues, or other critical details that buyers should be aware of.\n\"\"\"\n\nmental_health_counseling_focus_presets = \"\"\"\n    Empathetic: Demonstrating understanding and compassion for the client's feelings.\n    Directive: Providing clear guidance or instructions to help the client make changes.\n    Non-Directive: Allowing the client to lead the direction of the therapy.\n    Supportive: Offering encouragement and reassurance as the client explores feelings.\n    Challenging: Encouraging the client to confront difficult emotions or behaviors.\n    Holistic: Considering all aspects of the client's life, including physical, mental, and social factors.\n    Solution-Focused: Concentrating on finding immediate and practical solutions to problems.\n    Insight-Oriented: Aiming to increase the client's understanding of themselves and their behaviors.\n    Structured: Following a specific therapeutic model or plan.\n    Flexible: Adapting the approach to meet the unique needs and preferences of the client.\n    Collaborative: Working together with the client to develop goals and strategies.\n    Individualized: Tailoring the approach to the specific characteristics and needs of the client.\n    Crisis-Intervention: Providing immediate support and guidance for acute mental health crises.\n    Long-Term: Focusing on deep and lasting change through extended therapy.\n    Short-Term: Offering brief therapy aimed at addressing specific issues or goals.\n    Integrative: Combining various therapeutic methods or theories.\n    Eclectic: Selecting the best therapy techniques from various approaches, tailored to the client.\n    Culturally-Competent: Recognizing and respecting the client's cultural background and values.\n    Strength-Based: Focusing on the client's strengths and resources rather than weaknesses.\n    Trauma-Informed: Considering the impact of trauma on the client's mental health and well-being.\n    Mindfulness-Based: Encouraging present-moment awareness and acceptance.\n    Family-Focused: Involving family members in therapy to address relational dynamics.\n    Confidential: Ensuring privacy and secrecy of the client's information, in line with ethical guidelines.\n    Evidence-Based: Utilizing methods that have been scientifically proven to be effective.\n\"\"\"\n\nhuman_resources_recruitment_focus_presets = \"\"\"\n    Inclusive: Promoting diversity and equal opportunity for all candidates.\n    Exclusive: Targeting a specific skill set or demographic for specialized roles.\n    Transparent: Providing clear and open information about the recruitment process.\n    Ambiguous: Lacking clarity in job roles, expectations, or recruitment stages.\n    Competitive: Emphasizing a challenging and high-standards selection process.\n    Welcoming: Creating a friendly and approachable recruitment environment.\n    Structured: Having a well-defined and organized recruitment process.\n    Flexible: Allowing for adjustments or accommodations in the recruitment process.\n    Traditional: Following conventional recruitment methods and interviews.\n    Innovative: Utilizing new and creative ways to attract and assess candidates.\n    Candidate-Centric: Focusing on candidate experience and needs.\n    Employer-Centric: Emphasizing the company's needs and expectations.\n    Ethical: Adhering to fair and moral practices in recruitment.\n    Expedient: Prioritizing a quick and efficient recruitment process.\n    Thorough: Ensuring detailed evaluation and consideration of candidates.\n    Collaborative: Encouraging teamwork and internal alignment in hiring decisions.\n    Autonomous: Allowing individual recruiters or managers to make hiring decisions.\n    Local: Targeting recruitment efforts in specific geographical areas.\n    Global: Expanding recruitment outreach to international talent pools.\n    Performance-Based: Assessing candidates primarily on skills and achievements.\n    Potential-Based: Evaluating candidates on future potential and growth capacity.\n    Legal-Compliant: Ensuring adherence to legal regulations and labor laws.\n    Risky: Taking chances on unconventional or unproven recruitment methods.\n    Conservative: Adhering to tried-and-tested recruitment principles.\n    Growth-Oriented: Focusing on candidates who can contribute to company growth.\n    Cultural-Fit: Emphasizing alignment with company culture and values.\n\"\"\"\n\npolitical_campaign_focus_presets = \"\"\"\n    Grassroots: Focusing on community-level engagement and local issues.\n    Populist: Appealing to the concerns and interests of ordinary people.\n    Elitist: Targeting upper-class or highly educated voters with specialized messages.\n    Inclusive: Striving to include all demographics and social groups.\n    Polarizing: Sharpening divisions and emphasizing differences between groups or parties.\n    Unifying: Seeking to bring together disparate groups or interests.\n    Progressive: Emphasizing social reform and forward-thinking policies.\n    Conservative: Focusing on traditional values and resistance to rapid change.\n    Attack-Oriented: Centering on criticism or negative portrayals of opponents.\n    Positive: Highlighting own achievements, plans, and constructive ideas.\n    Issue-Focused: Concentrating on specific political or social issues.\n    Personality-Driven: Relying on the charisma or character of a candidate.\n    Pragmatic: Emphasizing practical solutions and realistic goals.\n    Idealistic: Promoting visionary goals or utopian ideals.\n    Transparent: Providing clear and open disclosure of plans and intentions.\n    Ambiguous: Using vague or unclear language to avoid commitment.\n    Inspirational: Encouraging hope, motivation, or a positive outlook.\n    Fear-Mongering: Using fear or alarmist tactics to gain support.\n    Ethical: Emphasizing integrity, honesty, and moral principles.\n    Opportunistic: Adapting messages to exploit current events or trends.\n    Strategic: Focusing on tactical planning and targeted voter outreach.\n    Tactical: Utilizing specific techniques or maneuvers to gain an advantage.\n    Cooperative: Encouraging collaboration with other parties or groups.\n    Adversarial: Reflecting opposition or conflict with other political entities.\n    Localized: Concentrating on regional or local interests and issues.\n    Globalized: Addressing international relations and global concerns.\n\"\"\"\n\ngovernment_policy_response_focus_presets = \"\"\"\n    Proactive: Taking early action to address potential problems or opportunities.\n    Reactive: Responding to events or situations as they occur.\n    Comprehensive: Addressing all aspects of an issue in a detailed and thorough manner.\n    Incremental: Making gradual changes or adjustments to policy.\n    Radical: Implementing substantial or fundamental changes to existing structures or policies.\n    Conservative: Preserving existing structures and resisting drastic changes.\n    Collaborative: Working with other governments, organizations, or stakeholders.\n    Unilateral: Acting independently without consultation or cooperation with others.\n    Transparent: Providing clear, open disclosure of policy decisions and rationales.\n    Opaque: Keeping decisions and rationales hidden or unclear to the public.\n    Inclusive: Ensuring that policies consider the needs and interests of all demographic groups.\n    Exclusive: Targeting specific groups or interests, potentially at the expense of others.\n    Humanitarian: Focusing on the welfare and rights of individuals.\n    Economic-Centric: Prioritizing economic growth and fiscal responsibility.\n    Environmental: Emphasizing sustainability and protection of natural resources.\n    Security-Focused: Concentrating on national security and defense concerns.\n    Health-Oriented: Giving priority to public health and wellness.\n    Educational: Focusing on educational access, quality, and reform.\n    Innovation-Driven: Encouraging technological advancement and creativity.\n    Regulatory: Implementing rules and restrictions to govern behavior.\n    Deregulatory: Reducing or eliminating governmental rules and oversight.\n    Participatory: Encouraging public involvement in policy development or implementation.\n    Authoritative: Imposing policies without significant public input or consultation.\n    Adaptive: Flexibly adjusting policies in response to changing conditions or feedback.\n    Rigid: Sticking to a predetermined policy course regardless of changing circumstances.\n    Fair: Ensuring equitable treatment and opportunities for all individuals.\n    Biased: Favoring certain groups, interests, or perspectives in policy decisions.\n\"\"\"\n\nretail_customer_feedback_focus_presets = \"\"\"\n    Product_Quality: Commenting on the quality, durability, or functionality of products.\n    Customer_Service: Reflecting on the level of service, friendliness, or responsiveness.\n    Price_Value: Assessing the value for money, affordability, or pricing structure.\n    Shopping_Experience: Describing the overall experience, ambiance, or layout of the store.\n    Convenience: Focusing on the ease of shopping, location, or online accessibility.\n    Delivery: Providing feedback on shipping, packaging, or delivery times.\n    Product_Selection: Evaluating the variety, availability, or range of products offered.\n    Product_Information: Commenting on the clarity, accuracy, or completeness of product descriptions.\n    Responsiveness: Rating the speed and effectiveness of responses to inquiries or issues.\n    Personalization: Assessing the degree of personalized attention or tailored recommendations.\n    Loyalty_Programs: Reflecting on rewards, loyalty schemes, or special offers.\n    Ethical_Concerns: Highlighting sustainability, fair trade, or ethical practices.\n    Accessibility: Focusing on the accessibility for customers with disabilities or special needs.\n    Safety: Commenting on the safety measures, cleanliness, or health protocols.\n    Brand_Image: Reflecting on the perception, reputation, or alignment with brand values.\n    Technology_Use: Evaluating the use of technology, apps, or online platforms.\n    Return_Policy: Assessing the ease, fairness, or flexibility of return or exchange policies.\n    Complaint_Handling: Rating the handling, resolution, or satisfaction of complaints.\n    Marketing_Communication: Commenting on advertising, promotions, or marketing messages.\n    Community_Involvement: Reflecting on the store's engagement with local community or charity.\n    Environmental_Impact: Assessing the store's impact on the environment or eco-friendly practices.\n\"\"\"\n\nlegal_document_focus_presets = \"\"\"\n    Precise: Using exact and clear language to define terms or conditions.\n    Ambiguous: Containing unclear or vague language that may lead to confusion.\n    Binding: Reflecting a strong commitment or obligation to adhere to the terms.\n    Permissive: Allowing flexibility or granting permissions within the agreement.\n    Restrictive: Imposing limitations or constraints on actions or behavior.\n    Comprehensive: Covering all necessary aspects or details in a thorough manner.\n    Confrontational: Taking an aggressive or adversarial stance in the language.\n    Conciliatory: Seeking to smooth over differences or reach an amicable agreement.\n    Formal: Adhering to conventional legal standards and language.\n    Informal: Using a more casual or non-traditional tone.\n    Fair: Reflecting an even-handed or equitable approach to the subject matter.\n    Biased: Showing favoritism or prejudice towards a particular party or interest.\n    Transparent: Providing clear and open disclosure of all relevant information.\n    Confidential: Emphasizing the privacy or secrecy of the information contained.\n    Enforceable: Reflecting terms that are legally binding and can be enforced.\n    Unenforceable: Containing provisions that may not be legally upheld.\n    Protective: Including clauses or terms to safeguard interests or rights.\n    Risky: Reflecting potential legal risks or liabilities.\n    Cooperative: Encouraging collaboration or mutual agreement between parties.\n    Adversarial: Reflecting opposition or conflict between the interests of parties.\n\"\"\"\n\nnews_article_focus_presets = \"\"\"\n    Objective: Presenting facts without personal emotion or bias.\n    Subjective: Reflecting personal opinions, feelings, or interpretations.\n    Informative: Providing valuable information, details, or insights.\n    Sensational: Using exciting or shocking language to attract attention.\n    Critical: Offering criticism or analysis, often pointing out flaws or problems.\n    Praise: Expressing approval, admiration, or complimenting aspects of the subject.\n    Neutral: Lacking strong emotions, opinions, or bias; presenting a balanced view.\n    Biased: Showing favoritism or prejudice towards a particular side or perspective.\n    Alarming: Conveying a sense of urgency or warning, often through dramatic language.\n    Reassuring: Providing comfort or confidence, often calming concerns or fears.\n    Analytical: Offering a detailed examination or analysis of the subject matter.\n    Opinionated: Expressing strong personal beliefs or judgments.\n    Factual: Sticking strictly to verifiable facts and data.\n    Speculative: Engaging in conjecture or hypothesis without firm evidence.\n    Positive: Conveying a favorable or uplifting sentiment.\n    Negative: Indicating an unfavorable, critical, or discouraging sentiment.\n    Cautious: Reflecting careful or guarded language, often related to uncertain matters.\n    Aggressive: Using forceful or assertive language, often to make a strong point.\n    Sympathetic: Showing empathy, understanding, or compassion towards a subject.\n    Controversial: Addressing topics or opinions that may provoke strong disagreement.\n\"\"\"\n\nresearch_paper_focus_presets = \"\"\"\n    Insightful: Demonstrating a deep understanding or novel perspective on the subject.\n    Rigorous: Reflecting strict adherence to scientific methods and principles.\n    Innovative: Introducing new ideas, methods, or approaches.\n    Confusing: Lacking clarity or coherence in arguments or explanations.\n    Comprehensive: Providing an extensive and thorough examination of the topic.\n    Biased: Showing a lack of objectivity or impartiality in presentation or analysis.\n    Objective: Presenting information and arguments in a neutral and unbiased manner.\n    Superficial: Lacking depth or detailed exploration of the subject.\n    Relevant: Directly related or applicable to the field or subject of interest.\n    Irrelevant: Containing content that does not pertain to the main subject or focus.\n    Credible: Demonstrating reliability, supported by valid evidence or citations.\n    Speculative: Based on conjecture or hypothesis without solid evidence.\n    Well-Structured: Organized and logically structured, facilitating understanding.\n    Disorganized: Lacking clear structure or logical flow, hindering comprehension.\n    Convincing: Presenting compelling arguments or evidence that persuades the reader.\n    Unconvincing: Failing to provide persuasive evidence or reasoning.\n    Original: Offering unique contributions or fresh perspectives to the field.\n    Derivative: Lacking originality, heavily relying on existing works.\n    Accessible: Written in a manner that is easily understood by a broader audience.\n    Technical: Utilizing specialized language or concepts that may be challenging for a general audience.\n\"\"\"\n\nblog_post_focus_presets = \"\"\"\n    Informative: Providing valuable insights, facts, or information.\n    Personal: Sharing personal experiences, feelings, or opinions.\n    Entertaining: Engaging readers with humor, stories, or amusing content.\n    Inspirational: Encouraging positive feelings, creativity, or motivation.\n    Technical: Delving into technical details, specifications, or processes.\n    Opinionated: Expressing strong personal beliefs or judgments.\n    Reflective: Encouraging thoughtful contemplation or introspection.\n    Conversational: Using a casual, friendly tone to foster reader engagement.\n    Professional: Maintaining a formal, business-like tone and content.\n    Trendy: Covering current trends, pop culture, or hot topics.\n    Educational: Offering instructional content, tutorials, or learning resources.\n    Controversial: Addressing divisive or hotly debated topics.\n    Encouraging: Providing support, encouragement, or positive reinforcement.\n    Critical: Analyzing subjects in depth, often pointing out flaws or areas for improvement.\n    Collaborative: Inviting reader interaction, comments, or collaboration.\n    Visual: Relying heavily on images, videos, or visual elements to convey the message.\n    Narrative: Telling a story or creating a narrative thread throughout the post.\n    Promotional: Highlighting products, services, or events for marketing purposes.\n    Analytical: Offering detailed analysis or examination of a particular subject.\n    Concise: Providing information in a clear and brief manner, avoiding unnecessary details.\n\"\"\"\n\nbook_focus_presets = \"\"\"\n    Engaging: Capturing the reader's interest or attention; compelling.\n    Boring: Lacking excitement or interest; monotonous.\n    Thoughtful: Providing deep insights or reflections; contemplative.\n    Shallow: Lacking depth or complexity in theme or characterization.\n    Inspirational: Offering motivation or encouragement; uplifting.\n    Tragic: Conveying a sense of sorrow, loss, or despair.\n    Comedic: Incorporating humor or satire; entertaining.\n    Informative: Educating the reader on a particular subject or idea.\n    Confusing: Leading to misunderstandings or difficulty following the plot.\n    Realistic: Portraying believable characters, settings, or events.\n    Fantastical: Incorporating magical or supernatural elements; imaginative.\n    Suspenseful: Creating tension or anticipation; thrilling.\n    Predictable: Lacking surprises or originality in plot or character development.\n    Original: Offering unique or innovative concepts, themes, or storytelling.\n    Emotional: Evoking strong feelings or reactions from the reader.\n    Detached: Lacking emotional connection or empathy with characters.\n    Complex: Featuring intricate plotlines, themes, or character relationships.\n    Simplistic: Overly straightforward or lacking nuance; elementary.\n    Poetic: Utilizing beautiful or artistic language; lyrical.\n    Dry: Lacking liveliness or emotion; dull or academic.\n\"\"\"\n\nmovie_focus_presets = \"\"\"\n    Artistic: Emphasizing visual aesthetics, design, or creative expression.\n    Commercial: Focused on mainstream appeal, entertainment value, or box office potential.\n    Realistic: Striving for authenticity, factual accuracy, or true-to-life depiction.\n    Fantastical: Embracing imagination, otherworldly elements, or magical realism.\n    Thought-provoking: Encouraging deep reflection, intellectual engagement, or philosophical exploration.\n    Light-hearted: Aimed at amusement, fun, or easy-going entertainment.\n    Dark: Delving into serious, grim, or disturbing themes and moods.\n    Action-packed: Featuring intense physical activity, stunts, or thrilling sequences.\n    Dialogue-driven: Relying on strong writing, character interaction, or verbal storytelling.\n    Visual-driven: Utilizing striking imagery, special effects, or cinematography.\n    Character-focused: Concentrating on character development, personalities, or emotional arcs.\n    Plot-focused: Prioritizing a well-structured, engaging, or complex storyline.\n    Original: Offering unique concepts, unconventional approaches, or innovative ideas.\n    Formulaic: Following established patterns, genre conventions, or predictable structures.\n    Inclusive: Promoting diversity, representation, or social awareness.\n    Exclusive: Lacking in diversity or representing a narrow perspective.\n    Historical: Based on or inspired by real historical events, figures, or periods.\n    Futuristic: Exploring future scenarios, technological advancements, or speculative ideas.\n    Inspirational: Encouraging motivation, positivity, or uplifting emotions.\n    Depressing: Conveying sadness, despair, or disheartening themes.\n    Balanced: Presenting multiple viewpoints, or a fair and nuanced portrayal of events.\n    Biased: Favoring a particular ideology, perspective, or interpretation.\n    Family-friendly: Suitable for all ages, emphasizing moral values or wholesome content.\n    Mature: Containing content for adult audiences, such as violence, language, or sexuality.\n    Experimental: Challenging traditional forms, embracing avant-garde techniques or ideas.\n    Conventional: Adhering to well-established styles, formats, or genre expectations.\n\"\"\"\n\ntv_show_focus_presets = \"\"\"\n    Entertaining: Providing amusement or enjoyment through engaging content.\n    Educational: Offering information or insights in an instructive manner.\n    Dramatic: Emphasizing tension, conflict, or emotional intensity.\n    Comedic: Focused on humor, wit, or satire to provoke laughter.\n    Thrilling: Creating suspense or excitement through plot twists and action.\n    Realistic: Depicting characters or scenarios in a believable and relatable way.\n    Fantastical: Delving into imaginative or supernatural themes and settings.\n    Family-Friendly: Suitable for all ages, often with moral lessons or wholesome themes.\n    Mature: Containing content aimed at adult audiences, possibly with strong language or themes.\n    Inspiring: Encouraging positive attitudes, motivation, or personal growth.\n    Controversial: Tackling subjects that may provoke debate or strong opinions.\n    Romantic: Focusing on love, relationships, or emotional connections.\n    Action-Packed: Emphasizing physical action, stunts, or high-energy sequences.\n    Reflective: Encouraging thought or introspection on deeper themes or values.\n    Serialized: Following a continuous storyline across multiple episodes or seasons.\n    Episodic: Each episode stands alone, with no overarching plot connecting them.\n    Innovative: Breaking new ground or experimenting with unconventional storytelling.\n    Traditional: Adhering to established genres or narrative conventions.\n    Inclusive: Representing diverse characters or perspectives in an equitable way.\n    Exclusive: Focused on a specific niche, culture, or group, possibly lacking wider diversity.\n    High-Budget: Featuring high production values, with extensive resources spent on quality.\n    Low-Budget: Produced with limited resources, possibly reflecting in production quality.\n    Live: Broadcasted in real-time or featuring live performances.\n    Scripted: Following a predetermined script or structure in the content.\n    Unscripted: Allowing for improvisation, often seen in reality or talk shows.\n    Celebrity-Focused: Centering on the lives, careers, or appearances of famous individuals.\n    Documentary-Style: Presenting information in a factual or investigative manner.\n\"\"\"\n\npodcast_focus_presets = \"\"\"\n    Engaging: Capturing interest or attention through compelling content or delivery.\n    Informative: Providing valuable information, insights, or expertise.\n    Entertaining: Offering amusement or enjoyment through humor, storytelling, or personality.\n    Monotonous: Lacking variation or excitement; potentially boring or tedious.\n    Thoughtful: Reflecting careful consideration, depth, or insight.\n    Superficial: Lacking depth or substance; possibly glossing over complex topics.\n    Controversial: Addressing topics that may provoke disagreement or debate.\n    Inspirational: Motivating or uplifting, often through personal stories or encouragement.\n    Confrontational: Engaging in aggressive or challenging discussions or debates.\n    Collaborative: Featuring cooperative interactions between hosts, guests, or listeners.\n    Technical: Delving into specialized or complex subjects, possibly requiring background knowledge.\n    Accessible: Easily understood by a general audience; avoiding jargon or complexity.\n    Empathetic: Expressing understanding or compassion, especially in sensitive topics.\n    Analytical: Providing a detailed examination or interpretation of a subject.\n    Casual: Using a relaxed and conversational tone or approach.\n    Structured: Following a clear and organized format or agenda.\n    Chaotic: Lacking organization or clarity; possibly confusing or disjointed.\n    Inclusive: Encouraging participation or representation of diverse perspectives.\n    Biased: Reflecting a particular stance or viewpoint, possibly to the exclusion of others.\n    Professional: Maintaining a formal, polished, or business-like tone or approach.\n\"\"\"\n\nsong_focus_presets = \"\"\"\n    Emotional: Conveying strong feelings or sentiments.\n    Narrative: Telling a story or depicting a series of events.\n    Inspirational: Encouraging positive thoughts, motivation, or self-belief.\n    Romantic: Focusing on love, affection, or intimate relationships.\n    Political: Addressing political issues, activism, or social commentary.\n    Party: Creating an energetic, fun, or danceable vibe.\n    Reflective: Encouraging introspection, contemplation, or self-awareness.\n    Experimental: Utilizing unconventional sounds or structures.\n    Traditional: Adhering to classical or established musical forms.\n    Modern: Embracing contemporary styles, sounds, or themes.\n    Spiritual: Focusing on religious, mystical, or philosophical concepts.\n    Aggressive: Utilizing intense, forceful, or confrontational language and music.\n    Soothing: Creating a calming, relaxing, or tranquil atmosphere.\n    Nostalgic: Recalling or celebrating the past, often with a sentimental tone.\n    Optimistic: Conveying a positive, hopeful, or forward-looking perspective.\n    Pessimistic: Expressing a negative, cynical, or downbeat view.\n    Celebratory: Marking a special occasion, achievement, or joyful event.\n    Mourning: Reflecting grief, loss, or sorrow.\n    Humorous: Incorporating comedy, satire, or playful elements.\n    Abstract: Emphasizing non-literal or symbolic expressions and themes.\n    Personal: Detailing personal experiences, thoughts, or feelings.\n    Universal: Addressing themes or ideas that resonate with a wide audience.\n    Upbeat: Featuring lively, cheerful, or uplifting musical elements.\n    Melancholic: Creating a mood of sadness, longing, or wistfulness.\n    Empowering: Encouraging self-confidence, strength, or empowerment.\n\"\"\"\n\nvideo_game_focus_presets = \"\"\"\n    Immersive: Providing a deep and engaging experience that draws players in.\n    Repetitive: Lacking variety or innovation, leading to monotonous gameplay.\n    Challenging: Offering difficulties or obstacles that test players' skills.\n    Casual: Designed for relaxed play without intense commitment or challenge.\n    Competitive: Encouraging rivalry and competition between players.\n    Cooperative: Promoting teamwork and collaboration among players.\n    Story-driven: Centered around a strong narrative or plot.\n    Mechanic-focused: Emphasizing gameplay mechanics and systems over story.\n    Innovative: Introducing new ideas, concepts, or gameplay mechanics.\n    Traditional: Sticking to well-established genres or gameplay conventions.\n    Accessible: Designed to be easily understood and played by a wide audience.\n    Complex: Requiring deep understanding or mastery of intricate mechanics.\n    Visually-Stunning: Featuring impressive graphics, art, or visual effects.\n    Auditory: Utilizing impactful sound design or music to enhance the experience.\n    Ethical: Reflecting moral choices or values within the game's content.\n    Violent: Containing intense or graphic violence as a core aspect.\n    Educational: Offering learning opportunities or educational value.\n    Social: Encouraging interaction, communication, or connection between players.\n    Solo: Focused on single-player experiences without multiplayer components.\n    Multiplatform: Available on various gaming platforms or devices.\n    Exclusive: Restricted to a specific platform or device.\n    Inclusive: Incorporating diverse characters, themes, or accessibility options.\n    Microtransaction-heavy: Relying on in-game purchases for content or progression.\n    Mod-friendly: Allowing or encouraging player-created modifications or content.\n    Family-friendly: Suitable for players of all ages without mature content.\n    Realistic: Aiming for lifelike graphics, physics, or simulation.\n    Stylized: Utilizing a unique or artistic visual style distinct from realism.\n\"\"\"\n\nrestaurant_review_focus_presets = \"\"\"\n    Delicious: Expressing enjoyment or satisfaction with the taste of the food.\n    Disappointing: Indicating dissatisfaction or unmet expectations with the overall experience.\n    Friendly: Reflecting warm or welcoming service from the staff.\n    Unprofessional: Indicating a lack of professionalism or courtesy in service.\n    Cozy: Describing a comfortable or intimate ambiance.\n    Noisy: Referring to a loud or disruptive environment.\n    Overpriced: Believing that the prices are too high for the value provided.\n    Bargain: Finding the prices to be reasonable or a good value for the money.\n    Creative: Appreciating innovative or unique dishes and presentation.\n    Bland: Describing food that lacks flavor or seasoning.\n    Fresh: Valuing the use of fresh ingredients and quality produce.\n    Stale: Indicating that food seemed old or not freshly prepared.\n    Clean: Observing cleanliness in the dining area, kitchen, or restrooms.\n    Crowded: Reflecting a packed or overly busy dining space.\n    Romantic: Finding the setting or atmosphere suitable for a romantic occasion.\n    Family-friendly: Appreciating an environment suitable for family dining or children.\n    Authentic: Valuing a genuine or traditional culinary experience.\n    Fusion: Enjoying a blend or mix of culinary traditions or flavors.\n    Attentive: Praising the attentive and responsive service from the staff.\n    Ignored: Feeling neglected or overlooked by the service staff.\n\"\"\"\n\nhotel_review_focus_presets = \"\"\"\n    Comfortable: Emphasizing the comfort and coziness of the rooms and facilities.\n    Uncomfortable: Highlighting discomfort or issues with room amenities.\n    Luxurious: Focusing on opulence, high-end services, and lavish features.\n    Budget-friendly: Stressing affordability and value for money.\n    Clean: Noting the cleanliness and tidiness of the premises.\n    Dirty: Pointing out hygiene issues or unclean conditions.\n    Friendly: Emphasizing warmth, friendliness, or hospitality of the staff.\n    Rude: Discussing impoliteness or unsatisfactory interactions with staff.\n    Convenient: Highlighting the convenient location or accessibility.\n    Isolated: Mentioning challenges related to location or accessibility.\n    Safe: Noting the safety features and sense of security in the property.\n    Unsafe: Pointing out security concerns or potential risks.\n    Relaxing: Focusing on the calming ambiance, spa, or relaxation amenities.\n    Noisy: Discussing noise issues or disturbances during the stay.\n    Family-friendly: Emphasizing facilities and services suitable for families.\n    Not-family-friendly: Lacking features or ambiance suitable for family stays.\n    Pet-friendly: Noting accommodations and facilities for pets.\n    Not-pet-friendly: Highlighting restrictions or inconveniences related to pets.\n    Culinary: Focusing on the quality, variety, and taste of the food and beverages.\n    Disappointing-food: Criticizing the food quality, taste, or presentation.\n    Eco-friendly: Emphasizing sustainability practices and eco-conscious efforts.\n    Not-eco-friendly: Pointing out a lack of environmental responsibility.\n    Well-maintained: Noting the good condition and maintenance of the property.\n    Poorly-maintained: Discussing wear and tear, or maintenance issues.\n    Informative: Providing detailed information about the hotel and its services.\n    Vague: Lacking clarity or detailed information about the experience.\n    Romantic: Emphasizing aspects suitable for couples or romantic getaways.\n    Business-oriented: Focusing on facilities and services for business travelers.\n\"\"\"\n\ntravel_review_focus_presets = \"\"\"\n    Informative: Providing detailed information about the location, amenities, or experience.\n    Misleading: Containing information that may confuse or misrepresent the actual experience.\n    Enthusiastic: Expressing great excitement and positive feelings about the travel experience.\n    Disappointed: Reflecting dissatisfaction or unmet expectations during the travel.\n    Adventurous: Highlighting unique or thrilling aspects of the trip, such as outdoor activities.\n    Relaxing: Focusing on the calming and leisurely aspects of the travel destination.\n    Cultural: Emphasizing the cultural experiences, local traditions, or historical sites.\n    Touristy: Focusing on popular attractions or experiences typically sought by tourists.\n    Off-the-Beaten-Path: Describing lesser-known or unconventional travel experiences.\n    Luxurious: Detailing high-end accommodations, gourmet dining, or exclusive services.\n    Budget-Friendly: Emphasizing affordable options, deals, or value for money.\n    Family-Friendly: Highlighting aspects suitable for family travel, such as child-friendly activities.\n    Romantic: Capturing the intimate or romantic ambiance of a destination or accommodation.\n    Eco-Friendly: Focusing on sustainability, environmental practices, or eco-tourism.\n    Accessible: Detailing facilities or services catering to travelers with disabilities.\n    Photogenic: Emphasizing scenic beauty or opportunities for photography.\n    Gastronomic: Centering on culinary experiences, local cuisines, or dining options.\n    Nightlife: Highlighting entertainment, clubs, bars, or nighttime activities.\n    Safety-Conscious: Reflecting on the safety measures, local crime, or health concerns.\n    Crowded: Describing the level of tourist traffic, congestion, or crowdedness.\n    Seasonal: Focusing on the best time to visit or seasonal attractions and events.\n    Biased: Showing favoritism towards certain aspects or a particular view of the destination.\n    Balanced: Presenting a fair and comprehensive overview, including both positives and negatives.\n\"\"\"\n\n\nfocus_presets = {\n    \"financial_investor_focus\": financial_investor_focus_presets,\n    \"public_company_earnings_call_focus\": public_company_earnings_call_text,\n    \"customer_feedback_focus\": customer_feedback_focus_presets,\n    \"customer_service_interaction_focus\": customer_service_interaction_focus_presets,\n    \"marketing_campaign_focus\": marketing_campaign_focus_presets,\n    \"product_review_focus\": product_review_focus_presets,\n    \"email_correspondence_focus\": email_correspondence_focus_presets,\n    \"github_issues_focus\": github_issues_focus_presets,\n    \"social_media_sentiment_focus\": social_media_sentiment_focus_presets,\n    \"employee_feedback_focus\": employee_feedback_focus_presets,\n    \"crisis_communication_focus\": crisis_communication_focus_presets,\n    \"political_speech_focus\": political_speech_focus_presets,\n    \"healthcare_patient_feedback_focus\": healthcare_patient_feedback_focus_presets,\n    \"educational_content_focus\": educational_content_focus_presets,\n    \"job_interview_focus\": job_interview_focus_presets,\n    \"sales_call_focus\": sales_call_focus_presets,\n    \"real_estate_listing_focus\": real_estate_listing_focus_presets,\n    \"mental_health_counseling_focus\": mental_health_counseling_focus_presets,\n    \"human_resources_recruitment_focus\": human_resources_recruitment_focus_presets,\n    \"political_campaign_focus\": political_campaign_focus_presets,\n    \"government_policy_response_focus\": government_policy_response_focus_presets,\n    \"retail_customer_feedback_focus\": retail_customer_feedback_focus_presets,\n    \"legal_document_focus\": legal_document_focus_presets,\n    \"news_article_focus\": news_article_focus_presets,\n    \"research_paper_focus\": research_paper_focus_presets,\n    \"blog_post_focus\": blog_post_focus_presets,\n    \"book_focus\": book_focus_presets,\n    \"movie_focus\": movie_focus_presets,\n    \"tv_show_focus\": tv_show_focus_presets,\n    \"podcast_focus\": podcast_focus_presets,\n    \"song_focus\": song_focus_presets,\n    \"video_game_focus\": video_game_focus_presets,\n    \"restaurant_review_focus\": restaurant_review_focus_presets,\n    \"hotel_review_focus\": hotel_review_focus_presets,\n    \"travel_review_focus\": travel_review_focus_presets,\n}\n\nfocus_areas_dict = {\n    \"financial_investor_focus\": \"Public markets investors trying to evaluate the comments of management teams and other professional investors and stock analysts.\",\n    \"public_company_earnings_call_focus\": \"Investors, analysts, and shareholders seeking insights into a company's financial health and future prospects through earnings calls.\",\n    \"customer_feedback_focus\": \"Businesses and product managers looking to understand customer satisfaction, preferences, and areas for improvement.\",\n    \"customer_service_interaction_focus\": \"Customer service professionals and management aiming to evaluate and enhance customer support interactions.\",\n    \"marketing_campaign_focus\": \"Marketers and business strategists interested in evaluating the effectiveness and reach of marketing campaigns.\",\n    \"product_review_focus\": \"Consumers, retailers, and manufacturers seeking insights into product quality, usability, and appeal.\",\n    \"email_correspondence_focus\": \"Professionals and organizations aiming to improve communication efficiency and clarity in email interactions.\",\n    \"github_issues_focus\": \"Developers, project managers, and contributors looking to identify, track, and resolve issues in software development projects.\",\n    \"social_media_sentiment_focus\": \"Social media analysts and brand managers monitoring public sentiment, trends, and reactions on social platforms.\",\n    \"employee_feedback_focus\": \"HR professionals and management focusing on employee satisfaction, workplace culture, and opportunities for growth.\",\n    \"crisis_communication_focus\": \"PR and crisis management teams analyzing and formulating responses to urgent and sensitive situations.\",\n    \"political_speech_focus\": \"Political analysts, campaigners, and engaged citizens evaluating the content, strategy, and impact of political speeches.\",\n    \"healthcare_patient_feedback_focus\": \"Healthcare providers and administrators seeking to understand patient experiences, concerns, and satisfaction.\",\n    \"educational_content_focus\": \"Educators, students, and educational institutions assessing the quality, relevance, and effectiveness of educational materials.\",\n    \"job_interview_focus\": \"Job seekers and hiring managers interested in evaluating the qualifications, experience, and fit of candidates for a specific role.\",\n    \"sales_call_focus\": \"Sales professionals and potential customers assessing the value, features, and benefits of a product or service during a sales interaction.\",\n    \"real_estate_listing_focus\": \"Property buyers, sellers, and real estate agents examining the details, location, and pricing of real estate properties listed for sale or rent.\",\n    \"mental_health_counseling_focus\": \"Mental health professionals and individuals seeking therapy, focusing on the assessment, treatment, and support for mental and emotional well-being.\",\n    \"human_resources_recruitment_focus\": \"HR professionals, recruiters, and job seekers evaluating the recruitment process, candidate sourcing, and talent acquisition strategies.\",\n    \"political_campaign_focus\": \"Political strategists, voters, and media analyzing the strategies, messages, and objectives of a political campaign or candidate.\",\n    \"government_policy_response_focus\": \"Policymakers, citizens, and analysts evaluating government actions, policies, and responses to specific issues or crises.\",\n    \"retail_customer_feedback_focus\": \"Retail business owners, managers, and customers reviewing feedback on products, services, and overall customer experience in a retail setting.\",\n    \"legal_document_focus\": \"Lawyers, legal scholars, and interested parties examining legal documents, contracts, and agreements for compliance, interpretation, and understanding.\",\n    \"news_article_focus\": \"Journalists, readers, and media analysts focusing on the content, accuracy, and relevance of news articles, reports, and journalistic pieces.\",\n    \"research_paper_focus\": \"Academics, researchers, students, and professionals seeking in-depth analysis, findings, methodologies, and insights in a specific field of study.\",\n    \"blog_post_focus\": \"General readers, enthusiasts, or professionals interested in insights, opinions, updates, or personal narratives on specific topics, industries, or hobbies.\",\n    \"book_focus\": \"Readers of various age groups and interests seeking entertainment, knowledge, or literary enrichment through novels, non-fiction, educational texts, etc.\",\n    \"movie_focus\": \"Moviegoers, film enthusiasts, critics, and general audiences seeking entertainment, artistic expression, or thematic exploration through cinema.\",\n    \"tv_show_focus\": \"Television viewers, fans, and critics looking for entertainment, information, or engagement with serialized stories, documentaries, talk shows, etc.\",\n    \"podcast_focus\": \"Listeners interested in audio content that provides insights, entertainment, interviews, or discussions on specific subjects, industries, or trends.\",\n    \"song_focus\": \"Music lovers, musicians, critics, and general audiences seeking emotional expression, entertainment, or artistic exploration through musical compositions.\",\n    \"video_game_focus\": \"Gamers, game developers, critics, and enthusiasts seeking interactive entertainment, challenges, storytelling, or virtual experiences through gaming.\",\n    \"restaurant_review_focus\": \"Diners, food enthusiasts, tourists, and locals looking for insights into the quality, ambiance, service, and culinary offerings of eateries and restaurants.\",\n    \"hotel_review_focus\": \"Travelers, tourists, business professionals, and event planners seeking information on the comfort, amenities, location, and services of hotels and accommodations.\",\n    \"travel_review_focus\": \"Travelers, adventure seekers, families, and tourists looking for insights into destinations, experiences, accommodations, attractions, and travel services.\"\n}\n\n\ndef generate_all_prompts(focus_key, scoring_scale_explanation):\n    preset_text = focus_presets[focus_key]\n    target_audience = focus_areas_dict[focus_key]\n    populated_prompts = {}\n    for line in preset_text.strip().split(\"\\n\"):\n        sentiment_adjective, sentiment_explanation = line.strip().split(\": \", 1)\n        populated_prompt_text = generate_llm_sentiment_score_prompt(sentiment_adjective, sentiment_explanation, target_audience, scoring_scale_explanation)\n        populated_prompts[sentiment_adjective] = populated_prompt_text\n    return populated_prompts\n\ndef generate_llm_sentiment_score_prompt(sentiment_adjective, sentiment_explanation, target_audience, scoring_scale_explanation):\n    populated_prompt_text = (\n        f\"We are seeking to evaluate the sentiment expressed in a given text with regard to a specific adjective. \"\n        f\"The goal is to determine the extent to which the text reflects the sentiment described by the adjective \"\n        f\"{scoring_scale_explanation} \"\n        f\"Focus on adjectives that would also be relevant to {target_audience}.\\n\\n\"\n        f\"<sentiment_adjective>:  {sentiment_adjective}\\n\"\n        f\"<sentiment_explanation>:  {sentiment_explanation}\\n\\n\"\n        f\"Please provide your analysis in the following format:\\n\"\n        f\"`sentiment_score` | `score_justification`\\n\"\n        f\"where `sentiment_score` is the score on the aforementioned scale, and `score_justification` is a textual description of what exactly made you come up with the score that you did.\"\n    )\n    return populated_prompt_text\n\ndef combine_populated_prompts_with_source_text(populated_prompt, source_text):\n    combined_prompt = populated_prompt + \"\\n---\\n\" + source_text\n    return combined_prompt\n\nclass suppress_stdout_stderr(object):\n    def __enter__(self):\n        self.outnull_file = open(os.devnull, 'w')\n        self.errnull_file = open(os.devnull, 'w')\n        self.old_stdout_fileno_undup    = sys.stdout.fileno()\n        self.old_stderr_fileno_undup    = sys.stderr.fileno()\n        self.old_stdout_fileno = os.dup ( sys.stdout.fileno() )\n        self.old_stderr_fileno = os.dup ( sys.stderr.fileno() )\n        self.old_stdout = sys.stdout\n        self.old_stderr = sys.stderr\n        os.dup2 ( self.outnull_file.fileno(), self.old_stdout_fileno_undup )\n        os.dup2 ( self.errnull_file.fileno(), self.old_stderr_fileno_undup )\n        sys.stdout = self.outnull_file\n        sys.stderr = self.errnull_file\n        return self\n    def __exit__(self, *_):        \n        sys.stdout = self.old_stdout\n        sys.stderr = self.old_stderr\n        os.dup2 ( self.old_stdout_fileno, self.old_stdout_fileno_undup )\n        os.dup2 ( self.old_stderr_fileno, self.old_stderr_fileno_undup )\n        os.close ( self.old_stdout_fileno )\n        os.close ( self.old_stderr_fileno )\n        self.outnull_file.close()\n        self.errnull_file.close()\n\ndef load_inference_model(model_name: str):\n    try:\n        models_dir = os.path.join(BASE_DIRECTORY, 'models') # Determine the model directory path\n        matching_files = glob.glob(os.path.join(models_dir, f\"{model_name}*\")) # Search for matching model files\n        if not matching_files:\n            logger.error(f\"No model file found matching: {model_name}\")\n            raise FileNotFoundError\n        matching_files.sort(key=os.path.getmtime, reverse=True) # Sort the files based on modification time (recently modified files first)\n        model_file_path = matching_files[0]\n        model_instance = Llama(model_path=model_file_path, embedding=True, n_ctx=LLM_CONTEXT_SIZE_IN_TOKENS, verbose=False, use_mlock=True) # Load the model\n        return model_instance\n    except TypeError as e:\n        logger.error(f\"TypeError occurred while loading the model: {e}\")\n        raise\n    except Exception as e:\n        logger.error(f\"Exception occurred while loading the model: {e}\")\n        raise FileNotFoundError(f\"No model file found matching: {model_name}\")\n        \ndef validate_llm_generated_sentiment_response(llm_raw_output, lowest_possible_score, highest_possible_score):\n    use_verbose = 1\n    if use_verbose:\n        logger.info(f\"Validating LLM raw output: {llm_raw_output}\")\n    llm_raw_output = llm_raw_output.strip().strip(\"`\")  # Remove surrounding whitespace and backticks if present\n    llm_raw_output = llm_raw_output.replace(\"&amp;\", \"&\").replace(\"&lt;\", \"<\").replace(\"&gt;\", \">\") # Replace any HTML-encoded characters\n    parts = llm_raw_output.split(\"|\")\n    if '|' not in llm_raw_output: # Check if delimiter '|' is present; if not, try alternative delimiters\n        parts = llm_raw_output.split(\"_|_\")\n        if len(parts) < 2:\n            # If no delimiter found, try extracting the first numerical value\n            match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", llm_raw_output)\n            if match:\n                sentiment_score_str = match.group(0)\n                score_justification = llm_raw_output.replace(sentiment_score_str, '').strip()\n            else:\n                raise ValueError(\"Unable to extract sentiment score and justification from the LLM raw output.\")\n        else:\n            sentiment_score_str, score_justification = parts[0], \"|\".join(parts[1:])\n    else:\n        sentiment_score_str, score_justification = parts[0], \"|\".join(parts[1:])\n        logger.info(f\"Extracted sentiment_score_str: {sentiment_score_str}\")\n        logger.info(f\"Extracted score_justification: {score_justification}\")\n    sentiment_score_str = sentiment_score_str.strip().rstrip(\"`\")  # Trim any leading or trailing whitespace from both parts and trailing backtick if present\n    score_justification = score_justification.strip().rstrip(\"`\") \n    if use_verbose:\n        logger.info(f\"Trimmed sentiment_score_str: {sentiment_score_str}\")\n    sentiment_score_str = \"\".join(char for char in sentiment_score_str if char in \"0123456789.-\") # Remove any non-numeric characters except the decimal point and negative sign from sentiment_score_str\n    if use_verbose:\n        logger.info(f\"Removed non-numeric characters from sentiment_score_str: {sentiment_score_str}\")\n    if sentiment_score_str.count(\".\") > 1: # If multiple decimal points or negative signs are present, keep only the first occurrence\n        sentiment_score_str = sentiment_score_str.replace(\".\", \"\", sentiment_score_str.count(\".\") - 1)\n    if sentiment_score_str.count(\"-\") > 1:\n        sentiment_score_str = sentiment_score_str.replace(\"-\", \"\", sentiment_score_str.count(\"-\") - 1)\n    if use_verbose:\n        logger.info(f\"Removed extra decimal points and negative signs from sentiment_score_str: {sentiment_score_str}\")\n    try: # Attempt to cast sentiment_score into a float\n        if use_verbose:\n            logger.info(f\"Attempting to cast sentiment_score_str {sentiment_score_str} into a float...\")\n        sentiment_score = float(sentiment_score_str)\n        if sentiment_score < lowest_possible_score: # If out of range, bound to the nearest limit\n            logger.warning(f\"Sentiment score {sentiment_score} is below the lower bound {lowest_possible_score}.\")\n            sentiment_score = lowest_possible_score\n        elif sentiment_score > highest_possible_score:\n            logger.warning(f\"Sentiment score {sentiment_score} is above the upper bound {highest_possible_score}.\")\n            sentiment_score = highest_possible_score\n    except ValueError:\n        logger.error(f\"Sentiment score {sentiment_score_str} could not be cast into a float after attempted corrections.\")\n        raise ValueError(\"Sentiment score could not be cast into a float after attempted corrections.\")\n    if len(score_justification) < 30 or len(score_justification.split()) < 5: # If score justification is too short, return a warning in the justification itself\n        logger.warning(f\"Justification is too short: {score_justification}\")\n        score_justification = f\"Warning: Justification is too short. Original response: {score_justification}\"\n    return sentiment_score, score_justification\n\ndef run_llm_in_process(combined_prompt_text, model_name):\n    try:\n        logger.info(\"Running run_llm_in_process with model_name: \" + model_name)\n        logger.info(\"Combined prompt text: \" + combined_prompt_text[:100])  # Log the first 100 characters\n        with suppress_stdout_stderr():\n            logger.info(\"Loading LLM model...\")\n            llm_local = load_inference_model(model_name)\n            logger.info(\"Model loaded successfully.\")\n        logger.info(\"Running model with combined prompt text...\")\n        result = llm_local(combined_prompt_text)\n        logger.info(\"Model run successful.\")\n        return result\n    except Exception as e:\n        traceback.print_exc()\n        logger.error(f\"An error occurred in run_llm_in_process: {e}\", exc_info=True, stack_info=True)\n\nasync def parallel_attempt(combined_prompt_text, model_name):\n    global lowest_possible_score, highest_possible_score\n    loop = asyncio.get_event_loop()\n    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n        result_dict = await loop.run_in_executor(executor, partial(run_llm_in_process, combined_prompt_text, model_name))\n        result_text = result_dict['choices'][0]['text']  # Extract the text from the dictionary\n        return result_text  # Returning raw result text\n\ndef combine_llm_generated_sentiment_responses(llm_raw_outputs, lowest_possible_score, highest_possible_score):\n    scores = []\n    score_justification_strings = []\n    failed_runs = 0\n    min_successful_runs = 3 # Set as appropriate\n    for llm_raw_output in llm_raw_outputs:\n        try:\n            sentiment_score, score_justification = validate_llm_generated_sentiment_response(llm_raw_output, lowest_possible_score, highest_possible_score)\n            scores.append(sentiment_score)\n            score_justification_strings.append(score_justification)\n        except ValueError:\n            failed_runs += 1\n    if len(scores) < min_successful_runs: # Fallback strategy, e.g., return an error, use a default value, etc.\n        raise ValueError(\"Insufficient successful parallel runs.\")\n    mean_sentiment_score = sum(scores) / len(scores)\n    bootstrap_samples = [np.random.choice(scores, len(scores)) for _ in range(1000)] # Calculate the 95% confidence interval using the bootstrap method\n    sentiment_score_95_pct_confidence_interval = np.percentile(bootstrap_samples, [2.5, 97.5])\n    interquartile_range_of_sentiment_scores = np.percentile(bootstrap_samples, [25, 75])\n    iqr_of_sentiment_score_as_pct_of_mean_score = (interquartile_range_of_sentiment_scores[1] - interquartile_range_of_sentiment_scores[0]) / mean_sentiment_score\n    return mean_sentiment_score, sentiment_score_95_pct_confidence_interval, interquartile_range_of_sentiment_scores, iqr_of_sentiment_score_as_pct_of_mean_score, score_justification_strings\n\ndef update_summary_table(update_dict, global_table=False):\n    table_path = GLOBAL_SUMMARY_TABLE_PATH if global_table else SUMMARY_TABLE_PATH\n    try:\n        lock = FileLock(table_path + \".lock\")\n        with lock:\n            if os.path.exists(table_path):\n                summary_table = pd.read_csv(table_path)\n            else:\n                columns = GLOBAL_SUMMARY_COLUMNS if global_table else SUMMARY_COLUMNS\n                summary_table = pd.DataFrame(columns=columns)\n            summary_table = summary_table.append(update_dict, ignore_index=True)\n            summary_table.to_csv(table_path, index=False)\n            logger.info(f\"{'Global' if global_table else ''} Summary table updated successfully.\")\n    except Exception as e:\n        logger.error(f\"An error occurred while updating the {'global' if global_table else ''} summary table: {e}\", exc_info=True)\n\nasync def get_sentiment_score_from_llm(focus_key, sentiment_adjective, sentiment_explanation, target_audience, scoring_scale_explanation, source_text, model_name):\n    start_time = datetime.utcnow()\n    summary_table = pd.DataFrame(columns=SUMMARY_COLUMNS)\n    populated_prompt_text = generate_llm_sentiment_score_prompt(sentiment_adjective, sentiment_explanation, target_audience, scoring_scale_explanation)\n    combined_prompt_text = combine_populated_prompts_with_source_text(populated_prompt_text, source_text)\n    backoff_time = 1\n    raw_outputs = []\n    for attempt in range(MAX_ATTEMPTS // PARALLEL_ATTEMPTS):\n        logger.info(f\"Starting attempt {attempt + 1} out of {MAX_ATTEMPTS // PARALLEL_ATTEMPTS}.\")\n        tasks = [parallel_attempt(combined_prompt_text, model_name) for _ in range(PARALLEL_ATTEMPTS)]\n        results = await asyncio.gather(*tasks)\n        raw_outputs.extend(results)\n        successful_runs = 0\n        failed_runs = 0\n        for llm_raw_output in raw_outputs:\n            try:\n                validate_llm_generated_sentiment_response(llm_raw_output, lowest_possible_score, highest_possible_score)\n                print()\n                successful_runs += 1\n            except ValueError:\n                failed_runs += 1\n        logger.info(f\"Attempt {attempt + 1}: {successful_runs} successful runs, {failed_runs} failed runs.\")\n        if successful_runs >= MIN_SUCCESSFUL_RUNS_TO_GENERATE_SENTIMENT_SCORE: # Check if enough valid results have been gathered\n            try:\n                \n                mean_sentiment_score, sentiment_score_95_pct_confidence_interval, interquartile_range_of_sentiment_scores, iqr_of_sentiment_score_as_pct_of_mean_score, score_justification_strings = combine_llm_generated_sentiment_responses(raw_outputs, lowest_possible_score, highest_possible_score)\n                logger.info(f\"Output validation successful! Mean Sentiment Score: {mean_sentiment_score} | Confidence Interval: {sentiment_score_95_pct_confidence_interval}\")\n                logger.info(f\"Interquartile Range of Sentiment Scores: {interquartile_range_of_sentiment_scores} | IQR as Percentage of Mean Score: {iqr_of_sentiment_score_as_pct_of_mean_score * 100}%\")\n                finish_time = datetime.utcnow()\n                time_taken_in_seconds = (finish_time - start_time).total_seconds()\n                update_dict = { 'Attempt': attempt + 1,\n                                'Successful Runs': successful_runs,\n                                'Failed Runs': failed_runs,\n                                'Time Taken in Seconds': time_taken_in_seconds,\n                                'Mean Score': mean_sentiment_score,\n                                '95% CI Lower': sentiment_score_95_pct_confidence_interval[0],\n                                '95% CI Upper': sentiment_score_95_pct_confidence_interval[1],\n                                'IQR Lower': interquartile_range_of_sentiment_scores[0],\n                                'IQR Upper': interquartile_range_of_sentiment_scores[1],\n                                'IQR as Pct of Mean': iqr_of_sentiment_score_as_pct_of_mean_score}\n                update_summary_table(update_dict, global_table=False)\n                return mean_sentiment_score, sentiment_score_95_pct_confidence_interval, interquartile_range_of_sentiment_scores, iqr_of_sentiment_score_as_pct_of_mean_score, score_justification_strings, summary_table, combined_prompt_text, attempt, successful_runs, failed_runs, time_taken_in_seconds\n            except ValueError:\n                logger.error(f\"Attempt {attempt + 1} failed to combine outputs despite having enough successful runs. Trying again.\")\n                finish_time = datetime.utcnow()\n                time_taken_in_seconds = (finish_time - start_time).total_seconds()                \n                update_dict = { 'Attempt': attempt + 1,\n                                'Successful Runs': successful_runs,\n                                'Failed Runs': failed_runs,\n                                'Time Taken in Seconds': time_taken_in_seconds}\n                update_summary_table(update_dict, global_table=False)\n        else:\n            logger.warning(f\"Not enough successful runs in attempt {attempt + 1}. Trying again.\")\n            finish_time = datetime.utcnow()\n            time_taken_in_seconds = (finish_time - start_time).total_seconds()                 \n            update_dict = { 'Attempt': attempt + 1,\n                            'Successful Runs': successful_runs,\n                            'Failed Runs': failed_runs,\n                            'Time Taken in Seconds': time_taken_in_seconds}\n            update_summary_table(update_dict, global_table=False)\n        await asyncio.sleep(backoff_time) # Async sleep\n        backoff_time *= 2 # Double the backoff time for the next iteration\n    logger.error(\"Maximum attempts reached without valid output. Please review the LLM's responses.\")\n    raise Exception(\"Maximum attempts reached without valid output. Please review the LLM's responses.\")\n\nasync def analyze_focus_area_sentiments(focus_key, scoring_scale_explanation, source_text, model_name):\n    analysis_start_time = datetime.utcnow()\n    populated_prompts_dict = generate_all_prompts(focus_key, scoring_scale_explanation)\n    target_audience = focus_areas_dict[focus_key]\n    combined_sentiment_analysis_dict = {\n        \"focus_area\": focus_key,\n        \"target_audience\": target_audience,\n        \"scoring_scale_explanation\": scoring_scale_explanation,\n        \"source_text\": source_text,\n        \"model_name\": model_name,\n        \"lowest_possible_score\": lowest_possible_score,\n        \"highest_possible_score\": highest_possible_score,\n        \"individual_sentiment_report_dict\": {}\n    }\n    for sentiment_adjective, populated_prompt_text in populated_prompts_dict.items():\n        sentiment_explanation = populated_prompt_text.split(\"<sentiment_explanation>:  \")[1].split(\"\\n\")[0]\n        mean_sentiment_score, sentiment_score_95_pct_confidence_interval, interquartile_range_of_sentiment_scores, iqr_of_sentiment_score_as_pct_of_mean_score, score_justification_strings, summary_table, combined_prompt_text, attempt, successful_runs, failed_runs, time_taken_in_seconds = await get_sentiment_score_from_llm(focus_key, sentiment_adjective, sentiment_explanation, target_audience, scoring_scale_explanation, source_text, model_name)\n        update_dict = {\n            'Focus Area': focus_key,\n            'Sentiment Adjective': sentiment_adjective,\n            'Prompt': combined_prompt_text,\n            'Attempt': attempt + 1,\n            'Successful Runs': successful_runs,\n            'Failed Runs': failed_runs,\n            'Time Taken in Seconds': time_taken_in_seconds,\n            'Mean Score': mean_sentiment_score,\n            '95% CI Lower': sentiment_score_95_pct_confidence_interval[0],\n            '95% CI Upper': sentiment_score_95_pct_confidence_interval[1],\n            'IQR Lower': interquartile_range_of_sentiment_scores[0],\n            'IQR Upper': interquartile_range_of_sentiment_scores[1],\n            'IQR as Pct of Mean': iqr_of_sentiment_score_as_pct_of_mean_score\n        }\n        update_summary_table(update_dict, global_table=True)\n        combined_sentiment_analysis_dict[\"individual_sentiment_report_dict\"][sentiment_adjective] = {\n            \"sentiment_adjective\": sentiment_adjective,\n            \"sentiment_explanation\": sentiment_explanation,\n            \"populated_prompt_text\": populated_prompt_text,\n            \"sentiment_scores_dict\": {\"mean_sentiment_score\": mean_sentiment_score,\n                            \"sentiment_score_95_pct_confidence_interval\": sentiment_score_95_pct_confidence_interval,\n                            \"interquartile_range_of_sentiment_scores\": interquartile_range_of_sentiment_scores,\n                            \"iqr_of_sentiment_score_as_pct_of_mean_score\": iqr_of_sentiment_score_as_pct_of_mean_score,\n                            \"score_justification_strings\": score_justification_strings,\n                            \"run_summary_table\": summary_table.to_dict(orient=\"records\")\n            }\n        }\n    analysis_finish_time = datetime.utcnow()\n    analysis_time_taken_in_seconds = (analysis_finish_time - analysis_start_time).total_seconds()\n    logger.info(f\"Analysis of {focus_key} took {analysis_time_taken_in_seconds} seconds.\")\n    return combined_sentiment_analysis_dict\n\ndef calculate_max_workers(model_memory_requirement, safety_factor=0.5):\n    available_memory = psutil.virtual_memory().available / (1024 ** 2) # Convert to MB\n    max_workers = int((available_memory * safety_factor) / model_memory_requirement)\n    return max_workers\n\ndef get_model_memory_requirement(model_name):\n    model_name_pattern = f\"models/{model_name}*.bin\"\n    model_files = glob.glob(model_name_pattern)\n    if model_files:\n        model_file_path = model_files[0] # Assuming there's only one match\n        model_memory_requirement = os.path.getsize(model_file_path) / (1024 ** 2) # Convert to MB\n        return model_memory_requirement\n    else:\n        raise FileNotFoundError(f\"No model file found matching pattern: {model_name_pattern}\")\n\n\n# Example usage:\nlowest_possible_score = -100.0\nhighest_possible_score = 100.0\nmodel_name = \"Meta-Llama-3-8B-Instruct.Q3_K_S\"\nsource_text = \"The food was delicious but the service was slow. Overall I would try it again but not on a busy night.\" # Example source text\nfocus_key = \"restaurant_review_focus\" # The bundle of sentiment adjectives to use to analyze the source text\n\nmodel_memory_requirement = get_model_memory_requirement(model_name)\nprint(f\"Model memory requirement: {model_memory_requirement} MB\")\nMAX_WORKERS = calculate_max_workers(model_memory_requirement)\nprint(f\"MAX_WORKERS: {MAX_WORKERS}\")\nMAX_ATTEMPTS = 12 # How many times to attempt to generate a valid output before giving up\nPARALLEL_ATTEMPTS = 3 # How many parallel attempts to make per iteration\nMIN_SUCCESSFUL_RUNS_TO_GENERATE_SENTIMENT_SCORE = 5 # How many successful runs are required to consider the output valid\nUSE_RAMDISK = False\nLLM_CONTEXT_SIZE_IN_TOKENS = 1024\nSUMMARY_TABLE_PATH = 'combined_summary_table.csv'\nGLOBAL_SUMMARY_TABLE_PATH = 'global_combined_summary_table.csv'\nSUMMARY_COLUMNS = ['Attempt', 'Successful Runs', 'Failed Runs', 'Time Taken in Seconds', 'Mean Score', '95% CI Lower', '95% CI Upper', 'IQR Lower', 'IQR Upper', 'IQR as Pct of Mean']\nGLOBAL_SUMMARY_COLUMNS = ['Focus Area', 'Sentiment Adjective', 'Prompt'] + SUMMARY_COLUMNS\n\nneutral_score = lowest_possible_score + (highest_possible_score - lowest_possible_score) / 2\nscoring_scale_explanation = f\"on a scale from {lowest_possible_score} (strongly does NOT exhibit the adjective) to +{highest_possible_score} (strongly exhibits the adjective)-- so that {neutral_score} implies that nothing can be determined about the extent to which the adjective is reflected-- based on the contents of a sentence/paragraph/utterance.\"\nlogger.info(f\"Scoring scale explanation: {scoring_scale_explanation}\")\nif not os.path.exists(GLOBAL_SUMMARY_TABLE_PATH):\n    initial_global_summary_table = pd.DataFrame(columns=GLOBAL_SUMMARY_COLUMNS)\n    initial_global_summary_table.to_csv(GLOBAL_SUMMARY_TABLE_PATH, index=False)\n\ncombined_sentiment_analysis_dict = asyncio.run(analyze_focus_area_sentiments(focus_key, scoring_scale_explanation, source_text, model_name))  \nwith open('combined_sentiment_analysis.json', 'w') as file:\n    json.dump(combined_sentiment_analysis_dict, file, indent=4)\nprint(combined_sentiment_analysis_dict)\n"}
