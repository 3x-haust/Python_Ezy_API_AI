{"repo_info": {"repo_name": "megabots", "repo_owner": "momegas", "repo_url": "https://github.com/momegas/megabots"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/test_ui.py", "content": "import gradio as gr\nfrom megabots import create_interface\n\n\ndef test_create_interface():\n    # create a mock Bot object\n    class MockBot:\n        def ask(self, question: str):\n            return \"Answer\"\n\n    markdown = \"test\"\n\n    # call the function with the mock bot and example\n    interface = create_interface(MockBot(), markdown=markdown)\n\n    # check if the interface has the correct properties\n    assert isinstance(interface, gr.Blocks)\n"}
{"type": "test_file", "path": "tests/test_memory.py", "content": "from pytest import raises\nfrom megabots import memory\nfrom megabots.memory import ConversationBuffer, ConversationBufferWindow\n\n\ndef test_memory_conversation_buffer():\n    mem = memory(name=\"conversation-buffer\")\n    assert isinstance(mem, ConversationBuffer)\n\n\ndef test_memory_conversation_buffer_window():\n    mem = memory(name=\"conversation-buffer-window\", k=10)\n    assert isinstance(mem, ConversationBufferWindow)\n\n\ndef test_memory_unsupported_name():\n    with raises(ValueError, match=r\"Memory invalid-name is not supported.\"):\n        memory(name=\"invalid-name\")\n\n\ndef test_memory_no_name():\n    with raises(\n        RuntimeError, match=r\"Impossible to instantiate memory without a name.\"\n    ):\n        memory(name=None)\n"}
{"type": "test_file", "path": "tests/test_bots.py", "content": "import os\nimport tempfile\nfrom megabots import bot\nimport pickle\nfrom langchain.vectorstores.faiss import FAISS\n\n\n# Define test data\ntest_directory = \"./examples/files\"\ntest_question = \"what is megabots?\"\ncorrect_answer = \"state-of-the-art, production\"\nsources = \"SOURCES:\"\n\n\ndef test_ask():\n    qnabot = bot(\"qna-over-docs\", index=test_directory)\n    answer = qnabot.ask(test_question)\n\n    print(answer)\n\n    # Assert that the answer contains the correct answer\n    assert correct_answer in answer\n    # Assert that the answer contains the sources\n    assert sources not in answer\n\n\ndef test_save_load_index():\n    # Create a temporary directory and file path for the test index\n    with tempfile.TemporaryDirectory() as temp_dir:\n        index_path = os.path.join(temp_dir, \"test_index.pkl\")\n\n        # Create a bot and save the index to the temporary file path\n        qnabot = bot(\"qna-over-docs\", index=test_directory)\n        qnabot.save_index(index_path)\n\n        # Load the saved index and assert that it is the same as the original index\n        with open(index_path, \"rb\") as f:\n            saved_index = pickle.load(f)\n        assert isinstance(saved_index, FAISS)\n\n        bot_with_predefined_index = bot(\"qna-over-docs\", index=index_path)\n\n        # Assert that the bot returns the correct answer to the test question\n        assert correct_answer in bot_with_predefined_index.ask(test_question)\n"}
{"type": "test_file", "path": "tests/test_api.py", "content": "# def _ignore_warnings():\n#     import logging\n#     import warnings\n\n#     logging.captureWarnings(True)\n#     warnings.filterwarnings(\n#         \"ignore\",\n#         category=DeprecationWarning,\n#         message=\"Deprecated call to `pkg_resources.declare_namespace('google')`.\",\n#     )\n\n\n# _ignore_warnings()\n\n# import os\n# import signal\n# import subprocess\n\n# import requests\n# from requests.adapters import HTTPAdapter, Retry\n\n# megabot_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\n# def _session_with_retry() -> requests.Session:\n#     s = requests.Session()\n#     retries = Retry(\n#         total=50, backoff_factor=1, status_forcelist=[404, 500, 502, 503, 504]\n#     )\n#     s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n#     return s\n\n\n# class LCServeLocally:\n#     def __init__(self, port: int = 8000):\n#         self.port = port\n#         self.command = \" \".join(\n#             [\n#                 \"lc-serve\",\n#                 \"deploy\",\n#                 \"local\",\n#                 \"megabots.api\",\n#                 \"--port\",\n#                 str(self.port),\n#             ]\n#         )\n\n#     def __enter__(self):\n#         self.p = subprocess.Popen(\n#             self.command, cwd=megabot_dir, shell=True, preexec_fn=os.setsid\n#         )\n\n#     def __exit__(self, exc_type, exc_val, exc_tb):\n#         self.p.terminate()\n#         os.killpg(os.getpgid(self.p.pid), signal.SIGTERM)\n\n\n# def test_lcserve_successful():\n#     port = 8000\n#     lcserve_host = f\"http://localhost:{port}\"\n\n#     with LCServeLocally(port=port):\n#         resp = _session_with_retry().post(\n#             url=f\"{lcserve_host}/ask\",\n#             json={\"question\": \"What is your name?\"},\n#         )\n#         assert resp.status_code == 200\n#         assert \"result\" in resp.json()\n#         assert isinstance(resp.json()[\"result\"], str)\n\n\n# def test_lcserve_invalid_request():\n#     port = 8000\n#     lcserve_host = f\"http://localhost:{port}\"\n\n#     with LCServeLocally(port=port):\n#         resp = _session_with_retry().post(\n#             url=f\"{lcserve_host}/ask\",\n#             json={\"foo\": \"bar\"},\n#         )\n#         assert resp.status_code == 422\n#         assert \"detail\" in resp.json()\n#         assert resp.json()[\"detail\"] == [\n#             {\n#                 \"loc\": [\"body\", \"question\"],\n#                 \"msg\": \"field required\",\n#                 \"type\": \"value_error.missing\",\n#             }\n#         ]\n"}
{"type": "source_file", "path": "app.py", "content": "\"\"\"\nThis file is an example of what you can build with ðŸ¤–Megabots.\nIt is hosted here: https://huggingface.co/spaces/momegas/megabots\n\n\"\"\"\n\nfrom megabots import bot, create_interface\n\nprompt = \"\"\"\nYou are programming assistant that helps programmers develop apps with the Megabots library.\nUse the following pieces of context to answer the question at the end. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\nIf the question asks for python code you can provide it.\n\nContext:\n{context}\n\nConversation history:\n{history}\nHuman: {question}\nAI:\n\"\"\"\n\nqnabot = bot(\n    \"qna-over-docs\",\n    index=\"./examples/files\",\n    memory=\"conversation-buffer-window\",\n    prompt=prompt,\n)\n\n\ntext = \"\"\"\nYou can ask this bot anything about ðŸ¤–Megabots. Here are some examples:\n- What is Megabots?\n- How can I create a bot?\n- How can I change the prompt?\n- How can I create a bot that has memory and can connect to a milvus vector database?\n- How can I customise the bot function?\n- How can I an API out of my bot?\n- How can I an intrface out of my bot?\n- Where can i find the megabots repo?\n\"\"\"\n\niface = create_interface(qnabot, text)\niface.launch()\n"}
{"type": "source_file", "path": "megabots/api.py", "content": "import os\nfrom megabots import bot\nfrom megabots.utils import create_api\n\n# from lcserve import serving\n\ncur_dir = os.path.dirname(os.path.abspath(__file__))\nindex_dir = os.path.join(cur_dir, \"..\", \"examples\", \"files\")\n\n\nmybot = bot(\"qna-over-docs\", index=\"./index.pkl\")\n\n\n# @serving\n# def ask(question: str) -> str:\n#     return mybot.ask(question)\n\n\napp = create_api(mybot)\n"}
{"type": "source_file", "path": "megabots/utils.py", "content": "import gradio as gr\nfrom fastapi import FastAPI\nfrom megabots.bot import Bot\nfrom fastapi.openapi.utils import get_openapi\nfrom pydantic import BaseModel\n\n\ndef _custom_openapi(app: FastAPI, version: str):\n    if app.openapi_schema:\n        return app.openapi_schema\n\n    openapi_schema = get_openapi(\n        title=\"ðŸ¤– Megabots API\",\n        version=version,\n        description=\"Use this API to interact with the bot.\",\n        routes=app.routes,\n    )\n    return openapi_schema\n\n\nclass Answer(BaseModel):\n    text: str\n\n\ndef create_api(bot: Bot, version: str = \"0.0.1\"):\n    app = FastAPI()\n\n    @app.get(\n        \"/v1/bot/ask/{question}\",\n        tags=[\"Bot\"],\n        summary=\"Ask bot\",\n        description=\"Send question to the bot.\",\n        responses={200: {\"description\": \"Bot answer\"}},\n        response_model=Answer,\n    )\n    async def ask(question: str) -> Answer:\n        answer = bot.ask(question)\n        return Answer(text=answer)\n\n    app.openapi_schema = _custom_openapi(app, version)\n\n    return app\n\n\ndef create_interface(bot_instance: Bot, markdown: str = \"\"):\n    with gr.Blocks() as interface:\n        gr.Markdown(markdown)\n        chatbot = gr.Chatbot([], elem_id=\"chatbot\").style(height=450)\n        msg = gr.Textbox(\n            show_label=False,\n            placeholder=\"Enter text and press enter\",\n        ).style(container=False)\n\n        def user(user_message, history):\n            return \"\", history + [[user_message, None]]\n\n        def bot(history):\n            print(\"im here\")\n            response = bot_instance.ask(history[-1][0])\n            history[-1][1] = response\n            return history\n\n        msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n            bot, chatbot, chatbot\n        )\n\n    return interface\n"}
{"type": "source_file", "path": "megabots/memory.py", "content": "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n\n\nclass ConversationBuffer:\n    def __init__(self):\n        self.memory = ConversationBufferMemory(input_key=\"question\")\n\n\nclass ConversationBufferWindow:\n    def __init__(self, k: int):\n        self.k: int = k\n        self.memory = ConversationBufferWindowMemory(k=self.k, input_key=\"question\")\n\n\nSUPPORTED_MEMORY = {\n    \"conversation-buffer\": {\n        \"impl\": ConversationBuffer,\n        \"default\": {},\n    },\n    \"conversation-buffer-window\": {\n        \"impl\": ConversationBufferWindow,\n        \"default\": {\"k\": 3},\n    },\n}\n\n\nMemory = type(\"Memory\", (ConversationBuffer, ConversationBufferWindow), {})\n\n\ndef memory(\n    name: str = \"conversation-buffer-window\",\n    k: int | None = None,\n) -> Memory:\n    if name is None:\n        raise RuntimeError(\"Impossible to instantiate memory without a name.\")\n\n    if name not in SUPPORTED_MEMORY:\n        raise ValueError(f\"Memory {name} is not supported.\")\n\n    cl = SUPPORTED_MEMORY[name][\"impl\"]\n\n    if name == \"conversation-buffer-window\":\n        return cl(k=k or SUPPORTED_MEMORY[name][\"default\"][\"k\"])\n\n    return SUPPORTED_MEMORY[name][\"impl\"]()\n"}
{"type": "source_file", "path": "megabots/vectorstore.py", "content": "from typing import Type, TypeVar\nfrom langchain.vectorstores import Milvus\nfrom abc import ABC\n\n\nclass MilvusVectorStore:\n    def __init__(self, host: str, port: int):\n        self.host = host\n        self.port = port\n        self.client = Milvus\n\n\nclass ChromaVectorStore:\n    pass\n\n\n# Generic type variable for all vectorstores\nVectorStore = type(\"VectorStore\", (MilvusVectorStore, ChromaVectorStore), {})\n\n\nSUPPORTED_VECTORSTORES = {\n    \"milvus\": {\n        \"impl\": MilvusVectorStore,\n        \"default\": {\"host\": \"localhost\", \"port\": 19530},\n    }\n}\n\n\ndef vectorstore(\n    name: str, host: str | None = None, port: int | None = None\n) -> VectorStore:\n    \"\"\"Return a vectorstore object.\"\"\"\n\n    if name is None:\n        raise RuntimeError(\"Impossible to instantiate a vectorstore without a name.\")\n\n    if name not in SUPPORTED_VECTORSTORES:\n        raise ValueError(f\"Vectorstore {name} is not supported.\")\n\n    return SUPPORTED_VECTORSTORES[name][\"impl\"](\n        host=host or SUPPORTED_VECTORSTORES[name][\"default\"][\"host\"],\n        port=port or SUPPORTED_VECTORSTORES[name][\"default\"][\"port\"],\n    )\n"}
{"type": "source_file", "path": "megabots/prompt.py", "content": "from typing import List\nfrom langchain import PromptTemplate\n\nQNA_TEMPLATE = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\n{history}\nHuman: {question}\nAI:\"\"\"\n\nQA_MEMORY_PROMPT = PromptTemplate(\n    template=QNA_TEMPLATE, input_variables=[\"context\", \"history\", \"question\"]\n)\n\n\ndef prompt(template: str, variables: List[str]):\n    return PromptTemplate(template=template, input_variables=variables)\n"}
{"type": "source_file", "path": "megabots/__init__.py", "content": "from megabots.vectorstore import VectorStore, vectorstore\nfrom megabots.memory import Memory, memory\nfrom megabots.bot import Bot, bot\nfrom megabots.prompt import prompt\nfrom megabots.utils import create_api, create_interface\n\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n"}
{"type": "source_file", "path": "megabots/bot.py", "content": "from typing import Any\nfrom langchain.llms import OpenAI\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\nfrom langchain.vectorstores.faiss import FAISS\nimport pickle\nimport os\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.chains.conversational_retrieval.prompts import QA_PROMPT\nfrom langchain.document_loaders import DirectoryLoader\nfrom megabots.prompt import QA_MEMORY_PROMPT\nfrom megabots.vectorstore import VectorStore\nfrom megabots.memory import Memory\nimport megabots\n\n\nclass Bot:\n    def __init__(\n        self,\n        model: str | None = None,\n        prompt: PromptTemplate | None = None,\n        index: str | None = None,\n        sources: bool | None = False,\n        vectorstore: VectorStore | None = None,\n        memory: Memory | None = None,\n        verbose: bool = False,\n        temperature: int = 0,\n    ):\n        self.vectorstore = vectorstore\n        self.memory = memory\n        self.prompt = prompt or QA_MEMORY_PROMPT if self.memory else QA_PROMPT\n        self.select_model(model, temperature)\n        self.create_loader(index)\n        self.load_or_create_index(index, vectorstore)\n\n        # Load the question-answering chain for the selected model\n        self.chain = self.create_chain(sources=sources, verbose=verbose)\n\n    def create_chain(\n        self,\n        sources: bool | None = False,\n        verbose: bool = False,\n    ):\n        # TODO: Changing the prompt here is not working. Leave it as is for now.\n        # Reference: https://github.com/hwchase17/langchain/issues/2858\n        if sources:\n            return load_qa_with_sources_chain(\n                self.llm,\n                chain_type=\"stuff\",\n                memory=self.memory.memory if self.memory else None,\n                verbose=verbose,\n            )\n        return load_qa_chain(\n            self.llm,\n            chain_type=\"stuff\",\n            verbose=verbose,\n            prompt=self.prompt,\n            memory=self.memory.memory if self.memory else None,\n        )\n\n    def select_model(self, model: str | None, temperature: float):\n        # Select and set the appropriate model based on the provided input\n        if model is None or model == \"gpt-3.5-turbo\":\n            print(\"Using model: gpt-3.5-turbo\")\n            self.llm = ChatOpenAI(temperature=temperature)\n\n        if model == \"text-davinci-003\":\n            print(\"Using model: text-davinci-003\")\n            self.llm = OpenAI(temperature=temperature)\n\n    def create_loader(self, index: str | None):\n        # Create a loader based on the provided directory (either local or S3)\n        if index is None:\n            raise RuntimeError(\n                \"\"\"\n            Impossible to find a valid index. \n            Either provide a valid path to a pickle file or a directory.               \n            \"\"\"\n            )\n        self.loader = DirectoryLoader(index, recursive=True)\n\n    def load_or_create_index(self, index: str, vectorstore: VectorStore | None = None):\n        # Load an existing index from disk or create a new one if not available\n        if vectorstore is not None:\n            self.search_index = vectorstore.client.from_documents(\n                self.loader.load_and_split(),\n                OpenAIEmbeddings(),\n                connection_args={\"host\": vectorstore.host, \"port\": vectorstore.port},\n            )\n            return\n\n        # Is pickle\n        if index is not None and \"pkl\" in index or \"pickle\" in index:\n            print(\"Loading path from pickle file: \", index, \"...\")\n            with open(index, \"rb\") as f:\n                self.search_index = pickle.load(f)\n            return\n\n        # Is directory\n        if index is not None and os.path.isdir(index):\n            print(\"Creating index...\")\n            self.search_index = FAISS.from_documents(\n                self.loader.load_and_split(), OpenAIEmbeddings()\n            )\n            return\n\n        raise RuntimeError(\n            \"\"\"\n            Impossible to find a valid index. \n            Either provide a valid path to a pickle file or a directory.               \n            \"\"\"\n        )\n\n    def save_index(self, index_path: str):\n        # Save the index to the specified path\n        with open(index_path, \"wb\") as f:\n            pickle.dump(self.search_index, f)\n\n    def ask(self, question: str, k=1) -> str:\n        # Retrieve the answer to the given question and return it\n        input_documents = self.search_index.similarity_search(question, k=k)\n        answer = self.chain.run(input_documents=input_documents, question=question)\n        return answer\n\n\nSUPPORTED_TASKS = {\n    \"qna-over-docs\": {\n        \"impl\": Bot,\n        \"default\": {\n            \"model\": \"gpt-3.5-turbo\",\n            \"temperature\": 0,\n            \"index\": \"./index\",\n            \"input_variables\": [\"context\", \"question\"],\n        },\n    }\n}\n\nSUPPORTED_MODELS = {}\n\n\ndef bot(\n    task: str | None = None,\n    *,\n    model: str | None = None,\n    index: str | None = None,\n    prompt: str | None = None,\n    memory: str | Memory | None = None,\n    vectorstore: str | VectorStore | None = None,\n    verbose: bool = False,\n    temperature: int = 0,\n) -> Bot:\n    \"\"\"Instanciate a bot based on the provided task. Each supported tasks has it's own default sane defaults.\n\n    Args:\n        task (str | None, optional): The given task. Can be one of the SUPPORTED_TASKS.\n\n        model (str | None, optional): Model to be used. Can be one of the SUPPORTED_MODELS.\n\n        index (str | None, optional): Data that the model will load and store index info.\n        Can be either a local file path, a pickle file, or a url of a vector database.\n        By default it will look for a local directory called \"files\" in the current working directory.\n\n        prompt (str | None, optional): The prompt that the bot will take in. Mark variables like this: {variable}.\n        Variables are context, question, and history if the bot has memory.\n\n        vectorstore: (str | VectorStore | None, optional): The vectorstore that the bot will save the index to.\n        If only a string is passed, the defaults values willl be used.\n\n        verbose (bool, optional): Verbocity. Defaults to False.\n\n        temperature (int, optional): Temperature. Defaults to 0.\n\n    Raises:\n        RuntimeError: _description_\n        ValueError: _description_\n\n    Returns:\n        Bot: Bot instance\n    \"\"\"\n\n    if task is None:\n        raise RuntimeError(\"Impossible to instantiate a bot without a task.\")\n    if task not in SUPPORTED_TASKS:\n        raise ValueError(f\"Task {task} is not supported.\")\n\n    task_defaults = SUPPORTED_TASKS[task][\"default\"]\n\n    if memory is not None:\n        task_defaults[\"input_variables\"].append(\"history\")\n\n    return SUPPORTED_TASKS[task][\"impl\"](\n        model=model or task_defaults[\"model\"],\n        index=index or task_defaults[\"index\"],\n        prompt=None\n        if prompt is None\n        else PromptTemplate(\n            template=prompt, input_variables=task_defaults[\"input_variables\"]\n        ),\n        temperature=temperature,\n        verbose=verbose,\n        vectorstore=megabots.vectorstore(vectorstore)\n        if isinstance(vectorstore, str)\n        else vectorstore,\n        memory=megabots.memory(memory) if isinstance(memory, str) else memory,\n    )\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\nVERSION = \"0.0.11\"\n\nsetup(\n    name=\"megabots\",\n    version=VERSION,\n    packages=find_packages(),\n    install_requires=[\n        \"langchain\",\n        \"tiktoken\",\n        \"unstructured\",\n        \"fastapi\",\n        \"faiss-cpu\",\n        \"pdfminer.six\",\n        \"gradio\",\n        \"python-dotenv\",\n        \"openai\",\n        \"langchain-serve\",\n    ],\n    author=\"Megaklis Vasilakis\",\n    author_email=\"megaklis.vasilakis@gmail.com\",\n    description=\"ðŸ¤– Megabots provides State-of-the-art, production ready bots made mega-easy, so you don't have to build them from scratch ðŸ¤¯ Create a bot, now ðŸ«µ\",\n    long_description=open(\"README.md\").read(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/momegas/megabots\",\n    keywords=[\n        \"bot\",\n        \"qna-bot\",\n        \"information-retrieval\",\n        \"chatbot\",\n        \"question-answering\",\n        \"prompt-engineering\",\n    ],\n    license=\"MIT\",\n    classifiers=[\n        # Choose appropriate classifiers from\n        # https://pypi.org/classifiers/\n        \"Development Status :: 4 - Beta\"\n    ],\n)\n"}
