{"repo_info": {"repo_name": "aurora", "repo_owner": "W3ndige", "repo_url": "https://github.com/W3ndige/aurora"}}
{"type": "test_file", "path": "tests/__init__.py", "content": ""}
{"type": "test_file", "path": "tests/api/test_api_sample.py", "content": "from tests.utils import upload_file, add_minhash, add_string\n\n\ndef test_add_sample(client, monkeypatch):\n    def mock_push_sample(a, b, c):\n        return None\n\n    def mock_push_ssdeep(a, b, c):\n        return None\n\n    monkeypatch.setattr(\"aurora.core.karton.push_file\", mock_push_sample)\n    monkeypatch.setattr(\"aurora.core.karton.push_ssdeep\", mock_push_ssdeep)\n\n    file = upload_file()\n    response = client.post(\n        \"/api/v1/sample/\", files={\"file\": (file[\"filename\"], file[\"content\"])}\n    ).json()\n\n    assert response[\"filename\"] == file[\"filename\"]\n    assert response[\"sha256\"] == file[\"sha256\"]\n    assert response[\"filesize\"] == len(file[\"content\"])\n\n\ndef test_get_sample(client):\n    file = upload_file()\n\n    response = client.get(f\"/api/v1/sample/{file['sha256']}\").json()\n\n    assert response[\"filename\"] == file[\"filename\"]\n    assert response[\"sha256\"] == file[\"sha256\"]\n    assert response[\"filesize\"] == len(file[\"content\"])\n\n\ndef test_add_minhash(client):\n    file = upload_file()\n    minhash = add_minhash()\n    minhash.update({\"minhash_type\": \"strings\"})\n\n    response = client.post(\n        f\"api/v1/sample/{file['sha256']}/minhash\", json=minhash\n    ).json()\n\n    assert response[\"minhash_type\"] == minhash[\"minhash_type\"]\n    assert response[\"seed\"] == minhash[\"seed\"]\n    assert response[\"hash_values\"] == minhash[\"hash_values\"]\n\n\ndef test_get_sample_minhashes(client):\n    file = upload_file()\n\n    response = client.post(f\"api/v1/sample/{file['sha256']}/minhash\").json()\n\n    assert len(response) == 1\n\n\ndef test_add_minhash_unknown_sample(client):\n    # Unknown file\n    file = upload_file(content=\"Unknown content\")\n    minhash = add_minhash()\n    minhash.update({\"minhash_type\": \"strings\"})\n\n    response = client.post(f\"api/v1/sample/{file['sha256']}/minhash\", json=minhash)\n\n    assert response.status_code == 404\n\n\ndef test_get_ssdeep(client):\n    file = upload_file()\n\n    response = client.get(f\"/api/v1/sample/{file['sha256']}/ssdeep\").json()\n\n    assert response[\"ssdeep\"] == file[\"ssdeep\"]\n\n\ndef test_add_string(client):\n    file = upload_file()\n    string = add_string()\n\n    response = client.post(f\"api/v1/sample/{file['sha256']}/string\", json=string).json()\n\n    assert response[\"value\"] == string[\"value\"]\n    assert response[\"sha256\"] == string[\"sha256\"]\n    assert response[\"heuristic\"] == string[\"heuristic\"]\n\n\ndef test_add_string_incorrect_sha256(client):\n    file = upload_file()\n    string = add_string()\n    modified_string = add_string(string=\"Modified string\")\n\n    modified_string[\"value\"] = string[\"value\"]\n\n    response = client.post(\n        f\"api/v1/sample/{file['sha256']}/string\", json=modified_string\n    ).json()\n\n    assert response[\"value\"] == string[\"value\"]\n    assert response[\"sha256\"] == string[\"sha256\"]\n    assert response[\"heuristic\"] == string[\"heuristic\"]\n"}
{"type": "test_file", "path": "tests/api/test_api_relations.py", "content": "import random\n\nfrom tests.utils import upload_file\n\n\ndef test_add_relation(client, monkeypatch):\n    file_1 = upload_file(content=\"File_1\")\n    file_2 = upload_file(content=\"File_2\")\n\n    def mock_push_sample(a, b, c):\n        return None\n\n    def mock_push_ssdeep(a, b, c):\n        return None\n\n    monkeypatch.setattr(\"aurora.core.karton.push_file\", mock_push_sample)\n    monkeypatch.setattr(\"aurora.core.karton.push_ssdeep\", mock_push_ssdeep)\n\n    parent_response = client.post(\n        \"/api/v1/sample/\", files={\"file\": (file_1[\"filename\"], file_1[\"content\"])}\n    ).json()\n    parent_id = parent_response[\"id\"]\n\n    child_response = client.post(\n        \"/api/v1/sample/\", files={\"file\": (file_2[\"filename\"], file_2[\"content\"])}\n    ).json()\n    child_id = child_response[\"id\"]\n\n    relation = {\n        \"parent_sha256\": file_1[\"sha256\"],\n        \"child_sha256\": file_2[\"sha256\"],\n        \"type\": \"test\",\n        \"confidence\": random.uniform(0.0, 1.0),\n    }\n\n    response = client.post(\"/api/v1/relation/\", json=relation).json()\n\n    assert response[\"parent_id\"] == parent_id\n    assert response[\"child_id\"] == child_id\n    assert response[\"confidence\"] == relation[\"confidence\"]\n\n\ndef test_add_relation_invalid_confidence(client):\n    file_1 = upload_file(content=\"File_1\")\n    file_2 = upload_file(content=\"File_2\")\n\n    relation = {\n        \"parent_sha256\": file_1[\"sha256\"],\n        \"child_sha256\": file_2[\"sha256\"],\n        \"type\": \"test\",\n        \"confidence\": 1.1,\n    }\n\n    response = client.post(\"/api/v1/relation/\", json=relation)\n\n    assert response.status_code == 400\n\n\ndef test_relation_by_parent(client):\n    file_1 = upload_file(content=\"File_1\")\n    file_2 = upload_file(content=\"File_2\")\n\n    file_2_id = client.get(f\"/api/v1/sample/{file_2['sha256']}\").json()[\"id\"]\n\n    response = client.get(f\"/api/v1/relation/parent/{file_1['sha256']}\").json()\n\n    assert len(response) == 1\n    assert response[0][\"child_id\"] == file_2_id\n\n\ndef test_relation_by_child(client):\n    file_1 = upload_file(content=\"File_1\")\n    file_2 = upload_file(content=\"File_2\")\n\n    file_1_id = client.get(f\"/api/v1/sample/{file_1['sha256']}\").json()[\"id\"]\n\n    response = client.get(f\"/api/v1/relation/child/{file_2['sha256']}\").json()\n\n    assert len(response) == 1\n    assert response[0][\"parent_id\"] == file_1_id\n"}
{"type": "test_file", "path": "tests/conftest.py", "content": "import pytest\n\nfrom sqlalchemy import create_engine\nfrom fastapi.testclient import TestClient\nfrom sqlalchemy.orm import Session, sessionmaker\n\nfrom aurora.app import app\nfrom aurora.database import get_db, Base\nfrom aurora.config import DATABASE_URL, POSTGRES_DB\n\ndefault_engine = create_engine(DATABASE_URL)\n\nDB_URL = f\"{DATABASE_URL}_test\"\n\nwith default_engine.connect() as default_conn:\n    default_conn.execution_options(isolation_level=\"AUTOCOMMIT\").execute(\n        f\"DROP DATABASE IF EXISTS {POSTGRES_DB}_test\"\n    )\n    default_conn.execution_options(isolation_level=\"AUTOCOMMIT\").execute(\n        f\"CREATE DATABASE {POSTGRES_DB}_test\"\n    )\n\nengine = create_engine(f\"{DATABASE_URL}_test\")\n\nTestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n\nBase.metadata.drop_all(bind=engine)\nBase.metadata.create_all(bind=engine)\n\n\n@pytest.fixture(scope=\"session\")\ndef db_fixture() -> Session:\n    try:\n        db = TestingSessionLocal()\n        yield db\n    finally:\n        db.close()\n\n\n@pytest.fixture(scope=\"session\")\ndef client(db_fixture) -> TestClient:\n    def _get_db_override():\n        return db_fixture\n\n    app.dependency_overrides[get_db] = _get_db_override\n    return TestClient(app)\n"}
{"type": "test_file", "path": "tests/utils.py", "content": "import os\nimport ssdeep\nimport hashlib\nimport datasketch\n\nfrom typing import Dict, Any\n\n\ndef upload_file(filename: str = None, content: str = None) -> Dict[str, str]:\n    if not filename:\n        filename = \"filename\"\n\n    if not content:\n        content = \"content\"\n\n    ssdeep_object = ssdeep.Hash()\n    ssdeep_object.update(content.encode(\"utf-8\"))\n    ssdeep_hash = ssdeep_object.digest()\n\n    sha256_object = hashlib.sha256()\n    sha256_object.update(content.encode(\"utf-8\"))\n    sha256 = sha256_object.hexdigest()\n\n    return {\n        \"filename\": filename,\n        \"content\": content,\n        \"sha256\": sha256,\n        \"ssdeep\": ssdeep_hash,\n    }\n\n\ndef add_minhash() -> Dict[str, Any]:\n    minhash = datasketch.MinHash()\n\n    random_bytes = os.urandom(256)\n    minhash.update(random_bytes)\n\n    lean_minhash = datasketch.LeanMinHash(minhash)\n\n    return {\"seed\": lean_minhash.seed, \"hash_values\": lean_minhash.hashvalues.tolist()}\n\n\ndef add_string(string: str = None, heuristic: str = None) -> Dict[str, str]:\n    if not string:\n        string = \"string\"\n\n    if not heuristic:\n        heuristic = \"heuristic\"\n\n    sha256_object = hashlib.sha256()\n    sha256_object.update(string.encode(\"utf-8\"))\n    sha256 = sha256_object.hexdigest()\n\n    return {\"value\": string, \"sha256\": sha256, \"heuristic\": heuristic}\n"}
{"type": "source_file", "path": "aurora/api/relation.py", "content": "from typing import List, Optional\nfrom fastapi import APIRouter, Depends, HTTPException\n\nfrom aurora.database import get_db, queries, schemas\n\nrouter = APIRouter(\n    prefix=\"/relation\",\n    tags=[\"relation\"],\n)\n\n\n@router.get(\"/\", response_model=List[schemas.Relation])\ndef get_relations(\n    relation_type: Optional[str] = None,\n    confidence: Optional[float] = None,\n    db=Depends(get_db),\n):\n    filters = schemas.RelationFilter(relation_type=relation_type, confidence=confidence)\n\n    return queries.relation.get_relations(db, filters)\n\n\n@router.post(\"/\", response_model=schemas.Relation)\ndef add_relation(relation_input: schemas.InputRelation, db=Depends(get_db)):\n    if relation_input.confidence < 0 or relation_input.confidence > 1:\n        raise HTTPException(\n            status_code=400,\n            detail=f\"Confidence must be >= 0 or <= 1. Got {relation_input.confidence}\",\n        )\n\n    parent = queries.sample.get_sample_by_sha256(db, relation_input.parent_sha256)\n    child = queries.sample.get_sample_by_sha256(db, relation_input.child_sha256)\n    if not parent:\n        raise HTTPException(\n            status_code=404, detail=f\"Parent {relation_input.parent_sha256} not found.\"\n        )\n\n    if not child:\n        raise HTTPException(\n            status_code=404, detail=f\"Child {relation_input.child_sha256} not found.\"\n        )\n\n    relation = queries.relation.get_exact_relation(\n        db, parent, child, relation_input.type\n    )\n    if not relation:\n        relation = queries.relation.add_relation(\n            db, parent, child, relation_input.type, relation_input.confidence\n        )\n        db.commit()\n\n    return relation\n\n\n@router.get(\"/parent/{sha256}\", response_model=List[schemas.Relation])\ndef get_relations_by_parent(\n    sha256: str,\n    relation_type: Optional[str] = None,\n    confidence: Optional[float] = None,\n    db=Depends(get_db),\n):\n\n    filters = schemas.RelationFilter(relation_type=relation_type, confidence=confidence)\n\n    parent = queries.sample.get_sample_by_sha256(db, sha256)\n    if not parent:\n        raise HTTPException(status_code=404, detail=f\"Parent {sha256} not found.\")\n\n    return queries.relation.get_relations_by_parent(db, parent, filters)\n\n\n@router.get(\"/child/{sha256}\", response_model=List[schemas.Relation])\ndef get_relations_by_child(\n    sha256: str,\n    relation_type: Optional[str] = None,\n    confidence: Optional[float] = None,\n    db=Depends(get_db),\n):\n\n    filters = schemas.RelationFilter(relation_type=relation_type, confidence=confidence)\n\n    child = queries.sample.get_sample_by_sha256(db, sha256)\n    if not child:\n        raise HTTPException(status_code=404, detail=f\"Child {sha256} not found.\")\n\n    return queries.relation.get_relations_by_child(db, child, filters)\n\n\n@router.get(\"/{sha256}\", response_model=List[schemas.Relation])\ndef get_relations_by_hash(\n    sha256: str,\n    relation_type: Optional[str] = None,\n    confidence: Optional[float] = None,\n    db=Depends(get_db),\n):\n    filters = schemas.RelationFilter(relation_type=relation_type, confidence=confidence)\n\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return queries.relation.get_relations_by_hash(db, sample, filters)\n"}
{"type": "source_file", "path": "aurora/api/string.py", "content": "from typing import List\nfrom fastapi import APIRouter, Depends\n\nfrom aurora.database import get_db, queries, schemas\n\nrouter = APIRouter(\n    prefix=\"/string\",\n    tags=[\"string\"],\n)\n\n\n@router.get(\"/\", response_model=List[schemas.String])\ndef get_strings(db=Depends(get_db)):\n    return queries.string.get_strings(db)\n\n\n@router.get(\"/{sha256}\", response_model=schemas.String)\ndef get_string(sha256: str, db=Depends(get_db)):\n    string = queries.string.get_string_by_sha256(db, sha256)\n\n    return string\n"}
{"type": "source_file", "path": "aurora/config.py", "content": "from starlette.config import Config\nfrom starlette.datastructures import Secret\n\n\nconfig = Config(\".env\")\nPROJECT_NAME = \"aurora\"\nVERSION = \"1.0.0\"\nAPI_PREFIX = \"/api/v1\"\n\nPOSTGRES_USER = config(\"POSTGRES_USER\", cast=str)\nPOSTGRES_PASSWORD = config(\"POSTGRES_PASSWORD\", cast=Secret)\nPOSTGRES_SERVER = config(\"POSTGRES_SERVER\", cast=str, default=\"localhost\")\nPOSTGRES_PORT = config(\"POSTGRES_PORT\", cast=str, default=\"5432\")\nPOSTGRES_DB = config(\"POSTGRES_DB\", cast=str, default=PROJECT_NAME)\n\nDATABASE_URL = config(\n    \"DATABASE_URL\",\n    cast=str,\n    default=f\"postgresql://{POSTGRES_USER}:{POSTGRES_PASSWORD}@{POSTGRES_SERVER}:{POSTGRES_PORT}/{POSTGRES_DB}\",\n)\n"}
{"type": "source_file", "path": "aurora/core/search.py", "content": "from enum import Enum\nfrom functools import partial\nfrom typing import Tuple, Optional\n\nfrom aurora.database import queries\n\n\nclass SampleSearch(partial, Enum):\n    MD5 = partial(queries.sample.get_sample_by_md5)\n    SHA1 = partial(queries.sample.get_sample_by_sha1)\n    SHA256 = partial(queries.sample.get_sample_by_sha256)\n    SHA512 = partial(queries.sample.get_sample_by_sha512)\n\n    def __call__(self, *args, **kwargs):\n        return self.value(args[0], args[1])\n\n\nclass StringSearch(partial, Enum):\n    SHA256 = partial(queries.string.get_string_by_sha256)\n    VALUE = partial(queries.string.get_string_by_value)\n\n    def __call__(self, *args, **kwargs):\n        return self.value(args[0], args[1])\n\n\ndef prepare_search(query: str) -> Tuple[str, str]:\n    query = query.replace(\" \", \"\")\n    prefix, term = query.split(\":\", 1)\n    prefix = prefix.lower()\n\n    return prefix, term\n\n\ndef sample_search(db, attribute: str, term: str) -> Optional[str]:\n    sample = None\n    term = term.lower()\n    if attribute == \"md5\":\n        sample = SampleSearch.MD5(db, term)\n    elif attribute == \"sha1\":\n        sample = SampleSearch.SHA1(db, term)\n    elif attribute == \"sha256\":\n        sample = SampleSearch.SHA256(db, term)\n    elif attribute == \"sha512\":\n        sample = SampleSearch.SHA512(db, term)\n\n    return sample\n\n\ndef string_search(db, attribute: str, term: str) -> Optional[str]:\n    string = None\n    if attribute == \"sha256\":\n        term = term.lower()\n        string = StringSearch.SHA256(db, term)\n    elif attribute == \"value\":\n        string = StringSearch.VALUE(db, term)\n\n    return string\n"}
{"type": "source_file", "path": "aurora/database/alembic/env.py", "content": "from logging.config import fileConfig\n\nfrom sqlalchemy import engine_from_config\nfrom sqlalchemy import pool\n\nfrom alembic import context\n\nfrom aurora.database import Base\nfrom aurora.config import DATABASE_URL\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nfileConfig(config.config_file_name)\n\nconfig.set_main_option(\"sqlalchemy.url\", DATABASE_URL)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = Base.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline():\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(connection=connection, target_metadata=target_metadata)\n\n        with context.begin_transaction():\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n"}
{"type": "source_file", "path": "aurora/api/ssdeep.py", "content": "import ssdeep  # type: ignore\n\nfrom typing import List, Optional\nfrom fastapi import APIRouter, Depends, HTTPException\n\nfrom aurora.database import get_db, queries, schemas\n\nrouter = APIRouter(\n    prefix=\"/ssdeep\",\n    tags=[\"ssdeep\"],\n)\n\n\n@router.get(\"/\", response_model=List[schemas.SsDeep])\ndef get_ssdeep_hashes(chunksize: Optional[int] = None, db=Depends(get_db)):\n    return queries.ssdeep.get_ssdeep_hashes(db, chunksize=chunksize)\n\n\n@router.post(\"/compare\", response_model=float)\ndef compare_ssdeep(s1: str, s2: str):\n    try:\n        coefficient = ssdeep.compare(s1, s2) / 100.0\n    except ssdeep.InternalError:\n        raise HTTPException(status_code=500, detail=\"SsDeep compare failed.\")\n\n    return coefficient\n"}
{"type": "source_file", "path": "aurora/app.py", "content": "from fastapi import FastAPI, APIRouter\nfrom fastapi.staticfiles import StaticFiles\n\nfrom aurora.api import sample\nfrom aurora.api import minhash\nfrom aurora.api import relation\nfrom aurora.api import ssdeep\nfrom aurora.api import string\n\nfrom aurora.front import front\n\napi_v1 = APIRouter()\n\napi_v1.include_router(sample.router)\napi_v1.include_router(minhash.router)\napi_v1.include_router(relation.router)\napi_v1.include_router(ssdeep.router)\napi_v1.include_router(string.router)\n\napp = FastAPI()\n\napp.mount(\"/static\", StaticFiles(directory=\"aurora/front/static\"), name=\"static\")\n\napp.include_router(api_v1, prefix=\"/api/v1\")\napp.include_router(front.router, prefix=\"\")\n"}
{"type": "source_file", "path": "aurora/api/sample.py", "content": "import logging\nimport hashlib\n\nfrom typing import List, Optional\nfrom fastapi import APIRouter, UploadFile, File, Depends, HTTPException\n\nfrom aurora.core import karton\nfrom aurora.core.utils import get_sha256\nfrom aurora.database import get_db, queries, schemas\n\nlogger = logging.getLogger(__name__)\n\nrouter = APIRouter(\n    prefix=\"/sample\",\n    tags=[\"sample\"],\n)\n\n\n@router.get(\"/\")\ndef get_samples(db=Depends(get_db)):\n    return queries.sample.get_samples(db)\n\n\n@router.post(\"/\", response_model=schemas.Sample)\ndef add_sample(file: UploadFile = File(...), db=Depends(get_db)):\n    sha256 = get_sha256(file.file)\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if sample:\n        return sample\n\n    sample = queries.sample.add_sample(db, file)\n    if not sample.ssdeep:\n        ssdeep = queries.ssdeep.add_ssdeep(db, file)\n        sample.ssdeep = ssdeep\n\n        try:\n            karton.push_ssdeep(sample.sha256, ssdeep.chunksize, ssdeep.ssdeep)\n        except RuntimeError:\n            logger.exception(f\"Couldn't push ssdeep from {sample.sha256} to karton.\")\n\n    db.commit()\n\n    try:\n        karton.push_file(file, sample.filetype, sample.sha256)\n    except RuntimeError:\n        logger.exception(f\"Couldn't push sample {sample.sha256} to karton\")\n\n    return sample\n\n\n@router.get(\"/{sha256}\", response_model=schemas.Sample)\ndef get_sample(sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return sample\n\n\n@router.post(\"/{sha256}/reanalyze\")\ndef reanalyze(sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    if sample.minhashes:\n        for minhash in sample.minhashes:\n            karton.push_minhash(\n                sha256, minhash.seed, minhash.hash_values, minhash.minhash_type\n            )\n\n    if sample.ssdeep:\n        karton.push_ssdeep(sha256, sample.ssdeep.chunksize, sample.ssdeep.ssdeep)\n\n    return \"OK\"\n\n\n@router.post(\"/{sha256}/minhash\", response_model=schemas.Minhash)\ndef add_minhash(sha256: str, minhash: schemas.InputMinhash, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    if sample.minhashes:\n        if any(x.minhash_type == minhash.minhash_type for x in sample.minhashes):\n            raise HTTPException(\n                status_code=500,\n                detail=f\"Sample {sha256} already contains {minhash.minhash_type} minhash.\",\n            )\n\n    new_minhash = queries.minhash.add_minhash(\n        db, minhash.seed, minhash.hash_values, minhash.minhash_type\n    )\n    sample.minhashes.append(new_minhash)\n    db.commit()\n\n    return new_minhash\n\n\n@router.get(\"/{sha256}/minhash\", response_model=List[schemas.Minhash])\ndef get_minhashes(sha256: str, minhash_type: Optional[str] = None, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return queries.minhash.get_sample_minhash(db, sample, minhash_type)\n\n\n@router.get(\"/{sha256}/ssdeep\", response_model=schemas.SsDeep)\ndef get_ssdeep(sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return sample.ssdeep\n\n\n@router.post(\"/{sha256}/string\", response_model=schemas.String)\ndef add_string(sha256: str, string: schemas.InputString, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    sha256_object = hashlib.sha256()\n    sha256_object.update(string.value.encode(\"utf-8\"))\n    sha256 = sha256_object.hexdigest()\n\n    db_string = queries.string.add_string(db, string.value, sha256, string.heuristic)\n\n    sample.strings.append(db_string)\n    db.commit()\n\n    return db_string\n\n\n@router.get(\"/{sha256}/string\", response_model=List[schemas.String])\ndef get_strings(sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return sample.strings\n\n\n@router.get(\"/{sha256}/parents\", response_model=List[schemas.Sample])\ndef get_parents(sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return list(queries.sample.get_sample_parents(db, sample))\n\n\n@router.get(\"/{sha256}/children\", response_model=List[schemas.Sample])\ndef get_children(sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return list(queries.sample.get_sample_children(db, sample))\n\n\n@router.get(\"/{sha256}/related\", response_model=List[schemas.Sample])\ndef get_related(sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    return list(queries.sample.get_sample_related(db, sample))\n"}
{"type": "source_file", "path": "aurora/database/__init__.py", "content": "from sqlalchemy import create_engine\nfrom sqlalchemy.engine import Engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.ext.declarative import declarative_base\n\nfrom aurora.config import DATABASE_URL\n\n\nengine: Engine = create_engine(DATABASE_URL)\n\ndb_session: sessionmaker = sessionmaker(bind=engine, autocommit=False, autoflush=False)\n\nBase = declarative_base(name=\"Base\")\n\nfrom .models import Relation  # noqa E402, E401\nfrom .models import Sample  # noqa E402, E401\nfrom .models import Minhash  # noqa E402, E401\nfrom .models import SsDeep  # noqa E402, E401\nfrom .models import String  # noqa E402, E401\n\n\ndef get_db():\n    db = db_session()\n    try:\n        yield db\n    finally:\n        db.close()\n\n\n__all__ = [\"db_session\", \"get_db\", \"Base\"]\n"}
{"type": "source_file", "path": "aurora/core/network.py", "content": "from typing import List, Tuple\n\nfrom aurora.database import models\n\n\ndef prepare_large_graph(relations: List[models.Relation]) -> Tuple[List, List]:\n    nodes = {}\n    edges = []\n    for relation in relations:\n        nodes[relation.parent_id] = {\n            \"id\": relation.parent_id,\n            \"label\": relation.parent.filename,\n            \"shape\": \"dot\",\n        }\n\n        nodes[relation.child_id] = {\n            \"id\": relation.child_id,\n            \"label\": relation.child.filename,\n            \"shape\": \"dot\",\n        }\n\n        edges.append(\n            {\n                \"from\": relation.parent_id,\n                \"to\": relation.child_id,\n            }\n        )\n\n    return (list(nodes.values()), edges)\n\n\ndef prepare_sample_graph(relations: List[models.Relation]) -> Tuple[List, List]:\n    nodes = {}\n    edges = []\n    for relation in relations:\n        nodes[relation.parent_id] = {\n            \"id\": relation.parent_id,\n            \"label\": relation.parent.filename,\n            \"shape\": \"dot\",\n        }\n\n        nodes[relation.child_id] = {\n            \"id\": relation.child_id,\n            \"label\": relation.child.filename,\n            \"shape\": \"dot\",\n        }\n\n        edges.append(\n            {\n                \"from\": relation.parent_id,\n                \"to\": relation.child_id,\n            }\n        )\n\n    return (list(nodes.values()), edges)\n"}
{"type": "source_file", "path": "aurora/__init__.py", "content": ""}
{"type": "source_file", "path": "aurora/api/__init__.py", "content": ""}
{"type": "source_file", "path": "aurora/core/karton.py", "content": "\"\"\"Helper functions to interact with Karton pipeline.\n\nThis module exhibits helper functions that allows to interact with the Karton pipeline,\npushing elements for further analysis..\n\n\nAttributes:\n    config: (Config): Karton config.\n    producer (Producer): Karton producer used to send tasks to the pipeline.\n\"\"\"\n\nimport os\n\nfrom typing import List\nfrom fastapi import UploadFile\nfrom karton.core import Config, Producer, Task, Resource\n\ndirectory = os.path.dirname(__file__)\nos.path.join(directory, \"logger.conf\")\n\nconfig = Config(os.path.join(directory, \"../../karton.ini\"))\nproducer = Producer(config)\n\n\ndef push_file(file: UploadFile, magic: str, sha256: str) -> None:\n    \"\"\"Push file to the Karton pipeline.\n\n    Creates a Karton task with the content of passed file as a Resource. Sends that task using Producer.\n\n    Args:\n        file (UploadFile): File to be uploaded to pipeline.\n        magic (str): Magic value of the passed file, used to assign task to different kartons.\n        sha256 (str): SHA256 hash of the passed file.\n\n    Returns:\n        None\n    \"\"\"\n\n    file.file.seek(0, 0)\n\n    filename = file.filename\n    content = file.file.read()\n\n    resource = Resource(filename, content, sha256=sha256)\n\n    task = Task({\"type\": \"sample\", \"kind\": \"raw\"})\n\n    task.add_payload(\"sample\", resource)\n    task.add_payload(\"magic\", magic)\n\n    producer.send_task(task)\n\n\ndef push_minhash(\n    sha256: str, seed: int, hash_values: List[int], minhash_type: str\n) -> None:\n    \"\"\"Push minhash to the Karton pipeline.\n\n    Creates a Karton task with the minhash contents as payload values.\n\n    Args:\n        sha256 (str): SHA256 hash of the owner sample.\n        seed (int): Minhash seed.\n        hash_values (List(int)): List of hash values of the minhash.\n        minhash_type (str): Type of minhash.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    task = Task({\"type\": \"feature\", \"stage\": \"minhash\", \"kind\": minhash_type})\n\n    task.add_payload(\"sha256\", sha256)\n    task.add_payload(\"seed\", seed)\n    task.add_payload(\"hash_values\", hash_values)\n\n    producer.send_task(task)\n\n\ndef push_ssdeep(sha256: str, chunksize: int, ssdeep: str) -> None:\n    \"\"\"Push ssdeep to the Karton pipeline.\n\n    Creates a Karton task with the ssdeep contents as payload values.\n\n    Args:\n        sha256 (str): SHA256 hash of the owner sample.\n        chunksize (int): Chunksize part of the ssdeep hash.\n        ssdeep (List(int)): Whole ssdeep hash.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    task = Task({\"type\": \"feature\", \"stage\": \"ssdeep\"})\n\n    task.add_payload(\"sha256\", sha256)\n    task.add_payload(\"chunksize\", chunksize)\n    task.add_payload(\"ssdeep\", ssdeep)\n\n    producer.send_task(task)\n"}
{"type": "source_file", "path": "aurora/core/utils.py", "content": "\"\"\"Basic utils for extracting common information from file.\n\nThis module exhibits functions for extracting hashes from the file stream.\n\"\"\"\n\n\nimport os\nimport magic  # type: ignore\nimport ssdeep  # type: ignore\nimport hashlib\n\nfrom typing import IO\n\n\ndef get_magic(stream: IO, mimetype: bool = False) -> str:\n    \"\"\"Get magic value from the stream.\n\n    Extracts the magic of the passed stream using `python-magic` module.\n\n    Args:\n        stream (IO): File stream.\n        mimetype (bool)): Decides if the magic or mimetype is extracted. Defaults to `false`.\n\n    Returns:\n        str: Magic value of the passed stream.\n    \"\"\"\n\n    stream.seek(0, os.SEEK_SET)\n    return magic.from_buffer(stream.read(), mime=mimetype)\n\n\ndef get_hash(stream: IO, hash_obj, digest_func) -> str:\n    \"\"\"Helper functions for calculating hash_obj digest based on passed parameters.\n\n    Helper function that accepts hash object and digest function to calculate digest value of a stream.\n\n    Args:\n        stream (IO): File stream.\n        mimetype (bool)): Decides if the magic or mimetype is extracted. Defaults to `false`.\n\n    Returns:\n        str: Return value of digest function on the hash object.\n    \"\"\"\n\n    stream.seek(0, os.SEEK_SET)\n\n    for chunk in iter(lambda: stream.read(4096), b\"\"):\n        hash_obj.update(chunk)\n\n    return digest_func(hash_obj)\n\n\ndef get_md5(stream: IO) -> str:\n    \"\"\"Calculate MD5 hash of a stream.\n\n    Helper function that calculates the MD5 digest of a stream using `get_hash` helper function.\n\n    Args:\n        stream (IO): File stream.\n\n    Returns:\n        str: MD5 hash.\n    \"\"\"\n\n    return get_hash(stream, hashlib.md5(), lambda h: h.hexdigest())\n\n\ndef get_sha1(stream: IO) -> str:\n    \"\"\"Calculate SHA1 hash of a stream.\n\n    Helper function that calculates the SHA1 digest of a stream using `get_hash` helper function.\n\n    Args:\n        stream (IO): File stream.\n\n    Returns:\n        str: SHA1 hash.\n    \"\"\"\n\n    return get_hash(stream, hashlib.sha1(), lambda h: h.hexdigest())\n\n\ndef get_sha256(stream: IO) -> str:\n    \"\"\"Calculate SHA256 hash of a stream.\n\n    Helper function that calculates the SHA256 digest of a stream using `get_hash` helper function.\n\n    Args:\n        stream (IO): File stream.\n\n    Returns:\n        str: SHA256 hash.\n    \"\"\"\n\n    return get_hash(stream, hashlib.sha256(), lambda h: h.hexdigest())\n\n\ndef get_sha512(stream: IO) -> str:\n    \"\"\"Calculate SHA512 hash of a stream.\n\n    Helper function that calculates the SHA512 digest of a stream using `get_hash` helper function.\n\n    Args:\n        stream (IO): File stream.\n\n    Returns:\n        str: SHA512 hash.\n    \"\"\"\n\n    return get_hash(stream, hashlib.sha512(), lambda h: h.hexdigest())\n\n\ndef get_ssdeep(stream: IO) -> str:\n    \"\"\"Calculate SSDEEP hash of a stream.\n\n    Helper function that calculates the SSDEEP digest of a stream using `get_hash` helper function.\n\n    Args:\n        stream (IO): File stream.\n\n    Returns:\n        str: SSDEEP hash.\n    \"\"\"\n\n    return get_hash(stream, ssdeep.Hash(), lambda h: h.digest())\n"}
{"type": "source_file", "path": "aurora/core/__init__.py", "content": "\"\"\"Package with core functionality.\n\nThis package contains modules with common functions used across Aurora.\n\"\"\"\n"}
{"type": "source_file", "path": "aurora/api/minhash.py", "content": "import datasketch  # type: ignore\n\nfrom typing import List\nfrom fastapi import APIRouter, Depends\n\nfrom aurora.database import get_db, queries, schemas, models\n\nrouter = APIRouter(\n    prefix=\"/minhash\",\n    tags=[\"minhash\"],\n)\n\n\n@router.get(\"/\", response_model=List[schemas.Minhash])\ndef get_minhashes(minhash_type: str = None, db=Depends(get_db)):\n    return queries.minhash.get_minhashes(db, minhash_type)\n\n\n@router.get(\"/types\", response_model=List[str])\ndef get_minhash_types():\n    return list(models.MinhashType)\n\n\n@router.post(\"/compare\", response_model=float)\ndef compare_minhash(m1: schemas.InputMinhash, m2: schemas.InputMinhash):\n    m1_lean = datasketch.LeanMinHash(seed=m1.seed, hashvalues=m1.hash_values)\n    m2_lean = datasketch.LeanMinHash(seed=m2.seed, hashvalues=m2.hash_values)\n\n    return m1_lean.jaccard(m2_lean)\n"}
{"type": "source_file", "path": "aurora/database/alembic/versions/305048399549_base.py", "content": "# type: ignore\n\n\"\"\"Base\n\nRevision ID: 305048399549\nRevises:\nCreate Date: 2021-03-15 19:04:04.143914\n\n\"\"\"\nfrom alembic import op\nimport sqlalchemy as sa\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"305048399549\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"sample\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"filename\", sa.String(), nullable=False),\n        sa.Column(\"filesize\", sa.Integer(), nullable=False),\n        sa.Column(\"filetype\", sa.String(), nullable=False),\n        sa.Column(\"md5\", sa.String(length=32), nullable=False),\n        sa.Column(\"sha1\", sa.String(length=40), nullable=False),\n        sa.Column(\"sha256\", sa.String(length=64), nullable=False),\n        sa.Column(\"sha512\", sa.String(length=128), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_sample_md5\"), \"sample\", [\"md5\"], unique=False)\n    op.create_index(op.f(\"ix_sample_sha1\"), \"sample\", [\"sha1\"], unique=False)\n    op.create_index(op.f(\"ix_sample_sha256\"), \"sample\", [\"sha256\"], unique=True)\n    op.create_index(op.f(\"ix_sample_sha512\"), \"sample\", [\"sha512\"], unique=False)\n    op.create_table(\n        \"minhash\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"sample_id\", sa.Integer(), nullable=True),\n        sa.Column(\"seed\", sa.BIGINT(), nullable=False),\n        sa.Column(\"hash_values\", sa.ARRAY(sa.BIGINT()), nullable=False),\n        sa.Column(\"minhash_type\", sa.String(), nullable=False),\n        sa.Column(\"extra_data\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"sample_id\"],\n            [\"sample.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_minhash_minhash_type\"), \"minhash\", [\"minhash_type\"], unique=False\n    )\n    op.create_table(\n        \"relation\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"parent_id\", sa.Integer(), nullable=False),\n        sa.Column(\"child_id\", sa.Integer(), nullable=False),\n        sa.Column(\"relation_type\", sa.String(), nullable=False),\n        sa.Column(\n            \"confidence\",\n            sa.Float(),\n            nullable=False,\n        ),\n        sa.ForeignKeyConstraint(\n            [\"child_id\"],\n            [\"sample.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"parent_id\"],\n            [\"sample.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_relation_parent_id\"), \"relation\", [\"parent_id\"], unique=False\n    )\n    op.create_index(\n        op.f(\"ix_relation_child_id\"), \"relation\", [\"child_id\"], unique=False\n    )\n    op.create_table(\n        \"ssdeep\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"sample_id\", sa.Integer(), nullable=True),\n        sa.Column(\"chunksize\", sa.Integer(), nullable=False),\n        sa.Column(\"ssdeep\", sa.String(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"sample_id\"],\n            [\"sample.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"sample_id\"),\n    )\n    op.create_table(\n        \"string\",\n        sa.Column(\"id\", sa.Integer(), nullable=False),\n        sa.Column(\"sample_id\", sa.Integer(), nullable=True),\n        sa.Column(\"value\", sa.String(), nullable=False),\n        sa.Column(\"sha256\", sa.String(length=64), nullable=False),\n        sa.Column(\"heuristic\", sa.String(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"sample_id\"],\n            [\"sample.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_string_sha256\"), \"string\", [\"sha256\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade():\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_string_sha256\"), table_name=\"string\")\n    op.drop_table(\"string\")\n    op.drop_table(\"ssdeep\")\n    op.drop_table(\"relation\")\n    op.drop_index(op.f(\"ix_minhash_minhash_type\"), table_name=\"minhash\")\n    op.drop_table(\"minhash\")\n    op.drop_index(op.f(\"ix_sample_sha512\"), table_name=\"sample\")\n    op.drop_index(op.f(\"ix_sample_sha256\"), table_name=\"sample\")\n    op.drop_index(op.f(\"ix_sample_sha1\"), table_name=\"sample\")\n    op.drop_index(op.f(\"ix_sample_md5\"), table_name=\"sample\")\n    op.drop_table(\"sample\")\n    # ### end Alembic commands ###\n"}
{"type": "source_file", "path": "aurora/database/models/ssdeep.py", "content": "from __future__ import annotations\n\nimport sqlalchemy as sql\n\nfrom fastapi import UploadFile\nfrom typing import TYPE_CHECKING\nfrom sqlalchemy.orm import relationship\n\nfrom aurora.core import utils\nfrom aurora.database import Base\n\nif TYPE_CHECKING:\n    from aurora.database.models import Sample  # noqa: F401\n\n\nclass SsDeep(Base):\n    __tablename__ = \"ssdeep\"\n\n    id = sql.Column(sql.Integer, primary_key=True)\n    sample_id = sql.Column(sql.Integer, sql.ForeignKey(\"sample.id\"), unique=True)\n    chunksize = sql.Column(sql.Integer, nullable=False)\n    ssdeep = sql.Column(sql.String, nullable=False)\n\n    sample = relationship(\"Sample\")\n\n    @staticmethod\n    def from_uploadfile(file: UploadFile) -> SsDeep:\n        ssdeep_hash = utils.get_ssdeep(file.file)\n\n        chunksize = int(ssdeep_hash.split(\":\")[0])\n\n        ssdeep = SsDeep(chunksize=chunksize, ssdeep=ssdeep_hash)\n\n        return ssdeep\n"}
{"type": "source_file", "path": "aurora/database/schemas.py", "content": "from pydantic import BaseModel\nfrom typing import List, Optional, Dict, Any\n\n\nclass Sample(BaseModel):\n    id: int\n    filename: str\n    filesize: int\n    filetype: str\n    md5: str\n    sha1: str\n    sha256: str\n    sha512: str\n\n    class Config:\n        orm_mode = True\n\n\nclass InputMinhash(BaseModel):\n    seed: int\n    hash_values: List[int]\n    minhash_type: str\n    extra_data: Optional[Dict[str, Any]]\n\n\nclass Minhash(BaseModel):\n    id: int\n    seed: int\n    hash_values: List[int]\n    minhash_type: str\n    sample: Sample\n    extra_data: Optional[Dict[str, Any]]\n\n    class Config:\n        orm_mode = True\n\n\nclass InputRelation(BaseModel):\n    parent_sha256: str\n    child_sha256: str\n    type: str\n    confidence: float\n\n\nclass RelationFilter(BaseModel):\n    relation_type: Optional[str]\n    confidence: Optional[float]\n\n\nclass Relation(BaseModel):\n    id: int\n    parent_id: int\n    child_id: int\n    relation_type: str\n    confidence: float\n\n    class Config:\n        orm_mode = True\n\n\nclass SsDeep(BaseModel):\n    id: int\n    chunksize: int\n    ssdeep: str\n    sample: Sample\n\n    class Config:\n        orm_mode = True\n        arbitrary_types_allowed = True\n\n\nclass InputString(BaseModel):\n    value: str\n    heuristic: str\n\n\nclass String(BaseModel):\n    id: int\n    value: str\n    sha256: str\n    sample: Sample\n    heuristic: str\n\n    class Config:\n        orm_mode = True\n        arbitrary_types_allowed = True\n"}
{"type": "source_file", "path": "aurora/database/queries/ssdeep.py", "content": "from fastapi import UploadFile\nfrom typing import List, Optional\nfrom sqlalchemy.orm import Session\n\nfrom aurora.database import models\n\n\ndef get_ssdeep_hashes(\n    db: Session, chunksize: Optional[int] = None\n) -> List[models.SsDeep]:\n\n    \"\"\"Queries Ssdeep objects from the database.\n\n    Returns a list of ssdeep hashes.\n\n    Args:\n        db (Session): Database session.\n        chunksize (int): Optional chunksize to filter hashes.\n\n    Returns:\n        List(SsDeep) List of ssdeep hashes in a database.\n\n    \"\"\"\n\n    filters = []\n    if chunksize:\n        filters.append(models.SsDeep.chunksize == chunksize)\n\n    hashes = db.query(models.SsDeep).filter(*filters).all()\n\n    return hashes\n\n\ndef add_ssdeep(db: Session, file: UploadFile) -> models.SsDeep:\n\n    \"\"\"Add ssdeep.\n\n    Add new ssdeep hash to the database.\n\n    Args:\n       db (Session): Database session.\n       file (UploadFile): Sample file from which ssdeep will be calculated.\n\n    Returns:\n        SsDeep Newly added ssdeep hash.\n\n    \"\"\"\n\n    ssdeep = models.SsDeep.from_uploadfile(file)\n\n    db.add(ssdeep)\n\n    return ssdeep\n"}
{"type": "source_file", "path": "aurora/database/queries/__init__.py", "content": "from aurora.database.queries import sample\nfrom aurora.database.queries import minhash\nfrom aurora.database.queries import relation\nfrom aurora.database.queries import ssdeep\nfrom aurora.database.queries import string\n\n__all__ = [\"sample\", \"minhash\", \"relation\", \"ssdeep\", \"string\"]\n"}
{"type": "source_file", "path": "aurora/database/models/string.py", "content": "import sqlalchemy as sql\n\nfrom typing import TYPE_CHECKING\nfrom sqlalchemy.orm import relationship\n\nfrom aurora.database import Base\n\nif TYPE_CHECKING:\n    from aurora.database.models import Sample  # noqa: F401\n\n\nclass String(Base):\n    __tablename__ = \"string\"\n\n    id = sql.Column(sql.Integer, primary_key=True)\n    sample_id = sql.Column(sql.Integer, sql.ForeignKey(\"sample.id\"))\n    value = sql.Column(sql.String, nullable=False)\n    sha256 = sql.Column(sql.String(64), nullable=False, index=True)\n    heuristic = sql.Column(sql.String, nullable=False)\n\n    sql.UniqueConstraint(\"sample_id\", \"sha256\", \"unique_string\")\n\n    sample = relationship(\"Sample\")\n"}
{"type": "source_file", "path": "aurora/database/queries/sample.py", "content": "from sqlalchemy import or_\nfrom sqlalchemy.orm import Session\nfrom fastapi import UploadFile\nfrom typing import List, Optional\n\n\nfrom aurora.database import models\n\n\ndef get_samples(db: Session, offset: int = 0, limit: int = 50) -> List[models.Sample]:\n\n    \"\"\"Queries Sample objects from the database.\n\n    Returns a list of samples.\n\n    Args:\n        db (Session): Database session.\n        offset (int): Offset from which the query starts.\n        limit (int): Max number of relations returned in a single query.\n\n    Returns:\n        List(Relation) List of relations in a database.\n\n    \"\"\"\n\n    return db.query(models.Sample).offset(offset).limit(limit).all()\n\n\ndef get_number_of_samples(db: Session) -> int:\n\n    \"\"\"Returns a total number of samples.\n\n    Args:\n        db (Session): Database session.\n\n    Returns:\n        int Total number of samples in database.\n\n    \"\"\"\n\n    return db.query(models.Sample).count()\n\n\ndef get_sample_by_md5(db: Session, md5: str) -> Optional[models.Sample]:\n\n    \"\"\"Returns a sample by the MD5 hash.\n\n    Queries a sample which MD5 hash is equal to the passed one.\n\n    Args:\n         db (Session): Database session.\n         sha256 (str): MD5 hash of the sample.\n\n    Returns:\n        Sample Sample with the specified hash.\n\n    \"\"\"\n\n    return db.query(models.Sample).filter(models.Sample.md5 == md5).first()\n\n\ndef get_sample_by_sha1(db: Session, sha1: str) -> Optional[models.Sample]:\n\n    \"\"\"Returns a sample by the SHA1 hash.\n\n    Queries a sample which SHA1 hash is equal to the passed one.\n\n    Args:\n         db (Session): Database session.\n         sha256 (str): SHA1 hash of the sample.\n\n    Returns:\n        Sample Sample with the specified hash.\n\n    \"\"\"\n\n    return db.query(models.Sample).filter(models.Sample.sha1 == sha1).first()\n\n\ndef get_sample_by_sha256(db: Session, sha256: str) -> Optional[models.Sample]:\n\n    \"\"\"Returns a sample by the SHA256 hash.\n\n    Queries a sample which SHA256 hash is equal to the passed one.\n\n    Args:\n         db (Session): Database session.\n         sha256 (str): SHA256 hash of the sample.\n\n    Returns:\n        Sample Sample with the specified hash.\n\n    \"\"\"\n\n    return db.query(models.Sample).filter(models.Sample.sha256 == sha256).first()\n\n\ndef get_sample_by_sha512(db: Session, sha512: str) -> Optional[models.Sample]:\n\n    \"\"\"Returns a sample by the SHA512 hash.\n\n    Queries a sample which SHA512 hash is equal to the passed one.\n\n    Args:\n         db (Session): Database session.\n         sha256 (str): SHA512 hash of the sample.\n\n    Returns:\n        Sample Sample with the specified hash.\n\n    \"\"\"\n\n    return db.query(models.Sample).filter(models.Sample.sha512 == sha512).first()\n\n\ndef get_sample_parents(db: Session, sample: models.Sample) -> List[models.Sample]:\n\n    \"\"\"Returns parents of the sample.\n\n    Queries samples which child is the sample specified as argument.\n\n    Args:\n         db (Session): Database session.\n         sample (Sample): Sample whose parents are queried.\n\n    Returns:\n        List(Sample) List of parent samples related to the argument sample.\n\n    \"\"\"\n\n    return (\n        db.query(models.Sample)\n        .distinct()\n        .filter(models.Sample.children.any(models.Relation.child_id == sample.id))\n        .all()\n    )\n\n\ndef get_sample_children(db: Session, sample: models.Sample) -> List[models.Sample]:\n\n    \"\"\"Returns children of the sample.\n\n    Queries samples which parent is the sample specified as argument.\n\n    Args:\n         db (Session): Database session.\n         sample (Sample): Sample whose children are queried.\n\n    Returns:\n        List(Sample) List of child samples related to the argument sample.\n\n    \"\"\"\n\n    return (\n        db.query(models.Sample)\n        .distinct()\n        .filter(models.Sample.parents.any(models.Relation.parent_id == sample.id))\n        .all()\n    )\n\n\ndef get_sample_related(db: Session, sample: models.Sample) -> List[models.Sample]:\n\n    \"\"\"Returns related samples.\n\n    Returns both parent and children samples.\n\n    Args:\n         db (Session): Database session.\n         sample (Sample): Sample whose related samples are queried.\n\n    Returns:\n        List(Sample) List of related samples related to the argument sample.\n\n    \"\"\"\n\n    return (\n        db.query(models.Sample)\n        .distinct()\n        .filter(\n            or_(\n                models.Sample.parents.any(models.Relation.parent_id == sample.id),\n                models.Sample.children.any(models.Relation.child_id == sample.id),\n            )\n        )\n        .all()\n    )\n\n\ndef get_samples_with_string(db: Session, string: models.String) -> List[models.Sample]:\n\n    \"\"\"Returns samples containing string.\n\n    Queries samples which contain string specified as argument..\n\n    Args:\n         db (Session): Database session.\n         string (String): String which queried samples have to contain..\n\n    Returns:\n        List(Sample) List of samples containing specified string.\n\n    \"\"\"\n\n    return (\n        db.query(models.Sample)\n        .filter(models.Sample.strings.any(models.String.sha256 == string.sha256))\n        .all()\n    )\n\n\ndef add_sample(db: Session, file: UploadFile) -> models.Sample:\n\n    \"\"\"Add sample.\n\n    Add new sample to the database.\n\n    Args:\n       db (Session): Database session.\n       file (UploadFile): Sample file.\n\n    Returns:\n        Sample Newly added sample.\n\n    \"\"\"\n\n    sample = models.Sample.from_uploadfile(file)\n    db.add(sample)\n\n    return sample\n"}
{"type": "source_file", "path": "aurora/database/queries/minhash.py", "content": "from typing import List, Dict, Any, Optional\nfrom sqlalchemy.orm import Session\n\nfrom aurora.database import models\n\n\ndef get_minhashes(\n    db: Session, minhash_type: Optional[str] = None\n) -> List[models.Minhash]:\n\n    \"\"\"Queries Minhash objects from the database.\n\n    Returns a list of Minhash objects from the database.\n\n    Args:\n        db (Session): Database session.\n        minhash_type (Optional(str)): Optional type of minhash to filter out objects.\n\n    Returns:\n        List(Minhash) Returns a list of Minhash objects in the database.\n\n    \"\"\"\n\n    filters = []\n    if minhash_type:\n        filters.append(models.Minhash.minhash_type == minhash_type)\n\n    return db.query(models.Minhash).filter(*filters).all()\n\n\ndef get_sample_minhash(\n    db: Session, sample: models.Sample, type: Optional[str] = None\n) -> List[models.Minhash]:\n\n    \"\"\"Queries Minhash objects tat belong to the passed sample.\n\n    Returns a list of Minhash objects whose `sample_id` is equal to the `id` of passed sample..\n\n    Args:\n        db (Session): Database session.\n        sample (Sample): Sample object to which minhashes will be related.\n        minhash_type (Optional(MinhashType)): Optional MinhashType to filter out objects.\n\n    Returns:\n        List(Minhash) Returns a list of Minhash objects in the database.\n\n    \"\"\"\n    filters = []\n    if type:\n        filters.append(models.Minhash.minhash_type == type)\n\n    return (\n        db.query(models.Minhash)\n        .filter(models.Minhash.sample_id == sample.id)\n        .filter(*filters)\n        .all()\n    )\n\n\ndef add_minhash(\n    db: Session,\n    seed: int,\n    hash_values: List[int],\n    minhash_type: str,\n    extra_data: Optional[Dict[str, Any]] = None,\n) -> models.Minhash:\n    \"\"\"Creates a new Minhash object..\n\n    Creates and adds to databse a new minhash object using the passed parameters.\n\n    Args:\n        db (Session): Database session.\n        seed (int): Seed of the minhash.\n        hash_values (List(int)): List of hash values of the minhash.\n        minhash_type (str): Type of the minhash.\n        extra_data (Optional(Dict[(str, Any))): Optional extra data stored with the minhash.\n\n    Returns:\n        Minhash Returns created minhash object..\n\n    \"\"\"\n\n    minhash_model = models.Minhash(\n        seed=seed,\n        hash_values=hash_values,\n        minhash_type=minhash_type,\n        extra_data=extra_data,\n    )\n\n    db.add(minhash_model)\n\n    return minhash_model\n"}
{"type": "source_file", "path": "aurora/front/__init__.py", "content": ""}
{"type": "source_file", "path": "aurora/database/models/minhash.py", "content": "\"\"\"Models for storing Minhashes.\n\nThis module describes a model for storing Minhash values.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sqlalchemy as sql\n\nfrom typing import TYPE_CHECKING\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.dialects.postgresql import JSONB\n\nfrom aurora.database import Base\n\n\nif TYPE_CHECKING:\n    from aurora.database.models import Sample  # noqa: F401\n\n\nclass Minhash(Base):\n\n    __tablename__ = \"minhash\"\n\n    id = sql.Column(sql.Integer, primary_key=True)\n    sample_id = sql.Column(sql.Integer, sql.ForeignKey(\"sample.id\"))\n    seed = sql.Column(sql.BIGINT, nullable=False)\n    hash_values = sql.Column(sql.ARRAY(sql.BIGINT()), nullable=False)\n    minhash_type = sql.Column(sql.String, nullable=False, index=True)\n    extra_data = sql.Column(JSONB)\n\n    sql.UniqueConstraint(\"sample_id\", \"analysis_type\", name=\"unique_analysis_sample\")\n\n    sample = relationship(\"Sample\", back_populates=\"minhashes\")\n"}
{"type": "source_file", "path": "aurora/database/models/relation.py", "content": "from __future__ import annotations\n\nimport sqlalchemy as sql\n\nfrom typing import TYPE_CHECKING\nfrom sqlalchemy.orm import relationship, backref\n\nfrom aurora.database import Base\n\nif TYPE_CHECKING:\n    from aurora.database.models import Sample  # noqa: F401\n\n\nclass Relation(Base):\n    __tablename__ = \"relation\"\n\n    id = sql.Column(sql.Integer, primary_key=True)\n    parent_id = sql.Column(\n        sql.Integer, sql.ForeignKey(\"sample.id\"), nullable=False, index=True\n    )\n    child_id = sql.Column(\n        sql.Integer, sql.ForeignKey(\"sample.id\"), nullable=False, index=True\n    )\n    relation_type = sql.Column(sql.String, nullable=False)\n    confidence = sql.Column(sql.Float, nullable=False)\n\n    sql.UniqueConstraint(\"parent_id\", \"child_id\", \"relation_type\")\n\n    parent = relationship(\n        \"Sample\", foreign_keys=[parent_id], backref=backref(\"related_children\")\n    )\n    child = relationship(\n        \"Sample\", foreign_keys=[child_id], backref=backref(\"related_parents\")\n    )\n"}
{"type": "source_file", "path": "aurora/database/models/sample.py", "content": "from __future__ import annotations\n\nimport sqlalchemy as sql\n\nfrom fastapi import UploadFile\nfrom collections import namedtuple\nfrom typing import List, TYPE_CHECKING\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.orm.relationships import RelationshipProperty\nfrom sqlalchemy.ext.associationproxy import association_proxy\n\nfrom aurora.core import utils\nfrom aurora.database import Base\nfrom aurora.database.models.relation import Relation\n\nRelationInput = namedtuple(\"RelationInput\", [\"parent\", \"child\", \"type\", \"confidence\"])\n\nif TYPE_CHECKING:\n    from aurora.database.models import Minhash  # noqa: F401\n    from aurora.database.models import String  # noqa: F401\n    from aurora.database.models import SsDeep  # noqa: F401\n\n\nclass Sample(Base):\n    __tablename__ = \"sample\"\n\n    id = sql.Column(sql.Integer, primary_key=True)\n    filename = sql.Column(sql.String, nullable=False)\n    filesize = sql.Column(sql.Integer, nullable=False)\n    filetype = sql.Column(sql.String, nullable=False)\n    md5 = sql.Column(sql.String(32), nullable=False, index=True)\n    sha1 = sql.Column(sql.String(40), nullable=False, index=True)\n    sha256 = sql.Column(sql.String(64), nullable=False, index=True, unique=True)\n    sha512 = sql.Column(sql.String(128), nullable=False, index=True)\n\n    minhashes: RelationshipProperty[List[Minhash]] = relationship(\"Minhash\")\n    strings: RelationshipProperty[List[String]] = relationship(\"String\")\n    ssdeep = relationship(\"SsDeep\", uselist=False)\n\n    children = association_proxy(\n        \"related_children\",\n        \"child\",\n        creator=lambda relation_input: Relation(\n            parent=relation_input.parent,  # type: ignore\n            child=relation_input.child,  # type: ignore\n            relation_type=relation_input.type,  # type: ignore\n            confidence=relation_input.confidence,  # type: ignore\n        ),\n    )\n\n    parents = association_proxy(\n        \"related_parents\",\n        \"parent\",\n        creator=lambda relation_input: Relation(\n            parent=relation_input.parent,  # type: ignore\n            child=relation_input.child,  # type: ignore\n            relation_type=relation_input.type,  # type: ignore\n            confidence=relation_input.confidence,  # type: ignore\n        ),\n    )\n\n    @staticmethod\n    def from_uploadfile(file: UploadFile) -> Sample:\n        file.file.seek(0, 2)\n\n        filename = file.filename\n        filesize = file.file.tell()\n        filetype = utils.get_magic(file.file)\n        md5 = utils.get_md5(file.file)\n        sha1 = utils.get_sha1(file.file)\n        sha256 = utils.get_sha256(file.file)\n        sha512 = utils.get_sha512(file.file)\n\n        sample = Sample(\n            filename=filename,\n            filesize=filesize,\n            filetype=filetype,\n            md5=md5,\n            sha1=sha1,\n            sha256=sha256,\n            sha512=sha512,\n        )\n\n        return sample\n\n    def add_child(self, child: Sample, analysis_type: str, confidence: str) -> None:\n        relation_input = RelationInput(\n            parent=self, child=child, type=analysis_type, confidence=confidence\n        )\n\n        self.children.append(relation_input)\n"}
{"type": "source_file", "path": "aurora/database/queries/relation.py", "content": "import logging\n\nfrom typing import List, Optional\nfrom sqlalchemy import func, tuple_, or_\nfrom sqlalchemy.orm import Session\n\nfrom aurora.database import models, schemas\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_relations(\n    db: Session,\n    filters: Optional[schemas.RelationFilter] = None,\n    offset: int = 0,\n    limit: int = 50,\n) -> List[models.Relation]:\n\n    \"\"\"Queries Relation objects from the database.\n\n    Returns a list of relations between samples.\n\n    Args:\n        db (Session): Database session.\n        filters (Optional[schemas.RelationFilter]): Optional filters used in query.\n        offset (int): Offset from which the query starts.\n        limit (int): Max number of relations returned in a single query.\n\n    Returns:\n        List(Relation) List of relations in a database.\n\n    \"\"\"\n\n    query_filters = []\n    if filters:\n        if filters.relation_type:\n            query_filters.append(models.Relation.relation_type == filters.relation_type)\n        if filters.confidence:\n            query_filters.append(models.Relation.confidence >= filters.confidence)\n\n    relations = (\n        db.query(models.Relation)\n        .filter(*query_filters)\n        .offset(offset)\n        .limit(limit)\n        .all()\n    )\n    return relations\n\n\ndef get_confident_relation(db: Session) -> List[models.Relation]:\n    relations_with_bigger_count = (\n        db.query(models.Relation.parent_id, models.Relation.child_id)\n        .group_by(models.Relation.parent_id, models.Relation.child_id)\n        .having(func.count(models.Relation.parent_id) >= 2)\n        .subquery()\n    )\n\n    confident_relations = (\n        db.query(models.Relation)\n        .filter(\n            tuple_(models.Relation.parent_id, models.Relation.child_id).in_(\n                relations_with_bigger_count\n            )\n        )\n        .all()\n    )\n\n    return confident_relations\n\n\ndef get_exact_relation(\n    db: Session, parent: models.Sample, child: models.Sample, rel_type: str\n) -> Optional[models.Relation]:\n\n    \"\"\"Queries Relation objects from the database with specified both child and parent, together with a type.\n\n    Returns an exact relation between two passed samples of a specified type.\n\n    Args:\n        db (Session): Database session.\n        parent (Sample): Parent sample of the relationship.\n        child (Sample): Child sample of the relationship.\n        rel_type (str): Type of the relationship.\n\n    Returns:\n        Relation Exact relation in a database.\n\n    \"\"\"\n\n    return (\n        db.query(models.Relation)\n        .filter(\n            models.Relation.parent_id == parent.id,\n            models.Relation.child_id == child.id,\n            models.Relation.relation_type == rel_type,\n        )\n        .first()\n    )\n\n\ndef get_relations_by_parent(\n    db: Session, parent: models.Sample, filters: Optional[schemas.RelationFilter] = None\n) -> List[models.Relation]:\n\n    \"\"\"Queries Relation objects from the database with specified parent.\n\n    Returns a list of relations between samples where the parent sample is passed as a parameter.\n\n    Args:\n        db (Session): Database session.\n        parent (Sample): Sample used to filter relations with specified parent.\n        filters (Optional[schemas.RelationFilter]): Optional filters used in query.\n\n    Returns:\n        List(Relation) List of relations in a database.\n\n    \"\"\"\n\n    query_filters = []\n    if filters:\n        if filters.relation_type:\n            query_filters.append(models.Relation.relation_type == filters.relation_type)\n        if filters.confidence:\n            query_filters.append(models.Relation.confidence >= filters.confidence)\n\n    query_filters.append(models.Relation.parent_id == parent.id)\n\n    relations = db.query(models.Relation).filter(*query_filters).all()\n    return relations\n\n\ndef get_relations_by_child(\n    db: Session, child: models.Sample, filters: Optional[schemas.RelationFilter] = None\n) -> List[models.Relation]:\n\n    \"\"\"Queries Relation objects from the database with specified child.\n\n    Returns a list of relations between samples where the child sample is passed as a parameter.\n\n    Args:\n        db (Session): Database session.\n        parent (Sample): Sample used to filter relations with specified child.\n        filters (Optional[schemas.RelationFilter]): Optional filters used in query.\n\n    Returns:\n        List(Relation) List of relations in a database.\n\n    \"\"\"\n\n    query_filters = []\n    if filters:\n        if filters.relation_type:\n            query_filters.append(models.Relation.relation_type == filters.relation_type)\n        if filters.confidence:\n            query_filters.append(models.Relation.confidence >= filters.confidence)\n\n    query_filters.append(models.Relation.child_id == child.id)\n\n    relations = db.query(models.Relation).filter(*query_filters).all()\n    return relations\n\n\ndef get_relations_by_hash(\n    db: Session, sample: models.Sample, filters: Optional[schemas.RelationFilter] = None\n) -> List[models.Relation]:\n\n    \"\"\"Queries Relation objects from the database with specified hash.\n\n    Returns a list of relations between samples where the parent or the child sample is passed as a parameter.\n\n    Args:\n        db (Session): Database session.\n        sample (Sample): Sample used to filter relations with specified parent or child.\n        filters (Optional[schemas.RelationFilter]): Optional filters used in query.\n\n    Returns:\n        List(Relation) List of relations in a database.\n\n    \"\"\"\n\n    query_filters = []\n    if filters:\n        if filters.relation_type:\n            query_filters.append(models.Relation.relation_type == filters.relation_type)\n        if filters.confidence:\n            query_filters.append(models.Relation.confidence >= filters.confidence)\n\n    relations = (\n        db.query(models.Relation)\n        .filter(*query_filters)\n        .filter(\n            or_(\n                models.Relation.parent_id == sample.id,\n                models.Relation.child_id == sample.id,\n            )\n        )\n        .all()\n    )\n\n    return relations\n\n\ndef get_simplified_relations(\n    db: Session, filters: schemas.RelationFilter = None\n) -> List[models.Relation]:\n\n    \"\"\"Queries distinct Relation objects from the database.\n\n    Returns a distinct list of relations between samples where.\n\n    Args:\n        db (Session): Database session.\n        filters (Optional[schemas.RelationFilter]): Optional filters used in query.\n\n    Returns:\n        List(Relation) List of relations in a database.\n\n    \"\"\"\n\n    query_filters = []\n    if filters:\n        if filters.relation_type:\n            query_filters.append(models.Relation.relation_type == filters.relation_type)\n        if filters.confidence:\n            query_filters.append(models.Relation.confidence >= filters.confidence)\n\n    relations = (\n        db.query(models.Relation)\n        .distinct(models.Relation.parent_id, models.Relation.child_id)\n        .filter(*query_filters)\n        .all()\n    )\n\n    return relations\n\n\ndef add_relation(\n    db: Session,\n    parent: models.Sample,\n    child: models.Sample,\n    rel_type: str,\n    confidence: float,\n) -> models.Relation:\n\n    \"\"\"Add relation.\n\n    Add new relation to the database.\n\n    Args:\n        db (Session): Database session.\n       parent (Sample): Parent sample of the relation.\n       child (Sample): Child sample of the relation.\n       rel_type (str): Relation type.\n       confidence (float): Float value describing confidence in relationship.\n                           Value is mostly the same as similarity coefficient.\n\n    Returns:\n        Relation Newly added relation.\n\n    \"\"\"\n\n    relation = models.Relation(\n        parent=parent, child=child, relation_type=rel_type, confidence=confidence  # type: ignore\n    )\n\n    db.add(relation)\n\n    return relation\n"}
{"type": "source_file", "path": "aurora/database/queries/string.py", "content": "from typing import List, Optional\nfrom sqlalchemy.orm import Session\n\nfrom aurora.database import models\n\n\ndef get_strings(db: Session, offset: int = 0, limit: int = 50) -> List[models.String]:\n\n    \"\"\"Get strings.\n\n    Returns a list of strings in the database.\n\n    Args:\n        db (Session): Database session.\n        offset (int): Offset from which the query starts.\n        limit (int): Max number of strings returned in a single query.\n\n    Returns:\n        List(String) List of string objects.\n\n    \"\"\"\n\n    strings = db.query(models.String).offset(offset).limit(limit).all()\n    return strings\n\n\ndef get_unique_strings(\n    db: Session, offset: int = 0, limit: int = 50\n) -> List[models.String]:\n\n    \"\"\"Get unique strings.\n\n    Returns a list of distinct strings in the database.\n\n    Args:\n        db (Session): Database session.\n        offset (int): Offset from which the query starts.\n        limit (int): Max number of strings returned in a single query.\n\n    Returns:\n        List(String) List of unique string objects.\n\n    \"\"\"\n\n    strings = (\n        db.query(models.String)\n        .distinct(models.String.sha256)\n        .offset(offset)\n        .limit(limit)\n        .all()\n    )\n    return strings\n\n\ndef get_string_by_sha256(db: Session, sha256: str) -> Optional[models.String]:\n\n    \"\"\"Get string by SHA256 hash.\n\n    Returns a string with a specified SHA256 hash.\n\n    Args:\n        db (Session): Database session.\n        sha256 (str): SHA256 hash of the string.\n\n    Returns:\n        String String with the specified hash.\n\n    \"\"\"\n\n    string = db.query(models.String).filter(models.String.sha256 == sha256).first()\n\n    return string\n\n\ndef get_string_by_value(db: Session, value: str) -> Optional[models.String]:\n\n    \"\"\"Get string by SHA256 hash.\n\n    Returns a string with a specified SHA256 hash.\n\n    Args:\n        db (Session): Database session.\n        value (str): Value hash of the string.\n\n    Returns:\n        String String with the specified hash.\n\n    \"\"\"\n\n    string = db.query(models.String).filter(models.String.value == value).first()\n\n    return string\n\n\ndef add_string(db: Session, value: str, sha256: str, heuristic: str) -> models.String:\n\n    \"\"\"Add string.\n\n    Add new string to the database.\n\n    Args:\n       db (Session): Database session.\n       value (str): Value of the string.\n       sha256 (str): SHA256 hash of the string.\n       heuristic (str): Heuristic name from which the string was recovered.\n\n    Returns:\n        String Newly added string.\n\n    \"\"\"\n\n    string = models.String(value=value, sha256=sha256, heuristic=heuristic)\n\n    db.add(string)\n\n    return string\n"}
{"type": "source_file", "path": "aurora/database/models/__init__.py", "content": "\"\"\"Database models.\n\nThis package contains database models.\n\"\"\"\n\n\nfrom aurora.database.models.sample import Sample\nfrom aurora.database.models.minhash import Minhash\nfrom aurora.database.models.relation import Relation\nfrom aurora.database.models.ssdeep import SsDeep\nfrom aurora.database.models.string import String\n\n__all__ = [\n    \"Sample\",\n    \"Minhash\",\n    \"Relation\",\n    \"SsDeep\",\n    \"String\",\n]\n"}
{"type": "source_file", "path": "scripts/load_dir.py", "content": "import sys\nimport requests\n\nfrom pathlib import Path\n\n\ndef upoad_file(url, filename):\n    files = {'file': open(filename, 'rb')}\n    r = requests.post(url, files=files)\n\n    print(r.json())\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 3:\n        print(\"Usage:\\tpython load_dir.py <url> <dir_path>\")\n        sys.exit()\n\n    host = sys.argv[1]\n    dir_path = Path(sys.argv[2])\n\n    for entry in dir_path.iterdir():\n        if entry.is_file():\n            upoad_file(host, entry)\n"}
{"type": "source_file", "path": "setup.py", "content": "from setuptools import setup, find_packages\n\nsetup(name=\"aurora\",\n      version=\"1.0.0\",\n      description=\"Aurora malware similarity platform \",\n      long_description=\"Automated malware similarity platform with modularity in mind.\",\n      author=\"W3ndige\",\n      author_email=\"w3ndige@gmail.com\",\n      packages=find_packages(),\n      include_package_data=True,\n      url=\"https://github.com/W3ndige/aurora\",\n      install_requires=open(\"requirements.txt\").read().splitlines(),\n      python_requires='>=3.6',\n      classifiers=[\n        \"Development Status :: 5 - Production/Stable\",\n        \"Operating System :: POSIX :: Linux\",\n        \"Programming Language :: Python :: 3\"\n      ]\n)\n"}
{"type": "source_file", "path": "aurora/front/front.py", "content": "import logging\nimport starlette.status as status\n\nfrom typing import cast\nfrom fastapi.responses import HTMLResponse, RedirectResponse\nfrom fastapi import APIRouter, Request, Depends, UploadFile, File, HTTPException, Form\nfrom starlette.templating import Jinja2Templates\n\nfrom aurora.core import karton\nfrom aurora.core import search\nfrom aurora.core import network as net\nfrom aurora.core.utils import get_magic, get_sha256\nfrom aurora.database import get_db, queries, models\n\ntemplates = Jinja2Templates(directory=\"aurora/front/templates/\")\n\nrouter = APIRouter()\n\n\nlogger = logging.getLogger(__name__)\n\n\n@router.get(\"/\", response_class=HTMLResponse)\ndef index(request: Request, offset: int = 0, db=Depends(get_db)):\n    samples = queries.sample.get_samples(db, offset=offset)\n\n    samples_with_info = []\n    for sample in samples:\n        samples_with_info.append(\n            {\n                \"sample\": sample,\n                \"rel_size\": len(queries.sample.get_sample_related(db, sample)),\n            }\n        )\n\n    return templates.TemplateResponse(\n        \"index.html\",\n        {\"request\": request, \"samples_with_info\": samples_with_info, \"offset\": offset},\n    )\n\n\n@router.get(\"/upload\", response_class=HTMLResponse)\ndef get_upload(request: Request):\n    return templates.TemplateResponse(\"upload.html\", {\"request\": request})\n\n\n@router.post(\"/upload\", response_class=RedirectResponse)\ndef post_upload(file: UploadFile = File(...), db=Depends(get_db)):\n    sha256 = get_sha256(file.file)\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n    if not sample:\n        sample = queries.sample.add_sample(db, file)\n\n    if not sample.ssdeep:\n        ssdeep = queries.ssdeep.add_ssdeep(db, file)\n        sample.ssdeep = ssdeep\n\n        try:\n            karton.push_ssdeep(sample.sha256, ssdeep.chunksize, ssdeep.ssdeep)\n        except RuntimeError:\n            logger.exception(f\"Couldn't push Sample to karton. Sample {sample.sha256}\")\n\n    db.commit()\n\n    try:\n        sample_mime = get_magic(file.file, mimetype=True)\n        karton.push_file(file, sample_mime, sample.sha256)\n    except RuntimeError:\n        pass\n\n    return RedirectResponse(f\"/sample/{sha256}\", status_code=status.HTTP_302_FOUND)\n\n\n@router.get(\"/sample/{sha256}\", response_class=HTMLResponse)\ndef sample_index(request: Request, sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    sample_ssdeep = sample.ssdeep.ssdeep\n    related_samples = list(queries.sample.get_sample_related(db, sample))\n\n    db_relations = queries.relation.get_relations_by_hash(db, sample)\n\n    nodes, edges = net.prepare_sample_graph(db_relations)\n\n    return templates.TemplateResponse(\n        \"sample/related.html\",\n        {\n            \"request\": request,\n            \"sample\": sample,\n            \"sample_ssdeep\": sample_ssdeep,\n            \"related_samples\": related_samples,\n            \"nodes\": nodes,\n            \"edges\": edges,\n        },\n    )\n\n\n@router.get(\"/sample/{sha256}/relations\", response_class=HTMLResponse)\ndef get_sample_relations(request: Request, sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    sample_ssdeep = sample.ssdeep.ssdeep\n    db_relations = queries.relation.get_relations_by_hash(db, sample)\n\n    nodes, edges = net.prepare_sample_graph(db_relations)\n\n    return templates.TemplateResponse(\n        \"sample/relations.html\",\n        {\n            \"request\": request,\n            \"sample\": sample,\n            \"sample_ssdeep\": sample_ssdeep,\n            \"relations\": db_relations,\n            \"nodes\": nodes,\n            \"edges\": edges,\n        },\n    )\n\n\n@router.get(\"/sample/{sha256}/strings\", response_class=HTMLResponse)\ndef get_sample_strings(request: Request, sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    sample_ssdeep = sample.ssdeep.ssdeep\n    strings = sample.strings\n\n    db_relations = queries.relation.get_relations_by_hash(db, sample)\n\n    nodes, edges = net.prepare_sample_graph(db_relations)\n\n    return templates.TemplateResponse(\n        \"sample/strings.html\",\n        {\n            \"request\": request,\n            \"sample\": sample,\n            \"sample_ssdeep\": sample_ssdeep,\n            \"strings\": strings,\n            \"nodes\": nodes,\n            \"edges\": edges,\n        },\n    )\n\n\n@router.get(\"/sample/{sha256}/network\", response_class=HTMLResponse)\ndef get_sample_network(request: Request, sha256: str, db=Depends(get_db)):\n    sample = queries.sample.get_sample_by_sha256(db, sha256)\n\n    if not sample:\n        raise HTTPException(status_code=404, detail=f\"Sample {sha256} not found.\")\n\n    db_relations = queries.relation.get_relations_by_hash(db, sample)\n\n    nodes, edges = net.prepare_sample_graph(db_relations)\n\n    return templates.TemplateResponse(\n        \"network.html\",\n        {\"request\": request, \"nodes\": nodes, \"edges\": edges},\n    )\n\n\n@router.get(\"/string\", response_class=HTMLResponse)\ndef get_strings(request: Request, offset: int = 0, db=Depends(get_db)):\n    strings = queries.string.get_unique_strings(db, offset=offset)\n\n    return templates.TemplateResponse(\n        \"string/strings.html\",\n        {\"request\": request, \"offset\": offset, \"strings\": strings},\n    )\n\n\n@router.get(\"/string/{sha256}\", response_class=HTMLResponse)\ndef get_string(request: Request, sha256: str, db=Depends(get_db)):\n    string = queries.string.get_string_by_sha256(db, sha256)\n\n    if not string:\n        raise HTTPException(status_code=404, detail=f\"String {sha256} not found.\")\n\n    string_samples = queries.sample.get_samples_with_string(db, string)\n\n    return templates.TemplateResponse(\n        \"string/index.html\",\n        {\"request\": request, \"string\": string, \"related_samples\": string_samples},\n    )\n\n\n@router.get(\"/relations\", response_class=HTMLResponse)\ndef get_relations(request: Request, offset: int = 0, db=Depends(get_db)):\n    relations = queries.relation.get_relations(db, offset=offset)\n\n    return templates.TemplateResponse(\n        \"relations.html\", {\"request\": request, \"offset\": offset, \"relations\": relations}\n    )\n\n\n@router.post(\"/search\", response_class=HTMLResponse)\ndef post_search(query: str = Form(...), db=Depends(get_db)):\n    prefix, term = search.prepare_search(query)\n\n    if \".\" not in prefix:\n        return None\n\n    model, attribute = prefix.split(\".\")\n\n    if model == \"sample\":\n        sample = cast(models.Sample, search.sample_search(db, attribute, term))\n        if sample:\n            return RedirectResponse(\n                f\"/sample/{sample.sha256}\", status_code=status.HTTP_302_FOUND\n            )\n\n    elif model == \"string\":\n        string = cast(models.String, search.string_search(db, attribute, term))\n        if string:\n            return RedirectResponse(\n                f\"/string/{string.sha256}\", status_code=status.HTTP_302_FOUND\n            )\n"}
{"type": "source_file", "path": "docs/source/conf.py", "content": "# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath('../../'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = 'Aurora'\ncopyright = '2021, W3ndige'\nauthor = 'W3ndige'\n\n# The full version, including alpha/beta/rc tags\nrelease = '0.1'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\"sphinx.ext.autodoc\", \"sphinx.ext.napoleon\"]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'alabaster'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\nhtml_static_path = ['_static']"}
